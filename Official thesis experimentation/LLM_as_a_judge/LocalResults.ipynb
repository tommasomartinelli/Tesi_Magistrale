{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valuto le query locali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_correctness_eval_prompt_template = \"\"\"### Task Description:\n",
    "You are given a query, a generated answer, a reference answer (which receives a score of 5), and a scoring rubric representing the evaluation criteria for correctness.\n",
    "\n",
    "Instructions:\n",
    "1. Provide detailed feedback that assesses the correctness of the generated answer strictly based on the scoring rubric.\n",
    "2. After writing the feedback, assign a score from 1 to 5 according to the rubric.\n",
    "3. The output format should be:\n",
    "   - Feedback: (your detailed feedback)\n",
    "   - [RESULT] (1-5) or Score: (1-5)\n",
    "4. Do not include any additional text beyond the feedback and the score.\n",
    "5. Focus your evaluation only on the content present in both the generated answer and the reference answer. Do not penalize for information missing from the generated answer if it is also missing in the reference answer.\n",
    "\n",
    "### Query:\n",
    "{query}\n",
    "\n",
    "### Generated Answer:\n",
    "{generated_answer}\n",
    "\n",
    "### Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "### Scoring Rubric for Correctness:\n",
    "- **Score 1**: The generated answer is completely incorrect and does not relate to the query or the reference answer.\n",
    "- **Score 2**: The generated answer has significant inaccuracies and fails to correctly address the main points of the query or the reference answer.\n",
    "- **Score 3**: The generated answer is partially correct but contains notable errors or misconceptions.\n",
    "- **Score 4**: The generated answer is mostly correct with minor inaccuracies.\n",
    "- **Score 5**: The generated answer is entirely correct and aligns perfectly with the reference answer in terms of factual accuracy.\n",
    "\n",
    "### Feedback:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_relevance_eval_prompt_template = \"\"\" ### Task Description:\n",
    "You are an expert evaluator tasked with assessing a generated answer in response to a specific query. Evaluate the answer based on the criterion of relevance.\n",
    "\n",
    "### Instructions:\n",
    "1. Provide detailed feedback that assesses the relevance of the generated answer strictly based on the provided rubric.\n",
    "2. After writing the feedback, assign a score from 1 to 5 according to the rubric.\n",
    "3. The output format should be:\n",
    "\n",
    "**Feedback**: (your detailed feedback)\n",
    "\n",
    "**Score**: (1-5)\n",
    "\n",
    "4. Do not include any additional text beyond the feedback and the score.\n",
    "5. Focus your evaluation on how well the generated answer addresses the query in comparison to the reference answer. Do not penalize for information present in the generated answer that is also present in the reference answer.\n",
    "\n",
    "Note: Please ensure that you strictly follow the output format.\n",
    "\n",
    "### Query:\n",
    "{query}\n",
    "\n",
    "### Generated Answer:\n",
    "{generated_answer}\n",
    "\n",
    "### Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "### Evaluation Criterion and Rubric:\n",
    "\n",
    "**Relevance**\n",
    "\n",
    "- **Score 1**: The generated answer is completely irrelevant to the query and does not align with the reference answer.\n",
    "- **Score 2**: The generated answer has minimal relation to the query and the reference answer but lacks significant pertinent information.\n",
    "- **Score 3**: The generated answer is partially relevant. It addresses some aspects of the query and reference answer but omits key points.\n",
    "- **Score 4**: The generated answer is mostly relevant. It covers most key points of the query and reference answer with few irrelevant details.\n",
    "- **Score 5**: The generated answer is highly relevant. It fully addresses the query directly and completely, aligning closely with the reference answer without including irrelevant information.\n",
    "\n",
    "### Evaluation: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_completness_eval_prompt_template = \"\"\" ### Task Description:\n",
    "You are given a query, a generated answer, a reference answer (which receives a score of 5), and a scoring rubric representing the evaluation criteria for completeness.\n",
    "\n",
    "Instructions:\n",
    "1. Provide detailed feedback that assesses the completeness of the generated answer strictly based on the scoring rubric.\n",
    "2. After writing the feedback, assign a score from 1 to 5 according to the rubric.\n",
    "3. The output format should be:\n",
    "   - Feedback: (your detailed feedback)\n",
    "   - Score: (1-5)\n",
    "4. Do not include any additional text beyond the feedback and the score.\n",
    "5. Focus on whether the generated answer includes all relevant information present in the reference answer. Do not penalize for including additional information not present in the reference answer.\n",
    "\n",
    "### Query:\n",
    "{query}\n",
    "\n",
    "### Generated Answer:\n",
    "{generated_answer}\n",
    "\n",
    "### Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "### Scoring Rubric for Completeness:\n",
    "- **Score 1**: The generated answer provides minimal or no relevant information.\n",
    "- **Score 2**: The generated answer includes some relevant points but misses most key information.\n",
    "- **Score 3**: The generated answer covers several relevant points but lacks important details.\n",
    "- **Score 4**: The generated answer includes most key information but may miss minor details.\n",
    "- **Score 5**: The generated answer thoroughly covers all relevant information from the reference answer.\n",
    "\n",
    "### Feedback: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(feedback):\n",
    "    \"\"\"\n",
    "    Funzione che estrae il punteggio (da 1 a 5) dal feedback usando regex.\n",
    "    Gestisce sia il formato '[RESULT] X' che 'Score: X'.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(?:\\[RESULT\\]|Score:|\\*\\*Score\\*\\*:)\\s*(\\d+)', feedback)\n",
    "    if match:\n",
    "        return int(match.group(1)) \n",
    "    else:\n",
    "        print(f\"Impossibile estrarre il punteggio dal feedback:\\n{feedback}\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_prompt(query, generated_answer, reference_answer, criterion):\n",
    "\n",
    "    if criterion == 'correctness':\n",
    "        prompt_template = llama3_correctness_eval_prompt_template  # Usa il template aggiornato\n",
    "    elif criterion == 'completeness':\n",
    "        prompt_template = llama3_completness_eval_prompt_template  # Definisci questo template\n",
    "    elif criterion == 'relevance' :\n",
    "        prompt_template = llama3_relevance_eval_prompt_template\n",
    "\n",
    "    return prompt_template.format(\n",
    "        query=query,\n",
    "        generated_answer=generated_answer,\n",
    "        reference_answer=reference_answer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifica_e_estrai(json_generated, json_reference):\n",
    "    \"\"\"\n",
    "    Funzione che verifica se le domande nei due JSON corrispondono e,\n",
    "    se corrispondono, estrae le domande, le risposte generate e le risposte di riferimento.\n",
    "    \n",
    "    :param json_generated: Path del file JSON con le risposte generate.\n",
    "    :param json_reference: Path del file JSON con le risposte di riferimento.\n",
    "    :return: Dizionario con domande, risposte generate e risposte di riferimento.\n",
    "    :raises: AssertionError se le domande non corrispondono.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carica il file JSON con le risposte generate dal modello\n",
    "    with open(json_generated, \"r\") as file:\n",
    "        generated_data = json.load(file)\n",
    "\n",
    "    # Carica il file JSON con le risposte di riferimento\n",
    "    with open(json_reference, \"r\") as file:\n",
    "        reference_data = json.load(file)\n",
    "\n",
    "    # Estrai le domande e le risposte dai due file\n",
    "    generated_questions = [item['question'] for item in generated_data['questions']]\n",
    "    generated_answers = [item['answer'] for item in generated_data['questions']]\n",
    "    \n",
    "    reference_questions = [item['question'] for item in reference_data['questions']]\n",
    "    reference_answers = [item['answer'] for item in reference_data['questions']]\n",
    "\n",
    "    # Verifica che le domande corrispondano tra i due file\n",
    "    for gq, rq in zip(generated_questions, reference_questions):\n",
    "        assert gq == rq, f\"Le domande non corrispondono: {gq} != {rq}\"\n",
    "\n",
    "    # Se tutto combacia, ritorna le domande e le risposte\n",
    "    print(\"Tutte le domande corrispondono tra i due file.\")\n",
    "    return {\n",
    "        \"questions\": generated_questions,\n",
    "        \"generated_answers\": generated_answers,\n",
    "        \"reference_answers\": reference_answers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llama(prompt, model):\n",
    "    \"\"\"\n",
    "    Funzione che invia un prompt all'API LLaMA e restituisce il feedback.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model[\"name\"],  # Nome del modello LLaMA\n",
    "        \"prompt\": prompt,        # Prompt da inviare\n",
    "        \"temperature\": model[\"temperature\"],\n",
    "        \"max_tokens\": model[\"max_tokens\"]\n",
    "    }\n",
    "    \n",
    "    # Invia la richiesta POST all'endpoint\n",
    "    response = requests.post(f\"{model['url']}/completions\", json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Restituisci il testo generato\n",
    "        return response.json()[\"choices\"][0][\"text\"].strip()\n",
    "    else:\n",
    "        # In caso di errore, stampa il messaggio\n",
    "        raise Exception(f\"Errore nell'API: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, criterion):\n",
    "    \"\"\"\n",
    "    Esegue il processo di valutazione:\n",
    "    - Verifica i file JSON\n",
    "    - Itera su tutte le domande per generare il prompt e ottenere il feedback\n",
    "    - Salva i risultati in un file JSON\n",
    "    \"\"\"\n",
    "    # 1. Verifica e estrazione\n",
    "    dati_estratti = verifica_e_estrai(json_generated, json_reference)\n",
    "    generated_questions = dati_estratti[\"questions\"]\n",
    "    generated_answers = dati_estratti[\"generated_answers\"]\n",
    "    reference_answers = dati_estratti[\"reference_answers\"]\n",
    "\n",
    "    # 2. Lista per memorizzare i risultati\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(generated_questions)):\n",
    "        # Crea il prompt per il criterio specifico\n",
    "        prompt = create_evaluation_prompt(generated_questions[i], generated_answers[i], reference_answers[i], criterion)\n",
    "\n",
    "        # Ottieni il feedback dall'API LLaMA\n",
    "        feedback = query_llama(prompt, model)\n",
    "\n",
    "        # Estrai il voto dal feedback\n",
    "        score = extract_score(feedback)\n",
    "\n",
    "        # Aggiungi i risultati\n",
    "        result = {\n",
    "            \"question\": generated_questions[i],\n",
    "            \"generated_answer\": generated_answers[i],\n",
    "            \"reference_answer\": reference_answers[i],\n",
    "            f\"{criterion}_feedback\": feedback,\n",
    "            f\"{criterion}_score\": score\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Salva i risultati in un file JSON\n",
    "    with open(nome_output_json, \"w\") as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "\n",
    "    print(f\"Risultati salvati in {nome_output_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def estrai_statistiche_punteggi(json_file, criterion):\n",
    "    \"\"\"\n",
    "    Funzione che estrae i punteggi da un file JSON e calcola la statistica dei voti (1, 2, 3, 4, 5).\n",
    "\n",
    "    :param json_file: Path del file JSON con le risposte e i punteggi.\n",
    "    :param criterion: Il criterio di valutazione ('correctness', 'completeness', ecc.).\n",
    "    :return: Dizionario con la distribuzione dei punteggi.\n",
    "    \"\"\"\n",
    "    # Carica il file JSON\n",
    "    with open(json_file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Definisci la chiave del punteggio in base al criterio\n",
    "    score_key = f\"{criterion}_score\"\n",
    "\n",
    "    # Estrai i punteggi\n",
    "    scores = [item[score_key] for item in data if item.get(score_key) is not None]\n",
    "\n",
    "    # Conta quanti 1, 2, 3, 4, 5 ci sono\n",
    "    score_distribution = Counter(scores)\n",
    "\n",
    "    # Stampa le statistiche dei punteggi\n",
    "    print(f\"Distribuzione dei punteggi per il criterio '{criterion}':\")\n",
    "    for score in range(1, 6):\n",
    "        print(f\"Punteggio {score}: {score_distribution.get(score, 0)} occorrenze\")\n",
    "\n",
    "    # Restituisci il conteggio\n",
    "    return dict(score_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"url\": \"http://172.18.21.137:8000/v1\",  # URL dell'endpoint LLaMA\n",
    "    \"name\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Dati estratti correttamente.\n"
     ]
    }
   ],
   "source": [
    "json_generated = \"../Naive/Naive_local_responses.json\"\n",
    "json_reference = \"../DatasetCreation/Local_questions.json\"\n",
    "\n",
    "try:\n",
    "    risultati = verifica_e_estrai(json_generated, json_reference)\n",
    "    print(\"Dati estratti correttamente.\")\n",
    "except AssertionError as e:\n",
    "    print(f\"Errore: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_output_json = \"naive_local_results/naive_results_correctness.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Risultati salvati in naive_local_results/naive_results_correctness.json\n"
     ]
    }
   ],
   "source": [
    "esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, 'correctness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione dei punteggi per il criterio 'correctness':\n",
      "Punteggio 1: 1 occorrenze\n",
      "Punteggio 2: 15 occorrenze\n",
      "Punteggio 3: 12 occorrenze\n",
      "Punteggio 4: 17 occorrenze\n",
      "Punteggio 5: 0 occorrenze\n",
      "{2: 15, 3: 12, 4: 17, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "stat = estrai_statistiche_punteggi(nome_output_json, 'correctness')\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_output_json = \"naive_local_results/naive_results_completeness.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Risultati salvati in naive_local_results/naive_results_completeness.json\n"
     ]
    }
   ],
   "source": [
    "esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, 'completeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione dei punteggi per il criterio 'completeness':\n",
      "Punteggio 1: 6 occorrenze\n",
      "Punteggio 2: 4 occorrenze\n",
      "Punteggio 3: 16 occorrenze\n",
      "Punteggio 4: 14 occorrenze\n",
      "Punteggio 5: 5 occorrenze\n",
      "{3: 16, 4: 14, 1: 6, 5: 5, 2: 4}\n"
     ]
    }
   ],
   "source": [
    "stat = estrai_statistiche_punteggi(nome_output_json, 'completeness')\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_output_json = \"naive_local_results/naive_results_relevance.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Risultati salvati in naive_local_results/naive_results_relevance.json\n"
     ]
    }
   ],
   "source": [
    "esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, 'relevance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione dei punteggi per il criterio 'relevance':\n",
      "Punteggio 1: 3 occorrenze\n",
      "Punteggio 2: 2 occorrenze\n",
      "Punteggio 3: 9 occorrenze\n",
      "Punteggio 4: 30 occorrenze\n",
      "Punteggio 5: 1 occorrenze\n",
      "{4: 30, 3: 9, 5: 1, 1: 3, 2: 2}\n"
     ]
    }
   ],
   "source": [
    "stat = estrai_statistiche_punteggi(nome_output_json, 'relevance')\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Dati estratti correttamente.\n"
     ]
    }
   ],
   "source": [
    "json_generated = \"../GraphRAG-test/GraphRAG_local_responses.json\"\n",
    "json_reference = \"../DatasetCreation/Local_questions.json\"\n",
    "\n",
    "try:\n",
    "    risultati = verifica_e_estrai(json_generated, json_reference)\n",
    "    print(\"Dati estratti correttamente.\")\n",
    "except AssertionError as e:\n",
    "    print(f\"Errore: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_output_json = \"gr_local_results/gr_results_correctness.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Risultati salvati in gr_local_results/gr_results_correctness.json\n"
     ]
    }
   ],
   "source": [
    "esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, 'correctness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione dei punteggi per il criterio 'correctness':\n",
      "Punteggio 1: 15 occorrenze\n",
      "Punteggio 2: 16 occorrenze\n",
      "Punteggio 3: 7 occorrenze\n",
      "Punteggio 4: 7 occorrenze\n",
      "Punteggio 5: 0 occorrenze\n",
      "{4: 7, 3: 7, 1: 15, 2: 16}\n"
     ]
    }
   ],
   "source": [
    "stat = estrai_statistiche_punteggi(nome_output_json, 'correctness')\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_output_json = \"gr_local_results/gr_results_completeness.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Impossibile estrarre il punteggio dal feedback:\n",
      "(Please provide your detailed feedback)\n",
      "The generated answer provides a detailed explanation of the motivation behind the use of deep learning and transformers in time series analysis, including the concepts of representation learning and transfer learning. However, it does not explicitly state that the primary motivation lies in their ability to automatically learn comprehensive representations from raw data, thus capturing complex nonlinear relationships and temporal dependencies without the need for manual feature engineering. The generated answer also includes additional information not present in the reference answer, such as the specific benefits of transformers and the mention of authors and papers. While this additional information is relevant, it does not directly address the primary motivation.\n",
      "\n",
      "### Score:  (Please assign a score from 1 to 5)\n",
      "4\n",
      "\n",
      "Risultati salvati in gr_local_results/gr_results_completeness.json\n"
     ]
    }
   ],
   "source": [
    "esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, 'completeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione dei punteggi per il criterio 'completeness':\n",
      "Punteggio 1: 15 occorrenze\n",
      "Punteggio 2: 10 occorrenze\n",
      "Punteggio 3: 11 occorrenze\n",
      "Punteggio 4: 6 occorrenze\n",
      "Punteggio 5: 2 occorrenze\n",
      "{5: 2, 3: 11, 4: 6, 1: 15, 2: 10}\n"
     ]
    }
   ],
   "source": [
    "stat = estrai_statistiche_punteggi(nome_output_json, 'completeness')\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_output_json = \"gr_local_results/gr_results_relevance.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Risultati salvati in gr_local_results/gr_results_relevance.json\n"
     ]
    }
   ],
   "source": [
    "esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, 'relevance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione dei punteggi per il criterio 'relevance':\n",
      "Punteggio 1: 14 occorrenze\n",
      "Punteggio 2: 7 occorrenze\n",
      "Punteggio 3: 11 occorrenze\n",
      "Punteggio 4: 10 occorrenze\n",
      "Punteggio 5: 3 occorrenze\n",
      "{5: 3, 3: 11, 1: 14, 2: 7, 4: 10}\n"
     ]
    }
   ],
   "source": [
    "stat = estrai_statistiche_punteggi(nome_output_json, 'relevance')\n",
    "print(stat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.1 (Python 3.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
