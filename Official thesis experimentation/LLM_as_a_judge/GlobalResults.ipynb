{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valuto le global query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt e funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_correctness_eval_prompt_template = \"\"\"### Task Description:\n",
    "You are given a query, a generated answer, a reference answer (which receives a score of 5), and a scoring rubric representing the evaluation criteria for correctness.\n",
    "\n",
    "Instructions:\n",
    "1. Provide detailed feedback that assesses the correctness of the generated answer strictly based on the scoring rubric.\n",
    "2. After writing the feedback, assign a score from 1 to 5 according to the rubric.\n",
    "3. The output format should be:\n",
    "   - Feedback: (your detailed feedback)\n",
    "   - [RESULT] (1-5) or Score: (1-5)\n",
    "4. Do not include any additional text beyond the feedback and the score.\n",
    "5. Focus your evaluation only on the content present in both the generated answer and the reference answer. Do not penalize for missing information not present in the generated answer.\n",
    "\n",
    "### Query:\n",
    "{query}\n",
    "\n",
    "### Generated Answer:\n",
    "{generated_answer}\n",
    "\n",
    "### Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "### Scoring Rubric for Correctness:\n",
    "- **Score 1**: The generated answer is completely incorrect and does not relate to the query or the reference answer.\n",
    "- **Score 2**: The generated answer has significant inaccuracies and fails to correctly address the main points of the query or the reference answer.\n",
    "- **Score 3**: The generated answer is partially correct but contains notable errors or misconceptions.\n",
    "- **Score 4**: The generated answer is mostly correct with minor inaccuracies.\n",
    "- **Score 5**: The generated answer is entirely correct and aligns perfectly with the reference answer in terms of factual accuracy.\n",
    "\n",
    "### Feedback:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_completness_eval_prompt_template = \"\"\" ### Task Description:\n",
    "You are given a query, a generated answer, a reference answer (which receives a score of 5), and a scoring rubric representing the evaluation criteria for completeness.\n",
    "\n",
    "Instructions:\n",
    "1. Provide detailed feedback that assesses the completeness of the generated answer strictly based on the scoring rubric.\n",
    "2. After writing the feedback, assign a score from 1 to 5 according to the rubric.\n",
    "3. The output format should be:\n",
    "   - Feedback: (your detailed feedback)\n",
    "   - Score: (1-5)\n",
    "4. Do not include any additional text beyond the feedback and the score.\n",
    "5. Focus on whether the generated answer includes all relevant information present in the reference answer.\n",
    "\n",
    "### Query:\n",
    "{query}\n",
    "\n",
    "### Generated Answer:\n",
    "{generated_answer}\n",
    "\n",
    "### Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "### Scoring Rubric for Completeness:\n",
    "- **Score 1**: The generated answer provides minimal or no relevant information.\n",
    "- **Score 2**: The generated answer includes some relevant points but misses most key information.\n",
    "- **Score 3**: The generated answer covers several relevant points but lacks important details.\n",
    "- **Score 4**: The generated answer includes most key information but may miss minor details.\n",
    "- **Score 5**: The generated answer thoroughly covers all relevant information from the reference answer.\n",
    "\n",
    "### Feedback: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_relevance_eval_prompt_template = \"\"\" ### Task Description:\n",
    "You are an expert evaluator tasked with assessing a generated answer in response to a specific query. Evaluate the answer based on the criterion of **Relevance**.\n",
    "\n",
    "Instructions:\n",
    "1. Provide detailed feedback that assesses the relevance of the generated answer strictly based on the provided rubric.\n",
    "2. After writing the feedback, assign a score from 1 to 5 according to the rubric.\n",
    "3. The output format should be:\n",
    "\n",
    "**Feedback**: (your detailed feedback)\n",
    "**Score**: (1-5)\n",
    "\n",
    "4. Do not include any additional text beyond the feedback and the score.\n",
    "5. Focus only on the content present in the generated answer and the query. Do not penalize for missing information that is not required by the query.\n",
    "\n",
    "### Query:\n",
    "\n",
    "{query}\n",
    "\n",
    "### Generated Answer:\n",
    "\n",
    "{generated_answer}\n",
    "\n",
    "### Evaluation Criterion and Rubric:\n",
    "\n",
    "**Relevance**\n",
    "\n",
    "- **Score 1**: The answer is completely irrelevant to the query. It does not address any aspect of the question.\n",
    "- **Score 2**: The answer has minimal relation to the query but lacks significant pertinent information.\n",
    "- **Score 3**: The answer is partially relevant. It addresses some aspects of the query but omits key points.\n",
    "- **Score 4**: The answer is mostly relevant. It covers most key points of the query with few irrelevant details.\n",
    "- **Score 5**: The answer is highly relevant. It fully addresses the query directly and completely without including irrelevant information.\n",
    "\n",
    "### Evaluation: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(feedback):\n",
    "    \"\"\"\n",
    "    Funzione che estrae il punteggio (da 1 a 5) dal feedback usando regex.\n",
    "    Gestisce sia il formato '[RESULT] X' che 'Score: X'.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(?:\\[RESULT\\]|Score:|\\*\\*Score\\*\\*:)\\s*(\\d+)', feedback)\n",
    "    if match:\n",
    "        return int(match.group(1))  \n",
    "    else:\n",
    "        print(f\"Impossibile estrarre il punteggio dal feedback:\\n{feedback}\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_prompt(query, generated_answer, reference_answer, criterion):\n",
    "    # Scegli il template del prompt in base al criterio\n",
    "    if criterion == 'correctness':\n",
    "        prompt_template = llama3_correctness_eval_prompt_template  # Usa il template aggiornato\n",
    "    elif criterion == 'completeness':\n",
    "        prompt_template = llama3_completness_eval_prompt_template  # Definisci questo template\n",
    "    elif criterion == 'relevance' :\n",
    "        prompt_template = llama3_relevance_eval_prompt_template\n",
    "\n",
    "    return prompt_template.format(\n",
    "        query=query,\n",
    "        generated_answer=generated_answer,\n",
    "        reference_answer=reference_answer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifica_e_estrai(json_generated, json_reference):\n",
    "    \"\"\"\n",
    "    Funzione che verifica se le domande nei due JSON corrispondono e,\n",
    "    se corrispondono, estrae le domande, le risposte generate e le risposte di riferimento.\n",
    "    \n",
    "    :param json_generated: Path del file JSON con le risposte generate.\n",
    "    :param json_reference: Path del file JSON con le risposte di riferimento.\n",
    "    :return: Dizionario con domande, risposte generate e risposte di riferimento.\n",
    "    :raises: AssertionError se le domande non corrispondono.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carica il file JSON con le risposte generate dal modello\n",
    "    with open(json_generated, \"r\") as file:\n",
    "        generated_data = json.load(file)\n",
    "\n",
    "    # Carica il file JSON con le risposte di riferimento\n",
    "    with open(json_reference, \"r\") as file:\n",
    "        reference_data = json.load(file)\n",
    "\n",
    "    # Estrai le domande e le risposte dai due file\n",
    "    generated_questions = [item['question'] for item in generated_data['questions']]\n",
    "    generated_answers = [item['answer'] for item in generated_data['questions']]\n",
    "    \n",
    "    reference_questions = [item['question'] for item in reference_data['questions']]\n",
    "    reference_answers = [item['answer'] for item in reference_data['questions']]\n",
    "\n",
    "    # Verifica che le domande corrispondano tra i due file\n",
    "    for gq, rq in zip(generated_questions, reference_questions):\n",
    "        assert gq == rq, f\"Le domande non corrispondono: {gq} != {rq}\"\n",
    "\n",
    "    # Se tutto combacia, ritorna le domande e le risposte\n",
    "    print(\"Tutte le domande corrispondono tra i due file.\")\n",
    "    return {\n",
    "        \"questions\": generated_questions,\n",
    "        \"generated_answers\": generated_answers,\n",
    "        \"reference_answers\": reference_answers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llama(prompt, model):\n",
    "    \"\"\"\n",
    "    Funzione che invia un prompt all'API LLaMA e restituisce il feedback.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model[\"name\"],  # Nome del modello LLaMA\n",
    "        \"prompt\": prompt,        # Prompt da inviare\n",
    "        \"temperature\": model[\"temperature\"],\n",
    "        \"max_tokens\": model[\"max_tokens\"]\n",
    "    }\n",
    "    \n",
    "    # Invia la richiesta POST all'endpoint\n",
    "    response = requests.post(f\"{model['url']}/completions\", json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Restituisci il testo generato\n",
    "        return response.json()[\"choices\"][0][\"text\"].strip()\n",
    "    else:\n",
    "        # In caso di errore, stampa il messaggio\n",
    "        raise Exception(f\"Errore nell'API: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, criterion):\n",
    "    \"\"\"\n",
    "    Esegue il processo di valutazione:\n",
    "    - Verifica i file JSON\n",
    "    - Itera su tutte le domande per generare il prompt e ottenere il feedback\n",
    "    - Salva i risultati in un file JSON\n",
    "    \"\"\"\n",
    "    # 1. Verifica e estrazione\n",
    "    dati_estratti = verifica_e_estrai(json_generated, json_reference)\n",
    "    generated_questions = dati_estratti[\"questions\"]\n",
    "    generated_answers = dati_estratti[\"generated_answers\"]\n",
    "    reference_answers = dati_estratti[\"reference_answers\"]\n",
    "\n",
    "    # 2. Lista per memorizzare i risultati\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(generated_questions)):\n",
    "        # Crea il prompt per il criterio specifico\n",
    "        prompt = create_evaluation_prompt(generated_questions[i], generated_answers[i], reference_answers[i], criterion)\n",
    "\n",
    "        # Ottieni il feedback dall'API LLaMA\n",
    "        feedback = query_llama(prompt, model)\n",
    "\n",
    "        # Estrai il voto dal feedback\n",
    "        score = extract_score(feedback)\n",
    "\n",
    "        # Aggiungi i risultati\n",
    "        result = {\n",
    "            \"question\": generated_questions[i],\n",
    "            \"generated_answer\": generated_answers[i],\n",
    "            \"reference_answer\": reference_answers[i],\n",
    "            f\"{criterion}_feedback\": feedback,\n",
    "            f\"{criterion}_score\": score\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Salva i risultati in un file JSON\n",
    "    with open(nome_output_json, \"w\") as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "\n",
    "    print(f\"Risultati salvati in {nome_output_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def estrai_statistiche_punteggi(json_file, criterion):\n",
    "    \"\"\"\n",
    "    Funzione che estrae i punteggi da un file JSON e calcola la statistica dei voti (1, 2, 3, 4, 5).\n",
    "\n",
    "    :param json_file: Path del file JSON con le risposte e i punteggi.\n",
    "    :param criterion: Il criterio di valutazione ('correctness', 'completeness', ecc.).\n",
    "    :return: Dizionario con la distribuzione dei punteggi.\n",
    "    \"\"\"\n",
    "    # Carica il file JSON\n",
    "    with open(json_file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Definisci la chiave del punteggio in base al criterio\n",
    "    score_key = f\"{criterion}_score\"\n",
    "\n",
    "    # Estrai i punteggi\n",
    "    scores = [item[score_key] for item in data if item.get(score_key) is not None]\n",
    "\n",
    "    # Conta quanti 1, 2, 3, 4, 5 ci sono\n",
    "    score_distribution = Counter(scores)\n",
    "\n",
    "    # Stampa le statistiche dei punteggi\n",
    "    print(f\"Distribuzione dei punteggi per il criterio '{criterion}':\")\n",
    "    for score in range(1, 6):\n",
    "        print(f\"Punteggio {score}: {score_distribution.get(score, 0)} occorrenze\")\n",
    "\n",
    "    # Restituisci il conteggio\n",
    "    return dict(score_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"url\": \"http://172.18.21.137:8000/v1\",  # URL dell'endpoint LLaMA\n",
    "    \"name\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 512\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Dati estratti correttamente.\n"
     ]
    }
   ],
   "source": [
    "json_generated = \"../Naive/Naive_responses.json\"\n",
    "json_reference = \"../DatasetCreation/Global_questions.json\"\n",
    "\n",
    "try:\n",
    "    risultati = verifica_e_estrai(json_generated, json_reference)\n",
    "    print(\"Dati estratti correttamente.\")\n",
    "except AssertionError as e:\n",
    "    print(f\"Errore: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_output_json = \"naive_results/naive_results_correctness.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risultati salvati in naive_results/naive_results_correctness.json\n"
     ]
    }
   ],
   "source": [
    "esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, 'correctness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione dei punteggi per il criterio 'correctness':\n",
      "Punteggio 1: 4 occorrenze\n",
      "Punteggio 2: 12 occorrenze\n",
      "Punteggio 3: 18 occorrenze\n",
      "Punteggio 4: 3 occorrenze\n",
      "Punteggio 5: 0 occorrenze\n",
      "{1: 4, 4: 3, 3: 18, 2: 12}\n"
     ]
    }
   ],
   "source": [
    "stat = estrai_statistiche_punteggi(nome_output_json, 'correctness')\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_output_json = \"naive_results/naive_results_completeness.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Risultati salvati in naive_results/naive_results_completeness.json\n"
     ]
    }
   ],
   "source": [
    "esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, 'completeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione dei punteggi per il criterio 'completeness':\n",
      "Punteggio 1: 7 occorrenze\n",
      "Punteggio 2: 11 occorrenze\n",
      "Punteggio 3: 17 occorrenze\n",
      "Punteggio 4: 2 occorrenze\n",
      "Punteggio 5: 0 occorrenze\n",
      "{2: 11, 3: 17, 4: 2, 1: 7}\n"
     ]
    }
   ],
   "source": [
    "stat = estrai_statistiche_punteggi(nome_output_json, 'completeness')\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_output_json = \"naive_results/naive_results_relevance.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Risultati salvati in naive_results/naive_results_relevance.json\n"
     ]
    }
   ],
   "source": [
    "esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, 'relevance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione dei punteggi per il criterio 'relevance':\n",
      "Punteggio 1: 1 occorrenze\n",
      "Punteggio 2: 2 occorrenze\n",
      "Punteggio 3: 14 occorrenze\n",
      "Punteggio 4: 14 occorrenze\n",
      "Punteggio 5: 6 occorrenze\n",
      "{4: 14, 5: 6, 3: 14, 2: 2, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "stat = estrai_statistiche_punteggi(nome_output_json, 'relevance')\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Dati estratti correttamente.\n"
     ]
    }
   ],
   "source": [
    "json_generated = \"../GraphRAG-test/GraphRAG_responses.json\"\n",
    "json_reference = \"../DatasetCreation/Global_questions.json\"\n",
    "\n",
    "try:\n",
    "    risultati = verifica_e_estrai(json_generated, json_reference)\n",
    "    print(\"Dati estratti correttamente.\")\n",
    "except AssertionError as e:\n",
    "    print(f\"Errore: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_output_json = \"gr_results/gr_results_correctness.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Risultati salvati in gr_results/gr_results_correctness.json\n"
     ]
    }
   ],
   "source": [
    "esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, 'correctness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione dei punteggi per il criterio 'correctness':\n",
      "Punteggio 1: 0 occorrenze\n",
      "Punteggio 2: 3 occorrenze\n",
      "Punteggio 3: 16 occorrenze\n",
      "Punteggio 4: 18 occorrenze\n",
      "Punteggio 5: 0 occorrenze\n",
      "{3: 16, 4: 18, 2: 3}\n"
     ]
    }
   ],
   "source": [
    "stat = estrai_statistiche_punteggi(nome_output_json, 'correctness')\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_output_json = \"gr_results/gr_results_completeness.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Impossibile estrarre il punteggio dal feedback:\n",
      "(Please provide detailed feedback based on the scoring rubric)\n",
      "The generated answer provides a comprehensive overview of how Chronos approaches time-series forecasting compared to traditional statistical models. It highlights the use of a deep transformer network, probabilistic forecasting, and pre-training and fine-tuning capabilities. However, it lacks specific details about how Chronos tokenizes time-series data into discrete values and utilizes multiple future trajectory samples based on historical context, which are key aspects of its approach as mentioned in the reference answer. Additionally, the generated answer does not explicitly compare Chronos to traditional statistical models like ARIMA or SARIMA, which is a crucial point for understanding its distinctiveness. Despite these omissions, the generated answer covers several relevant points and demonstrates a good understanding of Chronos's architecture and capabilities.\n",
      "\n",
      "### Score:  (Please assign a score from 1 to 5 based on the feedback)\n",
      "4\n",
      "\n",
      "Risultati salvati in gr_results/gr_results_completeness.json\n"
     ]
    }
   ],
   "source": [
    "esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, 'completeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione dei punteggi per il criterio 'completeness':\n",
      "Punteggio 1: 0 occorrenze\n",
      "Punteggio 2: 0 occorrenze\n",
      "Punteggio 3: 14 occorrenze\n",
      "Punteggio 4: 22 occorrenze\n",
      "Punteggio 5: 0 occorrenze\n",
      "{4: 22, 3: 14}\n"
     ]
    }
   ],
   "source": [
    "stat = estrai_statistiche_punteggi(nome_output_json, 'completeness')\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_output_json = \"gr_results/gr_results_relevance.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le domande corrispondono tra i due file.\n",
      "Risultati salvati in gr_results/gr_results_relevance.json\n"
     ]
    }
   ],
   "source": [
    "esegui_valutazione_completa(json_generated, json_reference, nome_output_json, model, 'relevance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione dei punteggi per il criterio 'relevance':\n",
      "Punteggio 1: 0 occorrenze\n",
      "Punteggio 2: 0 occorrenze\n",
      "Punteggio 3: 0 occorrenze\n",
      "Punteggio 4: 2 occorrenze\n",
      "Punteggio 5: 35 occorrenze\n",
      "{5: 35, 4: 2}\n"
     ]
    }
   ],
   "source": [
    "stat = estrai_statistiche_punteggi(nome_output_json, 'relevance')\n",
    "print(stat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.1 (Python 3.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
