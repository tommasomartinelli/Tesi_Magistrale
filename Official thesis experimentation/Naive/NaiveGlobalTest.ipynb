{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaiveRAG per query globali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbeddings(Embeddings):\n",
    "    def __init__(self, endpoint_url):\n",
    "        self.endpoint_url = endpoint_url\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            payload = {\n",
    "                \"input\": text,\n",
    "                \"model\": \"intfloat/multilingual-e5-large-instruct\"\n",
    "            }\n",
    "            response = requests.post(f\"{self.endpoint_url}/embeddings\", json=payload)\n",
    "            if response.status_code == 200:\n",
    "                embedding = response.json()['data'][0]['embedding']\n",
    "                embeddings.append(embedding)\n",
    "            else:\n",
    "                raise Exception(f\"Errore nell'embedder: {response.text}\")\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = CustomEmbeddings(endpoint_url=\"http://172.18.21.138:80/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento dei file di testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "document_paths = glob.glob('../input/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "documents = []\n",
    "for file_path in document_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        documents.append(Document(page_content=content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di documenti caricati: 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numero di documenti caricati: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suddivisione documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,       \n",
    "    chunk_overlap=200,     \n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero totale di chunk: 718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numero totale di chunk: {len(docs)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicizzazione dei documenti nel db vettoriale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di vettori indicizzati: 718\n"
     ]
    }
   ],
   "source": [
    "num_vectors = vectorstore.index.ntotal\n",
    "print(f\"Numero di vettori indicizzati: {num_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurazione del prompt per le query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    endpoint_url: str\n",
    "    model_name: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    temperature: float = 0.0\n",
    "    max_tokens: int = 1500\n",
    "    repetition_penalty: float = 1.2\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom_llm\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        payload = {\n",
    "            \"prompt\": prompt,\n",
    "            \"model\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"repetition_penalty\": self.repetition_penalty,\n",
    "            \"stop\": stop or [\"I don't know.\"],\n",
    "        }\n",
    "        print(\"Payload inviato all'API:\", payload)  \n",
    "        response = requests.post(f\"{self.endpoint_url}/completions\", json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['choices'][0]['text']\n",
    "        else:\n",
    "            raise Exception(f\"Errore nel LLM: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CustomLLM(\n",
    "    endpoint_url=\"http://172.18.21.132:8000/v1\",\n",
    "    temperature=0.0,\n",
    "    max_tokens= 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurazione del retrieval e sistema di question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_187614/3367573412.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  result = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Figure 1: The TranAD Model.\\n\\n# 3.3 Transformer Model\\n\\nTransformers are popular deep learning models that have been used in various natural language and vision processing tasks [51]. However, we use insightful refactoring of the transformer architecture for the task of anomaly detection in time-series data. Just like other encoder-decoder models, in a transformer, an input sequence undergoes several attention-based transformations. Figure 1 shows the architecture of the neural network used in TranAD. The encoder encodes the complete sequence until the current timestamp ùê∂ with a focus score (more details later). The window encoder uses this to create an encoded representation of the input window ùëä, which is then passed to two decoders to create its reconstruction.\\n\\nùëÇùëñ = Sigmoid(FeedForward(ùêº23)), (6)\\n\\nwhere ùëñ ‚àà {1, 2} for the first and second decoder respectively. The Sigmoid activation is used to generate an output in the range [0, 1], to match the normalized input window. Thus, the TranAD model takes the inputs ùê∂ and ùëä to generate two outputs ùëÇ1 and ùëÇ2.\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Algorithm 1 The TranAD training algorithm\\n\\nRequire:\\n\\n- Encoder ùê∏, Decoders ùê∑1 and ùê∑2\\n- Dataset used for training W\\n- Evolutionary hyperparameter ùúñ\\n- Iteration limit ùëÅ\\n\\n1. Initialize weights ùê∏, ùê∑1, ùê∑2\\n2. ùëõ ‚Üê 0\\n3. do\\n\\nwhile ùëõ < ùëÅ\\n\\n# 3.4 Offline Two-Phase Adversarial Training\\n\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\n. Specifically, TranAD achieves improvement of up to 17.06% in F1 score, 14.64% in F1* score, 11.69% in AUC and 11.06% in AUC* scores over the state-of-the-art baseline models.\\n\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\nThe Transformer model enables us to predict the reconstruction of each input time-series window. It does this by acting as an encoder-decoder network at each timestamp. However, traditional encoder-decoder models often are unable to capture short-term trends and tend to miss anomalies if the deviations are too small. To tackle this challenge, we develop an auto-regressive inference style that predicts the reconstructed window in two-phases. In the first phase, the model aims to generate an approximate reconstruction of the input window.\\n\\n# Phase 2 - Focused Input Reconstruction\\n\\nIn the second phase, we use the reconstruction loss for the first decoder as a focus score. Having the focus matrix for the second phase ùêπ = ùêø1, we rerun model inference to obtain the output of the second decoder as ùëÇÀÜ2.\\n\\nWe now provide details on the working of TranAD. A multivariate sequence like ùëä or ùê∂ is transformed first into a matrix form with modality ùëö. We define scaled-dot product attention [51] of three matrices ùëÑ (query), ùêæ (key) and ùëâ (value):\\n\\nAttention(ùëÑ, ùêæ, ùëâ) = softmax(ùëÑùêæT / ‚àöùëö) ùëâ. (2)\\n\\nHere, the softmax forms the convex combination weights for the values in ùëâ, allowing us to compress the matrix ùëâ into a smaller representative embedding for simplified inference in the downstream neural network operations. Unlike traditional attention operation, the scaled-dot product attention scales the weights by a ‚àöùëö term to reduce the variance of the weights, facilitating stable training [51]. For input matrices ùëÑ, ùêæ and ùëâ, we apply Multi-Head Self Attention [51] by first passing it through ‚Ñé (number of heads) feed-forward layers to get ùëÑùëñ, ùêæùëñ and ùëâùëñ for ùëñ ‚àà {1, . . . , ‚Ñé}, and then applying scaled-dot product attention as:\\n\\nQuestion:\\nDescribe how TranAD works?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Question: Describe how TranAD works?\n",
      "Answer: TranAD works by using a Transformer model as an encoder-decoder network to predict the reconstruction of each input time-series window. This prediction occurs in two phases. \n",
      "\n",
      "Firstly, during **Phase 1 - Input Reconstruction**, the model generates an approximate reconstruction of the input window. Then, in **Phase 2 - Focused Input Reconstruction**, the model utilizes the reconstruction loss for the first decoder as a focus score to refine its predictions. Specifically, having obtained the focus matrix for the second phase, the model re-runs inference to produce the output of the second decoder as O^2.\n",
      "\n",
      "This two-phase inference approach allows TranAD to effectively capture both long-term patterns and short-term trends within the input sequences, thereby improving its ability to detect anomalies. Furthermore, the use of multi-head self-attention mechanism facilitates stable training while enabling the model to learn complex relationships between different modalities within the input data. Overall, TranAD's unique design enables it to achieve significant improvements in performance compared to existing state-of-the-art baselines. I don't know whether these improvements translate directly to real-world applications; however, they demonstrate the potential effectiveness of TranAD in detecting anomalies within multivariate time series data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Describe how TranAD works?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {result['result']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definizione della funzione per eseguire le query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask_question(query):  \n",
    "    result = qa_chain({\"query\": query})  \n",
    "    answer = result['result']\n",
    "    display(Markdown(f\"**Answer to the question:** {query}\\n\\n{answer}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Figure 1: The TranAD Model.\\n\\n# 3.3 Transformer Model\\n\\nTransformers are popular deep learning models that have been used in various natural language and vision processing tasks [51]. However, we use insightful refactoring of the transformer architecture for the task of anomaly detection in time-series data. Just like other encoder-decoder models, in a transformer, an input sequence undergoes several attention-based transformations. Figure 1 shows the architecture of the neural network used in TranAD. The encoder encodes the complete sequence until the current timestamp ùê∂ with a focus score (more details later). The window encoder uses this to create an encoded representation of the input window ùëä, which is then passed to two decoders to create its reconstruction.\\n\\nùëÇùëñ = Sigmoid(FeedForward(ùêº23)), (6)\\n\\nwhere ùëñ ‚àà {1, 2} for the first and second decoder respectively. The Sigmoid activation is used to generate an output in the range [0, 1], to match the normalized input window. Thus, the TranAD model takes the inputs ùê∂ and ùëä to generate two outputs ùëÇ1 and ùëÇ2.\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Algorithm 1 The TranAD training algorithm\\n\\nRequire:\\n\\n- Encoder ùê∏, Decoders ùê∑1 and ùê∑2\\n- Dataset used for training W\\n- Evolutionary hyperparameter ùúñ\\n- Iteration limit ùëÅ\\n\\n1. Initialize weights ùê∏, ùê∑1, ùê∑2\\n2. ùëõ ‚Üê 0\\n3. do\\n\\nwhile ùëõ < ùëÅ\\n\\n# 3.4 Offline Two-Phase Adversarial Training\\n\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\n. Specifically, TranAD achieves improvement of up to 17.06% in F1 score, 14.64% in F1* score, 11.69% in AUC and 11.06% in AUC* scores over the state-of-the-art baseline models.\\n\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\nThe Transformer model enables us to predict the reconstruction of each input time-series window. It does this by acting as an encoder-decoder network at each timestamp. However, traditional encoder-decoder models often are unable to capture short-term trends and tend to miss anomalies if the deviations are too small. To tackle this challenge, we develop an auto-regressive inference style that predicts the reconstructed window in two-phases. In the first phase, the model aims to generate an approximate reconstruction of the input window.\\n\\n# Phase 2 - Focused Input Reconstruction\\n\\nIn the second phase, we use the reconstruction loss for the first decoder as a focus score. Having the focus matrix for the second phase ùêπ = ùêø1, we rerun model inference to obtain the output of the second decoder as ùëÇÀÜ2.\\n\\nWe now provide details on the working of TranAD. A multivariate sequence like ùëä or ùê∂ is transformed first into a matrix form with modality ùëö. We define scaled-dot product attention [51] of three matrices ùëÑ (query), ùêæ (key) and ùëâ (value):\\n\\nAttention(ùëÑ, ùêæ, ùëâ) = softmax(ùëÑùêæT / ‚àöùëö) ùëâ. (2)\\n\\nHere, the softmax forms the convex combination weights for the values in ùëâ, allowing us to compress the matrix ùëâ into a smaller representative embedding for simplified inference in the downstream neural network operations. Unlike traditional attention operation, the scaled-dot product attention scales the weights by a ‚àöùëö term to reduce the variance of the weights, facilitating stable training [51]. For input matrices ùëÑ, ùêæ and ùëâ, we apply Multi-Head Self Attention [51] by first passing it through ‚Ñé (number of heads) feed-forward layers to get ùëÑùëñ, ùêæùëñ and ùëâùëñ for ùëñ ‚àà {1, . . . , ‚Ñé}, and then applying scaled-dot product attention as:\\n\\nQuestion:\\nDescribe how TranAD works?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer to the question:** Describe how TranAD works?\n",
       "\n",
       "TranAD works by using a Transformer model as an encoder-decoder network to predict the reconstruction of each input time-series window. This prediction occurs in two phases. \n",
       "\n",
       "Firstly, during **Phase 1 - Input Reconstruction**, the model generates an approximate reconstruction of the input window. Then, in **Phase 2 - Focused Input Reconstruction**, the model utilizes the reconstruction loss for the first decoder as a focus score to refine its predictions. Specifically, having obtained the focus matrix for the second phase, the model re-runs inference to produce the output of the second decoder as O^2.\n",
       "\n",
       "This two-phase inference approach allows TranAD to effectively capture both long-term patterns and short-term trends within the input sequences, thereby improving its ability to detect anomalies. Furthermore, the use of multi-head self-attention mechanism facilitates stable training while reducing the variance of the weights involved in the attention operation. Overall, TranAD's design enables it to achieve significant improvements in performance metrics such as F1 score, F1*, AUC, and AUC* compared to existing state-of-the-art baselines. I don't know whether these results were achieved under specific conditions or datasets; however, they demonstrate the effectiveness of TranAD in addressing challenges related to anomaly detection in multivariate time series data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await ask_question(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione domande globali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../DatasetCreation/Global_questions.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What are the main topics covered by the data in the set of time-series papers?\n",
      "Question 2: How does RestAD leverage both statistical methods and machine learning to achieve robust anomaly detection in noisy time-series data?\n",
      "Question 3: What are the key features and benefits of RestAD in anomaly detection for time-series data?\n",
      "Question 4: What are the key features and benefits of RestAD in anomaly detection for time-series data?\n",
      "Question 5: How does TimeLLM differ from other models in time-series forecasting?\n",
      "Question 6: How does AnomalyBERT work?\n",
      "Question 7: How does TimeGPT approach time-series forecasting?\n",
      "Question 8: What types of real-world applications can benefit from models like TimeLLM, RestAD, TimeGPT, AnomalyBERT, LagLLama and the other models described?\n",
      "Question 9: What distinguishes LagLLama in its approach to time-series analysis?\n",
      "Question 10: How do models like AnomalyBERT handle non-stationary data, and why is this important?\n",
      "Question 11: What are the main trade-offs when choosing between transformer-based models and traditional time-series models?\n",
      "Question 12: How does TimeLLM incorporate historical data for better forecasting accuracy?\n",
      "Question 13: What kind of preprocessing steps are typically necessary for models like TimeGPT and AnomalyBERT to perform effectively on time-series data?\n",
      "Question 14: How do models like TimeGPT and RestAD handle seasonality and trends in time-series data?\n",
      "Question 15: How does Chronos approach time-series forecasting compared to traditional statistical models?\n",
      "Question 16: What are the strengths and weaknesses of TimesFM in handling multivariate time-series forecasting?\n",
      "Question 17: How does TranAD address the challenges of detecting anomalies in streaming time-series data?\n",
      "Question 18: How does Foundation Models for Time Series Analysis (FMTS) improve time-series forecasting compared to deep learning models?\n",
      "Question 19: What are the key differences between TimeLLM and TimeGPT in terms of model architecture and performance on time-series forecasting tasks?\n",
      "Question 20: How does LagLLama differ from Chronos in handling long-term dependencies in time-series forecasting?\n",
      "Question 21: What are the most common pitfalls when using anomaly detection models in financial datasets?\n",
      "Question 22: How has the use of foundation models impacted the field of time-series forecasting in terms of model accuracy and scalability?\n",
      "Question 23: How are transformer-based models improving the accuracy of anomaly detection in real-time applications?\n",
      "Question 24: What are the most effective techniques for handling seasonality and trend in time-series forecasting models?\n",
      "Question 25: How do foundation models like TimeGPT reduce the need for manual feature engineering in time-series forecasting?\n",
      "Question 26: How do foundation models such as TimeGPT, LagLLama, and TimesFM redefine time series forecasting compared to traditional statistical methods, and what are the implications of their zero-shot or few-shot capabilities in diverse industries?\n",
      "Question 27: What are the architectural innovations and challenges of adapting language models, like those in Chronos and TimeLLM, for time series data, particularly in terms of tokenization, temporal dependencies, and forecast accuracy?\n",
      "Question 28: In what ways do models like AnomalyBERT, TranAD, and RestAD approach anomaly detection in multivariate time series, and how do their self-supervised learning techniques compare in handling unlabeled data for real-time anomaly detection?\n",
      "Question 29: What role does probabilistic forecasting play in models like LagLLama and TimesFM, and how do these models handle uncertainty quantification across long prediction horizons?\n",
      "Question 30: How do models like TimeGPT and Chronos utilize synthetic data and real-world benchmarks in their training, and what trade-offs exist between their generalization abilities on new datasets and performance on specialized, domain-specific tasks?\n",
      "Question 31: How do the different models balance the trade-off between model complexity and interpretability when applied to real-world time series tasks such as anomaly detection and forecasting?\n",
      "Question 32: What are the shared architectural principles across the foundation models, and how do these designs influence their scalability and performance in large-scale forecasting tasks?\n",
      "Question 33: How do anomaly detection models like AnomalyBERT, TranAD, and RestAD compare in terms of their robustness and adaptability to various types of anomalies (contextual, point, collective) across different time series domains?\n",
      "Question 34: What are the main differences in how models like Chronos, TimesFM, and LagLLama handle long-term versus short-term dependencies in time series forecasting, and what impact does this have on their practical applications?\n",
      "Question 35: What challenges and breakthroughs have been encountered in applying self-supervised learning to time series data, and how does this compare with traditional supervised learning approaches in time series tasks?\n",
      "Question 36: How do state-of-the-art models for time series forecasting address the challenge of data sparsity and missing values in real-world datasets, and what techniques are most effective in mitigating the impact of such issues?\n",
      "Question 37: What are the main solutions presented in the paper Foundation Models for Time Series Data? What comparisons and tests were carried out?\n"
     ]
    }
   ],
   "source": [
    "questions_list = [item['question'] for item in data['questions']]\n",
    "\n",
    "# Print the questions to verify\n",
    "for idx, question in enumerate(questions_list, 1):\n",
    "    print(f\"Question {idx}: {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_sample_global = questions_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risposte del modello alle domande globali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question):\n",
    "    # Pass the question to your QA chain or model\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Categories of Time Series.\\n\\nA time series is commonly described as an ordered sequence of data points. Figure 2 illustrates various types of time series discussed in this survey, including standard time series, spatial time series, trajectories, and events. Note that trajectories and events can be regarded as time series since each data point is associated with a specific timestamp (and location), allowing for analysis using time series techniques such as anomaly detection. These time series are formulated as follows.\\n# Foundation Models for Time Series Analysis: A Tutorial and Survey\\n\\n# KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\\n\\n# 1. Introduction\\n\\nIn contrast to previous surveys, this manuscript incorporates the most extensive array of time series data types (see Table 1), spatial time series, as well as other types such as the trajectory and event. We further summarize the developmental roadmap of current TSFMs in Figure 1, in order to foster further innovations and understanding in the dynamic and ever-evolving landscape of TSFMs. In short, our major contributions lie in three aspects:\\n\\nTimeGPT was trained on, to our knowledge, the largest collection of publicly available time series, collectively encompassing over 100 billion data points. This training set incorporates time series from a broad array of domains, including finance, economics, demographics, healthcare, weather, IoT sensor data, energy, web traffic, sales, transport, and banking. Due to this diverse set of domains, the training dataset contains time series with a wide range of characteristics.\\n\\n# 2 RELATED WORKS\\n\\n# Time Series Anomaly Detection\\n\\n# 5 Pretraining Details\\n\\nWe would like our pretraining corpus to include large volumes of temporal data representing a variety of domains, trend and seasonality patterns and time granularities that ideally capture the forecasting use-cases which we are interested in serving by the deployed model. It is challenging to find a large time-series dataset that meets the volume and diversity of data needed for training our foundation model. We address this problem by sourcing the bulk of data used to train our models from three major sources: Google trends, Wiki Pageview statistics and synthetic time-series. In summary the main data sources are:\\n\\n# Google Trends\\n\\nQuestion:\\nWhat are the main topics covered by the data in the set of time-series papers?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nPreprint\\n\\nRamin Ghorbani1‚àó, Marcel J.T. Reinders1, and David M.J. Tax1\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\nAnomaly detection in time series data is crucial across various domains. The scarcity of labeled data for such tasks has increased the attention towards unsupervised learning methods. These approaches, often relying solely on reconstruction error, typically fail to detect subtle anomalies in complex datasets. To address this, we introduce RESTAD, an adaptation of the Transformer model by incorporating a layer of Radial Basis Function (RBF) neurons within its architecture. This layer fits a non-parametric density in the latent representation, such that a high RBF output indicates similarity with predominantly normal training data. RESTAD integrates the RBF similarity scores with the reconstruction errors to increase sensitivity to anomalies. Our empirical evaluations demonstrate that RESTAD outperforms various established baselines across multiple benchmark datasets.\\n\\nOur task is to determine if a given xi,t shows any anomalous behavior or not.\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Data\\n\\n|Input|Embedding|Encoder Layer 1|Encoder Layer 2|RBF Layer|Encoder Layer 3|Fully Connected Layer|Output Layer|\\n|---|---|---|---|---|---|---|---|\\n|Multi-Head Attention (8x)|Dropout|+|Norm|Feed-Forward|Norm|+| |\\n\\nFigure 2: Overview of the proposed RESTAD model. Here, the RBF layer is added after the second encoder layer.\\n\\n# Model Score\\n\\nRESTAD score(xi,t) = œµr √ó œµs (2)\\n\\nand œµs = ||xi,t PM=1 ||2 represents the reconstruction error, where œµr = 1 ‚àí M1 ‚àí ÀÜi,tzm,t measures dissimilarity. This combination highlights subtle anomalies characterized by both low reconstruction errors and RBF scores, as well as significant anomalies with high reconstruction errors or low RBF scores.\\n\\n# Initialization of RBF Layer Parameters\\n\\nThis paper presents an adaptation of the Transformer model, chosen for its ability to capture temporal dependencies in sequential data. By integrating the RBF neurons into the Transformer architecture, we develop a model that synergistically utilizes both similarity scores and reconstruction error to compute a distinctive anomaly score.\\n\\nThrough an extensive evaluation, we show that this new REconstruction and Similarity based Transformer for time series Anomaly Detection, RESTAD, outperforms existing baselines across a range of benchmark datasets.\\n\\n# 2 Methodology\\n\\nAssume that the observed time series dataset consists of N sequences with length T. Each sequence in this dataset is denoted by Xi = xi,t t=1 where xi,t represents the observed time point for i-th sequence at time t, having d dimensions, i.e., xi,t ‚àà Rd.\\n\\nOur task is to determine if a given xi,t shows any anomalous behavior or not.\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Evaluation\\n\\nAnomaly scores (Eq. 2) exceeding a threshold Œ¥ are identified as anomalies. Performance is evaluated using the F1-score for threshold-dependent evaluation. Here, we follow [17] by setting Œ¥ to label a predefined proportion of data points as anomalies (0.5% for SMD, 1% for others). For threshold-independent analysis, we use AUC-ROC, AUC-PR, VUS-ROC, and VUS-PR metrics [21]. We exclude the point-adjustment method [22] due to its overestimation [23]. Our model is compared against baselines and state-of-the-arts models: LSTM [9], vanilla Transformer [17], USAD [13], PatchAD [12], AnomalyTrans [17], and DCdetector [10].\\n\\n1 https://github.com/Raminghorbanii/RESTAD\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Table 1: Performance metrics of baselines and RESTAD on test sets.\\n\\nInitialization methods are denoted as (R) for Random and (K) for K-means. For all measures, a higher value indicates better anomaly detection performance.\\n\\nQuestion:\\nHow does RestAD leverage both statistical methods and machine learning to achieve robust anomaly detection in noisy time-series data?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nPreprint\\n\\nRamin Ghorbani1‚àó, Marcel J.T. Reinders1, and David M.J. Tax1\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\nOur task is to determine if a given xi,t shows any anomalous behavior or not.\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Data\\n\\n|Input|Embedding|Encoder Layer 1|Encoder Layer 2|RBF Layer|Encoder Layer 3|Fully Connected Layer|Output Layer|\\n|---|---|---|---|---|---|---|---|\\n|Multi-Head Attention (8x)|Dropout|+|Norm|Feed-Forward|Norm|+| |\\n\\nFigure 2: Overview of the proposed RESTAD model. Here, the RBF layer is added after the second encoder layer.\\n\\n# Model Score\\n\\nRESTAD score(xi,t) = œµr √ó œµs (2)\\n\\nand œµs = ||xi,t PM=1 ||2 represents the reconstruction error, where œµr = 1 ‚àí M1 ‚àí ÀÜi,tzm,t measures dissimilarity. This combination highlights subtle anomalies characterized by both low reconstruction errors and RBF scores, as well as significant anomalies with high reconstruction errors or low RBF scores.\\n\\n# Initialization of RBF Layer Parameters\\n\\n# Evaluation\\n\\nAnomaly scores (Eq. 2) exceeding a threshold Œ¥ are identified as anomalies. Performance is evaluated using the F1-score for threshold-dependent evaluation. Here, we follow [17] by setting Œ¥ to label a predefined proportion of data points as anomalies (0.5% for SMD, 1% for others). For threshold-independent analysis, we use AUC-ROC, AUC-PR, VUS-ROC, and VUS-PR metrics [21]. We exclude the point-adjustment method [22] due to its overestimation [23]. Our model is compared against baselines and state-of-the-arts models: LSTM [9], vanilla Transformer [17], USAD [13], PatchAD [12], AnomalyTrans [17], and DCdetector [10].\\n\\n1 https://github.com/Raminghorbanii/RESTAD\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Table 1: Performance metrics of baselines and RESTAD on test sets.\\n\\nInitialization methods are denoted as (R) for Random and (K) for K-means. For all measures, a higher value indicates better anomaly detection performance.\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\nAnomaly detection in time series data is crucial across various domains. The scarcity of labeled data for such tasks has increased the attention towards unsupervised learning methods. These approaches, often relying solely on reconstruction error, typically fail to detect subtle anomalies in complex datasets. To address this, we introduce RESTAD, an adaptation of the Transformer model by incorporating a layer of Radial Basis Function (RBF) neurons within its architecture. This layer fits a non-parametric density in the latent representation, such that a high RBF output indicates similarity with predominantly normal training data. RESTAD integrates the RBF similarity scores with the reconstruction errors to increase sensitivity to anomalies. Our empirical evaluations demonstrate that RESTAD outperforms various established baselines across multiple benchmark datasets.\\n\\nQuestion:\\nWhat are the key features and benefits of RestAD in anomaly detection for time-series data?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nPreprint\\n\\nRamin Ghorbani1‚àó, Marcel J.T. Reinders1, and David M.J. Tax1\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\nOur task is to determine if a given xi,t shows any anomalous behavior or not.\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Data\\n\\n|Input|Embedding|Encoder Layer 1|Encoder Layer 2|RBF Layer|Encoder Layer 3|Fully Connected Layer|Output Layer|\\n|---|---|---|---|---|---|---|---|\\n|Multi-Head Attention (8x)|Dropout|+|Norm|Feed-Forward|Norm|+| |\\n\\nFigure 2: Overview of the proposed RESTAD model. Here, the RBF layer is added after the second encoder layer.\\n\\n# Model Score\\n\\nRESTAD score(xi,t) = œµr √ó œµs (2)\\n\\nand œµs = ||xi,t PM=1 ||2 represents the reconstruction error, where œµr = 1 ‚àí M1 ‚àí ÀÜi,tzm,t measures dissimilarity. This combination highlights subtle anomalies characterized by both low reconstruction errors and RBF scores, as well as significant anomalies with high reconstruction errors or low RBF scores.\\n\\n# Initialization of RBF Layer Parameters\\n\\n# Evaluation\\n\\nAnomaly scores (Eq. 2) exceeding a threshold Œ¥ are identified as anomalies. Performance is evaluated using the F1-score for threshold-dependent evaluation. Here, we follow [17] by setting Œ¥ to label a predefined proportion of data points as anomalies (0.5% for SMD, 1% for others). For threshold-independent analysis, we use AUC-ROC, AUC-PR, VUS-ROC, and VUS-PR metrics [21]. We exclude the point-adjustment method [22] due to its overestimation [23]. Our model is compared against baselines and state-of-the-arts models: LSTM [9], vanilla Transformer [17], USAD [13], PatchAD [12], AnomalyTrans [17], and DCdetector [10].\\n\\n1 https://github.com/Raminghorbanii/RESTAD\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Table 1: Performance metrics of baselines and RESTAD on test sets.\\n\\nInitialization methods are denoted as (R) for Random and (K) for K-means. For all measures, a higher value indicates better anomaly detection performance.\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\nAnomaly detection in time series data is crucial across various domains. The scarcity of labeled data for such tasks has increased the attention towards unsupervised learning methods. These approaches, often relying solely on reconstruction error, typically fail to detect subtle anomalies in complex datasets. To address this, we introduce RESTAD, an adaptation of the Transformer model by incorporating a layer of Radial Basis Function (RBF) neurons within its architecture. This layer fits a non-parametric density in the latent representation, such that a high RBF output indicates similarity with predominantly normal training data. RESTAD integrates the RBF similarity scores with the reconstruction errors to increase sensitivity to anomalies. Our empirical evaluations demonstrate that RESTAD outperforms various established baselines across multiple benchmark datasets.\\n\\nQuestion:\\nWhat are the key features and benefits of RestAD in anomaly detection for time-series data?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# TIME-LLM: TIME SERIES FORECASTING BY REPROGRAMMING LARGE LANGUAGE MODELS\\n\\n# Ming Jin1‚àó, Shiyu Wang2‚àó, Lintao Ma2, Zhixuan Chu2, James Y. Zhang2, Xiaoming Shi2, Pin-Yu Chen3, Yuxuan Liang6, Yuan-Fang Li4, Shirui Pan4‚Ä†, Qingsong Wen5‚Ä†\\n\\n# Griffith University1, Alibaba Group5, Monash University1, Ant Group2, IBM Research3, The Hong Kong University of Science and Technology (Guangzhou)6\\n\\n{ming.jin, yuanfang.li}@monash.edu, pin-yu.chen@ibm.com, yuxliang@outlook.com, s.pan@griffith.edu.au, qingsongedu@gmail.com\\n\\n{weiming.wsy, lintao.mlt, chuzhixuan.czx, james.z, peter.sxm}@antgroup.com\\n\\n# ABSTRACT\\n\\nTime series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present TIME-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact\\n\\nIn both cases. TIME-LLM exhibits remarkable superiority in forecasting with limited data‚Äîa fact that becomes particularly salient when compared to GPT4TS.\\n# Published as a conference paper at ICLR 2024\\n\\n# Table 18: Efficiency comparison between model reprogramming and parameter-efficient fine-tuning (PEFT) with QLoRA (Dettmers et al., 2023) on ETTh1 dataset in forecasting two different steps ahead.\\n\\n|Length|ETTh1-96|ETTh1-96|ETTh1-96|ETTh1-336|ETTh1-336|ETTh1-336|\\n|---|---|---|\\n|Metric|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|Llama (8) QLoRA|12.60|14767|0.237|12.69|15982|0.335|\\n|Llama (8) Reprogram|5.62|11370|0.184|5.71|13188|0.203|\\n|Llama (32) QLoRA|50.29|45226|0.697|50.37|49374|0.732|\\n|Llama (32) Reprogram|6.39|32136|0.517|6.48|37988|0.632|\\n\\n# Table 19: Standard deviations of our approach and the second-best method (PatchTST) on all time series datasets for long-term forecasting.\\n\\n# Input statistics:\\n\\n- &lt;time series statistic 1&gt;\\n- &lt;time series statistic 2&gt; ‚Ä¶\\n\\n# Frozen Training\\n\\n# Prompt Embeddings\\n\\n# Patch Embeddings\\n\\n# Forward\\n\\n# Backward\\n\\nFigure 2: The model framework of TIME-LLM.\\nGiven an input time series, we first tokenize and embed it via 1 patching along with a 2 customized embedding layer. 3 These patch embeddings are then reprogrammed with condensed text prototypes to align two modalities. To augment the LLM‚Äôs reasoning ability, 4 additional prompt prefixes are added to the input to direct the transformation of input patches. 5 The output patches from the LLM are projected to generate the forecasts.\\n\\n# D.2 SHORT-TERM FORECASTING\\n\\nOur complete results on short-term forecasting are presented in Tab. 12. TIME-LLM consistently outperforms the majority of baseline models in most cases.\\n\\nlarge margin (e.g., 8.7% overall, 13.4% on M4-Yearly, and an average of 21.5% on M4-Hourly, M4-Daily, and M4-Weekly), as well as TimesNet (e.g., 10% overall, 14.1% on M4-Yearly, and an average of 30.1% on M4-Hourly, M4-Daily, and M4-Weekly). Compared to the recent state-of-the-art forecasting models, N-HiTS and PatchTST, TIME-LLM exhibits comparable or superior performances without any parameter updates on the backbone LLM.\\n\\nQuestion:\\nHow does TimeLLM differ from other models in time-series forecasting?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n. Our model converts multivariate data points into temporal representations with relative position bias and yields anomaly scores from these representations. Our method, AnomalyBERT, shows a great capability of detecting anomalies contained in complex time series and surpasses previous state-of-the-art methods on five real-world benchmarks. Our code is available at https://github.com/Jhryu30/AnomalyBERT.\\n\\n# 4.4 MAIN RESULTS\\n\\nWe report the results of AnomalyBERT on the five real-world datasets introduced in Section 4.1, and compare them to those of the previous works. The reproduced evaluation scores are brought from Kim et al. (2022). In Table 2, we show that AnomalyBERT outperforms the previous methods on\\n# Presented at the ICLR 2023 workshop on Machine Learning for IoT: Datasets, Perception, and Understanding\\n\\n# Table 2: F1-scores for various anomaly detection methods and AnomalyBERT on five benchmark datasets.\\n\\nWe report the standard F1 and F1-scores after point adjustment (F1PA) following the protocol in Kim et al. (2022). Our method outperforms all existing methods with F1.\\n\\n# Figure 1\\n\\nExamples of anomaly detection for abnormal time series in SWaT (features in our model). Our method, AnomalyBERT, predicts anomaly scores that quantify abnormalities of data points in time series. We also visualize the original data and the final latent features in our model using t-SNE and show that our method separates abnormal points effectively.\\n\\n|(a) Abnormal component of the original data|(b) 2D-visualization of the original data and latent features in our model.|\\n|---|---|\\n|Predicted Score|Latent feature|\\n|Normal|Abnormal|\\n|Normal|Abnormal|\\n\\n# 1 INTRODUCTION\\n\\nIn many industrial environments, time series data is mainly dealt with to monitor machines, IT devices, spacecrafts, or engines. Anomaly detection is one of the essential tasks for time series analysis, which can find defects in machines and prevent potential harm. Recently, many deep learning-based approaches have been applied to this work. Several studies design recurrent neural networks.\\n\\n# Table 1: Examination of relationships between the typical types of outliers in Lai et al. (2021) and our proposed synthetic outliers.\\n\\n|Proposed|Typical| | | | |\\n|---|---|---|---|---|---|\\n|Global|Contextual|Shapelet|Seasonal|Trend| |\\n|Soft replacement|‚úî|‚úî|‚úî|‚úî|‚úî|\\n|Uniform replacement|‚úî|‚úî| |‚úî| |\\n|Length adjustment| | | |‚úî| |\\n|Peak noise| |‚úî| | | |\\n\\n# Figure 4: Qualitative results of AnomalyBERT on abnormal sequences from SWaT, WADI, and SMAP datasets.\\n\\nWe visualize an abnormal component where the outlier occurs and anomaly scores for each sequence. Without any post-processing, our method finds out diverse types of anomalies.\\n\\n# B VISUAL RESULTS\\n\\nWe visualize the prediction results of AnomalyBERT and present the 2D projections of the original data and latent features in the model. Similar to Figure 1, we select three abnormal windows in each of SWaT, WADI, and SMAP test sets and fetch the corresponding anomaly scores after the score prediction process. We then project the original data points and the last latent features from the abnormal windows into 2D planes using t-SNE. As shown in Figure 5, our method distinguishes anomalies successfully and separates abnormal data points from normal points well.\\n\\n# SWaT\\n\\n# WADI\\n\\n# SMAP\\n\\n# Figure 5: Visualization of anomaly score predictions on abnormal sequences in SWaT, WADI, and SMAP datasets.\\n\\nQuestion:\\nHow does AnomalyBERT work?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 5 TimeGPT\\n\\n# 5.1 Architecture\\n\\nTimeGPT is a Transformer-based time series model with self-attention mechanisms based on [Vaswani et al., 2017]. TimeGPT takes a window of historical values to produce the forecast, adding local positional encoding to enrich the input. The architecture consists of an encoder-decoder structure with multiple layers, each with residual connections and layer normalization. Finally, a linear layer maps the decoder‚Äôs output to the forecasting window dimension. The general intuition is that attention-based mechanisms are able to capture the diversity of past events and correctly extrapolate potential future distributions.\\n\\nThe evaluation is performed in the last forecasting window of each time series, varying in length by the sampling frequency. TimeGPT uses the previous historical values as inputs, as shown in Figure 3, without re-training its weights (zero-shot). We specify a different forecasting horizon based on the frequency to represent common practical applications: 12 for monthly, 1 for weekly, 7 for daily, and 24 for hourly data. 4\\n\\nFurthermore, adjacent questions about foundation models for time series classification and the integration of truly multimodal (text, video) and multi-temporal foundation models promise to be engaging areas for future study. These areas will not only extend our understanding of time series data but also improve our ability to develop more powerful and generalized models for forecasting.\\n\\n# Access and early testing\\n\\nTimeGPT has undergone rigorous internal testing, demonstrating robust performance across various domains and frequencies. As we move forward, we invite practitioners and researchers to explore its capabilities on their own datasets and tasks.\\n\\nTo facilitate this, we have released comprehensive guides that explain how to utilize TimeGPT with features including uncertainty quantification, fine-tuning, forecasting multiple time series, integrating calendar and exogenous variables, and performing anomaly detection.\\n\\nIn this paper, we embark on a novel path and introduce TimeGPT, the first pre-trained foundation model for time series forecasting that can produce accurate predictions across a diverse array of domains and applications without additional training. A general pre-trained model constitutes a\\n\\n‚àóAuthors contributed equally.\\n# Groundbreaking Innovation\\n\\nGroundbreaking innovation that opens the path to a new paradigm for the forecasting practice that is more accessible and accurate, less time-consuming, and drastically reduces computational complexity.\\n\\n# 2 Background\\n\\n# Multi-Head Attention\\n\\n# Inference\\n\\n# Retail\\n\\n# Tourism\\n\\n# IoT\\n\\n# Positional Encoding\\n\\n# Input Embedding\\n\\n# Output Embedding\\n\\n# Inputs (shifted right)\\n\\n# Healthcare\\n\\n# Transport\\n\\n# Economics\\n\\nFigure 2: TimeGPT was trained in the largest collection of publicly available time series, and can forecast unseen time series without re-training its parameters.\\n\\nA forecasting model provides a function fŒ∏ : X  ‚Ü¶ Y, with X the feature space and Y the dependent variable space. We consider the setting with X = {y[0:t], x[0:t+h]} and Y = {y[t+1:t+h]}, where h is the forecast horizon, y is the target time series, and x are exogenous covariates. The forecasting task objective is to estimate the following conditional distribution:\\n\\nP(y[t+1:t+h]| y[0:t], x[0:t+h]) = fŒ∏(y[0:t], x[0:t+h]) (1)\\n\\nQuestion:\\nHow does TimeGPT approach time-series forecasting?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Input statistics:\\n\\n- &lt;time series statistic 1&gt;\\n- &lt;time series statistic 2&gt; ‚Ä¶\\n\\n# Frozen Training\\n\\n# Prompt Embeddings\\n\\n# Patch Embeddings\\n\\n# Forward\\n\\n# Backward\\n\\nFigure 2: The model framework of TIME-LLM.\\nGiven an input time series, we first tokenize and embed it via 1 patching along with a 2 customized embedding layer. 3 These patch embeddings are then reprogrammed with condensed text prototypes to align two modalities. To augment the LLM‚Äôs reasoning ability, 4 additional prompt prefixes are added to the input to direct the transformation of input patches. 5 The output patches from the LLM are projected to generate the forecasts.\\n\\n# Easy optimization\\n\\nLLMs are trained once on massive computing and then can be applied to forecasting tasks without learning from scratch. Optimizing existing forecasting models often requires significant architecture search and hyperparameter tuning (Zhou et al., 2023b).\\n\\nIn summary, LLMs offer a promising path to make time series forecasting more general, efficient, synergistic, and accessible compared to current specialized modeling paradigms. Thus, adapting these powerful models for time series data can unlock significant untapped potential.\\n\\n|Type|Methodology|Domain|References| | |\\n|---|---|---|---|---|---|\\n|Pre-trained LLM, AM, VLM|General| |Time-LLM [47], OFA [120], LLM4TS [11], PromptCast [102], TEMPO [10], LLMTime [36], Voice2Series [105], AutoTimes [63], UniTime [60]| | |\\n| | |Finance| |Yu et al. [108], Chen et al. [19], Xie et al. [100], Wimmer et al. [96]| |\\n| | |Healthcare| |Liu et al. [62]| |\\n|Generative|General| |PatchTST [68], Moirai [97], Lag-Llama [75], TimeSiam [26], Timer [64], Das et al. [24], UniTS [34], TimeGPT-1 [35], Chronos [1], MTSMAE [84]| | |\\n| | |Self-supervised|Contrastive|General: TEST [83], TimeCLR [107]| |\\n| | | |Healthcare| |METS [52]|\\n|Hybrid| |General|SimMTM [27]| | |\\n|Fully-supervised| |General|TimeXer [90], UniTS [34]| | |\\n|non-Transformer-based (MLP RNN CNN)|Self-supervised|Generative|TSMixer [30]| | |\\n| | |Contrastive|General|TF-C [115], TS2Vec [111], CLUDA [69]| |\\n|Fully-supervised| |General|TTMs [31], TimesNet [98], RWKV-TS [40]| | |\\n\\nIn both cases. TIME-LLM exhibits remarkable superiority in forecasting with limited data‚Äîa fact that becomes particularly salient when compared to GPT4TS.\\n# Published as a conference paper at ICLR 2024\\n\\n# Table 18: Efficiency comparison between model reprogramming and parameter-efficient fine-tuning (PEFT) with QLoRA (Dettmers et al., 2023) on ETTh1 dataset in forecasting two different steps ahead.\\n\\n|Length|ETTh1-96|ETTh1-96|ETTh1-96|ETTh1-336|ETTh1-336|ETTh1-336|\\n|---|---|---|\\n|Metric|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|Llama (8) QLoRA|12.60|14767|0.237|12.69|15982|0.335|\\n|Llama (8) Reprogram|5.62|11370|0.184|5.71|13188|0.203|\\n|Llama (32) QLoRA|50.29|45226|0.697|50.37|49374|0.732|\\n|Llama (32) Reprogram|6.39|32136|0.517|6.48|37988|0.632|\\n\\n# Table 19: Standard deviations of our approach and the second-best method (PatchTST) on all time series datasets for long-term forecasting.\\n\\n- TIME-LLM consistently exceeds state-of-the-art performance in mainstream forecasting tasks, especially in few-shot and zero-shot scenarios. Moreover, this superior performance is achieved while maintaining excellent model reprogramming efficiency. Thus, our research is a concrete step in unleashing LLMs‚Äô untapped potential for time series and perhaps other sequential data.\\n\\nQuestion:\\nWhat types of real-world applications can benefit from models like TimeLLM, RestAD, TimeGPT, AnomalyBERT, LagLLama and the other models described?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nIn this paper, we present Lag-Llama‚Äî a foundation model for probabilistic time series forecasting trained on a large collection of open time series data, and evaluated on unseen time series datasets. We investigate the performance of Lag-Llama across several settings where unseen time series.\\n# Lag-Llama\\n\\ndatasets are encountered downstream with different levels of data history being available, and show that Lag-Llama performs comparably or better against state-of-the-art dataset-specific models.\\n\\n# Our contributions:\\n\\n# Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting\\n\\n# Authors\\n\\n*Kashif Rasul1 *Arjun Ashok2 3 4 ‚ô¢Andrew Robert Williams3 4 ‚ô¢Hena Ghonia3 4 Rishika Bhagwatkar ‚ô†Roland Riachi3 4 ‚ô†Arian Khorasani3 4 ‚ô†Mohammad Javad Darvishi Bayazi3 ‚ô†George Adamopoulos5 Nadhir Hassen3 4 Marin Bilo≈°1 ‚ñ≥Sahil Garg1 ‚ñ≥Anderson Schneider1 ‚ñ≥Nicolas Chapados2 4 ‚ñ≥Alexandre Drouin2 4 ‚ñ≥Valentina Zantedeschi2 ‚ô£Yuriy Nevmyvaka1 ‚ô£Irina Rish3 4\\n\\n# Abstract\\n\\n. Moreover, when fine-tuned on relatively small fractions of such previously unseen datasets, Lag-Llama achieves state-of-the-art performance, outperforming prior deep learning approaches, emerging as the best general-purpose model on average. Lag-Llama serves as a strong contender to the current state-of-art in time series forecasting and paves the way for future advancements in foundation models tailored to time series data.\\n\\n- TIME-LLM consistently exceeds state-of-the-art performance in mainstream forecasting tasks, especially in few-shot and zero-shot scenarios. Moreover, this superior performance is achieved while maintaining excellent model reprogramming efficiency. Thus, our research is a concrete step in unleashing LLMs‚Äô untapped potential for time series and perhaps other sequential data.\\n\\nIn both cases. TIME-LLM exhibits remarkable superiority in forecasting with limited data‚Äîa fact that becomes particularly salient when compared to GPT4TS.\\n# Published as a conference paper at ICLR 2024\\n\\n# Table 18: Efficiency comparison between model reprogramming and parameter-efficient fine-tuning (PEFT) with QLoRA (Dettmers et al., 2023) on ETTh1 dataset in forecasting two different steps ahead.\\n\\n|Length|ETTh1-96|ETTh1-96|ETTh1-96|ETTh1-336|ETTh1-336|ETTh1-336|\\n|---|---|---|\\n|Metric|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|Llama (8) QLoRA|12.60|14767|0.237|12.69|15982|0.335|\\n|Llama (8) Reprogram|5.62|11370|0.184|5.71|13188|0.203|\\n|Llama (32) QLoRA|50.29|45226|0.697|50.37|49374|0.732|\\n|Llama (32) Reprogram|6.39|32136|0.517|6.48|37988|0.632|\\n\\n# Table 19: Standard deviations of our approach and the second-best method (PatchTST) on all time series datasets for long-term forecasting.\\n\\nQuestion:\\nWhat distinguishes LagLLama in its approach to time-series analysis?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n. Our model converts multivariate data points into temporal representations with relative position bias and yields anomaly scores from these representations. Our method, AnomalyBERT, shows a great capability of detecting anomalies contained in complex time series and surpasses previous state-of-the-art methods on five real-world benchmarks. Our code is available at https://github.com/Jhryu30/AnomalyBERT.\\n\\n# 2 RELATED WORKS\\n\\n# Time Series Anomaly Detection\\n\\n# Figure 1\\n\\nExamples of anomaly detection for abnormal time series in SWaT (features in our model). Our method, AnomalyBERT, predicts anomaly scores that quantify abnormalities of data points in time series. We also visualize the original data and the final latent features in our model using t-SNE and show that our method separates abnormal points effectively.\\n\\n|(a) Abnormal component of the original data|(b) 2D-visualization of the original data and latent features in our model.|\\n|---|---|\\n|Predicted Score|Latent feature|\\n|Normal|Abnormal|\\n|Normal|Abnormal|\\n\\n# 1 INTRODUCTION\\n\\nIn many industrial environments, time series data is mainly dealt with to monitor machines, IT devices, spacecrafts, or engines. Anomaly detection is one of the essential tasks for time series analysis, which can find defects in machines and prevent potential harm. Recently, many deep learning-based approaches have been applied to this work. Several studies design recurrent neural networks.\\n\\n# Table 1: Examination of relationships between the typical types of outliers in Lai et al. (2021) and our proposed synthetic outliers.\\n\\n|Proposed|Typical| | | | |\\n|---|---|---|---|---|---|\\n|Global|Contextual|Shapelet|Seasonal|Trend| |\\n|Soft replacement|‚úî|‚úî|‚úî|‚úî|‚úî|\\n|Uniform replacement|‚úî|‚úî| |‚úî| |\\n|Length adjustment| | | |‚úî| |\\n|Peak noise| |‚úî| | | |\\n\\n# Figure 4: Qualitative results of AnomalyBERT on abnormal sequences from SWaT, WADI, and SMAP datasets.\\n\\nWe visualize an abnormal component where the outlier occurs and anomaly scores for each sequence. Without any post-processing, our method finds out diverse types of anomalies.\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nQuestion:\\nHow do models like AnomalyBERT handle non-stationary data, and why is this important?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nthat demand substantial computational resources and time for inference. Recent concurrent work (Dooley et al., 2023; Das et al., 2023; Rasul et al., 2023; Woo et al., 2024) also explores pretraining transformer-based models with sophisticated time-series-specific designs on a large corpus of real and (or) synthetic time series data.\\n\\n# 5.1 Architecture\\n\\nAs shown in Figure 4, we first delve into the architecture of TSFMs, including Transformer-based models, non-Transformer-based models and diffusion-based models, focusing on the underlying mechanisms that shape their capabilities, as well as how they could be applied on various time series.\\n\\n# 5.1.1 Transformer-based Models\\n\\nIn addition to conventional time series data, the Transformer architecture has demonstrated efficacy across a diverse array of temporal datasets, such as trajectory and healthcare records, as summarized in Figure 3. This expansion highlights the Transformer‚Äôs versatile capacity for temporal data analysis.\\n# Foundation Models for Time Series Analysis: A Tutorial and Survey\\n\\n# KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\\n\\n# 5.1.2 Non-Transformer-based Models\\n\\nExcluding the widespread adoption of Transformers, a diverse array of traditional pre-training methods leveraged models such as Multi-Layer Perceptrons (MLPs) [31], Recurrent Neural Networks (RNNs) [33], and Convolutional Neural Networks (CNNs) [98] as the backbone for pre-training. These models, each with their unique strengths, are notable for their effectiveness in both conventional and spatial time series data.\\n\\nalso unable to model long-term trends effectively [4, 14, 62]. This is because, at each timestamp, a recurrent model needs to first perform inference for all previous timestamps before proceeding further. Recent developments of the transformer models allow single-shot inference with the complete input series using position encoding [51]. Using transformers allows much faster detection compared to recurrent methods by parallelizing inference on GPUs [19]. However, transformers also provide the benefit of being able to encode large sequences with accuracy and training/inference times nearly agnostic to the sequence length [51]. Thus, we use transformers to grow the temporal context information sent to an anomaly detector without significantly increasing the computational overheads.\\n\\n# Our contributions\\n\\n# Trend and seasonality.\\n\\nWe generated time series following linear and exponential trends: Chronos-T5 (Base) predicts the linear trend accurately but struggles with the exponential trend, as shown in Figure 12b. This may be due to a limited representation of exponential trends in the training data. A potential resolution for generating better forecasts for time series with exponential trends is to perform logarithmic scaling before feeding the time series.\\n\\n16\\n# Forecasting with Chronos Models\\n\\nQuestion:\\nWhat are the main trade-offs when choosing between transformer-based models and traditional time-series models?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Input statistics:\\n\\n- &lt;time series statistic 1&gt;\\n- &lt;time series statistic 2&gt; ‚Ä¶\\n\\n# Frozen Training\\n\\n# Prompt Embeddings\\n\\n# Patch Embeddings\\n\\n# Forward\\n\\n# Backward\\n\\nFigure 2: The model framework of TIME-LLM.\\nGiven an input time series, we first tokenize and embed it via 1 patching along with a 2 customized embedding layer. 3 These patch embeddings are then reprogrammed with condensed text prototypes to align two modalities. To augment the LLM‚Äôs reasoning ability, 4 additional prompt prefixes are added to the input to direct the transformation of input patches. 5 The output patches from the LLM are projected to generate the forecasts.\\n\\nIn both cases. TIME-LLM exhibits remarkable superiority in forecasting with limited data‚Äîa fact that becomes particularly salient when compared to GPT4TS.\\n# Published as a conference paper at ICLR 2024\\n\\n# Table 18: Efficiency comparison between model reprogramming and parameter-efficient fine-tuning (PEFT) with QLoRA (Dettmers et al., 2023) on ETTh1 dataset in forecasting two different steps ahead.\\n\\n|Length|ETTh1-96|ETTh1-96|ETTh1-96|ETTh1-336|ETTh1-336|ETTh1-336|\\n|---|---|---|\\n|Metric|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|Llama (8) QLoRA|12.60|14767|0.237|12.69|15982|0.335|\\n|Llama (8) Reprogram|5.62|11370|0.184|5.71|13188|0.203|\\n|Llama (32) QLoRA|50.29|45226|0.697|50.37|49374|0.732|\\n|Llama (32) Reprogram|6.39|32136|0.517|6.48|37988|0.632|\\n\\n# Table 19: Standard deviations of our approach and the second-best method (PatchTST) on all time series datasets for long-term forecasting.\\n\\n- TIME-LLM consistently exceeds state-of-the-art performance in mainstream forecasting tasks, especially in few-shot and zero-shot scenarios. Moreover, this superior performance is achieved while maintaining excellent model reprogramming efficiency. Thus, our research is a concrete step in unleashing LLMs‚Äô untapped potential for time series and perhaps other sequential data.\\n\\nThe realization of the above benefits hinges on the effective alignment of the modalities of time series data and natural language. However, this is a challenging task as LLMs operate on discrete tokens, while time series data is inherently continuous. Furthermore, the knowledge and reasoning capabilities to interpret time series patterns are not naturally present within LLMs‚Äô pre-training. Therefore, it remains an open challenge to unlock the knowledge within LLMs in activating their ability for general time series forecasting in a way that is accurate, data-efficient, and task-agnostic.\\n\\n# Proposed Framework\\n\\n# VISUALIZATION\\n\\nIn this part, we visualize the forecasting results of TIME-LLM compared with the state-of-the-art and representative methods (e.g., GPT4TS (Zhou et al., 2023a), PatchTST (Nie et al., 2023), and Autoformer (Wu et al., 2021)) in various scenarios to demonstrate the superior performance of TIME-LLM. In Fig. 7 and Fig. 8, the long-term (input-96-predict-96) and short-term (input-36-predict-36) forecasts of various approaches are compared with the ground truth. Here, TIME-LLM showcases forecasting accuracy that is notably superior compared to GPT4TS, PatchTST, and a classical Transformer-based method, Autoformer. We also offer visual comparisons of the forecasting results in both few-shot and zero-shot scenarios, as depicted in Fig. 9 and Fig. 10. We adhere to the long-term (input-96-predict-96) forecasting setup.\\n# Published as a conference paper at ICLR 2024\\n\\nQuestion:\\nHow does TimeLLM incorporate historical data for better forecasting accuracy?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 3.2 Data Preprocessing\\n\\nTo make our model more robust, we normalize the data and convert it to time-series windows, both for training and testing. We normalize the time-series as:\\n\\n*ùë•ùë° ‚àí min(T)*\\n\\n*ùë•ùë° ‚Üê max(T) ‚àí min(T) + ùúñ ‚Ä≤,* (1)\\n\\nwhere *min(T) and max(T) are the mode wise minimum and maximum vectors in the training time-series. ùúñ ‚Ä≤* is a small constant vector to prevent zero-division. Knowing the ranges a-priori, we normalize the data to get it in the range [0, 1).\\n\\nTo model the dependence of a data point *ùë•ùë° at a timestamp ùë°, we consider a local contextual window of length K* as:\\n\\n*ùëäùë° = {ùë•ùë°‚àíùêæ+1, . . . , ùë•ùë°}.*\\n\\nthat demand substantial computational resources and time for inference. Recent concurrent work (Dooley et al., 2023; Das et al., 2023; Rasul et al., 2023; Woo et al., 2024) also explores pretraining transformer-based models with sophisticated time-series-specific designs on a large corpus of real and (or) synthetic time series data.\\n\\nTimeGPT was benchmarked against a broad spectrum of baseline, statistical, machine learning, and neural forecasting models to provide a comprehensive performance analysis. Baselines and statistical models are individually trained on each time series of the test set, utilizing the historical values preceding the last forecasting window. We opted for a global model approach for machine learning and deep learning methods for each frequency, leveraging all time series in the test set. Some popular models like Prophet [Taylor and Letham, 2018] and ARIMA were excluded from the analysis due to their prohibitive computational requirements and extensive training times.\\n\\nFurthermore, adjacent questions about foundation models for time series classification and the integration of truly multimodal (text, video) and multi-temporal foundation models promise to be engaging areas for future study. These areas will not only extend our understanding of time series data but also improve our ability to develop more powerful and generalized models for forecasting.\\n\\n# Access and early testing\\n\\nTimeGPT has undergone rigorous internal testing, demonstrating robust performance across various domains and frequencies. As we move forward, we invite practitioners and researchers to explore its capabilities on their own datasets and tasks.\\n\\nTo facilitate this, we have released comprehensive guides that explain how to utilize TimeGPT with features including uncertainty quantification, fine-tuning, forecasting multiple time series, integrating calendar and exogenous variables, and performing anomaly detection.\\n\\n# Aggregating across datasets\\n\\nSince the datasets have wildly different scales averaging unnormalized metrics like MAE is not kosher. Therefore following [GFQW23] we scale the metric of each baseline for a dataset by the same metric achieved by a naive baseline on that dataset. The naive baseline just makes the constant prediction yL repeated across the prediction length. We did not need to do that for the Informer datasets since on these datasets metrics are usually reported on standard normalized data [NNSK22].\\n# A decoder-only foundation model for time-series forecasting\\n\\n# A.3 Finetuning study on ETT\\n\\nQuestion:\\nWhat kind of preprocessing steps are typically necessary for models like TimeGPT and AnomalyBERT to perform effectively on time-series data?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Trend and seasonality.\\n\\nWe generated time series following linear and exponential trends: Chronos-T5 (Base) predicts the linear trend accurately but struggles with the exponential trend, as shown in Figure 12b. This may be due to a limited representation of exponential trends in the training data. A potential resolution for generating better forecasts for time series with exponential trends is to perform logarithmic scaling before feeding the time series.\\n\\n16\\n# Forecasting with Chronos Models\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nPreprint\\n\\nRamin Ghorbani1‚àó, Marcel J.T. Reinders1, and David M.J. Tax1\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\n# Chronos: Learning the Language of Time Series\\n\\n# Abdul Fatir Ansari1‚àó Lorenzo Stella1‚àó Caner Turkmen1, Xiyuan Zhang3‚Ä† Pedro Mercado4‚Ä†\\n\\n# Huibin Shen1, Oleksandr Shchur1, Syama Sundar Rangapuram1, Sebastian Pineda Arango1, Shubham Kapoor1, Jasper Zschiegner, Danielle C. Maddix1, Hao Wang1,5‚Ä†\\n\\n# honey2,6‚Ä† Kari Torkkola2, Andrew Gordon Wilson2,7‚Ä† Michael Bohlke-Schneider1, Yuyang‚Ä†, Michael W. Ma-Wang1\\n\\n# 1AWS AI Labs, 2Amazon Supply Chain Optimization Technologies, 3UC San Diego, {ansarnd, stellalo}@amazon.com 4University of Freiburg, 5Rutgers University, 6UC Berkeley, 7New York University\\n\\n# Abstract\\n\\n# A PREPRINT\\n\\n# Table 1: Composition of TimesFM pretraining dataset.\\n\\n|Dataset|Granularity|# Time series|# Time points|\\n|---|---|---|---|\\n|Synthetic| |3,000,000|6,144,000,000|\\n|Electricity|Hourly|321|8,443,584|\\n|Traffic| | | |\\n|Weather [ZZP+21]|Hourly|862|15,122,928|\\n| |10 Min|42|2,213,232|\\n|Favorita Sales [23]| | | |\\n|LibCity [WJJ]|Daily|111,840|6,159|\\n| |15 Min|139,179,538|34,253,622|\\n|M4 hourly|Hourly|414|353,500|\\n|M4 daily|Daily|4,227|9,964,658|\\n|M4 monthly|Monthly|48,000|10,382,411|\\n|M4 quarterly|Quarterly|24,000|2,214,108|\\n|M4 yearly|Yearly|22,739|840,644|\\n|Wiki hourly|Hourly|5,608,693|239,110,787,496|\\n|Wiki daily|Daily|68,448,204|115,143,501,240|\\n|Wiki weekly|Weekly|66,579,850|16,414,251,948|\\n|Wiki monthly|Monthly|63,151,306|3,789,760,907|\\n|Trends hourly|Hourly|22,435|393,043,680|\\n|Trends daily|Daily|22,435|122,921,365|\\n|Trends weekly|Weekly|22,435|16,585,438|\\n|Trends monthly|Monthly|22,435|3,821,760|\\n\\n# Figure 2:\\n\\n# Acknowledgements\\n\\nWe are indebted to Stefano Soatto for challenging us to think about the fundamental question regarding language models and time series modeling, ultimately leading to the creation of the present work.\\nare grateful to our fellow researchers who have contributed to this work with insightful discussions and valuable feedback, including but not limited to Devamanyu Hazarika, Imry Kissos, Laurent Callot, Baris Kurt, Valentin Flunkert, David Salinas, Boran Han, Xiaoyong Jin, Luke Huan, Youngsuk Park, Gaurav Gupta, Karthick Gopalswamy, Tim Januschowski, Jan Gasthaus, Bing Xiang, Kashif Rasul, Juba Nait Saada, Matthias Karlbauer, Mononito Goswami and Gerald Woo.\\n\\n# References\\n\\nQuestion:\\nHow do models like TimeGPT and RestAD handle seasonality and trends in time-series data?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nChronos represents one of the first endeavours in practical pretrained time series forecasting models, with remarkable zero-shot performance on a comprehensive collection of test datasets. This work opens up various research avenues, some of which we discuss below.\\n\\n# 6.1 Beyond Zero-shot Univariate Forecasting\\n\\n# Trend and seasonality.\\n\\nWe generated time series following linear and exponential trends: Chronos-T5 (Base) predicts the linear trend accurately but struggles with the exponential trend, as shown in Figure 12b. This may be due to a limited representation of exponential trends in the training data. A potential resolution for generating better forecasts for time series with exponential trends is to perform logarithmic scaling before feeding the time series.\\n\\n16\\n# Forecasting with Chronos Models\\n\\nIn this work, we take a step back and ask: what are the fundamental differences between a language model that predicts the next token, and a time series forecasting model that predicts the next values? Despite the apparent distinction ‚Äî tokens from a finite dictionary versus values from an unbounded, usually continuous domain ‚Äî both endeavors fundamentally aim to model the sequential structure of the data to predict future patterns. Shouldn‚Äôt good language models ‚Äújust work‚Äù on time series? This naive question prompts us to challenge the necessity of time-series-specific modifications, and answering it led us to develop Chronos, a language modeling framework minimally adapted for time series forecasting. Chronos tokenizes time series into discrete bins through simple scaling and quantization of real values\\n\\n# Forecast distributions from a Chronos model on series from the NN5 (Daily), Traffic, and Hospital datasets respectively.\\n\\n. A common approach in time series modeling is to incorporate time and frequency information, through features such as day-of-week, week-of-year, and so on. Perhaps counter-intuitively, in Chronos, we ignore time and frequency information, treating the ‚Äútime series‚Äù simply as a sequence.\\n\\nQuestion:\\nHow does Chronos approach time-series forecasting compared to traditional statistical models?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# A PREPRINT\\n\\nIn particular, we design TimesFM, a single foundation model for time-series forecasting that, when applied to a variety of previously-unseen forecasting datasets across different domains, obtains close to state-of-the-art zero-shot accuracy (compared to the best supervised models trained individually for these datasets). Our model can work well across different forecasting history lengths, prediction lengths and time granularities at inference time. The key elements of our foundation model are twofold: 1) a large-scale time-series corpus built using both real-world (mostly time-series data from web search queries1 and Wikipedia page visits2) and synthetic data, which meets the volume and diversity of data needed for training our foundation model, and 2) a decoder style attention architecture with input patching, that can be efficiently pre-trained on this time-series corpus.\\n\\n# A.9 Illustrative Examples\\n\\nWe conduct a visual inspection of the forecasts generated by TimesFM, first on some synthetic examples and then on the benchmark datasets.\\n\\nIn Figure 5 we show 4 different synthetic curves: (1) sum of 5 sine curves of different periods, (2) a sine curve linearly scaled, (3) a sine curve with a linear trend, and (4) minimum of two sine curves with a linear trend. Our results suggests that TimesFM picks up the trend and seasonal components readily interpretable by humans, while ARIMA and (to a lesser extent) llmtime fail in some of the instances.\\n\\nAs illustrated in Figure 6, TimesFM also effectively captures these subtle characteristics within both the trend and seasonal patterns of the depicted real world time-series. For instance, in the Air Passenger dataset, TimesFM correctly captures the amplitude increase with trend ‚Äìthis is also reflected by the fact that it attains the best MAE on this dataset (see Table 3). In the traffic hourly example on the left, it can be seen that TimesFM can correctly identify the seasonal peaks even in the presence of outliers in the context, while llmtime is thrown off.\\n\\nWe provide more visualization in Figure 7, Figure 8 and Figure 9.\\n# A decoder-only foundation model for time-series forecasting\\n\\n# A PREPRINT\\n\\n# Figure 5\\n\\nForecasts visualized on synthetic curves. The bottom row plots zoom in on the prediction horizon for the sake of clarity.\\n\\nground truth\\nTimesFM(ZS)\\narima\\nIlmtime(ZS)\\n\\n# Figure 6\\n\\nWe provide a finetuning study in the same setting as [ ZNW+23 ] in Appendix A.3, where our model performs better than all baselines on all the reported datasets. This shows the utility of our model on downstream tasks.\\n\\n# 7 Conclusion\\n\\nIn this paper, we presented TimesFM, a practical foundation model for forecasting whose zero-shot performance comes close to the accuracy of fully-supervised forecasting models on a diverse set of time-series data. This model is pretrained on real-world and synthetic datasets comprising O(100B) timepoints. We discuss limitations and future work in more detail in Appendix A.1.\\n\\n# 8 Impact Statement\\n\\n# Table 2: Short-term time series forecasting results on M4.\\n\\nThe forecasting horizons are in [6, 48] and the three rows provided are weighted averaged from all datasets under different sampling intervals. A lower value indicates better performance. Red: the best, Blue: the second best. More results are in Appendix D.\\n\\n|Methods|TIME-LLM (Ours)|GPT4TS (2023a)|TimesNet (2023)|PatchTST (2023)|N-HiTS (2023b)|N-BEATS (2020)|ETSformer (2022)|LightTS (2022a)|DLinear (2023)|FEDformer (2022)|Stationary (2022)|Autoformer (2021)|Informer (2021)|Reformer (2020)|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|SMAPE|11.983|12.69|12.88|12.059|12.035|12.25|14.718|13.525|13.639|13.16|12.780|12.909|14.086|18.200|\\n|MASE|1.595|1.808|1.836|1.623|1.625|1.698|2.408|2.111|2.095|1.775|1.756|1.771|2.718|4.223|\\n|OWA|0.859|0.94|0.955|0.869|0.869|0.896|1.172|1.051|1.051|0.949|0.930|0.939|1.230|1.775|\\n\\nQuestion:\\nWhat are the strengths and weaknesses of TimesFM in handling multivariate time-series forecasting?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Shreshth Tuli\\n\\n# Giuliano Casale\\n\\n# Nicholas R. Jennings\\n\\n# Imperial College London\\n\\n# Imperial College London\\n\\n# Loughborough University\\n\\n# London, UK\\n\\n# London, UK\\n\\n# London, UK\\n\\n# s.tuli20@imperial.ac.uk\\n\\n# g.casale@imperial.ac.uk\\n\\n# n.r.jennings@lboro.ac.uk\\n\\n# Abstract\\n\\n# 4 Experiments\\n\\nWe compare TranAD with state-of-the-art models for multivariate time-series anomaly detection, including MERLIN, LSTM-NDT (with autoencoder implementation from openGauss), DAGMM, OmniAnomaly, MSCRED, MAD-GAN, USAD, MTAD-GAT, CAE-M and GDN (with graph embedding implementation from GraphAn). For more details refer Section 2.1. We also tested the Isolation Forest method, but due to its low F1 scores, do not include the corresponding results in our discussion. Other classical methods have been omitted as well.\\n\\nEfficient anomaly detection and diagnosis in multivariate time-series data is of great importance for modern industrial applications. However, building a system that is able to quickly and accurately pinpoint anomalous observations is a challenging problem. This is due to the lack of anomaly labels, high data volatility and the demands of ultra-low inference times in modern applications. Despite the recent developments of deep learning approaches for anomaly detection, only a few of them can address all of these challenges. In this paper, we propose TranAD, a deep transformer network based anomaly detection and diagnosis model which uses attention-based sequence encoders to swiftly perform inference with the knowledge of the broader temporal trends in the data. TranAD uses focus score-based self-conditioning to enable robust multi-modal feature extraction and adversarial training to gain stability\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nThe rest of the paper is organized as follows. Section 2 overviews related work. Section 3 outlines the working of the TranAD model for multivariate anomaly detection and diagnosis. A performance evaluation of the proposed method is shown in Section 4. Section 5 presents additional analysis. Finally, Section 6 concludes.\\n\\n# 2 Related Work\\n\\nTime series anomaly detection is a long-studied problem in the VLDB community. The prior literature works on two types of time-series data: univariate and multivariate. For the former, various methods analyze and detect anomalies in time-series data with a single data source [34], while for the latter multiple time-series together [14, 45, 62].\\n\\n# Classical methods\\n\\nQuestion:\\nHow does TranAD address the challenges of detecting anomalies in streaming time-series data?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# A PREPRINT\\n\\nIn particular, we design TimesFM, a single foundation model for time-series forecasting that, when applied to a variety of previously-unseen forecasting datasets across different domains, obtains close to state-of-the-art zero-shot accuracy (compared to the best supervised models trained individually for these datasets). Our model can work well across different forecasting history lengths, prediction lengths and time granularities at inference time. The key elements of our foundation model are twofold: 1) a large-scale time-series corpus built using both real-world (mostly time-series data from web search queries1 and Wikipedia page visits2) and synthetic data, which meets the volume and diversity of data needed for training our foundation model, and 2) a decoder style attention architecture with input patching, that can be efficiently pre-trained on this time-series corpus.\\n\\n# 2 BACKGROUND\\n\\n# Foundation Models.\\n\\nFoundation models (FMs), also known as large pre-trained models, are a class of deep models that are pre-trained on vast amounts of data, thus equipped with a wide range of general knowledge and patterns. To this end, these models serve as a versatile starting point for various tasks across different domains. Specifically, FMs can be fine-tuned or adapted to specific tasks with relatively small amounts of task-specific data, showcasing remarkable flexibility and efficiency. In CV, FMs such as text-prompted model CLIP [73] and visual-prompted model SAM [51] have propelled advancements in image recognition, object detection, and more. In NLP, FMs such as BERT [25] and GPT-3 [8] have revolutionized text understanding and generation tasks. Inspired by the great success of FMs in the above domains, this survey delves into the utilization of these models in the realm of time series analysis.\\n\\nThis motivates the question: ‚ÄúCan large pretrained models trained on massive amounts of time-series data learn temporal patterns that can be useful for time-series forecasting on previously unseen datasets?‚Äù In particular, can we design a time-series foundation model that obtains good zero-shot out-of-the-box forecasting performance? Such a pretrained time-series foundation model, if possible, would bring significant benefits for downstream forecasting users in terms of no additional training burden and significantly reduced compute requirements. It is not immediately obvious that such a foundation model for time-series forecasting is possible. Unlike in NLP, there is no well defined vocabulary or grammar for time-series. Additionally, such a model would need to support forecasting with varying history lengths (context), prediction lengths (horizon) and time granularities\\n\\nThe potential of foundation models, namely large-scale models pre-trained on a large dataset and later fine-tuned for specific tasks, remains relatively under-explored for time series forecasting tasks. There are, however, early indicators of the possibility of forecasting foundational models. For instance, [Oreshkin et al., 2021] showed that pre-trained models can be transferred between tasks without performance degradation. Additionally, [Kunz et al., 2023] provided evidence on the existence of scaling laws on data and model sizes for Transformer architectures on time series forecasting tasks.\\n\\n# 4 Foundation model for time series\\n\\n. In fact we can see our 10% finetuned model‚Äôs performances are comparable or better than that of most baselines trained on the whole training dataset as reported in Table 14 of [ZNW+23]. This shows that the inductive biases encoded in our model weights by finetuning on a large time-series corpus are better for downstream forecasting task than an off the shelf language model like GPT2, even though our model is orders of magnitude smaller.\\n\\nQuestion:\\nHow does Foundation Models for Time Series Analysis (FMTS) improve time-series forecasting compared to deep learning models?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nIn both cases. TIME-LLM exhibits remarkable superiority in forecasting with limited data‚Äîa fact that becomes particularly salient when compared to GPT4TS.\\n# Published as a conference paper at ICLR 2024\\n\\n# Table 18: Efficiency comparison between model reprogramming and parameter-efficient fine-tuning (PEFT) with QLoRA (Dettmers et al., 2023) on ETTh1 dataset in forecasting two different steps ahead.\\n\\n|Length|ETTh1-96|ETTh1-96|ETTh1-96|ETTh1-336|ETTh1-336|ETTh1-336|\\n|---|---|---|\\n|Metric|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|Llama (8) QLoRA|12.60|14767|0.237|12.69|15982|0.335|\\n|Llama (8) Reprogram|5.62|11370|0.184|5.71|13188|0.203|\\n|Llama (32) QLoRA|50.29|45226|0.697|50.37|49374|0.732|\\n|Llama (32) Reprogram|6.39|32136|0.517|6.48|37988|0.632|\\n\\n# Table 19: Standard deviations of our approach and the second-best method (PatchTST) on all time series datasets for long-term forecasting.\\n\\nIn addition, we conduct a comparative analysis between TIME-LLM and the top-performing models on the M3-Quarterly dataset, with the findings presented in Tab. 13. We provide additional metrics, namely MRAE and MAPE, alongside the default SMAPE used in the M3 competition. On this dataset, TIME-LLM attains on-par performance compared to TimesNet and PatchTST, outperforming GPT4TS by substantial margins, achieving reductions of over 23%, 35%, and 26% in SMAPE, MRAE, and MAPE, respectively.\\n\\nNotably, we surpass GPT4TS by a\\n\\n# E FEW-SHOT AND ZERO-SHOT FORECASTING\\n\\n# E.1 FEW-SHOT FORECASTING\\n\\nTimeGPT was benchmarked against a broad spectrum of baseline, statistical, machine learning, and neural forecasting models to provide a comprehensive performance analysis. Baselines and statistical models are individually trained on each time series of the test set, utilizing the historical values preceding the last forecasting window. We opted for a global model approach for machine learning and deep learning methods for each frequency, leveraging all time series in the test set. Some popular models like Prophet [Taylor and Letham, 2018] and ARIMA were excluded from the analysis due to their prohibitive computational requirements and extensive training times.\\n\\n# Results\\n\\nOur brief results are shown in Tab. 1, where TIME-LLM outperforms all baselines in most cases and significantly so to the majority of them. The comparison with GPT4TS (Zhou et al., 2023a) is particularly noteworthy. GPT4TS is a very recent work that involves fine-tuning on the backbone language model. We note average performance gains of 12% and 20% over GPT4TS and TimesNet, respectively. When compared with the SOTA task-specific Transformer model PatchTST, by reprogramming the smallest Llama, TIME-LLM realizes an average MSE reduction of 1.4%. Relative to the other models, e.g., DLinear, our improvements are also pronounced, exceeding 12%.\\n\\n# 4.2 SHORT-TERM FORECASTING\\n\\n# Setups\\n\\nWe choose the M4 benchmark (Makridakis et al., 2018) as the testbed, which contains a collection of marketing data in different sampling frequencies. More details are provided in Appendix B. The prediction horizons in this case are relatively small and in [6, 48]. The input lengths.\\n\\n- TIME-LLM consistently exceeds state-of-the-art performance in mainstream forecasting tasks, especially in few-shot and zero-shot scenarios. Moreover, this superior performance is achieved while maintaining excellent model reprogramming efficiency. Thus, our research is a concrete step in unleashing LLMs‚Äô untapped potential for time series and perhaps other sequential data.\\n\\nQuestion:\\nWhat are the key differences between TimeLLM and TimeGPT in terms of model architecture and performance on time-series forecasting tasks?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n.e., they are fine-tuned and tested on each dataset separately. Furthermore, the aforementioned methods are based on prompting or fine-tuning pretrained LLMs. In contrast, Chronos trains language models from scratch on a large collection of time series, tokenized via scaling and quantization.\\n\\nIn this work, we take a step back and ask: what are the fundamental differences between a language model that predicts the next token, and a time series forecasting model that predicts the next values? Despite the apparent distinction ‚Äî tokens from a finite dictionary versus values from an unbounded, usually continuous domain ‚Äî both endeavors fundamentally aim to model the sequential structure of the data to predict future patterns. Shouldn‚Äôt good language models ‚Äújust work‚Äù on time series? This naive question prompts us to challenge the necessity of time-series-specific modifications, and answering it led us to develop Chronos, a language modeling framework minimally adapted for time series forecasting. Chronos tokenizes time series into discrete bins through simple scaling and quantization of real values\\n\\nIn this paper, we present Lag-Llama‚Äî a foundation model for probabilistic time series forecasting trained on a large collection of open time series data, and evaluated on unseen time series datasets. We investigate the performance of Lag-Llama across several settings where unseen time series.\\n# Lag-Llama\\n\\ndatasets are encountered downstream with different levels of data history being available, and show that Lag-Llama performs comparably or better against state-of-the-art dataset-specific models.\\n\\n# Our contributions:\\n\\nOur contributions are significant in two key aspects. First, we show that existing language model architectures are capable of performing forecasting without time-series-specific customizations. This paves the way for accelerated progress by leveraging developments in the area of LLMs and through better data strategies. Second, on a practical level, the strong performance of Chronos models suggests that large (by forecasting standards) pretrained language models can greatly simplify forecasting pipelines without sacrificing accuracy, offering an inference-only alternative to the conventional approach involving training and tuning a model on individual tasks.\\n\\n# Acknowledgements\\n\\nIn both cases. TIME-LLM exhibits remarkable superiority in forecasting with limited data‚Äîa fact that becomes particularly salient when compared to GPT4TS.\\n# Published as a conference paper at ICLR 2024\\n\\n# Table 18: Efficiency comparison between model reprogramming and parameter-efficient fine-tuning (PEFT) with QLoRA (Dettmers et al., 2023) on ETTh1 dataset in forecasting two different steps ahead.\\n\\n|Length|ETTh1-96|ETTh1-96|ETTh1-96|ETTh1-336|ETTh1-336|ETTh1-336|\\n|---|---|---|\\n|Metric|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|Llama (8) QLoRA|12.60|14767|0.237|12.69|15982|0.335|\\n|Llama (8) Reprogram|5.62|11370|0.184|5.71|13188|0.203|\\n|Llama (32) QLoRA|50.29|45226|0.697|50.37|49374|0.732|\\n|Llama (32) Reprogram|6.39|32136|0.517|6.48|37988|0.632|\\n\\n# Table 19: Standard deviations of our approach and the second-best method (PatchTST) on all time series datasets for long-term forecasting.\\n\\nQuestion:\\nHow does LagLLama differ from Chronos in handling long-term dependencies in time-series forecasting?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 2 RELATED WORKS\\n\\n# Time Series Anomaly Detection\\n\\n# Challenges\\n\\nThe problem of anomaly detection is becoming increasingly challenging in large-scale databases due to the increasing data modality. In particular, the increasing number of sensors and devices in contemporary IoT platforms with...\\n# Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nPreprint\\n\\nRamin Ghorbani1‚àó, Marcel J.T. Reinders1, and David M.J. Tax1\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# (1) Numenta Anomaly Benchmark (NAB)\\n\\nis a dataset of multiple real-world data traces, including readings from temperature sensors, CPU utilization of cloud machines, service request latencies and taxi demands in New York city [2]. However, this dataset is known to have sequences with incorrect anomaly labels such as the nyc-taxi trace [37], which we exclude in our experiments.\\n\\n# (2) HexagonML (UCR) dataset\\n\\nis a dataset of multiple univariate time series (included just for completeness) that was used in KDD 2021 cup [13, 26]. We include only the datasets obtained from natural sources (the InternalBleeding and ECG datasets) and ignore the synthetic sequences.\\n\\n# (3) MIT-BIH Supraventricular Arrhythmia Database (MBA)\\n\\nQuestion:\\nWhat are the most common pitfalls when using anomaly detection models in financial datasets?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nThe potential of foundation models, namely large-scale models pre-trained on a large dataset and later fine-tuned for specific tasks, remains relatively under-explored for time series forecasting tasks. There are, however, early indicators of the possibility of forecasting foundational models. For instance, [Oreshkin et al., 2021] showed that pre-trained models can be transferred between tasks without performance degradation. Additionally, [Kunz et al., 2023] provided evidence on the existence of scaling laws on data and model sizes for Transformer architectures on time series forecasting tasks.\\n\\n# 4 Foundation model for time series\\n\\n# A PREPRINT\\n\\nIn particular, we design TimesFM, a single foundation model for time-series forecasting that, when applied to a variety of previously-unseen forecasting datasets across different domains, obtains close to state-of-the-art zero-shot accuracy (compared to the best supervised models trained individually for these datasets). Our model can work well across different forecasting history lengths, prediction lengths and time granularities at inference time. The key elements of our foundation model are twofold: 1) a large-scale time-series corpus built using both real-world (mostly time-series data from web search queries1 and Wikipedia page visits2) and synthetic data, which meets the volume and diversity of data needed for training our foundation model, and 2) a decoder style attention architecture with input patching, that can be efficiently pre-trained on this time-series corpus.\\n\\nThis motivates the question: ‚ÄúCan large pretrained models trained on massive amounts of time-series data learn temporal patterns that can be useful for time-series forecasting on previously unseen datasets?‚Äù In particular, can we design a time-series foundation model that obtains good zero-shot out-of-the-box forecasting performance? Such a pretrained time-series foundation model, if possible, would bring significant benefits for downstream forecasting users in terms of no additional training burden and significantly reduced compute requirements. It is not immediately obvious that such a foundation model for time-series forecasting is possible. Unlike in NLP, there is no well defined vocabulary or grammar for time-series. Additionally, such a model would need to support forecasting with varying history lengths (context), prediction lengths (horizon) and time granularities\\n\\n- Comprehensive and up-to-date survey. We offer a comprehensive and up-to-date survey on foundation models for a wide spectrum of time series, encompassing standard time series, spatial time series, and other types (i.e., trajectories and events).\\n- Novel methodology-centric taxonomy. We introduce a novel taxonomy that offers a thorough analysis from a methodological standpoint on TSFMs with the first shot, enabling a full understanding of the mechanism on why and how FMs can achieve admirable performance in time series data.\\n- Future research opportunities. We discuss and highlight future avenues for enhancing time series analysis using foundation models, urging researchers to delve deeper into this area.\\n\\n# 2 BACKGROUND\\n\\n# Foundation Models.\\n\\nThere has been some very recent work on re-using or fine-tuning large language models for time-series forecasting. In particular, [GFQW23] benchmarks pretrained LLMs like GPT-3 and LLaMA-2 for zero-shot forecasting performance. As we show later, our model obtains much superior zero-shot performance at a tiny fraction of these model sizes. [ZNW+23] and [CPC23] show how to fine-tune a GPT-2 [RWC+19] backbone model for time-series forecasting tasks. With the exception of a transfer-learning study (forecasting on a target dataset after having trained on a source dataset), these papers mostly focus on fine-tuning a pretrained model on target datasets, and not on pretraining a single foundation model with good out-of-the box zero-shot performance on a variety of datasets. To the best of our knowledge, the very recent work in TimeGPT-1 [GMC23] is the only other parallel work on a zero-shot foundation model for time-series forecasting\\n\\nQuestion:\\nHow has the use of foundation models impacted the field of time-series forecasting in terms of model accuracy and scalability?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nalso unable to model long-term trends effectively [4, 14, 62]. This is because, at each timestamp, a recurrent model needs to first perform inference for all previous timestamps before proceeding further. Recent developments of the transformer models allow single-shot inference with the complete input series using position encoding [51]. Using transformers allows much faster detection compared to recurrent methods by parallelizing inference on GPUs [19]. However, transformers also provide the benefit of being able to encode large sequences with accuracy and training/inference times nearly agnostic to the sequence length [51]. Thus, we use transformers to grow the temporal context information sent to an anomaly detector without significantly increasing the computational overheads.\\n\\n# Our contributions\\n\\n# 6 Conclusions\\n\\nWe present a transformer based anomaly detection model (TranAD) that can detect and diagnose anomalies for multivariate time-series data. The transformer based encoder-decoder allows quick model training and high detection performance for a variety of datasets considered in this work. TranAD leverages self-conditioning and adversarial training to amplify errors and gain training stability. Moreover, meta-learning allows it to be able to identify data trends even with limited data. Specifically, TranAD achieves an improvement of 17% and 11% for F1 score on complete and limited training data, respectively. It is also able to correctly identify root causes for up to 75% of the detected anomalies, higher than the state-of-the-art models. It is able to achieve this with up to 99% lower training times compared to the baseline methods. This makes TranAD an ideal choice for modern industrial systems where accurate and quick anomaly predictions are required.\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nPreprint\\n\\nRamin Ghorbani1‚àó, Marcel J.T. Reinders1, and David M.J. Tax1\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\nThis work uses various tools, including Transformer neural networks and model-agnostic meta learning, as building blocks. However, each of these different technologies cannot be directly used and need necessary adaptations to create a generalizable model for anomaly detection. Specifically, we propose a transformer-based anomaly detection model (TranAD), that uses self-conditioning and an adversarial training process. Its architecture makes it fast for training and testing while maintaining stability with large input sequences. Simple transformer-based encoder-decoder networks tend to miss anomalies if the deviation is too small, i.e., it is relatively close to normal data. One of our contributions is to show that this can be alleviated by an adversarial training procedure that can amplify reconstruction errors. Further, using self-conditioning for robust multi-modal feature extraction can help gain training stability and allow generalization [32]\\n\\nThis paper presents an adaptation of the Transformer model, chosen for its ability to capture temporal dependencies in sequential data. By integrating the RBF neurons into the Transformer architecture, we develop a model that synergistically utilizes both similarity scores and reconstruction error to compute a distinctive anomaly score.\\n\\nThrough an extensive evaluation, we show that this new REconstruction and Similarity based Transformer for time series Anomaly Detection, RESTAD, outperforms existing baselines across a range of benchmark datasets.\\n\\n# 2 Methodology\\n\\nAssume that the observed time series dataset consists of N sequences with length T. Each sequence in this dataset is denoted by Xi = xi,t t=1 where xi,t represents the observed time point for i-th sequence at time t, having d dimensions, i.e., xi,t ‚àà Rd.\\n\\nOur task is to determine if a given xi,t shows any anomalous behavior or not.\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nQuestion:\\nHow are transformer-based models improving the accuracy of anomaly detection in real-time applications?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Trend and seasonality.\\n\\nWe generated time series following linear and exponential trends: Chronos-T5 (Base) predicts the linear trend accurately but struggles with the exponential trend, as shown in Figure 12b. This may be due to a limited representation of exponential trends in the training data. A potential resolution for generating better forecasts for time series with exponential trends is to perform logarithmic scaling before feeding the time series.\\n\\n16\\n# Forecasting with Chronos Models\\n\\n16\\n# Forecasting with Chronos Models\\n\\nWe also observed that Chronos models tend to underestimate the trend when the context is not sufficiently long. This phenomenon is depicted in Figure 13 where the model forecasts the pattern correctly but underpredicts the trend when a short context is provided. However, with a longer context, the model picks up the correct pattern and trend. In our analysis, we observed that Chronos models recognize seasonal patterns in time series particularly well. We generated purely seasonal time series using sinusoids with different frequencies. As shown in Figure 12c, Chronos-T5 (Base) precisely forecasts both time series. When fundamental patterns such as trend and seasonality are combined, either additively or multiplicatively, Chronos forecasts them accurately. This is demonstrated in Figure 12d on time series generated via addition and multiplication of a linear function with a sinusoid.\\n\\n# Table 2: Short-term time series forecasting results on M4.\\n\\nThe forecasting horizons are in [6, 48] and the three rows provided are weighted averaged from all datasets under different sampling intervals. A lower value indicates better performance. Red: the best, Blue: the second best. More results are in Appendix D.\\n\\n|Methods|TIME-LLM (Ours)|GPT4TS (2023a)|TimesNet (2023)|PatchTST (2023)|N-HiTS (2023b)|N-BEATS (2020)|ETSformer (2022)|LightTS (2022a)|DLinear (2023)|FEDformer (2022)|Stationary (2022)|Autoformer (2021)|Informer (2021)|Reformer (2020)|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|SMAPE|11.983|12.69|12.88|12.059|12.035|12.25|14.718|13.525|13.639|13.16|12.780|12.909|14.086|18.200|\\n|MASE|1.595|1.808|1.836|1.623|1.625|1.698|2.408|2.111|2.095|1.775|1.756|1.771|2.718|4.223|\\n|OWA|0.859|0.94|0.955|0.869|0.869|0.896|1.172|1.051|1.051|0.949|0.930|0.939|1.230|1.775|\\n\\n# Table 19: Standard deviations of our approach and the second-best method (PatchTST) on all time series datasets for long-term forecasting.\\n\\n|Model|Dataset|MSE|MAE|MSE|MAE| |\\n|---|---|---|---|---|---|---|\\n|TIME-LLM|ETTh1|0.408 ¬± 0.011|0.423 ¬± 0.012|PatchTST (2023)|0.413 ¬± 0.001|0.430 ¬± 0.002|\\n|TIME-LLM|ETTh2|0.334 ¬± 0.005|0.383 ¬± 0.009|PatchTST (2023)|0.330 ¬± 0.002|0.379 ¬± 0.007|\\n|TIME-LLM|ETTm1|0.329 ¬± 0.006|0.372 ¬± 0.007|PatchTST (2023)|0.351 ¬± 0.006|0.380 ¬± 0.002|\\n|TIME-LLM|ETTm2|0.251 ¬± 0.002|0.313 ¬± 0.003|PatchTST (2023)|0.255 ¬± 0.003|0.315 ¬± 0.002|\\n|TIME-LLM|Weather|0.225 ¬± 0.009|0.257 ¬± 0.008|PatchTST (2023)|0.225 ¬± 0.001|0.264 ¬± 0.001|\\n|TIME-LLM|Electricity|0.158 ¬± 0.004|0.252 ¬± 0.007|PatchTST (2023)|0.161 ¬± 0.001|0.252 ¬± 0.001|\\n|TIME-LLM|Traffic|0.388 ¬± 0.001|0.264 ¬± 0.006|PatchTST (2023)|0.390 ¬± 0.003|0.263 ¬± 0.003|\\n|TIME-LLM|ILI|1.435 ¬± 0.011|0.801 ¬± 0.008|PatchTST (2023)|1.443 ¬± 0.012|0.797 ¬± 0.002|\\n\\nIn both cases. TIME-LLM exhibits remarkable superiority in forecasting with limited data‚Äîa fact that becomes particularly salient when compared to GPT4TS.\\n# Published as a conference paper at ICLR 2024\\n\\n# Table 18: Efficiency comparison between model reprogramming and parameter-efficient fine-tuning (PEFT) with QLoRA (Dettmers et al., 2023) on ETTh1 dataset in forecasting two different steps ahead.\\n\\n|Length|ETTh1-96|ETTh1-96|ETTh1-96|ETTh1-336|ETTh1-336|ETTh1-336|\\n|---|---|---|\\n|Metric|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|Trainable Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|Llama (8) QLoRA|12.60|14767|0.237|12.69|15982|0.335|\\n|Llama (8) Reprogram|5.62|11370|0.184|5.71|13188|0.203|\\n|Llama (32) QLoRA|50.29|45226|0.697|50.37|49374|0.732|\\n|Llama (32) Reprogram|6.39|32136|0.517|6.48|37988|0.632|\\n\\n# Table 19: Standard deviations of our approach and the second-best method (PatchTST) on all time series datasets for long-term forecasting.\\n\\nQuestion:\\nWhat are the most effective techniques for handling seasonality and trend in time-series forecasting models?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# A PREPRINT\\n\\nIn particular, we design TimesFM, a single foundation model for time-series forecasting that, when applied to a variety of previously-unseen forecasting datasets across different domains, obtains close to state-of-the-art zero-shot accuracy (compared to the best supervised models trained individually for these datasets). Our model can work well across different forecasting history lengths, prediction lengths and time granularities at inference time. The key elements of our foundation model are twofold: 1) a large-scale time-series corpus built using both real-world (mostly time-series data from web search queries1 and Wikipedia page visits2) and synthetic data, which meets the volume and diversity of data needed for training our foundation model, and 2) a decoder style attention architecture with input patching, that can be efficiently pre-trained on this time-series corpus.\\n\\nThere has been some very recent work on re-using or fine-tuning large language models for time-series forecasting. In particular, [GFQW23] benchmarks pretrained LLMs like GPT-3 and LLaMA-2 for zero-shot forecasting performance. As we show later, our model obtains much superior zero-shot performance at a tiny fraction of these model sizes. [ZNW+23] and [CPC23] show how to fine-tune a GPT-2 [RWC+19] backbone model for time-series forecasting tasks. With the exception of a transfer-learning study (forecasting on a target dataset after having trained on a source dataset), these papers mostly focus on fine-tuning a pretrained model on target datasets, and not on pretraining a single foundation model with good out-of-the box zero-shot performance on a variety of datasets. To the best of our knowledge, the very recent work in TimeGPT-1 [GMC23] is the only other parallel work on a zero-shot foundation model for time-series forecasting\\n\\nThis motivates the question: ‚ÄúCan large pretrained models trained on massive amounts of time-series data learn temporal patterns that can be useful for time-series forecasting on previously unseen datasets?‚Äù In particular, can we design a time-series foundation model that obtains good zero-shot out-of-the-box forecasting performance? Such a pretrained time-series foundation model, if possible, would bring significant benefits for downstream forecasting users in terms of no additional training burden and significantly reduced compute requirements. It is not immediately obvious that such a foundation model for time-series forecasting is possible. Unlike in NLP, there is no well defined vocabulary or grammar for time-series. Additionally, such a model would need to support forecasting with varying history lengths (context), prediction lengths (horizon) and time granularities\\n\\nIn this paper, we embark on a novel path and introduce TimeGPT, the first pre-trained foundation model for time series forecasting that can produce accurate predictions across a diverse array of domains and applications without additional training. A general pre-trained model constitutes a\\n\\n‚àóAuthors contributed equally.\\n# Groundbreaking Innovation\\n\\nGroundbreaking innovation that opens the path to a new paradigm for the forecasting practice that is more accessible and accurate, less time-consuming, and drastically reduces computational complexity.\\n\\n# 2 Background\\n\\n# 7 Discussion and Future Research\\n\\nCurrent forecasting practice usually involves a complex pipeline, encompassing multiple steps from data processing to model training and selection. TimeGPT greatly simplifies this process by reducing pipelines to the inference step, substantially reducing complexity and time investment while still achieving state-of-the-art performance. Perhaps most significantly, TimeGPT democratizes the advantages of large transformers models, nowadays restricted to organizations with vast amounts of data, computational resources, and technical expertise. We believe that foundational models are set to profoundly impact the forecasting field and can redefine current practices.\\n\\nQuestion:\\nHow do foundation models like TimeGPT reduce the need for manual feature engineering in time-series forecasting?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# A PREPRINT\\n\\nIn particular, we design TimesFM, a single foundation model for time-series forecasting that, when applied to a variety of previously-unseen forecasting datasets across different domains, obtains close to state-of-the-art zero-shot accuracy (compared to the best supervised models trained individually for these datasets). Our model can work well across different forecasting history lengths, prediction lengths and time granularities at inference time. The key elements of our foundation model are twofold: 1) a large-scale time-series corpus built using both real-world (mostly time-series data from web search queries1 and Wikipedia page visits2) and synthetic data, which meets the volume and diversity of data needed for training our foundation model, and 2) a decoder style attention architecture with input patching, that can be efficiently pre-trained on this time-series corpus.\\n\\nCompared to the latest large language models, our time-series foundation model is much smaller in both parameter size (200M parameters) and pretraining data size (O(100B) timepoints); yet we show that even at such scales, it is possible to pretrain a practical foundation model for forecasting whose zero-shot performance comes close to the accuracy of fully-supervised approaches on a diverse set of time-series data. Our work also suggests that unlike recent work [GFQW23] that recommends Large Language Models such as GPT-3 and LLama-2 as out-of-the-box zero-shot forecasters, foundation models trained from scratch exclusively on time-series data can obtain much better zero-shot performance at a tiny fraction of its costs.\\n\\n# 2 Related Work\\n\\nIn the first group, Lag-Llama and TimeGPT-1 represent pioneering efforts as forecasting foundation models. Both models undergo pre-training on a vast collection of time series data spanning multiple domains. Lag-Llama employs a decoder-only transformer architecture, utilizing lags as covariates, whereas TimeGPT-1 features an encoder-decoder structure with several transformer layers, facilitating efficient zero-shot forecasting. Another noteworthy contribution is TTMs, a recent endeavor in creating a domain-agnostic forecasting model built upon TSMixer, which itself is pre-trained on diverse time series datasets from various.\\n# KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\\n\\n# Foundation Models for Time Series\\n\\n# Standard Time Series\\n\\nOver the past years, foundation models have caused a paradigm shift in machine learning due to their unprecedented capabilities for zero-shot and few-shot generalization. However, despite the success of foundation models in modalities such as natural language processing and computer vision, the development of foundation models for time series forecasting has lagged behind. We present Lag-Llama, a general-purpose foundation model for univariate probabilistic time series forecasting based on a decoder-only transformer architecture that uses lags as covariates. Lag-Llama is pretrained on a large corpus of diverse time series data from several domains, and demonstrates strong zero-shot generalization capabilities compared to a wide range of forecasting models on downstream datasets across domains\\n\\nThere has been some very recent work on re-using or fine-tuning large language models for time-series forecasting. In particular, [GFQW23] benchmarks pretrained LLMs like GPT-3 and LLaMA-2 for zero-shot forecasting performance. As we show later, our model obtains much superior zero-shot performance at a tiny fraction of these model sizes. [ZNW+23] and [CPC23] show how to fine-tune a GPT-2 [RWC+19] backbone model for time-series forecasting tasks. With the exception of a transfer-learning study (forecasting on a target dataset after having trained on a source dataset), these papers mostly focus on fine-tuning a pretrained model on target datasets, and not on pretraining a single foundation model with good out-of-the box zero-shot performance on a variety of datasets. To the best of our knowledge, the very recent work in TimeGPT-1 [GMC23] is the only other parallel work on a zero-shot foundation model for time-series forecasting\\n\\nQuestion:\\nHow do foundation models such as TimeGPT, LagLLama, and TimesFM redefine time series forecasting compared to traditional statistical methods, and what are the implications of their zero-shot or few-shot capabilities in diverse industries?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nOur contributions are significant in two key aspects. First, we show that existing language model architectures are capable of performing forecasting without time-series-specific customizations. This paves the way for accelerated progress by leveraging developments in the area of LLMs and through better data strategies. Second, on a practical level, the strong performance of Chronos models suggests that large (by forecasting standards) pretrained language models can greatly simplify forecasting pipelines without sacrificing accuracy, offering an inference-only alternative to the conventional approach involving training and tuning a model on individual tasks.\\n\\n# Acknowledgements\\n\\nIn this work, we take a step back and ask: what are the fundamental differences between a language model that predicts the next token, and a time series forecasting model that predicts the next values? Despite the apparent distinction ‚Äî tokens from a finite dictionary versus values from an unbounded, usually continuous domain ‚Äî both endeavors fundamentally aim to model the sequential structure of the data to predict future patterns. Shouldn‚Äôt good language models ‚Äújust work‚Äù on time series? This naive question prompts us to challenge the necessity of time-series-specific modifications, and answering it led us to develop Chronos, a language modeling framework minimally adapted for time series forecasting. Chronos tokenizes time series into discrete bins through simple scaling and quantization of real values\\n\\n. Chronos tokenizes time series into discrete bins through simple scaling and quantization of real values. In this way, we can train off-the-shelf language models on this ‚Äúlanguage of time series,‚Äù with no changes to the model architecture (see Figure 1 for a high-level depiction of Chronos). Remarkably, this straightforward approach proves to be effective and efficient, underscoring the potential for language model architectures to address a broad range of time series problems with minimal modifications.\\n\\nThe realization of the above benefits hinges on the effective alignment of the modalities of time series data and natural language. However, this is a challenging task as LLMs operate on discrete tokens, while time series data is inherently continuous. Furthermore, the knowledge and reasoning capabilities to interpret time series patterns are not naturally present within LLMs‚Äô pre-training. Therefore, it remains an open challenge to unlock the knowledge within LLMs in activating their ability for general time series forecasting in a way that is accurate, data-efficient, and task-agnostic.\\n\\n# Proposed Framework\\n\\n# Chronos: A Language Modeling Framework for Time Series\\n\\nIn this section we introduce Chronos, a framework adapting existing language model architectures and training procedures to probabilistic time series forecasting. While both language and time series are sequential in nature, they differ in terms of their representation ‚Äî natural language consists of words from a finite\\n# 3.1 Time Series Tokenization\\n\\nConsider a time series x1:C+H = [x1, . . . , xC+H], where the first C time steps constitute the historical context, and the remaining H represent the forecast horizon. Language models operate on tokens from a finite vocabulary, so using them for time series data requires mapping the observations xi ‚àà R to a finite set of tokens. To this end, we first scale and then quantize observations into a fixed number of bins.\\n\\n# Scaling\\n\\nQuestion:\\nWhat are the architectural innovations and challenges of adapting language models, like those in Chronos and TimeLLM, for time series data, particularly in terms of tokenization, temporal dependencies, and forecast accuracy?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 4 Experiments\\n\\nWe compare TranAD with state-of-the-art models for multivariate time-series anomaly detection, including MERLIN, LSTM-NDT (with autoencoder implementation from openGauss), DAGMM, OmniAnomaly, MSCRED, MAD-GAN, USAD, MTAD-GAT, CAE-M and GDN (with graph embedding implementation from GraphAn). For more details refer Section 2.1. We also tested the Isolation Forest method, but due to its low F1 scores, do not include the corresponding results in our discussion. Other classical methods have been omitted as well.\\n\\nIn this paper, we design a Transformer-based architecture and propose AnomalyBERT, a self-supervised method for time series anomaly detection. Inspired by BERT (Devlin et al., 2018) in the natural language processing (NLP) field, we modify the masked language modeling (MLM) by replacing a random portion of input data and training a model to find the degraded part. This data degradation scheme helps detect varied unnatural sequences in real-world time series, as shown in Figure 1. Furthermore, we apply 1D relative position bias (Raffel et al., 2019) to self-attention modules to insert appropriate temporal information into data. AnomalyBERT outperforms previous detection methods by achieving the highest F1-scores on five real-world benchmark datasets. We demonstrate that our data degradation scheme enables the Transformer-based model to understand the temporal context, and our method has strong capability in detecting real-world anomalies.\\n\\n# 2 RELATED WORKS\\n\\n. Our model converts multivariate data points into temporal representations with relative position bias and yields anomaly scores from these representations. Our method, AnomalyBERT, shows a great capability of detecting anomalies contained in complex time series and surpasses previous state-of-the-art methods on five real-world benchmarks. Our code is available at https://github.com/Jhryu30/AnomalyBERT.\\n\\n# Figure 1\\n\\nExamples of anomaly detection for abnormal time series in SWaT (features in our model). Our method, AnomalyBERT, predicts anomaly scores that quantify abnormalities of data points in time series. We also visualize the original data and the final latent features in our model using t-SNE and show that our method separates abnormal points effectively.\\n\\n|(a) Abnormal component of the original data|(b) 2D-visualization of the original data and latent features in our model.|\\n|---|---|\\n|Predicted Score|Latent feature|\\n|Normal|Abnormal|\\n|Normal|Abnormal|\\n\\n# 1 INTRODUCTION\\n\\nIn many industrial environments, time series data is mainly dealt with to monitor machines, IT devices, spacecrafts, or engines. Anomaly detection is one of the essential tasks for time series analysis, which can find defects in machines and prevent potential harm. Recently, many deep learning-based approaches have been applied to this work. Several studies design recurrent neural networks.\\n\\nThe recently proposed HitAnomaly [19] method uses vanilla transformers as encoder-decoder networks, but is only applicable to natural-language log data and not appropriate for generic continuous time-series data as inputs. In our experiments, we compare TranAD against the state-of-the-art methods MERLIN, LSTM-NDT, DAGMM, OmniAnomaly, MSCRED, MAD-GAN, USAD, MTAD-GAT, CAE-M and GDN. These methods have shown superiority in anomaly detection and diagnosis, but complement one another in terms of performance across different time-series datasets. Out of these, only USAD aims to reduce training times, but does this to a limited extent. Just like reconstruction based prior work [4, 29, 45, 60, 61], we develop a TranAD model that learns broad level trends using training data to find anomalies in test data. We specifically improve anomaly detection and diagnosis performance with also reducing the training times in this work.\\n\\n# 3 Methodology\\n\\n# 3.1 Problem Formulation\\n\\nQuestion:\\nIn what ways do models like AnomalyBERT, TranAD, and RestAD approach anomaly detection in multivariate time series, and how do their self-supervised learning techniques compare in handling unlabeled data for real-time anomaly detection?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting\\n\\n# Authors\\n\\n*Kashif Rasul1 *Arjun Ashok2 3 4 ‚ô¢Andrew Robert Williams3 4 ‚ô¢Hena Ghonia3 4 Rishika Bhagwatkar ‚ô†Roland Riachi3 4 ‚ô†Arian Khorasani3 4 ‚ô†Mohammad Javad Darvishi Bayazi3 ‚ô†George Adamopoulos5 Nadhir Hassen3 4 Marin Bilo≈°1 ‚ñ≥Sahil Garg1 ‚ñ≥Anderson Schneider1 ‚ñ≥Nicolas Chapados2 4 ‚ñ≥Alexandre Drouin2 4 ‚ñ≥Valentina Zantedeschi2 ‚ô£Yuriy Nevmyvaka1 ‚ô£Irina Rish3 4\\n\\n# Abstract\\n\\n# 5.4 Uncertainty quantification\\n\\nProbabilistic forecasting refers to estimating a model‚Äôs uncertainty around the predictions. Correctly assessing a forecasting model‚Äôs calibration enables risk assessment and informed decision-making. Conformal prediction, a non-parametric framework, offers a compelling approach to generating prediction intervals with a pre-specified level of coverage accuracy [Shafer and Vovk, 2008, Stankeviciute et al., 2021]. Unlike traditional methods, conformal prediction does not require strict distributional assumptions, making it more flexible and agnostic to the model or time series domain. During the inference of a new time series, we perform rolling forecasts on the latest available data to estimate the model‚Äôs errors in forecasting the particular target time series.\\n\\n# 6 Experimental Results\\n\\n¬∂This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nas in LLaMA (Touvron et al., 2023). After passing through the causally masked transformer layers, the model predicts the parameters œï of the forecast log prob distribution of the next timestep, where the parameters are output by a parametric distribution head, as described in Sec. 4.3. The negative log-likelihood of the predicted distribution of all predicted timesteps is minimized.\\n\\nIn this paper, we present Lag-Llama‚Äî a foundation model for probabilistic time series forecasting trained on a large collection of open time series data, and evaluated on unseen time series datasets. We investigate the performance of Lag-Llama across several settings where unseen time series.\\n# Lag-Llama\\n\\ndatasets are encountered downstream with different levels of data history being available, and show that Lag-Llama performs comparably or better against state-of-the-art dataset-specific models.\\n\\n# Our contributions:\\n\\n# 3. Probabilistic Time Series Forecasting\\n\\nWe consider a dataset of D ‚â• 1 univariate time series, Dtrain = {xi1:Ti}D sampled at a specific discrete set of time points t ‚àà {1, . . . , Ti} where Ti represents the length of the time series i. Given this dataset, we aim to train a predictive model that can accurately predict the values at the future P ‚â• 1 time points; we refer to these timesteps of our D time series as to the test dataset, denoted\\n# Lag-Llama\\n\\nDtest = {xiT + 1:T + P}Dii i=1.\\n\\n# The univariate probabilistic time series forecasting problem\\n\\ninvolves modelling an unknown joint distribution of the P future values of a one-dimensional sequence given its observed past until timestep t from which prediction should be performed, and covariates:\\n\\npœï(xit+1:t+P | xi1:t, ci1:t+P). (1)\\n\\nQuestion:\\nWhat role does probabilistic forecasting play in models like LagLLama and TimesFM, and how do these models handle uncertainty quantification across long prediction horizons?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n.e., they are fine-tuned and tested on each dataset separately. Furthermore, the aforementioned methods are based on prompting or fine-tuning pretrained LLMs. In contrast, Chronos trains language models from scratch on a large collection of time series, tokenized via scaling and quantization.\\n\\nTimeGPT was benchmarked against a broad spectrum of baseline, statistical, machine learning, and neural forecasting models to provide a comprehensive performance analysis. Baselines and statistical models are individually trained on each time series of the test set, utilizing the historical values preceding the last forecasting window. We opted for a global model approach for machine learning and deep learning methods for each frequency, leveraging all time series in the test set. Some popular models like Prophet [Taylor and Letham, 2018] and ARIMA were excluded from the analysis due to their prohibitive computational requirements and extensive training times.\\n\\n. Our pretrained models significantly outperform existing local models and task-specific deep learning baselines in terms of their in-domain performance. More remarkably, Chronos models obtain excellent results on unseen datasets (zero-shot performance), performing competitively with the best deep-learning baselines trained on these datasets, while showing promising evidence of further improvements through fine-tuning.\\n\\nThe selection of such a diverse training set is critical for developing a robust foundational model. This diversity encompasses the complex realities of non-stationary real-world data, where trends and patterns can shift over time due to a multitude of factors. Training TimeGPT on this rich dataset equips it to handle a wide range of scenarios, enhancing its robustness and generalization capabilities. This effectively enables TimeGPT to forecast unseen time series accurately while eliminating the need for individual model training and optimization.\\n\\n# 5.3 Training TimeGPT\\n\\nThe smaller Chronos-T5 models (Mini and Small) and Chronos-GPT2 also perform better than the majority of baselines, with the exception of PatchTST. Between the two baseline pretrained models studied in this experiment, Moirai-1.0-R clearly outperforms Lag-Llama. Notably, the best Moirai-1.0-R model (Large, 311M) is still outperformed by the smallest Chronos-T5 model (Mini, 20M) even though Moirai-1.0-R models were trained on a significantly larger corpus of time series data. Task-specific deep learning models, trained across multiple time series for a specific task, perform better than local statistical models that fit parameters for each time series. Interestingly, the Seasonal Naive baseline performs competitively.\\n# Model\\n\\nQuestion:\\nHow do models like TimeGPT and Chronos utilize synthetic data and real-world benchmarks in their training, and what trade-offs exist between their generalization abilities on new datasets and performance on specialized, domain-specific tasks?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nThe realization of the above benefits hinges on the effective alignment of the modalities of time series data and natural language. However, this is a challenging task as LLMs operate on discrete tokens, while time series data is inherently continuous. Furthermore, the knowledge and reasoning capabilities to interpret time series patterns are not naturally present within LLMs‚Äô pre-training. Therefore, it remains an open challenge to unlock the knowledge within LLMs in activating their ability for general time series forecasting in a way that is accurate, data-efficient, and task-agnostic.\\n\\n# Proposed Framework\\n\\nThe development of a generalized global model for time series entails numerous challenges, primarily due to the complex task of handling signals derived from a broad set of underlying processes. Characteristics such as frequency, sparsity, trend, seasonality, stationarity, and heteroscedasticity present distinct complications for both local and global models. Therefore, any foundational forecasting model must possess the ability to manage such heterogeneity. Our model, TimeGPT, is engineered to process time series of varied frequencies and characteristics while accommodating different input sizes and forecasting horizons. This adaptability is largely attributable to the underlying transformer-based architecture upon which TimeGPT is built.\\n# 5.2 Training dataset\\n\\nthat demand substantial computational resources and time for inference. Recent concurrent work (Dooley et al., 2023; Das et al., 2023; Rasul et al., 2023; Woo et al., 2024) also explores pretraining transformer-based models with sophisticated time-series-specific designs on a large corpus of real and (or) synthetic time series data.\\n\\nThe selection of such a diverse training set is critical for developing a robust foundational model. This diversity encompasses the complex realities of non-stationary real-world data, where trends and patterns can shift over time due to a multitude of factors. Training TimeGPT on this rich dataset equips it to handle a wide range of scenarios, enhancing its robustness and generalization capabilities. This effectively enables TimeGPT to forecast unseen time series accurately while eliminating the need for individual model training and optimization.\\n\\n# 5.3 Training TimeGPT\\n\\nIn this work, we take a step back and ask: what are the fundamental differences between a language model that predicts the next token, and a time series forecasting model that predicts the next values? Despite the apparent distinction ‚Äî tokens from a finite dictionary versus values from an unbounded, usually continuous domain ‚Äî both endeavors fundamentally aim to model the sequential structure of the data to predict future patterns. Shouldn‚Äôt good language models ‚Äújust work‚Äù on time series? This naive question prompts us to challenge the necessity of time-series-specific modifications, and answering it led us to develop Chronos, a language modeling framework minimally adapted for time series forecasting. Chronos tokenizes time series into discrete bins through simple scaling and quantization of real values\\n\\nQuestion:\\nHow do the different models balance the trade-off between model complexity and interpretability when applied to real-world time series tasks such as anomaly detection and forecasting?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 4 Model Architecture\\n\\nA foundation model for time-series forecasting should be able to adapt to variable context and horizon lengths, while having enough capacity to encode all patterns from a large pretraining datasets. Transformers have been shown to be able to adapt to different context lengths in NLP [RWC+19]. However, there are several time-series specific design choices. The main guiding principles for our architecture are the following:\\n\\n# Patching\\n\\nAccording to our discussions of related previous work, recent studies have begun to explore model versatility through pre-training and architectural innovations. However, further efforts are needed to realize the truly general-purpose forecasting systems that we are advancing in this research.\\n\\n# Cross-modality Adaptation\\n\\n# Other architectures\\n\\nGiven the cost of training foundation models we did not perform much hyper-parameter tuning in our pretraining, while following some well established best practices for training transformers. In a similar vein, it would also be interesting to try out alternatives like the exciting directions of all MLP structures like [CLY+23] or efficient linear state space models like Mamba [GD23] (and references there in).\\n\\n# Interpretability\\n\\nDeep foundation models trained on a huge corpuses of data could be inherently less interpretable compared to statistical methods like ARIMA, ETS [BJ68]. In this regard methods like LOCO, SHAP (see [VW23] and references there in) could be used to some extent to attribute feature importances to different lags in the context supplied to the model. However, this does not solve the problem to a full extent and one of the best things to do would be to open source a version of the model with a proper model card. [MWZ+19].\\n\\n# A.2 Metrics\\n\\n# A PREPRINT\\n\\nIn particular, we design TimesFM, a single foundation model for time-series forecasting that, when applied to a variety of previously-unseen forecasting datasets across different domains, obtains close to state-of-the-art zero-shot accuracy (compared to the best supervised models trained individually for these datasets). Our model can work well across different forecasting history lengths, prediction lengths and time granularities at inference time. The key elements of our foundation model are twofold: 1) a large-scale time-series corpus built using both real-world (mostly time-series data from web search queries1 and Wikipedia page visits2) and synthetic data, which meets the volume and diversity of data needed for training our foundation model, and 2) a decoder style attention architecture with input patching, that can be efficiently pre-trained on this time-series corpus.\\n\\n# 5.1 Architecture\\n\\nAs shown in Figure 4, we first delve into the architecture of TSFMs, including Transformer-based models, non-Transformer-based models and diffusion-based models, focusing on the underlying mechanisms that shape their capabilities, as well as how they could be applied on various time series.\\n\\n# 5.1.1 Transformer-based Models\\n\\nQuestion:\\nWhat are the shared architectural principles across the foundation models, and how do these designs influence their scalability and performance in large-scale forecasting tasks?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 4 Experiments\\n\\nWe compare TranAD with state-of-the-art models for multivariate time-series anomaly detection, including MERLIN, LSTM-NDT (with autoencoder implementation from openGauss), DAGMM, OmniAnomaly, MSCRED, MAD-GAN, USAD, MTAD-GAT, CAE-M and GDN (with graph embedding implementation from GraphAn). For more details refer Section 2.1. We also tested the Isolation Forest method, but due to its low F1 scores, do not include the corresponding results in our discussion. Other classical methods have been omitted as well.\\n\\nThe recently proposed HitAnomaly [19] method uses vanilla transformers as encoder-decoder networks, but is only applicable to natural-language log data and not appropriate for generic continuous time-series data as inputs. In our experiments, we compare TranAD against the state-of-the-art methods MERLIN, LSTM-NDT, DAGMM, OmniAnomaly, MSCRED, MAD-GAN, USAD, MTAD-GAT, CAE-M and GDN. These methods have shown superiority in anomaly detection and diagnosis, but complement one another in terms of performance across different time-series datasets. Out of these, only USAD aims to reduce training times, but does this to a limited extent. Just like reconstruction based prior work [4, 29, 45, 60, 61], we develop a TranAD model that learns broad level trends using training data to find anomalies in test data. We specifically improve anomaly detection and diagnosis performance with also reducing the training times in this work.\\n\\n# 3 Methodology\\n\\n# 3.1 Problem Formulation\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nPreprint\\n\\nRamin Ghorbani1‚àó, Marcel J.T. Reinders1, and David M.J. Tax1\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\n# Evaluation\\n\\nAnomaly scores (Eq. 2) exceeding a threshold Œ¥ are identified as anomalies. Performance is evaluated using the F1-score for threshold-dependent evaluation. Here, we follow [17] by setting Œ¥ to label a predefined proportion of data points as anomalies (0.5% for SMD, 1% for others). For threshold-independent analysis, we use AUC-ROC, AUC-PR, VUS-ROC, and VUS-PR metrics [21]. We exclude the point-adjustment method [22] due to its overestimation [23]. Our model is compared against baselines and state-of-the-arts models: LSTM [9], vanilla Transformer [17], USAD [13], PatchAD [12], AnomalyTrans [17], and DCdetector [10].\\n\\n1 https://github.com/Raminghorbanii/RESTAD\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Table 1: Performance metrics of baselines and RESTAD on test sets.\\n\\nInitialization methods are denoted as (R) for Random and (K) for K-means. For all measures, a higher value indicates better anomaly detection performance.\\n\\nOur task is to determine if a given xi,t shows any anomalous behavior or not.\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Data\\n\\n|Input|Embedding|Encoder Layer 1|Encoder Layer 2|RBF Layer|Encoder Layer 3|Fully Connected Layer|Output Layer|\\n|---|---|---|---|---|---|---|---|\\n|Multi-Head Attention (8x)|Dropout|+|Norm|Feed-Forward|Norm|+| |\\n\\nFigure 2: Overview of the proposed RESTAD model. Here, the RBF layer is added after the second encoder layer.\\n\\n# Model Score\\n\\nRESTAD score(xi,t) = œµr √ó œµs (2)\\n\\nand œµs = ||xi,t PM=1 ||2 represents the reconstruction error, where œµr = 1 ‚àí M1 ‚àí ÀÜi,tzm,t measures dissimilarity. This combination highlights subtle anomalies characterized by both low reconstruction errors and RBF scores, as well as significant anomalies with high reconstruction errors or low RBF scores.\\n\\n# Initialization of RBF Layer Parameters\\n\\nQuestion:\\nHow do anomaly detection models like AnomalyBERT, TranAD, and RestAD compare in terms of their robustness and adaptability to various types of anomalies (contextual, point, collective) across different time series domains?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nIn this work, we take a step back and ask: what are the fundamental differences between a language model that predicts the next token, and a time series forecasting model that predicts the next values? Despite the apparent distinction ‚Äî tokens from a finite dictionary versus values from an unbounded, usually continuous domain ‚Äî both endeavors fundamentally aim to model the sequential structure of the data to predict future patterns. Shouldn‚Äôt good language models ‚Äújust work‚Äù on time series? This naive question prompts us to challenge the necessity of time-series-specific modifications, and answering it led us to develop Chronos, a language modeling framework minimally adapted for time series forecasting. Chronos tokenizes time series into discrete bins through simple scaling and quantization of real values\\n\\n# 5.3 Baselines\\n\\nWe assessed the performance of Chronos models against a variety of time series forecasting baselines. From statistical forecasting literature (Hyndman & Athanasopoulos, 2018), we included Naive, Seasonal Naive, AutoETS, AutoARIMA (Hyndman et al., 2008) and AutoTheta (Assimakopoulos & Nikolopoulos, 2000). Additionally, we compared against several neural forecasting baselines, including WaveNet (Oord et al., 2016), DeepAR (Salinas et al., 2020), N-BEATS (Oreshkin et al., 2020), TFT (Lim et al., 2021), DLinear (Zeng et al., 2023), PatchTST (Nie et al., 2023), N-HiTS (Challu et al., 2023), and GPT4TS (Zhou et al., 2023a). Furthermore, from the recently proposed pretrained time series models, we included the ones with publicly available weights: Lag-Llama (Rasul et al., 2023) and Moirai-1.0-R (Woo et al., 2024).\\n\\n# Context length\\n\\nChronos-T5 (Small, 46M) models with four distinct context lengths. Figure 11b shows how the performance varies with increasing context length. We observe improvements on both in-domain and zero-shot metrics as context length increases, showing that a longer context helps the models to forecast better. However, this analysis may be limited due to our zero-shot evaluation setup, wherein the majority of datasets in the benchmark have low frequencies and time series shorter than 1000 steps. Hence, further evaluation is required to conclusively study the impact of longer context lengths. We posit that high-frequency datasets may benefit from a longer context, which may be necessary to correctly capture the long-term seasonal patterns.\\n\\n# Vocabulary size\\n\\n16\\n# Forecasting with Chronos Models\\n\\nWe also observed that Chronos models tend to underestimate the trend when the context is not sufficiently long. This phenomenon is depicted in Figure 13 where the model forecasts the pattern correctly but underpredicts the trend when a short context is provided. However, with a longer context, the model picks up the correct pattern and trend. In our analysis, we observed that Chronos models recognize seasonal patterns in time series particularly well. We generated purely seasonal time series using sinusoids with different frequencies. As shown in Figure 12c, Chronos-T5 (Base) precisely forecasts both time series. When fundamental patterns such as trend and seasonality are combined, either additively or multiplicatively, Chronos forecasts them accurately. This is demonstrated in Figure 12d on time series generated via addition and multiplication of a linear function with a sinusoid.\\n\\n# Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting\\n\\n# Authors\\n\\n*Kashif Rasul1 *Arjun Ashok2 3 4 ‚ô¢Andrew Robert Williams3 4 ‚ô¢Hena Ghonia3 4 Rishika Bhagwatkar ‚ô†Roland Riachi3 4 ‚ô†Arian Khorasani3 4 ‚ô†Mohammad Javad Darvishi Bayazi3 ‚ô†George Adamopoulos5 Nadhir Hassen3 4 Marin Bilo≈°1 ‚ñ≥Sahil Garg1 ‚ñ≥Anderson Schneider1 ‚ñ≥Nicolas Chapados2 4 ‚ñ≥Alexandre Drouin2 4 ‚ñ≥Valentina Zantedeschi2 ‚ô£Yuriy Nevmyvaka1 ‚ô£Irina Rish3 4\\n\\n# Abstract\\n\\nQuestion:\\nWhat are the main differences in how models like Chronos, TimesFM, and LagLLama handle long-term versus short-term dependencies in time series forecasting, and what impact does this have on their practical applications?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nThe realization of the above benefits hinges on the effective alignment of the modalities of time series data and natural language. However, this is a challenging task as LLMs operate on discrete tokens, while time series data is inherently continuous. Furthermore, the knowledge and reasoning capabilities to interpret time series patterns are not naturally present within LLMs‚Äô pre-training. Therefore, it remains an open challenge to unlock the knowledge within LLMs in activating their ability for general time series forecasting in a way that is accurate, data-efficient, and task-agnostic.\\n\\n# Proposed Framework\\n\\n- Misaligned or poorly defined evaluation settings: Unlike other fields that have benefited from the introduction of ideal testing datasets such as ImageNet for computer vision, the publicly available datasets for time series do not possess the necessary scale and volume for deep learning methods to excel.\\n- Suboptimal models: Given the limited and specific datasets, even well-conceived deep learning architectures might struggle with generalization or require considerable effort to find optimal settings and parameters.\\n\\nFurthermore, the lack of standardized large-scale datasets that cater to the requirements of deep learning methods could also be hindering progress in this area. While other fields have benefited from benchmark datasets and clear evaluation metrics, the time series community still needs to develop such resources to foster innovation and validate new techniques.\\n\\nthat demand substantial computational resources and time for inference. Recent concurrent work (Dooley et al., 2023; Das et al., 2023; Rasul et al., 2023; Woo et al., 2024) also explores pretraining transformer-based models with sophisticated time-series-specific designs on a large corpus of real and (or) synthetic time series data.\\n\\n. The key is to construct and utilize the self-supervision signals by generating informative positive pairs as well as filtering out unsuitable negative pairs when performing augmentation [61]. In addition to the aforementioned two self-supervised strategies, the efficacy of the hybrid variant has also been validated, where the pre-trained model on fewer time series data outperforms the supervised counterpart [43].\\n\\nKexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong Liu, James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, et al. Self-supervised learning for time series analysis: Taxonomy, progress, and prospects. arXiv preprint arXiv:2306.10125, 2023.\\n\\nTianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and Jian Li. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures. arXiv preprint arXiv:2207.01186, 2022a.\\n\\nXiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. Advances in Neural Information Processing Systems, 35:3988‚Äì4003, 2022b.\\n\\nQuestion:\\nWhat challenges and breakthroughs have been encountered in applying self-supervised learning to time series data, and how does this compare with traditional supervised learning approaches in time series tasks?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nFor the development of a useful general-purpose time series forecasting model, the scarcity of publicly available time series datasets, both in quantity and quality, is arguably more critical than the modeling framework. In addition to the comprehensive collection of public datasets we used to train Chronos, a central aspect of our approach is the integration of data augmentation strategies, including TSMixup and KernelSynth. TSMixup randomly samples a set of base time series from different training datasets, and generates new time series based on a convex combination of them; KernelSynth uses Gaussian processes to generate synthetic time series by randomly composing kernel functions. These techniques address the inherent limitations of small training datasets in time series forecasting, enhancing model robustness and generalization.\\n\\nAnother direction to address the problem of limited data involves developing better methods for generating synthetic time series. Our work has made significant strides in this direction by clearly demonstrating the utility of synthetic data generated using Gaussian processes, improving model performance when incorporated into the training data. Even models trained solely on synthetic data exhibit reasonable forecasting performance. Future research could delve into the failure modes of these models, proposing enhancements to bridge the gap between real and synthetic data.\\n\\n# 7 Conclusion\\n\\nThe realization of the above benefits hinges on the effective alignment of the modalities of time series data and natural language. However, this is a challenging task as LLMs operate on discrete tokens, while time series data is inherently continuous. Furthermore, the knowledge and reasoning capabilities to interpret time series patterns are not naturally present within LLMs‚Äô pre-training. Therefore, it remains an open challenge to unlock the knowledge within LLMs in activating their ability for general time series forecasting in a way that is accurate, data-efficient, and task-agnostic.\\n\\n# Proposed Framework\\n\\nIn this work, we take a step back and ask: what are the fundamental differences between a language model that predicts the next token, and a time series forecasting model that predicts the next values? Despite the apparent distinction ‚Äî tokens from a finite dictionary versus values from an unbounded, usually continuous domain ‚Äî both endeavors fundamentally aim to model the sequential structure of the data to predict future patterns. Shouldn‚Äôt good language models ‚Äújust work‚Äù on time series? This naive question prompts us to challenge the necessity of time-series-specific modifications, and answering it led us to develop Chronos, a language modeling framework minimally adapted for time series forecasting. Chronos tokenizes time series into discrete bins through simple scaling and quantization of real values\\n\\nThe development of a generalized global model for time series entails numerous challenges, primarily due to the complex task of handling signals derived from a broad set of underlying processes. Characteristics such as frequency, sparsity, trend, seasonality, stationarity, and heteroscedasticity present distinct complications for both local and global models. Therefore, any foundational forecasting model must possess the ability to manage such heterogeneity. Our model, TimeGPT, is engineered to process time series of varied frequencies and characteristics while accommodating different input sizes and forecasting horizons. This adaptability is largely attributable to the underlying transformer-based architecture upon which TimeGPT is built.\\n# 5.2 Training dataset\\n\\nQuestion:\\nHow do state-of-the-art models for time series forecasting address the challenge of data sparsity and missing values in real-world datasets, and what techniques are most effective in mitigating the impact of such issues?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Foundation Models for Time Series Analysis: A Tutorial and Survey\\n\\n# Authors\\n\\nYuxuan Liang, The Hong Kong University of Science and Technology (Guangzhou), yuxliang@outlook.com\\n\\nHaomin Wen, Beijing Jiao Tong University & HKUST(GZ), wenhaomin@bjtu.edu.cn\\n\\nYuqi Nie, Princeton University, ynie@princeton.edu\\n\\nYushan Jiang, University of Connecticut, yushan.jiang@uconn.edu\\n\\nMing Jin, Monash University, ming.jin@monash.edu\\n\\nDongjin Song, University of Connecticut, dongjin.song@uconn.edu\\n\\nShirui Pan, Griffith University, s.pan@griffith.edu.au\\n\\nQingsong Wen, Squirrel AI, USA, qingsongedu@gmail.com\\n\\n# ABSTRACT\\n\\n- Comprehensive and up-to-date survey. We offer a comprehensive and up-to-date survey on foundation models for a wide spectrum of time series, encompassing standard time series, spatial time series, and other types (i.e., trajectories and events).\\n- Novel methodology-centric taxonomy. We introduce a novel taxonomy that offers a thorough analysis from a methodological standpoint on TSFMs with the first shot, enabling a full understanding of the mechanism on why and how FMs can achieve admirable performance in time series data.\\n- Future research opportunities. We discuss and highlight future avenues for enhancing time series analysis using foundation models, urging researchers to delve deeper into this area.\\n\\n# 2 BACKGROUND\\n\\n# Foundation Models.\\n\\n# 3. Taxonomy\\n\\nThe proposed taxonomy is illustrated in Figure 3, and the related works can be found in Table 2. The proposed taxonomy unfolds a structured and comprehensive classification to enhance the understanding of foundation models on time series analysis. It is organized into four hierarchical levels, starting with the data category, followed by the model architecture, pre-training techniques, and finally, the application domain. Unlike previous taxonomies, ours distinguishes itself by delving deeper into the foundation models from the methodology perspective, with a keen focus on their architectural designs, pre-training, and adaptation techniques. This method-centric view is pivotal for researchers, providing valuable insights into the mechanisms of why and how foundation models show great potential for time series analysis.\\n\\n# 4. Data Perspective\\n\\n# 6 Empirical Results\\n\\nWe evaluate our model in zero-shot settings on three groups of well known public datasets against the best performing baselines for each group. These datasets have been intentionally held out from our pretraining data. We show that a single pretrained model can come close or surpass the performance of baselines models on the benchmarks even when the baselines are specially trained or tuned for each specific task. Subsequently, we perform ablation studies that justify different choices made in our architecture.\\n\\n# 6.1 Zero-shot Evaluation\\n\\nTo benchmark our model‚Äôs performance, we choose three groups of commonly used forecasting datasets that cover various domains, sizes, granularities, and horizon lengths: Darts [HLP+22], Monash [GBW+21] and Informer datasets [ZZP+21], to test the generalization power of our foundation model against other baselines.\\n# A decoder-only foundation model for time-series forecasting\\n\\n# A PREPRINT\\n\\n# A.5 Additional Empirical Results\\n\\nIn this section, we provide more detailed tables for our zero-shot datasets and experiments described in Section 6.1. The AM based aggregated metrics are presented in Figure 4.\\n# A decoder-only foundation model for time-series forecasting\\n\\n# A PREPRINT\\n\\n# Figure 4\\n\\nQuestion:\\nWhat are the main solutions presented in the paper Foundation Models for Time Series Data? What comparisons and tests were carried out?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model_answers = []\n",
    "for question in questions_list:\n",
    "    answer = process_question(question)\n",
    "    model_answers.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "total_time_global = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvataggio risposte in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {\n",
    "    \"questions\": model_answers\n",
    "}\n",
    "\n",
    "with open('Naive_responses.json', 'w') as outfile:\n",
    "    json.dump(output_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione delle domande locali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def extract_questions_from_json(file_path):\n",
    "    # extraction from single json file\n",
    "    questions = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        if 'examples' in data:\n",
    "            for example in data['examples']:\n",
    "                questions.append(example['question'])\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_questions_from_directory(directory_path):\n",
    "\n",
    "    all_questions = []\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):  \n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            questions = extract_questions_from_json(file_path)\n",
    "            all_questions.extend(questions) \n",
    "    \n",
    "    return all_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../DatasetCreation/Local'\n",
    "all_questions = extract_all_questions_from_directory(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero totale di domande estratte: 45\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numero totale di domande estratte: {len(all_questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risposte alle domande locali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question):\n",
    "    # Pass the question to your QA chain or model\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nChronos represents one of the first endeavours in practical pretrained time series forecasting models, with remarkable zero-shot performance on a comprehensive collection of test datasets. This work opens up various research avenues, some of which we discuss below.\\n\\n# 6.1 Beyond Zero-shot Univariate Forecasting\\n\\n# Fine tuning\\n\\nMotivated by the remarkable zero-shot performance of Chronos models, we conducted a preliminary investigation into fine-tuning Chronos models individually on datasets from Benchmark II.\\n\\n|Chronos-T5 (Small)|Zero Shot|Fine Tuned|\\n|---|---|---|\\n|WQL|0.672|0.608|\\n|MASE|0.856|0.782|\\n\\n. An example of this behaviour is given in Figure 16b. Improving the tokenization to overcome these edge cases is subject for future work, but the results from Section 5.5 suggest that the Chronos models performs well on real-world data despite the limitations.\\n\\nWe selected T5 (Raffel et al., 2020) as the main architecture for Chronos in our experiments, since it is available in a variety of sizes, ranging from 16M (Tiny) to 11B (XXL) parameters (Tay et al., 2021). We also conducted experiments with the decoder-only GPT-2 model to demonstrate the applicability of the Chronos framework to decoder-only models. In the following, we discuss the training configurations used for our main results (Section 5.5) and explore alternatives for some of the hyperparameters in Section 5.6.\\n\\nOur comprehensive evaluation across 42 datasets establishes Chronos as a benchmark for both in-domain and zero-shot forecasting, surpassing both traditional models and task-specific deep learning approaches. Notably, Chronos achieves impressive zero-shot forecasting performance out of the box, without necessitating task-specific adjustments. Its accuracy, coupled with its relatively modest model size, positions it as a preferable alternative to larger, more computationally demanding models for zero-shot forecasting applications.\\n# cations. By its very nature as a language model operating over a fixed vocabulary, Chronos can seamlessly integrate with future advancements in LLMs, making it an ideal candidate for further development as a generalist time series model.\\n\\nQuestion:\\nAccording to the paper \\'Chronos\\', what is the primary goal of the proposed framework?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Chronos: A Language Modeling Framework for Time Series\\n\\nIn this section we introduce Chronos, a framework adapting existing language model architectures and training procedures to probabilistic time series forecasting. While both language and time series are sequential in nature, they differ in terms of their representation ‚Äî natural language consists of words from a finite\\n# 3.1 Time Series Tokenization\\n\\nConsider a time series x1:C+H = [x1, . . . , xC+H], where the first C time steps constitute the historical context, and the remaining H represent the forecast horizon. Language models operate on tokens from a finite vocabulary, so using them for time series data requires mapping the observations xi ‚àà R to a finite set of tokens. To this end, we first scale and then quantize observations into a fixed number of bins.\\n\\n# Scaling\\n\\n. Chronos tokenizes time series into discrete bins through simple scaling and quantization of real values. In this way, we can train off-the-shelf language models on this ‚Äúlanguage of time series,‚Äù with no changes to the model architecture (see Figure 1 for a high-level depiction of Chronos). Remarkably, this straightforward approach proves to be effective and efficient, underscoring the potential for language model architectures to address a broad range of time series problems with minimal modifications.\\n\\n. An example of this behaviour is given in Figure 16b. Improving the tokenization to overcome these edge cases is subject for future work, but the results from Section 5.5 suggest that the Chronos models performs well on real-world data despite the limitations.\\n\\nIn this section, we analyze forecasts generated by Chronos models qualitatively, and we also highlight some limitations of our tokenization technique. We primarily focus on synthetically generated time series for a controlled analysis of different types of time series patterns. For example forecasts from real datasets, see Figures 22 to 24 in Appendix E.\\n\\n# I.I.D. Noise.\\n\\nWe generated time series comprised purely of Gaussian observations, N (0, 1) and N (100, 10), and used Chronos-T5 (Base) to forecast these. Figure 12a shows that Chronos generates plausible forecasts for such time series and the predicted 80% interval coincides with the ground truth 80% interval shown by the dashed blue lines.\\n\\n# Trend and seasonality.\\n\\n# Chronos: Learning the Language of Time Series\\n\\n# Abdul Fatir Ansari1‚àó Lorenzo Stella1‚àó Caner Turkmen1, Xiyuan Zhang3‚Ä† Pedro Mercado4‚Ä†\\n\\n# Huibin Shen1, Oleksandr Shchur1, Syama Sundar Rangapuram1, Sebastian Pineda Arango1, Shubham Kapoor1, Jasper Zschiegner, Danielle C. Maddix1, Hao Wang1,5‚Ä†\\n\\n# honey2,6‚Ä† Kari Torkkola2, Andrew Gordon Wilson2,7‚Ä† Michael Bohlke-Schneider1, Yuyang‚Ä†, Michael W. Ma-Wang1\\n\\n# 1AWS AI Labs, 2Amazon Supply Chain Optimization Technologies, 3UC San Diego, {ansarnd, stellalo}@amazon.com 4University of Freiburg, 5Rutgers University, 6UC Berkeley, 7New York University\\n\\n# Abstract\\n\\nQuestion:\\nHow does the Chronos framework tokenize time series values, as described in the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nIn this section, we analyze forecasts generated by Chronos models qualitatively, and we also highlight some limitations of our tokenization technique. We primarily focus on synthetically generated time series for a controlled analysis of different types of time series patterns. For example forecasts from real datasets, see Figures 22 to 24 in Appendix E.\\n\\n# I.I.D. Noise.\\n\\nWe generated time series comprised purely of Gaussian observations, N (0, 1) and N (100, 10), and used Chronos-T5 (Base) to forecast these. Figure 12a shows that Chronos generates plausible forecasts for such time series and the predicted 80% interval coincides with the ground truth 80% interval shown by the dashed blue lines.\\n\\n# Trend and seasonality.\\n\\n# Algorithm 2 KernelSynth: Synthetic Data Generation using Gaussian Processes\\n\\nInput: Kernel bank K, maximum kernels per time series J = 5, and length of the time series lsyn = 1024.\\n\\nOutput: A synthetic time series x1:lsyn.\\n\\n1. j ‚àº U{1, J}\\n2. {Œ∫1(t, t‚Ä≤‚Ä≤), . . . , Œ∫j(t, t‚Ä≤)} ‚àº Ki.i.d\\n3. Œ∫‚àó(t, t) ‚Üê Œ∫1(t, t‚Ä≤)\\n4. for i ‚Üê 2, j do\\n5. ‚ãÜ ‚àº {+, √ó}\\n6. Œ∫‚àó(t, t) ‚Üê Œ∫‚àó(t, t) ‚ãÜ Œ∫(t, t)‚Ä≤\\n7. end for\\n8. x1:lsyn ‚àº GP(0, Œ∫‚àó(t, t))‚Ä≤\\n9. return x1:lsyn\\n\\n# B Datasets\\n\\nThe complete list of datasets used for our empirical evaluation is provided in Table 2. The table is divided into three sections, representing how the datasets were used for Chronos models: in total, 55 datasets were used for experiments, 13 of which for pretraining only, 15 for in-domain evaluation, and 27 for zero-shot evaluation (see also Section 5). In the following, we provide a brief description of each dataset, organized by its domain.\\n\\n# B.1 Energy\\n\\nChronos represents one of the first endeavours in practical pretrained time series forecasting models, with remarkable zero-shot performance on a comprehensive collection of test datasets. This work opens up various research avenues, some of which we discuss below.\\n\\n# 6.1 Beyond Zero-shot Univariate Forecasting\\n\\n(c)\\n\\n(d)\\n\\nFigure 12: Forecasts generated by Chronos-T5 (Base) on synthetically generated patterns. (a) Noise: Chronos generates reasonable forecasts for Gaussian noise with the 80% prediction interval matching the interval of the underlying distribution (shown by the horizontal dashed blue line). (b) Trend: Chronos forecasts a linear trend (top) correctly but struggles with an exponential trend (bottom). (c) Seasonality: Chronos accurately models seasonal patterns of varying degrees of complexity (single seasonality at the top and three seasonalities at the bottom). (d) Combined Patterns: Chronos forecasts time series generated by the additive (top) or multiplicative (bottom) combination of trend and seasonal patterns accurately.\\n\\n# Flexible predictive distributions.\\n\\nUsing a categorical distribution to encode predictions gives Chronos flexibility in producing predictive distributions of different shapes. This is shown in Figure 15, illustrating kernel density estimate (KDE) plots of token IDs sampled from a Chronos model, for the first five time steps in the forecast horizon, across three datasets. Despite the fact that cross-entropy is not distance-aware, Chronos outputs predictive distributions over a contiguous set of tokens, and with different shapes, including multi-modal ones.\\n\\n# Overflow and loss of precision.\\n\\nQuestion:\\nWhat is the significance of the synthetic dataset generated via Gaussian processes in the Chronos framework, as mentioned in the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n.e., they are fine-tuned and tested on each dataset separately. Furthermore, the aforementioned methods are based on prompting or fine-tuning pretrained LLMs. In contrast, Chronos trains language models from scratch on a large collection of time series, tokenized via scaling and quantization.\\n\\nOur contributions are significant in two key aspects. First, we show that existing language model architectures are capable of performing forecasting without time-series-specific customizations. This paves the way for accelerated progress by leveraging developments in the area of LLMs and through better data strategies. Second, on a practical level, the strong performance of Chronos models suggests that large (by forecasting standards) pretrained language models can greatly simplify forecasting pipelines without sacrificing accuracy, offering an inference-only alternative to the conventional approach involving training and tuning a model on individual tasks.\\n\\n# Acknowledgements\\n\\nChronos represents one of the first endeavours in practical pretrained time series forecasting models, with remarkable zero-shot performance on a comprehensive collection of test datasets. This work opens up various research avenues, some of which we discuss below.\\n\\n# 6.1 Beyond Zero-shot Univariate Forecasting\\n\\nOur comprehensive evaluation across 42 datasets establishes Chronos as a benchmark for both in-domain and zero-shot forecasting, surpassing both traditional models and task-specific deep learning approaches. Notably, Chronos achieves impressive zero-shot forecasting performance out of the box, without necessitating task-specific adjustments. Its accuracy, coupled with its relatively modest model size, positions it as a preferable alternative to larger, more computationally demanding models for zero-shot forecasting applications.\\n# cations. By its very nature as a language model operating over a fixed vocabulary, Chronos can seamlessly integrate with future advancements in LLMs, making it an ideal candidate for further development as a generalist time series model.\\n\\nOn Benchmark II (i.e., zero-shot datasets for Chronos models), we also evaluated against two zero-shot methods: ForecastPFN (Dooley et al., 2023) which is a transformer model pretrained only on synthetic time series data and LLMTime (Gruver et al., 2023) which uses LLMs for zero-shot forecasting.\\n\\nWe categorize Chronos models and the baselines into three groups: local models that estimate parameters for each time series individually; task-specific models trained or fine-tuned for each task separately; and pretrained models which do not perform task-specific training, instead using a single model across all tasks. Further details on the implementation and training of these baselines can be found in Appendix C.\\n\\n# 5.4 Evaluation Metrics\\n\\nWhenever possible, we evaluated models both in terms of their probabilistic and point forecast performance. We used the weighted quantile loss (WQL) to assess the quality of the probabilistic forecasts: the WQL is\\n\\nQuestion:\\nHow does the Chronos framework differ from other LLM-based forecasting models, such as PromptCast and LLMTime, as discussed in the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 5.1 Datasets\\n\\nTo train and evaluate Chronos models, we collected a wide variety of publicly available datasets spanning various application domains including energy, transport, healthcare, retail, web, weather, finance, and with sampling frequencies ranging from 5 minutes up to yearly. The complete list of datasets, together with their respective sources and additional details, is given in Appendix B. In total, our dataset collection comprises 55 datasets from multiple sources, including the Monash Time Series Forecasting Repository (Godahewa et al., 2021), the M-competitions (Makridakis et al., 1979; Makridakis & Hibon, 2000; Makridakis et al., 2020; 2022), and public domain datasets from Kaggle.\\n\\n. Overall, we used 28 datasets for training Chronos models, consisting of about 890K univariate time series with approximately 84B observations (tokens) in total. For both in-domain (I) and zero-shot (II) benchmark datasets, we used the last H ‚àà N+ observations of each time series as a held-out test set: all models are judged by the accuracy of their forecast on such held-out set, which no model had access to for training purposes. The prediction\\n\\nWe categorize this collection into three subsets, based on how we use them for training and evaluating Chronos models: (a) datasets exclusively used for training (13 datasets); (b) Benchmark I datasets, employed for both training and evaluation, representing an in-domain evaluation (15 datasets); and (c) Benchmark II datasets, used solely for evaluation, constituting a zero-shot evaluation (27 datasets). In categorizing the datasets in this way, we tried to find a good balance between keeping as many datasets as possible for the zero-shot evaluation of Chronos models, among the ones most commonly used in the literature, while still having enough variety of domains and sampling frequencies in the training data. Overall, we used 28 datasets for training Chronos models, consisting of about 890K univariate time series with approximately 84B observations (tokens) in total\\n\\nOur code and model checkpoints are available at https://github.com/amazon-science/chronos-forecasting.\\n\\nSome models (GPT4TS and ForecastPFN) only generate point forecasts and we only evaluate those.\\n# 5.5 Main Results\\n\\nIn this section, we present our main results on 42 datasets, which comprise Benchmark I (15 datasets) and Benchmark II (27 datasets). Chronos models surpass classical statistical baselines, task-specific deep learning models, and other pretrained models on the in-domain datasets (Benchmark I; see Section 5.5.1). On the zero-shot datasets (Benchmark II; Section 5.5.2), Chronos models comfortably outperform statistical baselines and other pretrained models, while performing on par with the best deep learning models trained on these tasks. With an inexpensive fine-tuning regimen, our Chronos-T5 (Small) model achieves the top spot on Benchmark II, significantly outperforming all baselines.\\n\\n# 5.5.1 Benchmark I: In-domain Results\\n\\n# Fine tuning\\n\\nMotivated by the remarkable zero-shot performance of Chronos models, we conducted a preliminary investigation into fine-tuning Chronos models individually on datasets from Benchmark II.\\n\\n|Chronos-T5 (Small)|Zero Shot|Fine Tuned|\\n|---|---|---|\\n|WQL|0.672|0.608|\\n|MASE|0.856|0.782|\\n\\nQuestion:\\nWhat is the result of the comprehensive evaluation of Chronos models on 42 datasets, as reported in the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 5 TimeGPT\\n\\n# 5.1 Architecture\\n\\nTimeGPT is a Transformer-based time series model with self-attention mechanisms based on [Vaswani et al., 2017]. TimeGPT takes a window of historical values to produce the forecast, adding local positional encoding to enrich the input. The architecture consists of an encoder-decoder structure with multiple layers, each with residual connections and layer normalization. Finally, a linear layer maps the decoder‚Äôs output to the forecasting window dimension. The general intuition is that attention-based mechanisms are able to capture the diversity of past events and correctly extrapolate potential future distributions.\\n\\n# 5.3 Training TimeGPT\\n\\nTimeGPT underwent a multi-day training period on a cluster of NVIDIA A10G GPUs. During this process, we carried out extensive hyperparameter exploration to optimize learning rates, batch sizes, and other related parameters. We observed a pattern in alignment with findings from [Brown et al., 2020], where a larger batch size and a smaller learning rate proved beneficial. Implemented in PyTorch, TimeGPT was trained using the Adam with a learning rate decay strategy that reduced the rate to 12% of its initial value.\\n\\n# 5.4 Uncertainty quantification\\n\\n# 6.3 Time Comparison\\n\\nFor zero-shot inference, our internal tests recorded an average GPU inference speed of 0.6 milliseconds per series for TimeGPT, which nearly mirrors that of the simple Seasonal Naive. As points of comparison, we consider parallel computing-optimized statistical methods, which, when complemented with Numba compiling, averaged a speed of 600 milliseconds per series for training and inference. On the other hand, global models such as LGBM, LSTM, and NHITS demonstrated a more prolonged average of 57 milliseconds per series, considering both training and inference. Due to its zero-shot capabilities, TimeGPT outperforms traditional statistical methods and global models with total speed by orders of magnitude.\\n\\n# 7 Discussion and Future Research\\n\\nTimeGPT was benchmarked against a broad spectrum of baseline, statistical, machine learning, and neural forecasting models to provide a comprehensive performance analysis. Baselines and statistical models are individually trained on each time series of the test set, utilizing the historical values preceding the last forecasting window. We opted for a global model approach for machine learning and deep learning methods for each frequency, leveraging all time series in the test set. Some popular models like Prophet [Taylor and Letham, 2018] and ARIMA were excluded from the analysis due to their prohibitive computational requirements and extensive training times.\\n\\n# 7 Discussion and Future Research\\n\\nCurrent forecasting practice usually involves a complex pipeline, encompassing multiple steps from data processing to model training and selection. TimeGPT greatly simplifies this process by reducing pipelines to the inference step, substantially reducing complexity and time investment while still achieving state-of-the-art performance. Perhaps most significantly, TimeGPT democratizes the advantages of large transformers models, nowadays restricted to organizations with vast amounts of data, computational resources, and technical expertise. We believe that foundational models are set to profoundly impact the forecasting field and can redefine current practices.\\n\\nQuestion:\\nAccording to the abstract of the TimeGPT paper, what is the main contribution of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 5 TimeGPT\\n\\n# 5.1 Architecture\\n\\nTimeGPT is a Transformer-based time series model with self-attention mechanisms based on [Vaswani et al., 2017]. TimeGPT takes a window of historical values to produce the forecast, adding local positional encoding to enrich the input. The architecture consists of an encoder-decoder structure with multiple layers, each with residual connections and layer normalization. Finally, a linear layer maps the decoder‚Äôs output to the forecasting window dimension. The general intuition is that attention-based mechanisms are able to capture the diversity of past events and correctly extrapolate potential future distributions.\\n\\n# 5.3 Training TimeGPT\\n\\nTimeGPT underwent a multi-day training period on a cluster of NVIDIA A10G GPUs. During this process, we carried out extensive hyperparameter exploration to optimize learning rates, batch sizes, and other related parameters. We observed a pattern in alignment with findings from [Brown et al., 2020], where a larger batch size and a smaller learning rate proved beneficial. Implemented in PyTorch, TimeGPT was trained using the Adam with a learning rate decay strategy that reduced the rate to 12% of its initial value.\\n\\n# 5.4 Uncertainty quantification\\n\\nWe selected T5 (Raffel et al., 2020) as the main architecture for Chronos in our experiments, since it is available in a variety of sizes, ranging from 16M (Tiny) to 11B (XXL) parameters (Tay et al., 2021). We also conducted experiments with the decoder-only GPT-2 model to demonstrate the applicability of the Chronos framework to decoder-only models. In the following, we discuss the training configurations used for our main results (Section 5.5) and explore alternatives for some of the hyperparameters in Section 5.6.\\n\\n# A PREPRINT\\n\\n# Figure 1\\n\\nWe provide an illustration of the TimesFM model architecture during training, where we show a input time-series of a specific length that can be broken down into input patches. Each patch along is processed into a vector by a residual block (as defined in the model definition) to the model dimension of the transformer layers. The vector is then added to positional encodings and fed into nl stacked transformer layers. SA refers to self-attention (note that we use multi-head causal attention) and FFN is the fully connected layer in the transformer. The output tokens are then mapped through a residual block to an output of size output_patch_len, which is the forecast for the time window following the last input patch seen by the model so far.\\n\\n# Network\\n\\n# 6.3 Time Comparison\\n\\nFor zero-shot inference, our internal tests recorded an average GPU inference speed of 0.6 milliseconds per series for TimeGPT, which nearly mirrors that of the simple Seasonal Naive. As points of comparison, we consider parallel computing-optimized statistical methods, which, when complemented with Numba compiling, averaged a speed of 600 milliseconds per series for training and inference. On the other hand, global models such as LGBM, LSTM, and NHITS demonstrated a more prolonged average of 57 milliseconds per series, considering both training and inference. Due to its zero-shot capabilities, TimeGPT outperforms traditional statistical methods and global models with total speed by orders of magnitude.\\n\\n# 7 Discussion and Future Research\\n\\nQuestion:\\nWhat is the architecture of TimeGPT, as described in Section 5.1 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 5.3 Training TimeGPT\\n\\nTimeGPT underwent a multi-day training period on a cluster of NVIDIA A10G GPUs. During this process, we carried out extensive hyperparameter exploration to optimize learning rates, batch sizes, and other related parameters. We observed a pattern in alignment with findings from [Brown et al., 2020], where a larger batch size and a smaller learning rate proved beneficial. Implemented in PyTorch, TimeGPT was trained using the Adam with a learning rate decay strategy that reduced the rate to 12% of its initial value.\\n\\n# 5.4 Uncertainty quantification\\n\\nTimeGPT was trained on, to our knowledge, the largest collection of publicly available time series, collectively encompassing over 100 billion data points. This training set incorporates time series from a broad array of domains, including finance, economics, demographics, healthcare, weather, IoT sensor data, energy, web traffic, sales, transport, and banking. Due to this diverse set of domains, the training dataset contains time series with a wide range of characteristics.\\n\\nWe trained T5 models of 4 sizes, namely, Mini (20M), Small (46M), Base (200M) and Large (710M), and the GPT-2 base model (90M), on 10M TSMixup augmentations generated from the 28 training datasets, with K = 3 in Algorithm 1, and 1M synthetic time series generated using Gaussian processes. Note that with this setup, original time series are adequately represented since they are included in the TSMixup augmentations with probability 1/3. We sampled time series from the augmentations and synthetic data in the ratio 9:1 during training. Each model is trained with an effective batch size of 256 sequences, using distributed data parallelism and gradient accumulation, whenever necessary. These sequences are constructed by slicing random windows from the time series, and then scaling and quantizing them into equal-sized bins within the interval [l= ‚àí 15, r=15], as described in Section 3.1\\n\\n.1. The context length of the sequences was set to 512, the default for T5 models, and the prediction length is set to 64, a value greater than the prediction lengths of all tasks we consider in our evaluation.\\n\\n# Table 2: MAE of different methods on ETT datasets.\\n\\nAll methods use 10% of the original training set for training or finetuning. The baseline numbers are from Table 13 in [ZZP+21].\\n\\nQuestion:\\nWhat is the size of the training dataset used for TimeGPT, as mentioned in Section 5.2 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nIt must be noted that the validity of a forecasting model can only be assessed relative to its performance against competing alternatives. Although accuracy is commonly seen as the only relevant metric, computational cost and implementation complexity are key factors for practical applications. In this regard, it is noteworthy that the reported results of TimeGPT are the result of a simple and extremely fast invocation of the prediction method of a pre-trained model. In comparison, other models require a complete pipeline for training and then predicting.\\n\\n# 6.1.1 Comparison with recent Foundation Models\\n\\nWork in progress.\\n\\n# 6.2 Fine Tuning\\n\\nOur selected evaluation metrics include the relative Mean Absolute Error (rMAE) and the relative Root Mean Square Error (rRMSE), both normalized against the performance of the Seasonal Naive model. This choice is justified by the additional insights offered by these relative errors, as they show performance gains in relation to a known baseline, improving the interpretability of our results. The relative error metrics bring the additional benefit of scale independence, enabling comparisons.\\n\\n4Future work would profit from expanding and varying this testing set.\\n# Table 1: Main performance results for TimeGPT with zero-shot inference and benchmark models\\n\\nMeasured with rMAE and rRMSE, lower scores are better. The best model for each frequency and metric is highlighted in bold, the second best underlined, and the third best underlined with a dotted line.\\n\\nTimeGPT was benchmarked against a broad spectrum of baseline, statistical, machine learning, and neural forecasting models to provide a comprehensive performance analysis. Baselines and statistical models are individually trained on each time series of the test set, utilizing the historical values preceding the last forecasting window. We opted for a global model approach for machine learning and deep learning methods for each frequency, leveraging all time series in the test set. Some popular models like Prophet [Taylor and Letham, 2018] and ARIMA were excluded from the analysis due to their prohibitive computational requirements and extensive training times.\\n\\nThe evaluation is performed in the last forecasting window of each time series, varying in length by the sampling frequency. TimeGPT uses the previous historical values as inputs, as shown in Figure 3, without re-training its weights (zero-shot). We specify a different forecasting horizon based on the frequency to represent common practical applications: 12 for monthly, 1 for weekly, 7 for daily, and 24 for hourly data. 4\\n\\n# 5.3 Training TimeGPT\\n\\nTimeGPT underwent a multi-day training period on a cluster of NVIDIA A10G GPUs. During this process, we carried out extensive hyperparameter exploration to optimize learning rates, batch sizes, and other related parameters. We observed a pattern in alignment with findings from [Brown et al., 2020], where a larger batch size and a smaller learning rate proved beneficial. Implemented in PyTorch, TimeGPT was trained using the Adam with a learning rate decay strategy that reduced the rate to 12% of its initial value.\\n\\n# 5.4 Uncertainty quantification\\n\\nQuestion:\\nWhat is the evaluation metric used to compare the performance of TimeGPT with other models, as described in Section 6 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 6.3 Time Comparison\\n\\nFor zero-shot inference, our internal tests recorded an average GPU inference speed of 0.6 milliseconds per series for TimeGPT, which nearly mirrors that of the simple Seasonal Naive. As points of comparison, we consider parallel computing-optimized statistical methods, which, when complemented with Numba compiling, averaged a speed of 600 milliseconds per series for training and inference. On the other hand, global models such as LGBM, LSTM, and NHITS demonstrated a more prolonged average of 57 milliseconds per series, considering both training and inference. Due to its zero-shot capabilities, TimeGPT outperforms traditional statistical methods and global models with total speed by orders of magnitude.\\n\\n# 7 Discussion and Future Research\\n\\nAcross the results for each frequency. To ensure both robust numerical stability and consistency in evaluation, we apply this normalization at a global scale for each comprehensive dataset. The specific computations for these metrics, applicable to a dataset with n time series and a forecast horizon of h, are described in Equation 2.\\n\\nrMAE = Pn=1Ph=1 yi,t ‚àí ÀÜi,ti / Ph=1 |yi,t ‚àí ÀÜi,t|t\\n\\nrRMSE = Pn=1qPh=1 (yi,t ‚àí ÀÜi,t)2 / ybase\\n\\n# 6.1 Zero-shot inference\\n\\nWe first test TimeGPT capabilities on zero-shot inference, meaning that no additional fine-tuning is performed on the test set. Table 1 presents the zero-shot results. Remarkably, TimeGPT outperforms a comprehensive collection of battle-tested statistical models and SoTA deep learning approaches, ranking among the top-3 performers across frequencies.\\n\\n# 5.3 Training TimeGPT\\n\\nTimeGPT underwent a multi-day training period on a cluster of NVIDIA A10G GPUs. During this process, we carried out extensive hyperparameter exploration to optimize learning rates, batch sizes, and other related parameters. We observed a pattern in alignment with findings from [Brown et al., 2020], where a larger batch size and a smaller learning rate proved beneficial. Implemented in PyTorch, TimeGPT was trained using the Adam with a learning rate decay strategy that reduced the rate to 12% of its initial value.\\n\\n# 5.4 Uncertainty quantification\\n\\nThe evaluation is performed in the last forecasting window of each time series, varying in length by the sampling frequency. TimeGPT uses the previous historical values as inputs, as shown in Figure 3, without re-training its weights (zero-shot). We specify a different forecasting horizon based on the frequency to represent common practical applications: 12 for monthly, 1 for weekly, 7 for daily, and 24 for hourly data. 4\\n\\n. To the best of our knowledge, the very recent work in TimeGPT-1 [GMC23] is the only other parallel work on a zero-shot foundation model for time-series forecasting. However the model is not public access, and several model details and the benchmark dataset have not been revealed.\\n\\nQuestion:\\nWhat is the average GPU inference speed of TimeGPT for zero-shot inference, as reported in Section 6.3 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nWe provide a finetuning study in the same setting as [ ZNW+23 ] in Appendix A.3, where our model performs better than all baselines on all the reported datasets. This shows the utility of our model on downstream tasks.\\n\\n# 7 Conclusion\\n\\nIn this paper, we presented TimesFM, a practical foundation model for forecasting whose zero-shot performance comes close to the accuracy of fully-supervised forecasting models on a diverse set of time-series data. This model is pretrained on real-world and synthetic datasets comprising O(100B) timepoints. We discuss limitations and future work in more detail in Appendix A.1.\\n\\n# 8 Impact Statement\\n\\n# A PREPRINT\\n\\n# Figure 1\\n\\nWe provide an illustration of the TimesFM model architecture during training, where we show a input time-series of a specific length that can be broken down into input patches. Each patch along is processed into a vector by a residual block (as defined in the model definition) to the model dimension of the transformer layers. The vector is then added to positional encodings and fed into nl stacked transformer layers. SA refers to self-attention (note that we use multi-head causal attention) and FFN is the fully connected layer in the transformer. The output tokens are then mapped through a residual block to an output of size output_patch_len, which is the forecast for the time window following the last input patch seen by the model so far.\\n\\n# Network\\n\\n# A PREPRINT\\n\\nIn particular, we design TimesFM, a single foundation model for time-series forecasting that, when applied to a variety of previously-unseen forecasting datasets across different domains, obtains close to state-of-the-art zero-shot accuracy (compared to the best supervised models trained individually for these datasets). Our model can work well across different forecasting history lengths, prediction lengths and time granularities at inference time. The key elements of our foundation model are twofold: 1) a large-scale time-series corpus built using both real-world (mostly time-series data from web search queries1 and Wikipedia page visits2) and synthetic data, which meets the volume and diversity of data needed for training our foundation model, and 2) a decoder style attention architecture with input patching, that can be efficiently pre-trained on this time-series corpus.\\n\\n# A PREPRINT\\n\\n# Table 1: Composition of TimesFM pretraining dataset.\\n\\n|Dataset|Granularity|# Time series|# Time points|\\n|---|---|---|---|\\n|Synthetic| |3,000,000|6,144,000,000|\\n|Electricity|Hourly|321|8,443,584|\\n|Traffic| | | |\\n|Weather [ZZP+21]|Hourly|862|15,122,928|\\n| |10 Min|42|2,213,232|\\n|Favorita Sales [23]| | | |\\n|LibCity [WJJ]|Daily|111,840|6,159|\\n| |15 Min|139,179,538|34,253,622|\\n|M4 hourly|Hourly|414|353,500|\\n|M4 daily|Daily|4,227|9,964,658|\\n|M4 monthly|Monthly|48,000|10,382,411|\\n|M4 quarterly|Quarterly|24,000|2,214,108|\\n|M4 yearly|Yearly|22,739|840,644|\\n|Wiki hourly|Hourly|5,608,693|239,110,787,496|\\n|Wiki daily|Daily|68,448,204|115,143,501,240|\\n|Wiki weekly|Weekly|66,579,850|16,414,251,948|\\n|Wiki monthly|Monthly|63,151,306|3,789,760,907|\\n|Trends hourly|Hourly|22,435|393,043,680|\\n|Trends daily|Daily|22,435|122,921,365|\\n|Trends weekly|Weekly|22,435|16,585,438|\\n|Trends monthly|Monthly|22,435|3,821,760|\\n\\n# Figure 2:\\n\\n# A.9 Illustrative Examples\\n\\nWe conduct a visual inspection of the forecasts generated by TimesFM, first on some synthetic examples and then on the benchmark datasets.\\n\\nIn Figure 5 we show 4 different synthetic curves: (1) sum of 5 sine curves of different periods, (2) a sine curve linearly scaled, (3) a sine curve with a linear trend, and (4) minimum of two sine curves with a linear trend. Our results suggests that TimesFM picks up the trend and seasonal components readily interpretable by humans, while ARIMA and (to a lesser extent) llmtime fail in some of the instances.\\n\\nQuestion:\\nWhat is the main contribution of the paper \\'Timesfm\\'?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# A PREPRINT\\n\\n# Table 1: Composition of TimesFM pretraining dataset.\\n\\n|Dataset|Granularity|# Time series|# Time points|\\n|---|---|---|---|\\n|Synthetic| |3,000,000|6,144,000,000|\\n|Electricity|Hourly|321|8,443,584|\\n|Traffic| | | |\\n|Weather [ZZP+21]|Hourly|862|15,122,928|\\n| |10 Min|42|2,213,232|\\n|Favorita Sales [23]| | | |\\n|LibCity [WJJ]|Daily|111,840|6,159|\\n| |15 Min|139,179,538|34,253,622|\\n|M4 hourly|Hourly|414|353,500|\\n|M4 daily|Daily|4,227|9,964,658|\\n|M4 monthly|Monthly|48,000|10,382,411|\\n|M4 quarterly|Quarterly|24,000|2,214,108|\\n|M4 yearly|Yearly|22,739|840,644|\\n|Wiki hourly|Hourly|5,608,693|239,110,787,496|\\n|Wiki daily|Daily|68,448,204|115,143,501,240|\\n|Wiki weekly|Weekly|66,579,850|16,414,251,948|\\n|Wiki monthly|Monthly|63,151,306|3,789,760,907|\\n|Trends hourly|Hourly|22,435|393,043,680|\\n|Trends daily|Daily|22,435|122,921,365|\\n|Trends weekly|Weekly|22,435|16,585,438|\\n|Trends monthly|Monthly|22,435|3,821,760|\\n\\n# Figure 2:\\n\\n# A PREPRINT\\n\\n# Figure 1\\n\\nWe provide an illustration of the TimesFM model architecture during training, where we show a input time-series of a specific length that can be broken down into input patches. Each patch along is processed into a vector by a residual block (as defined in the model definition) to the model dimension of the transformer layers. The vector is then added to positional encodings and fed into nl stacked transformer layers. SA refers to self-attention (note that we use multi-head causal attention) and FFN is the fully connected layer in the transformer. The output tokens are then mapped through a residual block to an output of size output_patch_len, which is the forecast for the time window following the last input patch seen by the model so far.\\n\\n# Network\\n\\n# A PREPRINT\\n\\nIn particular, we design TimesFM, a single foundation model for time-series forecasting that, when applied to a variety of previously-unseen forecasting datasets across different domains, obtains close to state-of-the-art zero-shot accuracy (compared to the best supervised models trained individually for these datasets). Our model can work well across different forecasting history lengths, prediction lengths and time granularities at inference time. The key elements of our foundation model are twofold: 1) a large-scale time-series corpus built using both real-world (mostly time-series data from web search queries1 and Wikipedia page visits2) and synthetic data, which meets the volume and diversity of data needed for training our foundation model, and 2) a decoder style attention architecture with input patching, that can be efficiently pre-trained on this time-series corpus.\\n\\n# Table 3: Datasets used in the pretraining corpus and the unseen datasets on which we evaluate, grouped by the domains they are labelled against.\\n\\n|Transport & Tourism|Energy|Nature|\\n|---|---|---|\\n|Pretraining|San Francisco Traffic|Australian Electricity Demand|\\n| |Uber TLC Hourly|Electricity Hourly|\\n| |London Smart Meters|Solar|\\n| |Wind Farms|ETT H1|\\n| |ETT H2|ETT M1|\\n| |ETT M2| |\\n|Unseen|Pedestrian Counts|ETT M2|\\n| | |Weather|\\n|Air Quality|Cloud|Banking & Econ|\\n|Beijing Multisite|CPU Limit Minute| |\\n|UCI|CPU Usage Minute| |\\n| |Function Delay Minute| |\\n| |Instances Minute| |\\n| |Memory Limit Minute| |\\n| |Memory Usage Minute| |\\n|Beijing PM2.5|Requests Minute|Exchange Rate|\\n| |Platform Delay Minute| |\\n\\n# Table 4: Statistics of all the datasets used in the paper. Frequencies H stands for Hourly, T for minute, and B for business day. Tokens refers to the total number of windows of size 1 in the dataset, computed as the aggregate number of timesteps across all series in that dataset.\\n\\n# Synthetic Data\\n\\nAnother major component of our pretraining data is of synthetic origin. We create generators for ARMA processes, seasonal patterns (mixture of sines and cosines of different frequencies), trends (linear, exponential with a few change-points) and step functions. A synthetic time-series can be an additive combination of one or more of these processes. We create 3M synthetic time-series each of length 2048 time-points. More details about our synthetic data generation are presented in Appendix A.8.\\n\\n# Other real-world data sources\\n\\nQuestion:\\nWhat is the composition of the pretraining dataset used in the paper \\'Timesfm\\'?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# A PREPRINT\\n\\n# Figure 1\\n\\nWe provide an illustration of the TimesFM model architecture during training, where we show a input time-series of a specific length that can be broken down into input patches. Each patch along is processed into a vector by a residual block (as defined in the model definition) to the model dimension of the transformer layers. The vector is then added to positional encodings and fed into nl stacked transformer layers. SA refers to self-attention (note that we use multi-head causal attention) and FFN is the fully connected layer in the transformer. The output tokens are then mapped through a residual block to an output of size output_patch_len, which is the forecast for the time window following the last input patch seen by the model so far.\\n\\n# Network\\n\\n# A.6 More Details on Models\\n\\nWe now present implementation details about TimesFM and other baselines.\\n\\n# TimesFM\\n\\nFor our main 200M model we use 16 attention heads, 20 layers, a input patch length of 32 and output patch length of 128. The model dimension is set to 1280. We train with layer norm and a cosine decay learning rate schedule with peak learning rate of 5e ‚àí 4. The hyper-parameters of TimesFM for various sizes are provided in Table 6. Note that the settings are for the base models and not ablation models. The hidden dims of both the residual block and the FFN in the transformer layers are set as the same as model dimensions. We keep layer norm in transformer layers but not in the residual blocks.\\n\\n**Table 6: Hyper-parameters for TimesFM**\\n|Size|num_layers|model_dims|output_patch_len|input_patch_len|num_heads|dropout|\\n|---|---|---|---|---|---|---|\\n|200M|20|1280|128|32|16|0.2|\\n|70M|10|1024|128|32|16|0.2|\\n|17M|10|512|128|32|16|0.2|\\n\\n# Monash Baselines\\n\\n# Patch Masking\\n\\nIf we use patches naively, the model might only learn to predict well for context lengths that are multiples of the input patch length. Therefore we make a careful use of masking during training. Parts of patches as well as entire patches from the beginning of the context window can be masked in a data batch. We employ a specific random masking strategy (described later) during training that helps the model see all possible context lengths starting from 1 to a maximum context length.\\n\\nNow that we have mentioned the guiding principles, we next formally describe each component of our model architecture (illustrated in Figure 1), which we name as TimesFM (Time-series Foundation Model).\\n\\n# Input Layers\\n\\n# 5.1 Architecture\\n\\nAs shown in Figure 4, we first delve into the architecture of TSFMs, including Transformer-based models, non-Transformer-based models and diffusion-based models, focusing on the underlying mechanisms that shape their capabilities, as well as how they could be applied on various time series.\\n\\n# 5.1.1 Transformer-based Models\\n\\n**Table 5: MAE for ETT datasets for prediction horizons 96 and 192. Owing to expensive evaluations for llmtime, the results are reported on the last test window of the original test split.**\\n|Dataset|llmtime(ZS)*|PatchTST|PatchTST(ZS)|FEDFormer|AutoFormer|Informer|TimesFM(ZS)|\\n|---|---|---|---|---|---|---|---|\\n|ETTh1 (horizon=96)|0.42|0.41|0.39|0.58|0.55|0.76|0.45|\\n|ETTh1 (horizon=192)|0.50|0.49|0.50|0.64|0.64|0.78|0.53|\\n|ETTh2 (horizon=96)|0.33|0.28|0.37|0.67|0.65|1.94|0.35|\\n|ETTh2 (horizon=192)|0.70|0.68|0.59|0.82|0.82|2.02|0.62|\\n|ETTm1 (horizon=96)|0.37|0.33|0.24|0.41|0.54|0.71|0.19|\\n|ETTm1 (horizon=192)|0.71|0.31|0.26|0.49|0.46|0.68|0.26|\\n|ETTm2 (horizon=96)|0.29|0.23|0.22|0.36|0.29|0.48|0.24|\\n|ETTm2 (horizon=192)|0.31|0.25|0.22|0.25|0.30|0.51|0.27|\\n|Avg|0.45|0.37|0.35|0.53|0.53|0.99|0.36|\\n\\n# A.6 More Details on Models\\n\\nWe now present implementation details about TimesFM and other baselines.\\n\\n# TimesFM\\n\\nQuestion:\\nWhat is the architecture of the TimesFM model, and how does it differ from other models like PatchTST?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# A.5.1 Darts\\n\\nWe present the MAE results individually from all 8 datasets in Table 3. It can be seen that TimesFM performs well for all datasets with clear seasonal patterns. On an average we are within significant level of the best model. Note that there are only 8 time-series as a whole in Darts and therefore these evaluations have very wide confidence intervals. In Figure 8 we present visual comparisons of our forecasts vs some of the baselines.\\n\\n# Table 3: MAE for Darts datasets\\n\\nWe also include the naive baseline that predicts the last values in the context repeatedly.\\n\\nThe mean scaled MAE across all datasets is plotted in Figure 2a along with standard error bars. We compare the performance of TimesFM with the baseline models implemented in Monash, and the zero-shot llmtime [GFQW23] model that uses GPT-3 [RWC+19] with a specific prompting technique. Note that the zero-shot models are marked as (Zero-Shot). TimesFM is the top model even though we never trained on these datasets. It is slightly better but within significance of N-BEATS but outperforms deep supervised models like DeepAR [SFGJ20], and improves on llmtime‚Äôs performance by more than 25%.\\n\\n# A.6 More Details on Models\\n\\nWe now present implementation details about TimesFM and other baselines.\\n\\n# TimesFM\\n\\nFor our main 200M model we use 16 attention heads, 20 layers, a input patch length of 32 and output patch length of 128. The model dimension is set to 1280. We train with layer norm and a cosine decay learning rate schedule with peak learning rate of 5e ‚àí 4. The hyper-parameters of TimesFM for various sizes are provided in Table 6. Note that the settings are for the base models and not ablation models. The hidden dims of both the residual block and the FFN in the transformer layers are set as the same as model dimensions. We keep layer norm in transformer layers but not in the residual blocks.\\n\\n**Table 6: Hyper-parameters for TimesFM**\\n|Size|num_layers|model_dims|output_patch_len|input_patch_len|num_heads|dropout|\\n|---|---|---|---|---|---|---|\\n|200M|20|1280|128|32|16|0.2|\\n|70M|10|1024|128|32|16|0.2|\\n|17M|10|512|128|32|16|0.2|\\n\\n# Monash Baselines\\n\\n# Monash Baselines\\n\\nThe raw metrics for the Monash baselines are directly taken from Tables 9 and 11 of the supplementary material of the original paper [ GBW+21 ]. For llmtime, we use the precomputed outputs provided by the authors of [GFQW23].\\n\\n# Darts Baselines\\n\\nFor all the Darts baselines we use the precomputed outputs provided by the authors of [ GFQW23 ]. For more details please see Section C.1 in that paper.\\n\\n# Informer Baselines\\n\\n# A PREPRINT\\n\\n# Figure 4\\n\\nWe report average performance in three groups of datasets. In all figures, the lower the metric the better and the error bars represent one standard error. Note that among the baselines only TimesFM and llmtime are zero-shot. In (a) we report results on the Monash datasets. Since the datasets have different scales, we take the Arithmetic Mean (AM) the MAE‚Äôs scaled by the MAE of a naive baseline. We can see that TimesFM is within significance of the top model N-BEATS. In (b), we report the similarly scaled MAE on the Darts benchmarks. TimesFM is within significance of the top of method which is ARIMA in this case. Note that these datasets have one time-series each and therefore statistical methods are competitive with deep learning ones. Finally, in (c) we report the average MAE for 96 and 192 horizon prediction tasks on 4 ETT datasets i.e 8 tasks in total. TimesFM and PatchTST are the best performing models in this case.\\n\\n# A.5.1 Darts\\n\\nQuestion:\\nHow does the performance of TimesFM compare to other baselines on the Monash and Darts datasets?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nThe introduction of a foundation model in time series that resembles other fields and opens the possible path for future improvements could be considered an important milestone in the time series field. However, this work must be understood as part of a larger academic tradition with a plethora of open questions. While we believe that TimeGPT displays amazing results presenting for the first time a general global model capable of accurately predicting unseen series, there are still many important limitations and open questions. We hope this assessment is of help to current and future researchers.\\n\\nWe provide a finetuning study in the same setting as [ ZNW+23 ] in Appendix A.3, where our model performs better than all baselines on all the reported datasets. This shows the utility of our model on downstream tasks.\\n\\n# 7 Conclusion\\n\\nIn this paper, we presented TimesFM, a practical foundation model for forecasting whose zero-shot performance comes close to the accuracy of fully-supervised forecasting models on a diverse set of time-series data. This model is pretrained on real-world and synthetic datasets comprising O(100B) timepoints. We discuss limitations and future work in more detail in Appendix A.1.\\n\\n# 8 Impact Statement\\n\\n# A.6 More Details on Models\\n\\nWe now present implementation details about TimesFM and other baselines.\\n\\n# TimesFM\\n\\nFor our main 200M model we use 16 attention heads, 20 layers, a input patch length of 32 and output patch length of 128. The model dimension is set to 1280. We train with layer norm and a cosine decay learning rate schedule with peak learning rate of 5e ‚àí 4. The hyper-parameters of TimesFM for various sizes are provided in Table 6. Note that the settings are for the base models and not ablation models. The hidden dims of both the residual block and the FFN in the transformer layers are set as the same as model dimensions. We keep layer norm in transformer layers but not in the residual blocks.\\n\\n**Table 6: Hyper-parameters for TimesFM**\\n|Size|num_layers|model_dims|output_patch_len|input_patch_len|num_heads|dropout|\\n|---|---|---|---|---|---|---|\\n|200M|20|1280|128|32|16|0.2|\\n|70M|10|1024|128|32|16|0.2|\\n|17M|10|512|128|32|16|0.2|\\n\\n# Monash Baselines\\n\\n# Table 9: An overview of the experimental configurations for TIME-LLM. ‚ÄúLTF‚Äù and ‚ÄúSTF‚Äù denote long-term and short-term forecasting, respectively.\\n\\n**Table 5: MAE for ETT datasets for prediction horizons 96 and 192. Owing to expensive evaluations for llmtime, the results are reported on the last test window of the original test split.**\\n|Dataset|llmtime(ZS)*|PatchTST|PatchTST(ZS)|FEDFormer|AutoFormer|Informer|TimesFM(ZS)|\\n|---|---|---|---|---|---|---|---|\\n|ETTh1 (horizon=96)|0.42|0.41|0.39|0.58|0.55|0.76|0.45|\\n|ETTh1 (horizon=192)|0.50|0.49|0.50|0.64|0.64|0.78|0.53|\\n|ETTh2 (horizon=96)|0.33|0.28|0.37|0.67|0.65|1.94|0.35|\\n|ETTh2 (horizon=192)|0.70|0.68|0.59|0.82|0.82|2.02|0.62|\\n|ETTm1 (horizon=96)|0.37|0.33|0.24|0.41|0.54|0.71|0.19|\\n|ETTm1 (horizon=192)|0.71|0.31|0.26|0.49|0.46|0.68|0.26|\\n|ETTm2 (horizon=96)|0.29|0.23|0.22|0.36|0.29|0.48|0.24|\\n|ETTm2 (horizon=192)|0.31|0.25|0.22|0.25|0.30|0.51|0.27|\\n|Avg|0.45|0.37|0.35|0.53|0.53|0.99|0.36|\\n\\n# A.6 More Details on Models\\n\\nWe now present implementation details about TimesFM and other baselines.\\n\\n# TimesFM\\n\\nQuestion:\\nWhat are some potential limitations and future directions for the TimesFM model, as discussed in the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n¬∂This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nas in LLaMA (Touvron et al., 2023). After passing through the causally masked transformer layers, the model predicts the parameters œï of the forecast log prob distribution of the next timestep, where the parameters are output by a parametric distribution head, as described in Sec. 4.3. The negative log-likelihood of the predicted distribution of all predicted timesteps is minimized.\\n\\n. This clearly establishes Lag-Llama as a strong foundation model that can be used on a wide range of downstream datasets, without prior knowledge of these data distribution ‚Äî a key property that a foundation model should satisfy.\\n\\n# Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting\\n\\n# Authors\\n\\n*Kashif Rasul1 *Arjun Ashok2 3 4 ‚ô¢Andrew Robert Williams3 4 ‚ô¢Hena Ghonia3 4 Rishika Bhagwatkar ‚ô†Roland Riachi3 4 ‚ô†Arian Khorasani3 4 ‚ô†Mohammad Javad Darvishi Bayazi3 ‚ô†George Adamopoulos5 Nadhir Hassen3 4 Marin Bilo≈°1 ‚ñ≥Sahil Garg1 ‚ñ≥Anderson Schneider1 ‚ñ≥Nicolas Chapados2 4 ‚ñ≥Alexandre Drouin2 4 ‚ñ≥Valentina Zantedeschi2 ‚ô£Yuriy Nevmyvaka1 ‚ô£Irina Rish3 4\\n\\n# Abstract\\n\\n# D. Hyperparameters of Lag-Llama\\n\\nWe perform a random search of 100 different hyperparameter configurations and use the average validation loss over all datasets in the pretraining corpus to select our model. We list the possible hyperparameters of Lag-Llama and the optimal values obtained by our hyperparameter search in Tab. 5. Our final model obtained by hyperparameter search contains 2,449,299 parameters.\\n\\n# E. Forecast Visualizations\\n\\n. This is reflected in the results, as Lag-Llama is not the best performing model in each dataset. Still, Lag-Llama achieves a comparable average rank, and is among the models achieving the top average ranks.\\n\\nQuestion:\\nWhat is the primary goal of the Lag-Llama model, as described in the abstract of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 4.1. Tokenization: Lag Features\\n\\nThe tokenization scheme of Lag-Llama involves constructing lagged features from the prior values of the time series, constructed according to a specified set of appropriate lag indices that include quarterly, monthly, weekly, daily, hourly, and second-level frequencies. Given a sorted set of positive lag indices L = {1, . . . , L}*, we define the lag operation:\\n\\n*Note that L refers to the list of lag indices, while L is the last lag index in the sorted list L\\n\\nlag indices:\\n\\n- sec(t)\\n- min(t)\\n- month(t)\\n\\ntime\\n\\nFigure 1: For a time series, we depict the tokenization at the timestep t of the value xt which contains lag features constructed using an example set of lag indices L, where each value in the vector is from the past of xt (in blue), and F possible temporal covariates (date-time features) constructed from timestamp t (red).\\n\\n¬∂This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nas in LLaMA (Touvron et al., 2023). After passing through the causally masked transformer layers, the model predicts the parameters œï of the forecast log prob distribution of the next timestep, where the parameters are output by a parametric distribution head, as described in Sec. 4.3. The negative log-likelihood of the predicted distribution of all predicted timesteps is minimized.\\n\\n# 4.5 MODEL ANALYSIS\\n\\n# Language Model Variants\\n\\nWe compare two representative backbones with varying capacities. Our results indicate that the scaling law retains after the LLM reprogramming. We adopt Llama-7B by default in its full capacity, which manifestly outperforms its 1/4 capacity variant by 14.5%. An average MSE reduction of 14.7% is observed over GPT-2, which slightly outperforms its variant GPT-2 (6) by 2.7%.\\n\\n# Cross-modality Alignment\\n\\n# 4.4. Value Scaling\\n\\nWhen training on a large corpus of time series data from different datasets and domains, each time series can be of different numerical magnitude. Since we pretrain a foundation model over such data, we utilize the scaling heuristic (Salinas et al., 2019b) where for each univariate window, we calculate its mean value Œºi = ‚àët=1C xi / C and variance œÉi. We can then replace the time series xt with {(xit - Œºi) / œÉi}t=1. We also incorporate Œºi and œÉi as time independent real-valued covariates for each token, to give the model information of the statistics of the inputs, which we call summary statistics.\\n\\n# 4.5. Training Strategies\\n\\nWe employ a series of training strategies to effectively pre-train Lag-Llama on the corpus of datasets. Firstly, we find that employing a stratified sampling approach where the datasets in the corpus are weighed by the amount of total.\\n# Lag-Llama\\n\\n# 4.2. Lag-Llama Architecture\\n\\nLag-Llama‚Äôs architecture is based on the decoder-only transformer-based architecture LLaMA (Touvron et al., 2023). Fig. 2 shows a general schematic of this model with M decoder layers. A univariate sequence of length xi‚àíL:C along with its covariates is tokenized by concatenating the covariate vectors to a sequence of C tokens xi1:C. These tokens are passed through a shared linear projection layer that maps the features to the hidden dimension of the attention module. Similar to in Touvron et al. (2023), Lag-Llama incorporates pre-normalization via the RMSNorm (Zhang & Sennrich, 2019) and Rotary Positional Encoding (RoPE) (Su et al., 2021) at each attention layer‚Äôs query and key representations.\\n\\n¬∂This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nQuestion:\\nHow does the tokenization scheme of Lag-Llama work, as described in Section 4.1 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 4.2. Lag-Llama Architecture\\n\\nLag-Llama‚Äôs architecture is based on the decoder-only transformer-based architecture LLaMA (Touvron et al., 2023). Fig. 2 shows a general schematic of this model with M decoder layers. A univariate sequence of length xi‚àíL:C along with its covariates is tokenized by concatenating the covariate vectors to a sequence of C tokens xi1:C. These tokens are passed through a shared linear projection layer that maps the features to the hidden dimension of the attention module. Similar to in Touvron et al. (2023), Lag-Llama incorporates pre-normalization via the RMSNorm (Zhang & Sennrich, 2019) and Rotary Positional Encoding (RoPE) (Su et al., 2021) at each attention layer‚Äôs query and key representations.\\n\\n¬∂This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\n# 4.5 MODEL ANALYSIS\\n\\n# Language Model Variants\\n\\nWe compare two representative backbones with varying capacities. Our results indicate that the scaling law retains after the LLM reprogramming. We adopt Llama-7B by default in its full capacity, which manifestly outperforms its 1/4 capacity variant by 14.5%. An average MSE reduction of 14.7% is observed over GPT-2, which slightly outperforms its variant GPT-2 (6) by 2.7%.\\n\\n# Cross-modality Alignment\\n\\n¬∂This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nas in LLaMA (Touvron et al., 2023). After passing through the causally masked transformer layers, the model predicts the parameters œï of the forecast log prob distribution of the next timestep, where the parameters are output by a parametric distribution head, as described in Sec. 4.3. The negative log-likelihood of the predicted distribution of all predicted timesteps is minimized.\\n\\n. This clearly establishes Lag-Llama as a strong foundation model that can be used on a wide range of downstream datasets, without prior knowledge of these data distribution ‚Äî a key property that a foundation model should satisfy.\\n\\n# 4.1. Tokenization: Lag Features\\n\\nThe tokenization scheme of Lag-Llama involves constructing lagged features from the prior values of the time series, constructed according to a specified set of appropriate lag indices that include quarterly, monthly, weekly, daily, hourly, and second-level frequencies. Given a sorted set of positive lag indices L = {1, . . . , L}*, we define the lag operation:\\n\\n*Note that L refers to the list of lag indices, while L is the last lag index in the sorted list L\\n\\nlag indices:\\n\\n- sec(t)\\n- min(t)\\n- month(t)\\n\\ntime\\n\\nFigure 1: For a time series, we depict the tokenization at the timestep t of the value xt which contains lag features constructed using an example set of lag indices L, where each value in the vector is from the past of xt (in blue), and F possible temporal covariates (date-time features) constructed from timestamp t (red).\\n\\nQuestion:\\nWhat is the architecture of Lag-Llama, as described in Section 4.2 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 4.3. Choice of Distribution Head\\n\\nThe last layer of Lag-Llama is a distinct layer known as the distribution head, which projects the model‚Äôs features to the parameters of a probability distribution. We can combine different distribution heads with the representational capacity of the model to output the parameters œï of any parametric probability distribution. For our experiments, we adopt a Student‚Äôs t-distribution (Student, 1908) and output the three parameters corresponding to this distribution, namely its degrees of freedom, mean, and scale, with appropriate non-linearities to ensure the appropriate parameters stay positive.\\n\\n¬∂This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nas in LLaMA (Touvron et al., 2023). After passing through the causally masked transformer layers, the model predicts the parameters œï of the forecast log prob distribution of the next timestep, where the parameters are output by a parametric distribution head, as described in Sec. 4.3. The negative log-likelihood of the predicted distribution of all predicted timesteps is minimized.\\n\\nAt inference time, given a time series of size at least L, we can construct a feature vector that is passed to the model to obtain the distribution of the next time point. In this fashion, via greedy autoregressive decoding, we can obtain many simulated trajectories of the future up to our chosen prediction horizon P ‚â• 1. From these empirical samples, we can calculate the uncertainty intervals for downstream decision-making tasks and metrics with respect to held-out data.\\n\\n# 4.3. Choice of Distribution Head\\n\\n# Table 5: Hyperparameter choices for Lag-Llama\\n\\nThe values with * represent the optimal values obtained by hyperparameter search.\\n\\n‚Ä† Note that this is just the consecutive context that is sampled for each window; in practice we use a much larger context window due to the use of lags, as described in Sec. ¬ß 4.1\\n\\n|HYPERPARAMETER|LAG-LLAMA|\\n|---|---|\\n|NUMBER OF LAYERS|1,2,3,4,5,6,7,8*,9|\\n|NUMBER OF HEADS|1,2,3,4,5,6,7,8,9*|\\n|EMBEDDING DIMENSIONS PER HEAD| |\\n|CONTEXT LENGTH C ‚Ä†|16*, 32, 64, 128, 256, 512|\\n| |32*, 64, 128, 256, 512, 1024|\\n|AUGMENTATION PROBABILITY|0,0.25,0.5*,1.0|\\n|FREQUENCY MASKING RATE|0,0.25,0.5*,1.0|\\n|FREQUENCY MIXING RATE|0,0.25*,0.5,1.0|\\n|WEIGHT DECAY|0*,0.25,0.5,1.0|\\n|DROPOUT|0*,0.25,0.5,1.0|\\n\\nMore expressive choices of distributions, such as normalizing flows (Rasul et al., 2021b) and copulas (Salinas et al., 2019a; Drouin et al., 2022; Ashok et al., 2023) are potential choices of distribution heads, however with the potential overhead of difficulties in model training and optimization. The goal of our work was to keep the model as simple as possible, which led us to adopt a simple parametric distributional head. We leave the exploration of such distribution heads for future work.\\n\\n# 4.4. Value Scaling\\n\\nQuestion:\\nWhat is the purpose of the distribution head in Lag-Llama, as described in Section 4.3 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# D. Hyperparameters of Lag-Llama\\n\\nWe perform a random search of 100 different hyperparameter configurations and use the average validation loss over all datasets in the pretraining corpus to select our model. We list the possible hyperparameters of Lag-Llama and the optimal values obtained by our hyperparameter search in Tab. 5. Our final model obtained by hyperparameter search contains 2,449,299 parameters.\\n\\n# E. Forecast Visualizations\\n\\nlishes Lag-Llama as one with strong adaptation capabilities across all levels of data. As the amount of history available increases, Lag-Llama achieves increasingly better perfor-\\n\\nmance across all datasets, and the gap between the rank of Lag-Llama and the baselines widens, as expected. Note, however, that Lag-Llama is most often outperformed by TFT in the exchange-rate dataset, which is from an entirely new domain and has a new unseen frequency. Our observa-\\n\\ntion demonstrates that, in cases where the data is most dis-\\n\\nsimilar, as compared to the pretraining corpus, Lag-Llama requires increasing amounts of history to train on, and, when given enough history to adapt, performs comparable to state-\\n\\nof-the-art (as discussed in subsection 6.1). Overall, our empirical results demonstrate that Lag-Llama has strong few-shot adaptation capabilities, and that, based on the characteristics of the downstream dataset, Lag-Llama can adapt and generalize with the appropriate amount of data.\\n\\n# 4.4. Value Scaling\\n\\nWhen training on a large corpus of time series data from different datasets and domains, each time series can be of different numerical magnitude. Since we pretrain a foundation model over such data, we utilize the scaling heuristic (Salinas et al., 2019b) where for each univariate window, we calculate its mean value Œºi = ‚àët=1C xi / C and variance œÉi. We can then replace the time series xt with {(xit - Œºi) / œÉi}t=1. We also incorporate Œºi and œÉi as time independent real-valued covariates for each token, to give the model information of the statistics of the inputs, which we call summary statistics.\\n\\n# 4.5. Training Strategies\\n\\nWe employ a series of training strategies to effectively pre-train Lag-Llama on the corpus of datasets. Firstly, we find that employing a stratified sampling approach where the datasets in the corpus are weighed by the amount of total.\\n# Lag-Llama\\n\\nple decoder-only transformer architecture. We show that Lag-Llama, when pretrained from scratch on a large cor-\\n\\npus of datasets, has strong zero-shot generalization per-\\n\\nformance on unseen datasets, and performs comparably to dataset-specific models. Lag-Llama also demonstrates\\n\\nstate-of-the-art performance across diverse datasets from\\n\\ndifferent domains after finetuning, and emerges as the best general-purpose model without any knowledge of down-\\n\\nstream datasets. Lag-Llama also demonstrates a strong\\n\\nfew-shot adaptation performance across varying amounts\\n\\nof data history being available. Finally, we investigate the\\n\\ndiversity of the pretraining corpus used to train Lag-Llama. Our work opens up several potential directions for future\\n\\nwork. For now, collecting and collating a large scale time\\n\\nseries corpus of open dataset would be of high value, since\\n\\nthe largest time series dataset repositories (Godahewa et al., 2021) are themselves too small. Further, scaling up the\\n\\n# Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting\\n\\n# Authors\\n\\n*Kashif Rasul1 *Arjun Ashok2 3 4 ‚ô¢Andrew Robert Williams3 4 ‚ô¢Hena Ghonia3 4 Rishika Bhagwatkar ‚ô†Roland Riachi3 4 ‚ô†Arian Khorasani3 4 ‚ô†Mohammad Javad Darvishi Bayazi3 ‚ô†George Adamopoulos5 Nadhir Hassen3 4 Marin Bilo≈°1 ‚ñ≥Sahil Garg1 ‚ñ≥Anderson Schneider1 ‚ñ≥Nicolas Chapados2 4 ‚ñ≥Alexandre Drouin2 4 ‚ñ≥Valentina Zantedeschi2 ‚ô£Yuriy Nevmyvaka1 ‚ô£Irina Rish3 4\\n\\n# Abstract\\n\\nQuestion:\\nHow does Lag-Llama handle the diversity of time series data in the pretraining corpus, as described in Section 5.1 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nIn addition to conventional time series data, the Transformer architecture has demonstrated efficacy across a diverse array of temporal datasets, such as trajectory and healthcare records, as summarized in Figure 3. This expansion highlights the Transformer‚Äôs versatile capacity for temporal data analysis.\\n# Foundation Models for Time Series Analysis: A Tutorial and Survey\\n\\n# KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\\n\\n# 5.1.2 Non-Transformer-based Models\\n\\nExcluding the widespread adoption of Transformers, a diverse array of traditional pre-training methods leveraged models such as Multi-Layer Perceptrons (MLPs) [31], Recurrent Neural Networks (RNNs) [33], and Convolutional Neural Networks (CNNs) [98] as the backbone for pre-training. These models, each with their unique strengths, are notable for their effectiveness in both conventional and spatial time series data.\\n\\nthat demand substantial computational resources and time for inference. Recent concurrent work (Dooley et al., 2023; Das et al., 2023; Rasul et al., 2023; Woo et al., 2024) also explores pretraining transformer-based models with sophisticated time-series-specific designs on a large corpus of real and (or) synthetic time series data.\\n\\n# Foundation Models for Time Series Analysis: A Tutorial and Survey\\n\\n# Authors\\n\\nYuxuan Liang, The Hong Kong University of Science and Technology (Guangzhou), yuxliang@outlook.com\\n\\nHaomin Wen, Beijing Jiao Tong University & HKUST(GZ), wenhaomin@bjtu.edu.cn\\n\\nYuqi Nie, Princeton University, ynie@princeton.edu\\n\\nYushan Jiang, University of Connecticut, yushan.jiang@uconn.edu\\n\\nMing Jin, Monash University, ming.jin@monash.edu\\n\\nDongjin Song, University of Connecticut, dongjin.song@uconn.edu\\n\\nShirui Pan, Griffith University, s.pan@griffith.edu.au\\n\\nQingsong Wen, Squirrel AI, USA, qingsongedu@gmail.com\\n\\n# ABSTRACT\\n\\nDespite the promising prospects and rapid development of TSFMs, a systematic analysis of TSFMs from a methodological standpoint has been notably absent in prior literature. Existing studies, as depicted in Table 1, have concentrated on either the data perspective [48] or the pipeline perspective [45] of TSFMs. To bridge this gap, this survey aims to provide a comprehensive methodological analysis of foundation models for learning a variety of time series. This examination will center on scrutinizing their model architectures, pre-training techniques, adaptation methods, and data modalities. Through this endeavor, we seek to illuminate an overall picture of core elements in TSFMs, thereby enhancing comprehension regarding the rationale behind their efficacy and the mechanisms driving their substantial potential in time series analysis.\\n\\nThe potential of foundation models, namely large-scale models pre-trained on a large dataset and later fine-tuned for specific tasks, remains relatively under-explored for time series forecasting tasks. There are, however, early indicators of the possibility of forecasting foundational models. For instance, [Oreshkin et al., 2021] showed that pre-trained models can be transferred between tasks without performance degradation. Additionally, [Kunz et al., 2023] provided evidence on the existence of scaling laws on data and model sizes for Transformer architectures on time series forecasting tasks.\\n\\n# 4 Foundation model for time series\\n\\nQuestion:\\nAccording to the paper \\'Foundation Models for Time Series Analysis: A Tutorial and Survey\\', what is the primary motivation behind the use of deep learning and transformers in time series analysis?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 3. Taxonomy\\n\\nThe proposed taxonomy is illustrated in Figure 3, and the related works can be found in Table 2. The proposed taxonomy unfolds a structured and comprehensive classification to enhance the understanding of foundation models on time series analysis. It is organized into four hierarchical levels, starting with the data category, followed by the model architecture, pre-training techniques, and finally, the application domain. Unlike previous taxonomies, ours distinguishes itself by delving deeper into the foundation models from the methodology perspective, with a keen focus on their architectural designs, pre-training, and adaptation techniques. This method-centric view is pivotal for researchers, providing valuable insights into the mechanisms of why and how foundation models show great potential for time series analysis.\\n\\n# 4. Data Perspective\\n\\n- Comprehensive and up-to-date survey. We offer a comprehensive and up-to-date survey on foundation models for a wide spectrum of time series, encompassing standard time series, spatial time series, and other types (i.e., trajectories and events).\\n- Novel methodology-centric taxonomy. We introduce a novel taxonomy that offers a thorough analysis from a methodological standpoint on TSFMs with the first shot, enabling a full understanding of the mechanism on why and how FMs can achieve admirable performance in time series data.\\n- Future research opportunities. We discuss and highlight future avenues for enhancing time series analysis using foundation models, urging researchers to delve deeper into this area.\\n\\n# 2 BACKGROUND\\n\\n# Foundation Models.\\n\\n# Foundation Models for Time Series Analysis: A Tutorial and Survey\\n\\n# Authors\\n\\nYuxuan Liang, The Hong Kong University of Science and Technology (Guangzhou), yuxliang@outlook.com\\n\\nHaomin Wen, Beijing Jiao Tong University & HKUST(GZ), wenhaomin@bjtu.edu.cn\\n\\nYuqi Nie, Princeton University, ynie@princeton.edu\\n\\nYushan Jiang, University of Connecticut, yushan.jiang@uconn.edu\\n\\nMing Jin, Monash University, ming.jin@monash.edu\\n\\nDongjin Song, University of Connecticut, dongjin.song@uconn.edu\\n\\nShirui Pan, Griffith University, s.pan@griffith.edu.au\\n\\nQingsong Wen, Squirrel AI, USA, qingsongedu@gmail.com\\n\\n# ABSTRACT\\n\\n# 6 CONCLUSION\\n\\nThe rapid development of FMs has revolutionized the research fields in different domains. In this survey, we provide a comprehensive and updated review of FMs specifically designed for time series analysis. A novel taxonomy is proposed from a methodology-centric perspective by classifying FMs based on key components including model architecture, pre-training technique, adaptation technique, and data modality. Our survey facilitates understanding the underlying mechanism of applying the FMs to time series. Furthermore, we believe that the consolidation of the latest advancements, as well as the potential future direction (see Appendix), can inspire more innovative works within the field of time series analysis.\\n# Foundation Models for Time Series Analysis: A Tutorial and Survey\\n\\n# KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\\n\\n# ACKNOWLEDGEMENTS\\n\\nAuthor names are listed in alphabetical order.\\n# A decoder-only foundation model for time-series forecasting\\n\\n# A PREPRINT\\n\\nQuestion:\\nWhat is the main contribution of the paper \\'Foundation Models for Time Series Analysis: A Tutorial and Survey\\' in terms of taxonomy?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 3. Taxonomy\\n\\nThe proposed taxonomy is illustrated in Figure 3, and the related works can be found in Table 2. The proposed taxonomy unfolds a structured and comprehensive classification to enhance the understanding of foundation models on time series analysis. It is organized into four hierarchical levels, starting with the data category, followed by the model architecture, pre-training techniques, and finally, the application domain. Unlike previous taxonomies, ours distinguishes itself by delving deeper into the foundation models from the methodology perspective, with a keen focus on their architectural designs, pre-training, and adaptation techniques. This method-centric view is pivotal for researchers, providing valuable insights into the mechanisms of why and how foundation models show great potential for time series analysis.\\n\\n# 4. Data Perspective\\n\\n- Comprehensive and up-to-date survey. We offer a comprehensive and up-to-date survey on foundation models for a wide spectrum of time series, encompassing standard time series, spatial time series, and other types (i.e., trajectories and events).\\n- Novel methodology-centric taxonomy. We introduce a novel taxonomy that offers a thorough analysis from a methodological standpoint on TSFMs with the first shot, enabling a full understanding of the mechanism on why and how FMs can achieve admirable performance in time series data.\\n- Future research opportunities. We discuss and highlight future avenues for enhancing time series analysis using foundation models, urging researchers to delve deeper into this area.\\n\\n# 2 BACKGROUND\\n\\n# Foundation Models.\\n\\nAuthor names are listed in alphabetical order.\\n# A decoder-only foundation model for time-series forecasting\\n\\n# A PREPRINT\\n\\nIn contrast to previous surveys, this manuscript incorporates the most extensive array of time series data types (see Table 1), spatial time series, as well as other types such as the trajectory and event. We further summarize the developmental roadmap of current TSFMs in Figure 1, in order to foster further innovations and understanding in the dynamic and ever-evolving landscape of TSFMs. In short, our major contributions lie in three aspects:\\n\\n# 4. Data Perspective\\n\\nIn this section, we explore advancements in TSFMs from various data perspectives: standard time series, spatial time series, and others. We further categorize our discussion within each subsection into task-oriented or general-purpose foundation models.\\n\\n# 4.1 Standard Time Series\\n\\nStandard time series possess diverse properties, including varying sampling rates and temporal patterns, which pose significant challenges in developing relevant FMs. These models aim to identify universal patterns within extensive time series data from varied sources, either to enhance specific tasks or for broad analysis.\\n\\nQuestion:\\nAccording to the paper \\'Foundation Models for Time Series Analysis: A Tutorial and Survey\\', what is the primary difference between the proposed taxonomy and previous taxonomies?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nPre-training from scratch can be expensive, which has spurred the development of alternative approaches that leverage pre-trained models from other domains, such as large language, vision, and acoustic models. For instance, LLM4TS [11] and TEMPO [10] successfully perform time series forecasting across various datasets by fine-tuning GPT-2 [74] backbones, predicated on the notion that LLMs can be adapted to process non-linguistic datasets by activating their inherent capabilities. Similarly, Voice2Series [105] engages in the synchronization of time series and acoustic data to harness the classification prowess of an acoustic model for time series data.\\n# Foundation Models for Time Series Analysis: A Tutorial and Survey\\n\\n# KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\\n\\nOur contributions are significant in two key aspects. First, we show that existing language model architectures are capable of performing forecasting without time-series-specific customizations. This paves the way for accelerated progress by leveraging developments in the area of LLMs and through better data strategies. Second, on a practical level, the strong performance of Chronos models suggests that large (by forecasting standards) pretrained language models can greatly simplify forecasting pipelines without sacrificing accuracy, offering an inference-only alternative to the conventional approach involving training and tuning a model on individual tasks.\\n\\n# Acknowledgements\\n\\nthat demand substantial computational resources and time for inference. Recent concurrent work (Dooley et al., 2023; Das et al., 2023; Rasul et al., 2023; Woo et al., 2024) also explores pretraining transformer-based models with sophisticated time-series-specific designs on a large corpus of real and (or) synthetic time series data.\\n\\n. Additionally, such a model would need to support forecasting with varying history lengths (context), prediction lengths (horizon) and time granularities. Furthermore, unlike the huge volume of public text data for pretraining language models, vast amounts of time-series data is not readily available. In spite of these issues, we provide evidence to answer the above question in the affirmative.\\n\\nIn general, self-supervised pre-training enables foundation models to exploit the vast amounts of unlabeled time series data, providing generic temporal knowledge that can be further fine-tuned for specific downstream tasks. Compared with fully-supervised pre-training, it provides a more generic and realistic solution for the acquisition of a time series foundation model.\\n\\nNote that the above pre-training methods typically build the model from scratch and obtain the universal knowledge from data with the same modality (i.e., time series). Nevertheless, recent advancements in time series research have heightened the usage of LLMs [10, 11, 19, 36, 38, 47, 58, 62, 63, 81, 87, 100, 102, 103, 108, 120], VLMs [96], and AMs [105] that are pre-trained from other data modalities (text sequence, image-text sequence, acoustic signals).\\n# 5.2.2 Adaptation\\n\\nQuestion:\\nWhat is the main advantage of using pre-trained models from other domains, such as large language, vision, and acoustic models, in time series analysis?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Foundation Models for Time Series Analysis: A Tutorial and Survey\\n\\n# Authors\\n\\nYuxuan Liang, The Hong Kong University of Science and Technology (Guangzhou), yuxliang@outlook.com\\n\\nHaomin Wen, Beijing Jiao Tong University & HKUST(GZ), wenhaomin@bjtu.edu.cn\\n\\nYuqi Nie, Princeton University, ynie@princeton.edu\\n\\nYushan Jiang, University of Connecticut, yushan.jiang@uconn.edu\\n\\nMing Jin, Monash University, ming.jin@monash.edu\\n\\nDongjin Song, University of Connecticut, dongjin.song@uconn.edu\\n\\nShirui Pan, Griffith University, s.pan@griffith.edu.au\\n\\nQingsong Wen, Squirrel AI, USA, qingsongedu@gmail.com\\n\\n# ABSTRACT\\n\\n- Comprehensive and up-to-date survey. We offer a comprehensive and up-to-date survey on foundation models for a wide spectrum of time series, encompassing standard time series, spatial time series, and other types (i.e., trajectories and events).\\n- Novel methodology-centric taxonomy. We introduce a novel taxonomy that offers a thorough analysis from a methodological standpoint on TSFMs with the first shot, enabling a full understanding of the mechanism on why and how FMs can achieve admirable performance in time series data.\\n- Future research opportunities. We discuss and highlight future avenues for enhancing time series analysis using foundation models, urging researchers to delve deeper into this area.\\n\\n# 2 BACKGROUND\\n\\n# Foundation Models.\\n\\n. Under the assumption that diversity in the pre-training data is beneficial for foundation model pre-training (Brown et al., 2020b), pre-training a single time series model on a diverse combination of multiple datasets from multiple domains is beneficial to the foundation model‚Äôs zero-shot and few-shot adaptation performance.\\n\\n# 3. Taxonomy\\n\\nThe proposed taxonomy is illustrated in Figure 3, and the related works can be found in Table 2. The proposed taxonomy unfolds a structured and comprehensive classification to enhance the understanding of foundation models on time series analysis. It is organized into four hierarchical levels, starting with the data category, followed by the model architecture, pre-training techniques, and finally, the application domain. Unlike previous taxonomies, ours distinguishes itself by delving deeper into the foundation models from the methodology perspective, with a keen focus on their architectural designs, pre-training, and adaptation techniques. This method-centric view is pivotal for researchers, providing valuable insights into the mechanisms of why and how foundation models show great potential for time series analysis.\\n\\n# 4. Data Perspective\\n\\nDespite the promising prospects and rapid development of TSFMs, a systematic analysis of TSFMs from a methodological standpoint has been notably absent in prior literature. Existing studies, as depicted in Table 1, have concentrated on either the data perspective [48] or the pipeline perspective [45] of TSFMs. To bridge this gap, this survey aims to provide a comprehensive methodological analysis of foundation models for learning a variety of time series. This examination will center on scrutinizing their model architectures, pre-training techniques, adaptation methods, and data modalities. Through this endeavor, we seek to illuminate an overall picture of core elements in TSFMs, thereby enhancing comprehension regarding the rationale behind their efficacy and the mechanisms driving their substantial potential in time series analysis.\\n\\nQuestion:\\nAccording to the paper \\'Foundation Models for Time Series Analysis: A Tutorial and Survey\\', what is the primary benefit of using multi-modal time series analysis?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nOur contributions are significant in two key aspects. First, we show that existing language model architectures are capable of performing forecasting without time-series-specific customizations. This paves the way for accelerated progress by leveraging developments in the area of LLMs and through better data strategies. Second, on a practical level, the strong performance of Chronos models suggests that large (by forecasting standards) pretrained language models can greatly simplify forecasting pipelines without sacrificing accuracy, offering an inference-only alternative to the conventional approach involving training and tuning a model on individual tasks.\\n\\n# Acknowledgements\\n\\nTIME-LLM shows promise in adapting frozen large language models for time series forecasting by reprogramming time series data into text prototypes more natural for LLMs and providing natural language guidance via Prompt-as-Prefix to augment reasoning. Evaluations demonstrate the adapted LLMs can outperform specialized expert models, indicating their potential as effective time series machines. Our results also provide a novel insight that time series forecasting can be cast as yet another ‚Äúlanguage‚Äù task that can be tackled by an off-the-shelf LLM to achieve state-of-the-art performance through our Time-LLM framework. Further research should explore optimal reprogramming representations, enrich LLMs with explicit time series knowledge through continued pre-training, and build towards multimodal models with joint reasoning across time series, natural language, and other modalities\\n\\n# Easy optimization\\n\\nLLMs are trained once on massive computing and then can be applied to forecasting tasks without learning from scratch. Optimizing existing forecasting models often requires significant architecture search and hyperparameter tuning (Zhou et al., 2023b).\\n\\nIn summary, LLMs offer a promising path to make time series forecasting more general, efficient, synergistic, and accessible compared to current specialized modeling paradigms. Thus, adapting these powerful models for time series data can unlock significant untapped potential.\\n\\n# TIME-LLM: TIME SERIES FORECASTING BY REPROGRAMMING LARGE LANGUAGE MODELS\\n\\n# Ming Jin1‚àó, Shiyu Wang2‚àó, Lintao Ma2, Zhixuan Chu2, James Y. Zhang2, Xiaoming Shi2, Pin-Yu Chen3, Yuxuan Liang6, Yuan-Fang Li4, Shirui Pan4‚Ä†, Qingsong Wen5‚Ä†\\n\\n# Griffith University1, Alibaba Group5, Monash University1, Ant Group2, IBM Research3, The Hong Kong University of Science and Technology (Guangzhou)6\\n\\n{ming.jin, yuanfang.li}@monash.edu, pin-yu.chen@ibm.com, yuxliang@outlook.com, s.pan@griffith.edu.au, qingsongedu@gmail.com\\n\\n{weiming.wsy, lintao.mlt, chuzhixuan.czx, james.z, peter.sxm}@antgroup.com\\n\\n# ABSTRACT\\n\\n- We introduce a novel concept of reprogramming large language models for time series forecasting without altering the pre-trained backbone model. In doing so, we show that forecasting can be cast as yet another ‚Äúlanguage‚Äù task that can be effectively tackled by an off-the-shelf LLM.\\n- We propose a new framework, TIME-LLM, which encompasses reprogramming the input time series into text prototype representations that are more natural for the LLM, and augmenting the input context with declarative prompts (e.g., domain expert knowledge and task instructions) to guide LLM reasoning. Our technique points towards multimodal foundation models excelling in both language and time series.\\n\\nQuestion:\\nAccording to the abstract of the paper \\'TimeLLM\\', what are the benefits of leveraging large language models (LLMs) for time series forecasting?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Input statistics:\\n\\n- &lt;time series statistic 1&gt;\\n- &lt;time series statistic 2&gt; ‚Ä¶\\n\\n# Frozen Training\\n\\n# Prompt Embeddings\\n\\n# Patch Embeddings\\n\\n# Forward\\n\\n# Backward\\n\\nFigure 2: The model framework of TIME-LLM.\\nGiven an input time series, we first tokenize and embed it via 1 patching along with a 2 customized embedding layer. 3 These patch embeddings are then reprogrammed with condensed text prototypes to align two modalities. To augment the LLM‚Äôs reasoning ability, 4 additional prompt prefixes are added to the input to direct the transformation of input patches. 5 The output patches from the LLM are projected to generate the forecasts.\\n\\n# Table 9: An overview of the experimental configurations for TIME-LLM. ‚ÄúLTF‚Äù and ‚ÄúSTF‚Äù denote long-term and short-term forecasting, respectively.\\n\\n# Proposed Framework\\n\\nIn this work, we propose TIME-LLM, a reprogramming framework to adapt large language models for time series forecasting while keeping the backbone model intact. The core idea is to reprogram the input time series into text prototype representations that are more naturally suited to language models‚Äô capabilities. To further augment the model‚Äôs reasoning about time series concepts, we introduce Prompt-as-Prefix (PaP), a novel idea in enriching the input time series with additional context and providing task instructions in the modality of natural language. This provides declarative guidance about desired transformations to apply to the reprogrammed input. The output of the language model is then projected to generate time series forecasts.\\n\\n# Table 7: Efficiency analysis of TIME-LLM on ETTh1 in forecasting different steps ahead.\\n\\n|Length| |ETTh1-96| | |ETTh1-192| |ETTh1-336|ETTh1-512| | | | | | | |\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|Metric|Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|D.1 LLama (32)|3404.53|32136|0.517| |3404.57|33762|0.582| |3404.62|37988|0.632| |3404.69|39004|0.697|\\n|D.2 LLama (8)|975.83|11370|0.184| |975.87|12392|0.192| |975.92|13188|0.203| |976.11|13616|0.217|\\n|D.3 w/o LLM|6.39|3678|0.046| |6.42|3812|0.087| |6.48|3960|0.093| |6.55|4176|0.129|\\n\\nticipated as external knowledge can be naturally incorporated via prompting to facilitate the learning and inference. Additionally, providing the LLM with clear task instructions and input context (e.g., dataset captioning) is also beneficial (i.e., C.2 and C.1; eliciting over 7.7% and 9.6%, respectively).\\n\\n- TIME-LLM consistently exceeds state-of-the-art performance in mainstream forecasting tasks, especially in few-shot and zero-shot scenarios. Moreover, this superior performance is achieved while maintaining excellent model reprogramming efficiency. Thus, our research is a concrete step in unleashing LLMs‚Äô untapped potential for time series and perhaps other sequential data.\\n\\nQuestion:\\nWhat is the main idea behind the proposed framework TIME-LLM, as described in the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# (a) Patch Reprogramming\\n\\n# (b) Patch-as-Prefix and Prompt-as-Prefix\\n\\nFigure 3: Illustration of (a) patch reprogramming and (b) Patch-as-Prefix versus Prompt-as-Prefix.\\n\\nModalities that are identical or similar. Examples include repurposing a vision model to work with cross-domain images (Misra et al., 2023) or reprogramming an acoustic model to handle time series data (Yang et al., 2021). In both cases, there are explicit, learnable transformations between the source and target data, allowing for the direct editing of input samples. However, time series can neither be directly edited nor described losslessly in natural language, posing significant challenges to directly bootstrap the LLM for understanding time series without resource-intensive fine-tuning.\\n\\n# Patch Reprogramming.\\n\\nHere we reprogram patch embeddings into the source data representation space to align the modalities of time series and natural language to activate the backbone‚Äôs time series understanding and reasoning capabilities. A common practice is learning a form of ‚Äúnoise‚Äù that, when applied to target input samples, allows the pre-trained source model to produce the desired target outputs without requiring parameter updates. This is technically feasible for bridging data.\\n# Published as a conference paper at ICLR 2024\\n\\n# 0.6\\n\\n|Source|Target|the next value is|Projection|\\n|---|---|---|---|\\n|Vocab.|Prototypes|0.6|Pre-trained LLM|\\n|time|Patch Embeddings!| |Pre-trained LLM (Body + LM Head)|\\n|late| | | |\\n|early|Patch 1| | |\\n|down|Patch 2| | |\\n|up|‚Ä¶| | |\\n|steady|Patch| |Pre-trained LLM|\\n|short|Reprogram| |Pre-trained LLM (Text Embedder)|\\n|long|Patch 5| |Reprogram|\\n\\n# (a) Patch Reprogramming\\n\\n# (b) Patch-as-Prefix and Prompt-as-Prefix\\n\\n# Input statistics:\\n\\n- &lt;time series statistic 1&gt;\\n- &lt;time series statistic 2&gt; ‚Ä¶\\n\\n# Frozen Training\\n\\n# Prompt Embeddings\\n\\n# Patch Embeddings\\n\\n# Forward\\n\\n# Backward\\n\\nFigure 2: The model framework of TIME-LLM.\\nGiven an input time series, we first tokenize and embed it via 1 patching along with a 2 customized embedding layer. 3 These patch embeddings are then reprogrammed with condensed text prototypes to align two modalities. To augment the LLM‚Äôs reasoning ability, 4 additional prompt prefixes are added to the input to direct the transformation of input patches. 5 The output patches from the LLM are projected to generate the forecasts.\\n\\n# 3.1 MODEL STRUCTURE\\n\\n# Input Embedding.\\n\\nEach input channel X(i) is first individually normalized to have zero mean and unit standard deviation via reversible instance normalization (RevIN) in mitigating the time series distribution shift (Kim et al., 2021). Then, we divide X(i) into several consecutive overlapped or non-overlapped patches (Nie et al., 2023) with length Lp; thus the total number of input patches is P = ‚åä (T ‚àíLp)‚åã + 2, where S denotes the horizontal sliding stride. The underlying motivations are two-fold: (1) better preserving local semantic information by aggregating local information into each patch and (2) serving as tokenization to form a compact sequence of input tokens, reducing computational burdens. Given these patches XP ‚àà RP √óLp, we embed them as XP ‚àà RP √ódm, adopting a simple linear layer as the patch embedder to create dimensions dm.\\n\\n# Patch Reprogramming.\\n\\n# Cross-modality Alignment\\n\\nOur results indicate that ablating either patch reprogramming or Prompt-as-Prefix hurts knowledge transfer in reprogramming the LLM for effective time series forecasting. In the absence of representation alignment, we observe a notable average performance degradation of 9.2%, which becomes more pronounced (exceeding 17%) in few-shot tasks. In TIME-LLM, the act of prompting stands as a pivotal element in harnessing the LLM‚Äôs capacity for understanding the inputs and tasks. Ablation of this component results in over 8% and 19% degradation in standard and few-shot forecasting tasks, respectively. We find that removing the input statistics hurts the most, resulting in an average increase of 10.2% MSE.\\n# Published as a conference paper at ICLR 2024\\n\\n# Table 6: Ablations on ETTh1 and ETTm1 in predicting 96 and 192 steps ahead (MSE reported). Red: the best.\\n\\nQuestion:\\nHow does the patch reprogramming process work in TIME-LLM, as described in Section 3.1 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Input statistics:\\n\\n- &lt;time series statistic 1&gt;\\n- &lt;time series statistic 2&gt; ‚Ä¶\\n\\n# Frozen Training\\n\\n# Prompt Embeddings\\n\\n# Patch Embeddings\\n\\n# Forward\\n\\n# Backward\\n\\nFigure 2: The model framework of TIME-LLM.\\nGiven an input time series, we first tokenize and embed it via 1 patching along with a 2 customized embedding layer. 3 These patch embeddings are then reprogrammed with condensed text prototypes to align two modalities. To augment the LLM‚Äôs reasoning ability, 4 additional prompt prefixes are added to the input to direct the transformation of input patches. 5 The output patches from the LLM are projected to generate the forecasts.\\n\\nPrompt engineering is more specialized in LLM-based TSFMs. The prompt can be handcrafted with task-specific textual input and directly used to query the output for downstream prediction [87, 102] or intermediate embedding as feature enhancement [103]. Besides, the prompt can also be parameterized vectors and end-to-end learnable when optimizing the model on target datasets [10, 83]. In comparison to static prompts, the use of trainable prompts enhances the ability of LLMs to comprehend and match the context of given time series inputs. For example, TEMPO [10] constructs a trainable prompt pool with distinct key-value pairs, and retrieves the most representative prompt candidates with the highest similarity scores.\\n\\n# Table 7: Efficiency analysis of TIME-LLM on ETTh1 in forecasting different steps ahead.\\n\\n|Length| |ETTh1-96| | |ETTh1-192| |ETTh1-336|ETTh1-512| | | | | | | |\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|Metric|Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|D.1 LLama (32)|3404.53|32136|0.517| |3404.57|33762|0.582| |3404.62|37988|0.632| |3404.69|39004|0.697|\\n|D.2 LLama (8)|975.83|11370|0.184| |975.87|12392|0.192| |975.92|13188|0.203| |976.11|13616|0.217|\\n|D.3 w/o LLM|6.39|3678|0.046| |6.42|3812|0.087| |6.48|3960|0.093| |6.55|4176|0.129|\\n\\nticipated as external knowledge can be naturally incorporated via prompting to facilitate the learning and inference. Additionally, providing the LLM with clear task instructions and input context (e.g., dataset captioning) is also beneficial (i.e., C.2 and C.1; eliciting over 7.7% and 9.6%, respectively).\\n\\n‚Ä† H represents the forecasting horizon of the M4 and M3 datasets.\\n\\n* LR means the initial learning rate.\\n# HYPERPARAMETER SENSITIVITY\\n\\nWe conduct a hyperparameter sensitivity analysis focusing on the four important hyperparameters within TIME-LLM: namely, the number of backbone model layers, the number of text prototypes V ‚Ä≤, the time series input length T, and the number of patch reprogramming cross-attention heads K. The correlated results can be found in Fig. 6. From our analysis, we derive the following observations:\\n\\nWe provide additional technical details of TIME-LLM in three aspects: (1) the learning of text prototypes, (2) the calculation of trends and lags in time series for use in prompts, and (3) the implementation of the output projection. To identify a small set of text prototypes E‚Ä≤ ‚àà RV ‚Ä≤√óD from E ‚àà RV √óD, we learn a matrix W ‚àà RV ‚Ä≤√óV as the intermediary. To describe the overall time series trend in natural language, we calculate the sum of differences between consecutive time steps. A sum greater than 0 indicates an upward trend, while a lesser sum denotes a downward trend. In addition, we calculate the top-5 lags of the time series, identified by computing the autocorrelation using fast Fourier transformation and selecting the five lags with the highest correlation values. After we pack and feedforward the prompt and patch embeddings O(i) ‚àà RP √óD through the frozen LLM, we discard the prefixal part and obtain the output representations, denoted as Oi ‚àà RP √óD\\n\\nQuestion:\\nWhat are the three pivotal components for constructing effective prompts in TIME-LLM, as described in the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Table 7: Efficiency analysis of TIME-LLM on ETTh1 in forecasting different steps ahead.\\n\\n|Length| |ETTh1-96| | |ETTh1-192| |ETTh1-336|ETTh1-512| | | | | | | |\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|Metric|Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|D.1 LLama (32)|3404.53|32136|0.517| |3404.57|33762|0.582| |3404.62|37988|0.632| |3404.69|39004|0.697|\\n|D.2 LLama (8)|975.83|11370|0.184| |975.87|12392|0.192| |975.92|13188|0.203| |976.11|13616|0.217|\\n|D.3 w/o LLM|6.39|3678|0.046| |6.42|3812|0.087| |6.48|3960|0.093| |6.55|4176|0.129|\\n\\nticipated as external knowledge can be naturally incorporated via prompting to facilitate the learning and inference. Additionally, providing the LLM with clear task instructions and input context (e.g., dataset captioning) is also beneficial (i.e., C.2 and C.1; eliciting over 7.7% and 9.6%, respectively).\\n\\n¬∂This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nas in LLaMA (Touvron et al., 2023). After passing through the causally masked transformer layers, the model predicts the parameters œï of the forecast log prob distribution of the next timestep, where the parameters are output by a parametric distribution head, as described in Sec. 4.3. The negative log-likelihood of the predicted distribution of all predicted timesteps is minimized.\\n\\n# Table 9: An overview of the experimental configurations for TIME-LLM. ‚ÄúLTF‚Äù and ‚ÄúSTF‚Äù denote long-term and short-term forecasting, respectively.\\n\\n# Input statistics:\\n\\n- &lt;time series statistic 1&gt;\\n- &lt;time series statistic 2&gt; ‚Ä¶\\n\\n# Frozen Training\\n\\n# Prompt Embeddings\\n\\n# Patch Embeddings\\n\\n# Forward\\n\\n# Backward\\n\\nFigure 2: The model framework of TIME-LLM.\\nGiven an input time series, we first tokenize and embed it via 1 patching along with a 2 customized embedding layer. 3 These patch embeddings are then reprogrammed with condensed text prototypes to align two modalities. To augment the LLM‚Äôs reasoning ability, 4 additional prompt prefixes are added to the input to direct the transformation of input patches. 5 The output patches from the LLM are projected to generate the forecasts.\\n\\n**Table 5: MAE for ETT datasets for prediction horizons 96 and 192. Owing to expensive evaluations for llmtime, the results are reported on the last test window of the original test split.**\\n|Dataset|llmtime(ZS)*|PatchTST|PatchTST(ZS)|FEDFormer|AutoFormer|Informer|TimesFM(ZS)|\\n|---|---|---|---|---|---|---|---|\\n|ETTh1 (horizon=96)|0.42|0.41|0.39|0.58|0.55|0.76|0.45|\\n|ETTh1 (horizon=192)|0.50|0.49|0.50|0.64|0.64|0.78|0.53|\\n|ETTh2 (horizon=96)|0.33|0.28|0.37|0.67|0.65|1.94|0.35|\\n|ETTh2 (horizon=192)|0.70|0.68|0.59|0.82|0.82|2.02|0.62|\\n|ETTm1 (horizon=96)|0.37|0.33|0.24|0.41|0.54|0.71|0.19|\\n|ETTm1 (horizon=192)|0.71|0.31|0.26|0.49|0.46|0.68|0.26|\\n|ETTm2 (horizon=96)|0.29|0.23|0.22|0.36|0.29|0.48|0.24|\\n|ETTm2 (horizon=192)|0.31|0.25|0.22|0.25|0.30|0.51|0.27|\\n|Avg|0.45|0.37|0.35|0.53|0.53|0.99|0.36|\\n\\n# A.6 More Details on Models\\n\\nWe now present implementation details about TimesFM and other baselines.\\n\\n# TimesFM\\n\\nQuestion:\\nWhat is the significance of the output projection step in TIME-LLM, as described in Section 3.1 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nPreprint\\n\\nRamin Ghorbani1‚àó, Marcel J.T. Reinders1, and David M.J. Tax1\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nOur task is to determine if a given xi,t shows any anomalous behavior or not.\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Data\\n\\n|Input|Embedding|Encoder Layer 1|Encoder Layer 2|RBF Layer|Encoder Layer 3|Fully Connected Layer|Output Layer|\\n|---|---|---|---|---|---|---|---|\\n|Multi-Head Attention (8x)|Dropout|+|Norm|Feed-Forward|Norm|+| |\\n\\nFigure 2: Overview of the proposed RESTAD model. Here, the RBF layer is added after the second encoder layer.\\n\\n# Model Score\\n\\nRESTAD score(xi,t) = œµr √ó œµs (2)\\n\\nand œµs = ||xi,t PM=1 ||2 represents the reconstruction error, where œµr = 1 ‚àí M1 ‚àí ÀÜi,tzm,t measures dissimilarity. This combination highlights subtle anomalies characterized by both low reconstruction errors and RBF scores, as well as significant anomalies with high reconstruction errors or low RBF scores.\\n\\n# Initialization of RBF Layer Parameters\\n\\n# Evaluation\\n\\nAnomaly scores (Eq. 2) exceeding a threshold Œ¥ are identified as anomalies. Performance is evaluated using the F1-score for threshold-dependent evaluation. Here, we follow [17] by setting Œ¥ to label a predefined proportion of data points as anomalies (0.5% for SMD, 1% for others). For threshold-independent analysis, we use AUC-ROC, AUC-PR, VUS-ROC, and VUS-PR metrics [21]. We exclude the point-adjustment method [22] due to its overestimation [23]. Our model is compared against baselines and state-of-the-arts models: LSTM [9], vanilla Transformer [17], USAD [13], PatchAD [12], AnomalyTrans [17], and DCdetector [10].\\n\\n1 https://github.com/Raminghorbanii/RESTAD\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Table 1: Performance metrics of baselines and RESTAD on test sets.\\n\\nInitialization methods are denoted as (R) for Random and (K) for K-means. For all measures, a higher value indicates better anomaly detection performance.\\n\\nTo overcome the challenges of scoring based on reconstruction error and the limitations of the association discrepancy method, we propose combining the reconstruction error with a specialized non-linear transformation like the Radial Basis.\\n\\n*Corresponding Author: r.ghorbani@tudelft.nl\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n|Input|Transformer Layer|RBF Layer|Transformer Layer|Reconstructed Signal|\\n|---|---|---|---|---|\\n|Original Signal|Significant Anomaly|RBFCenter|Detected Anomaly|Normal Data Point|\\n|Reconstructed Signal|Subtle Anomaly|Missed Anomaly|Subtle Anomaly|Reconstruction Score|\\n|Threshold|Dis-Similarity Score|t0|t1|Time|\\n\\n# Figure 1: Comparison of traditional reconstruction and RBF-enhanced anomaly detection\\n\\n(a) Original signal with subtle and significant anomalies compared to its reconstruction.\\n\\n(b) Reconstruction errors for the signals in (a), highlighting challenges in detecting subtle anomalies.\\n\\nQuestion:\\nAccording to the abstract of the paper \\'RESTAD\\', what is the primary limitation of using reconstruction error for anomaly detection in time series data?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n(a) Original signal with subtle and significant anomalies compared to its reconstruction.\\n\\n(b) Reconstruction errors for the signals in (a), highlighting challenges in detecting subtle anomalies.\\n\\n(c) Visualization of a model integrated with an RBF, shown via a 2D scatter plot that includes typical data, subtle and significant anomalies, and the RBF center with its influence radius, showing the RBF‚Äôs ability to differentiate typical points from anomalies.\\n\\n(d) Enhanced anomaly score using the RBF, which shows improved detection of subtle anomalies.\\n\\n# 2.1 RESTAD Framework\\n\\nIn our study, we incorporate the anomaly detection mechanism into the vanilla Transformer through a specific layer of RBF neurons, see Figure 1.c. This RBF layer operates on the latent representations from the preceding layer, denoted by Hi = hi,t t=1, where hi,t ‚àà Rdh.\\n\\nOur task is to determine if a given xi,t shows any anomalous behavior or not.\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Data\\n\\n|Input|Embedding|Encoder Layer 1|Encoder Layer 2|RBF Layer|Encoder Layer 3|Fully Connected Layer|Output Layer|\\n|---|---|---|---|---|---|---|---|\\n|Multi-Head Attention (8x)|Dropout|+|Norm|Feed-Forward|Norm|+| |\\n\\nFigure 2: Overview of the proposed RESTAD model. Here, the RBF layer is added after the second encoder layer.\\n\\n# Model Score\\n\\nRESTAD score(xi,t) = œµr √ó œµs (2)\\n\\nand œµs = ||xi,t PM=1 ||2 represents the reconstruction error, where œµr = 1 ‚àí M1 ‚àí ÀÜi,tzm,t measures dissimilarity. This combination highlights subtle anomalies characterized by both low reconstruction errors and RBF scores, as well as significant anomalies with high reconstruction errors or low RBF scores.\\n\\n# Initialization of RBF Layer Parameters\\n\\n. However, the optimal number of RBF centers is data-dependent. These findings motivate future studies for the exploration of integrating RBF layers into other deep learning architectures for anomaly detection tasks.\\n\\n# Figure 5: RESTAD Performance with varying RBF layer\\n\\n# Figure 6: RESTAD mean performance across varying numbers of RBF centers.\\n\\nShaded areas indicate ¬± standard deviation, illustrating variability across multiple runs.\\n\\n# Discussion and Conclusion\\n\\n# AUC-ROC Metric Value\\n\\n**Table 2: Effect of integrating RBF layer and the choice of anomaly score. For all measures, a higher value indicates better anomaly detection performance.**\\n|Architecture|Anomaly Criterion|SMD|SMD|SMD|MSL|MSL|MSL|PSM|PSM|PSM|\\n|---|---|---|---|---|\\n|Transformer|œµr|0.11|0.75|0.19|0.80|0.22|0.06|0.56|0.14|0.63|\\n|RESTAD|œµr|0.11|0.77|0.18|0.81|0.21|0.07|0.63|0.16|0.69|\\n|RESTAD|œµr œµs œµs|0.01|0.44|0.03|0.52|0.07|0.01|0.43|0.08|0.48|\\n|RESTAD|œµr √ó œµs|0.23|0.78|0.23|0.82|0.24|0.07|0.68|0.18|0.72|\\n\\n# AUC-ROC\\n\\n# SMD Dataset\\n\\n# MSL Dataset\\n\\n# PSM Dataset\\n\\n# Figure 4: Effect of our composite anomaly score (œµr √ó œµs) compared to reconstruction error (œµr) across segments of all datasets.\\n\\nThe highlighted regions in red indicate the true anomaly periods (labeled by an expert).\\n\\n# Figure 5: RESTAD Performance with varying RBF layer\\n\\n# Figure 6: RESTAD mean performance across varying numbers of RBF centers.\\n\\nQuestion:\\nWhat is the role of the RBF layer in the RESTAD model, and how does it contribute to anomaly detection?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Evaluation\\n\\nAnomaly scores (Eq. 2) exceeding a threshold Œ¥ are identified as anomalies. Performance is evaluated using the F1-score for threshold-dependent evaluation. Here, we follow [17] by setting Œ¥ to label a predefined proportion of data points as anomalies (0.5% for SMD, 1% for others). For threshold-independent analysis, we use AUC-ROC, AUC-PR, VUS-ROC, and VUS-PR metrics [21]. We exclude the point-adjustment method [22] due to its overestimation [23]. Our model is compared against baselines and state-of-the-arts models: LSTM [9], vanilla Transformer [17], USAD [13], PatchAD [12], AnomalyTrans [17], and DCdetector [10].\\n\\n1 https://github.com/Raminghorbanii/RESTAD\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Table 1: Performance metrics of baselines and RESTAD on test sets.\\n\\nInitialization methods are denoted as (R) for Random and (K) for K-means. For all measures, a higher value indicates better anomaly detection performance.\\n\\n# Figure 5: RESTAD Performance with varying RBF layer\\n\\n# Figure 6: RESTAD mean performance across varying numbers of RBF centers.\\n\\nShaded areas indicate ¬± standard deviation, illustrating variability across multiple runs.\\n\\n# Discussion and Conclusion\\n\\n# 4 Results\\n\\n# 4.1 Ablation Analysis\\n\\nOur empirical results, as detailed in Table 1, highlight the effectiveness of the RESTAD for anomaly detection. The ablation experiments are based on the RBF layer with random initialization. This decision is based on our findings that random initialization is as effective as the K-means strategy (see Table 1), while offering greater simplicity and computational efficiency. While there are slight performance differences between initialization methods, these variations are not significant enough to establish the superiority of one method over another.\\n\\nWe report the standard F1 and F1-scores after point adjustment (F1PA) following the protocol in Kim et al. (2022). Our method outperforms all existing methods with F1.\\n\\n| |SWaT|SWaT|WADI|WADI|MSL|MSL|SMAP|SMAP|SMD|SMD|\\n|---|---|---|---|---|---|\\n| |F1|F1PA|F1|F1PA|F1|F1PA|F1|F1PA|F1|F1PA|\\n|DAGMM (2018)|0.550|0.853|0.121|0.209|0.199|0.701|0.333|0.712|0.238|0.723|\\n|LSTM-VAE (2018)|0.775|0.805|0.227|0.380|0.212|0.678|0.235|0.756|0.435|0.808|\\n|OmniAnomaly (2019)|0.782|0.866|0.223|0.417|0.207|0.899|0.227|0.805|0.474|0.944|\\n|MSCRED (2019)|0.662|0.868|0.087|0.346|0.199|0.775|0.232|0.945|0.097|0.389|\\n|THOC (2020)|0.612|0.880|0.130|0.506|0.190|0.891|0.240|0.781|0.168|0.541|\\n|USAD (2020)|0.791|0.846|0.232|0.429|0.211|0.927|0.228|0.818|0.426|0.938|\\n|GDN (2021)|0.808|0.935|0.569|0.855|0.217|0.903|0.252|0.708|0.529|0.716|\\n|AnomalyBERT (Ours)|0.854|0.925|0.580|0.798|0.302|0.585|0.457|0.914|0.535|0.830|\\n\\n# Table 3: Results of ablation studies on combinations of the synthetic outlier types.\\n\\n# Table 4: Diagnosis Performance.\\n\\n|Method|SMD|SMD|SMD|SMD|MSDS|MSDS|MSDS|MSDS|\\n|---|---|---|\\n| |H@100%|H@150%|N@100%|N@150%|H@100%|H@150%|N@100%|N@150%|\\n|MERLIN|0.5907|0.6177|0.4150|0.4912|0.3816|0.5626|0.3010|0.3947|\\n|LSTM-NDT|0.3808|0.5225|0.3603|0.4451|0.1504|0.2959|0.1124|0.1993|\\n|DAGMM|0.4927|0.6091|0.5169|0.5845|0.2617|0.4333|0.3153|0.4154|\\n|OmniAnomaly|0.4567|0.5652|0.4545|0.5125|0.2839|0.4365|0.3338|0.4231|\\n|MSCRED|0.4272|0.5180|0.4609|0.5164|0.2322|0.3469|0.2297|0.2962|\\n|MAD-GAN|0.4630|0.5785|0.4681|0.5522|0.3856|0.5589|0.4277|0.5292|\\n|USAD|0.4925|0.6055|0.5179|0.5781|0.3095|0.4769|0.3534|0.4515|\\n|MTAD-GAT|0.3493|0.4777|0.3759|0.4530|0.5812|0.5885|0.5926|0.6522|\\n|CAE-M|0.4707|0.5878|0.5474|0.6178|0.2530|0.4171|0.2047|0.3010|\\n|GDN|0.3143|0.4386|0.2980|0.3724|0.2276|0.3382|0.2921|0.3570|\\n|TranAD|0.4981|0.6401|0.4941|0.6178|0.4630|0.7533|0.5981|0.6963|\\n\\n# Table 5: Comparison of training times in seconds per epoch.\\n\\nQuestion:\\nAccording to Table 1, what is the F1-score of the RESTAD model with random initialization on the SMD dataset?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 5.1 Architecture\\n\\nAs shown in Figure 4, we first delve into the architecture of TSFMs, including Transformer-based models, non-Transformer-based models and diffusion-based models, focusing on the underlying mechanisms that shape their capabilities, as well as how they could be applied on various time series.\\n\\n# 5.1.1 Transformer-based Models\\n\\n. However, the optimal number of RBF centers is data-dependent. These findings motivate future studies for the exploration of integrating RBF layers into other deep learning architectures for anomaly detection tasks.\\n\\n# Figure 2: An overview of our framework.\\n\\nWe design a Transformer-based model and a self-supervised training strategy. In the training stage, a portion of an input window is randomly replaced and the model is directed to classify the degraded part. The main Transformer body is composed of Transformer layers with 1D relative position bias.\\n\\n# 3 METHOD\\n\\n# 3.1 OVERALL ARCHITECTURE\\n\\n# 3.2 Implementation\\n\\nRESTAD Model: The RESTAD model is an adaptation of a vanilla Transformer, incorporating an RBF kernel layer as detailed in Figure 2. It includes a DataEmbedding module that combines both token and positional embeddings, followed by an encoder with three layers. Each layer includes a multi-head self-attention mechanism and feed-forward networks. The model has a latent dimension of 32, an intermediate feed-forward network layer with a dimension of 128, and 8 attention heads. The RBF layer is placed after the second encoder layer (other placements are also possible, see section 4.1). Optimization is performed using the ADAM optimizer, and hyperparameters are determined through systematic search to optimize reconstruction task performance. Additional hyperparameter details are available in our code repository1.\\n\\n# Evaluation\\n\\nTo overcome the challenges of scoring based on reconstruction error and the limitations of the association discrepancy method, we propose combining the reconstruction error with a specialized non-linear transformation like the Radial Basis.\\n\\n*Corresponding Author: r.ghorbani@tudelft.nl\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n|Input|Transformer Layer|RBF Layer|Transformer Layer|Reconstructed Signal|\\n|---|---|---|---|---|\\n|Original Signal|Significant Anomaly|RBFCenter|Detected Anomaly|Normal Data Point|\\n|Reconstructed Signal|Subtle Anomaly|Missed Anomaly|Subtle Anomaly|Reconstruction Score|\\n|Threshold|Dis-Similarity Score|t0|t1|Time|\\n\\n# Figure 1: Comparison of traditional reconstruction and RBF-enhanced anomaly detection\\n\\n(a) Original signal with subtle and significant anomalies compared to its reconstruction.\\n\\n(b) Reconstruction errors for the signals in (a), highlighting challenges in detecting subtle anomalies.\\n\\nQuestion:\\nWhat is the effect of integrating the RBF layer into the Transformer model, as shown in Figure 4?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Figure 5: RESTAD Performance with varying RBF layer\\n\\n# Figure 6: RESTAD mean performance across varying numbers of RBF centers.\\n\\nShaded areas indicate ¬± standard deviation, illustrating variability across multiple runs.\\n\\n# Discussion and Conclusion\\n\\n# 4 Results\\n\\n# 4.1 Ablation Analysis\\n\\nOur empirical results, as detailed in Table 1, highlight the effectiveness of the RESTAD for anomaly detection. The ablation experiments are based on the RBF layer with random initialization. This decision is based on our findings that random initialization is as effective as the K-means strategy (see Table 1), while offering greater simplicity and computational efficiency. While there are slight performance differences between initialization methods, these variations are not significant enough to establish the superiority of one method over another.\\n\\n# 3.2 Implementation\\n\\nRESTAD Model: The RESTAD model is an adaptation of a vanilla Transformer, incorporating an RBF kernel layer as detailed in Figure 2. It includes a DataEmbedding module that combines both token and positional embeddings, followed by an encoder with three layers. Each layer includes a multi-head self-attention mechanism and feed-forward networks. The model has a latent dimension of 32, an intermediate feed-forward network layer with a dimension of 128, and 8 attention heads. The RBF layer is placed after the second encoder layer (other placements are also possible, see section 4.1). Optimization is performed using the ADAM optimizer, and hyperparameters are determined through systematic search to optimize reconstruction task performance. Additional hyperparameter details are available in our code repository1.\\n\\n# Evaluation\\n\\n# Evaluation\\n\\nAnomaly scores (Eq. 2) exceeding a threshold Œ¥ are identified as anomalies. Performance is evaluated using the F1-score for threshold-dependent evaluation. Here, we follow [17] by setting Œ¥ to label a predefined proportion of data points as anomalies (0.5% for SMD, 1% for others). For threshold-independent analysis, we use AUC-ROC, AUC-PR, VUS-ROC, and VUS-PR metrics [21]. We exclude the point-adjustment method [22] due to its overestimation [23]. Our model is compared against baselines and state-of-the-arts models: LSTM [9], vanilla Transformer [17], USAD [13], PatchAD [12], AnomalyTrans [17], and DCdetector [10].\\n\\n1 https://github.com/Raminghorbanii/RESTAD\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Table 1: Performance metrics of baselines and RESTAD on test sets.\\n\\nInitialization methods are denoted as (R) for Random and (K) for K-means. For all measures, a higher value indicates better anomaly detection performance.\\n\\n# Results\\n\\nOur results are given in Tab. 18. We see that model reprogramming remarkably results in better efficiency compared to parameter-efficient fine-tuning (PEFT) with QLoRA on long-range.\\n\\n# Table 14: Full few-shot learning results on 10% training data\\n\\nWe use the same protocol as in Tab. 1.\\n\\nQuestion:\\nAccording to the discussion section, what is the primary reason for the significant performance gains of the RESTAD model?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Table 1: Examination of relationships between the typical types of outliers in Lai et al. (2021) and our proposed synthetic outliers.\\n\\n|Proposed|Typical| | | | |\\n|---|---|---|---|---|---|\\n|Global|Contextual|Shapelet|Seasonal|Trend| |\\n|Soft replacement|‚úî|‚úî|‚úî|‚úî|‚úî|\\n|Uniform replacement|‚úî|‚úî| |‚úî| |\\n|Length adjustment| | | |‚úî| |\\n|Peak noise| |‚úî| | | |\\n\\n# Figure 4: Qualitative results of AnomalyBERT on abnormal sequences from SWaT, WADI, and SMAP datasets.\\n\\nWe visualize an abnormal component where the outlier occurs and anomaly scores for each sequence. Without any post-processing, our method finds out diverse types of anomalies.\\n\\n# 4.4 MAIN RESULTS\\n\\nWe report the results of AnomalyBERT on the five real-world datasets introduced in Section 4.1, and compare them to those of the previous works. The reproduced evaluation scores are brought from Kim et al. (2022). In Table 2, we show that AnomalyBERT outperforms the previous methods on\\n# Presented at the ICLR 2023 workshop on Machine Learning for IoT: Datasets, Perception, and Understanding\\n\\n# Table 2: F1-scores for various anomaly detection methods and AnomalyBERT on five benchmark datasets.\\n\\nWe report the standard F1 and F1-scores after point adjustment (F1PA) following the protocol in Kim et al. (2022). Our method outperforms all existing methods with F1.\\n\\n. Our model converts multivariate data points into temporal representations with relative position bias and yields anomaly scores from these representations. Our method, AnomalyBERT, shows a great capability of detecting anomalies contained in complex time series and surpasses previous state-of-the-art methods on five real-world benchmarks. Our code is available at https://github.com/Jhryu30/AnomalyBERT.\\n\\n# B VISUAL RESULTS\\n\\nWe visualize the prediction results of AnomalyBERT and present the 2D projections of the original data and latent features in the model. Similar to Figure 1, we select three abnormal windows in each of SWaT, WADI, and SMAP test sets and fetch the corresponding anomaly scores after the score prediction process. We then project the original data points and the last latent features from the abnormal windows into 2D planes using t-SNE. As shown in Figure 5, our method distinguishes anomalies successfully and separates abnormal data points from normal points well.\\n\\n# SWaT\\n\\n# WADI\\n\\n# SMAP\\n\\n# Figure 5: Visualization of anomaly score predictions on abnormal sequences in SWaT, WADI, and SMAP datasets.\\n\\n# 5 CONCLUSION\\n\\nThis paper presents AnomalyBERT, a novel method for time series anomaly detection that uses a data degradation scheme to train a Transformer-based model in a self-supervised manner. We design an appropriate Transformer architecture with 1D relative position embeddings for temporal data and propose four types of synthetic outliers that can cover all typical types of anomalies. Exploiting the synthetic outliers in the training phase, our proposed model can learn to distinguish anomalous behavior. We finally show that our method outperforms previous works and has a strong capability in detecting real-world anomalies in complex time series. Our data degradation scheme has the potential to improve the model performance by revising degradation algorithms to mimic real-world anomalies naturally or mixing proper types of outliers according to data characteristics. Therefore, future studies could be demonstrated on detailed analysis of outlier synthesis processes.\\n\\nQuestion:\\nWhat is the main contribution of the AnomalyBERT paper, as described in the abstract?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# ANOMALYBERT: SELF-SUPERVISED TRANSFORMER FOR TIME SERIES ANOMALY DETECTION USING DATA DEGRADATION SCHEME\\n\\n# Yungi Jeong, Eunseok Yang, Jung Hyun Ryu, Imseong Park, Myungjoo Kang*\\n\\nNumerical Computing & Image Analysis Lab\\n\\nSeoul National University, Seoul, Republic of Korea\\n\\n{jyg9628, mayth24, jhryu30, parkis, mkang}@snu.ac.kr\\n\\n# ABSTRACT\\n\\n# 5 CONCLUSION\\n\\nThis paper presents AnomalyBERT, a novel method for time series anomaly detection that uses a data degradation scheme to train a Transformer-based model in a self-supervised manner. We design an appropriate Transformer architecture with 1D relative position embeddings for temporal data and propose four types of synthetic outliers that can cover all typical types of anomalies. Exploiting the synthetic outliers in the training phase, our proposed model can learn to distinguish anomalous behavior. We finally show that our method outperforms previous works and has a strong capability in detecting real-world anomalies in complex time series. Our data degradation scheme has the potential to improve the model performance by revising degradation algorithms to mimic real-world anomalies naturally or mixing proper types of outliers according to data characteristics. Therefore, future studies could be demonstrated on detailed analysis of outlier synthesis processes.\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nPreprint\\n\\nRamin Ghorbani1‚àó, Marcel J.T. Reinders1, and David M.J. Tax1\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\n# B VISUAL RESULTS\\n\\nWe visualize the prediction results of AnomalyBERT and present the 2D projections of the original data and latent features in the model. Similar to Figure 1, we select three abnormal windows in each of SWaT, WADI, and SMAP test sets and fetch the corresponding anomaly scores after the score prediction process. We then project the original data points and the last latent features from the abnormal windows into 2D planes using t-SNE. As shown in Figure 5, our method distinguishes anomalies successfully and separates abnormal data points from normal points well.\\n\\n# SWaT\\n\\n# WADI\\n\\n# SMAP\\n\\n# Figure 5: Visualization of anomaly score predictions on abnormal sequences in SWaT, WADI, and SMAP datasets.\\n\\n# 2 RELATED WORKS\\n\\n# Time Series Anomaly Detection\\n\\nQuestion:\\nWhat is the purpose of the data degradation scheme in AnomalyBERT, as described in Section 3.2?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Table 1: Examination of relationships between the typical types of outliers in Lai et al. (2021) and our proposed synthetic outliers.\\n\\n|Proposed|Typical| | | | |\\n|---|---|---|---|---|---|\\n|Global|Contextual|Shapelet|Seasonal|Trend| |\\n|Soft replacement|‚úî|‚úî|‚úî|‚úî|‚úî|\\n|Uniform replacement|‚úî|‚úî| |‚úî| |\\n|Length adjustment| | | |‚úî| |\\n|Peak noise| |‚úî| | | |\\n\\n# Figure 4: Qualitative results of AnomalyBERT on abnormal sequences from SWaT, WADI, and SMAP datasets.\\n\\nWe visualize an abnormal component where the outlier occurs and anomaly scores for each sequence. Without any post-processing, our method finds out diverse types of anomalies.\\n\\nIn Table 1, we examine the relationships between the typical outlier types and our synthetic outliers using a simplified version of our model and the sine wave dataset. We measure F1 and area under ROC curve (AUROC) for comparisons of the model performances, and consider that a synthetic outlier covers a typical outlier type (‚úî) if a model trained with the synthetic one achieves both F1 and AUROC over 0.9 on the sine wave including the corresponding typical outlier. As shown in Table 1, the soft replacement covers all typical types of outliers, and the uniform replacement and peak noise partially cover them. Meanwhile, the length adjustment only covers the seasonal outlier. From the simple experiment, we draw inspiration and get ideas for imitating anomalous behavior using synthetic outliers.\\n\\n# 4.4 MAIN RESULTS\\n\\n# Table 3: Results of ablation studies on combinations of the synthetic outlier types.\\n\\nWe show the impacts of synthetic outliers by comparing F1-scores on various experimental conditions.\\n\\n# (a) Experimental results on the soft replacement, uniform replacement, and peak noise on WADI dataset.\\n\\n|Soft replacement|Uniform replacement|Peak noise|F1|F1PA|\\n|---|---|---|---|---|\\n|√ó|√ó|√ó|0.580|0.798|\\n|√ó|√ó| |0.504|0.756|\\n|√ó| |√ó|0.556|0.770|\\n| |√ó|√ó|0.402|0.757|\\n| | |√ó|0.403|0.706|\\n| |√ó| |0.330|0.888|\\n\\n# (b) Experimental results on the length adjustment on SWaT dataset.\\n\\n|Length adjustment|F1|F1PA|\\n|---|---|---|\\n|√ó|0.854|0.925|\\n| |0.837|0.914|\\n\\n# (c) Experimental results on the length adjustment on WADI dataset.\\n\\n|Length adjustment|F1|F1PA|\\n|---|---|---|\\n| |0.433|0.642|\\n|√ó|0.580|0.798|\\n\\n# Table 5: Different training settings for five benchmark datasets.\\n\\n|Setting|Dataset|SWaT|WADI|MSL|SMAP|SMD|\\n|---|---|---|---|---|---|---|\\n|Patch size| |14|8|2|4|4|\\n|Window size| |7,168|4,096|1,024|2,048|2,048|\\n|Max length % of outlier| |20%|15%|20%|15%|20%|\\n|Use of length adjustment| | |√ó|√ó|√ó| |\\n\\n# A.3 DETAILED RESULTS OF SECTION 4.3\\n\\nWe examine relationships between the typical types of outliers and our proposed synthetic outliers in Section 4.3 through F1 and AUROC. The detailed results of F1 and AUROC are presented in Table 7 and Table 6, respectively.\\n\\n# Table 6: AUROC results of the preliminary experiment in Section 4.3.\\n\\n# 4.4 MAIN RESULTS\\n\\nWe report the results of AnomalyBERT on the five real-world datasets introduced in Section 4.1, and compare them to those of the previous works. The reproduced evaluation scores are brought from Kim et al. (2022). In Table 2, we show that AnomalyBERT outperforms the previous methods on\\n# Presented at the ICLR 2023 workshop on Machine Learning for IoT: Datasets, Perception, and Understanding\\n\\n# Table 2: F1-scores for various anomaly detection methods and AnomalyBERT on five benchmark datasets.\\n\\nWe report the standard F1 and F1-scores after point adjustment (F1PA) following the protocol in Kim et al. (2022). Our method outperforms all existing methods with F1.\\n\\nQuestion:\\nWhat are the four types of synthetic outliers proposed in the AnomalyBERT paper, as described in Section 3.2?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# B VISUAL RESULTS\\n\\nWe visualize the prediction results of AnomalyBERT and present the 2D projections of the original data and latent features in the model. Similar to Figure 1, we select three abnormal windows in each of SWaT, WADI, and SMAP test sets and fetch the corresponding anomaly scores after the score prediction process. We then project the original data points and the last latent features from the abnormal windows into 2D planes using t-SNE. As shown in Figure 5, our method distinguishes anomalies successfully and separates abnormal data points from normal points well.\\n\\n# SWaT\\n\\n# WADI\\n\\n# SMAP\\n\\n# Figure 5: Visualization of anomaly score predictions on abnormal sequences in SWaT, WADI, and SMAP datasets.\\n\\n# 4.4 MAIN RESULTS\\n\\nWe report the results of AnomalyBERT on the five real-world datasets introduced in Section 4.1, and compare them to those of the previous works. The reproduced evaluation scores are brought from Kim et al. (2022). In Table 2, we show that AnomalyBERT outperforms the previous methods on\\n# Presented at the ICLR 2023 workshop on Machine Learning for IoT: Datasets, Perception, and Understanding\\n\\n# Table 2: F1-scores for various anomaly detection methods and AnomalyBERT on five benchmark datasets.\\n\\nWe report the standard F1 and F1-scores after point adjustment (F1PA) following the protocol in Kim et al. (2022). Our method outperforms all existing methods with F1.\\n\\n# (c) Experimental results on the length adjustment on WADI dataset.\\n\\n|Length adjustment|F1|F1PA|\\n|---|---|---|\\n| |0.433|0.642|\\n|√ó|0.580|0.798|\\n\\nAll datasets with F1. Our method particularly surpasses the others on MSL and SMAP, which may contain unlabeled outliers in the training set. Despite the difficulty in training networks, our method detects anomalies well in this kind of data. AnomalyBERT also performs well with F1PA, although F1PA tends to distort the model performance. Our qualitative results are visualized in Figure 4.\\n\\n# 4.5 IMPACT OF SYNTHETIC OUTLIERS\\n\\n. Our model converts multivariate data points into temporal representations with relative position bias and yields anomaly scores from these representations. Our method, AnomalyBERT, shows a great capability of detecting anomalies contained in complex time series and surpasses previous state-of-the-art methods on five real-world benchmarks. Our code is available at https://github.com/Jhryu30/AnomalyBERT.\\n\\n# Table 1: Examination of relationships between the typical types of outliers in Lai et al. (2021) and our proposed synthetic outliers.\\n\\n|Proposed|Typical| | | | |\\n|---|---|---|---|---|---|\\n|Global|Contextual|Shapelet|Seasonal|Trend| |\\n|Soft replacement|‚úî|‚úî|‚úî|‚úî|‚úî|\\n|Uniform replacement|‚úî|‚úî| |‚úî| |\\n|Length adjustment| | | |‚úî| |\\n|Peak noise| |‚úî| | | |\\n\\n# Figure 4: Qualitative results of AnomalyBERT on abnormal sequences from SWaT, WADI, and SMAP datasets.\\n\\nWe visualize an abnormal component where the outlier occurs and anomaly scores for each sequence. Without any post-processing, our method finds out diverse types of anomalies.\\n\\nQuestion:\\nWhat is the purpose of the 1D relative position bias in the AnomalyBERT model, as described in Section 3.1?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 4.4 MAIN RESULTS\\n\\nWe report the results of AnomalyBERT on the five real-world datasets introduced in Section 4.1, and compare them to those of the previous works. The reproduced evaluation scores are brought from Kim et al. (2022). In Table 2, we show that AnomalyBERT outperforms the previous methods on\\n# Presented at the ICLR 2023 workshop on Machine Learning for IoT: Datasets, Perception, and Understanding\\n\\n# Table 2: F1-scores for various anomaly detection methods and AnomalyBERT on five benchmark datasets.\\n\\nWe report the standard F1 and F1-scores after point adjustment (F1PA) following the protocol in Kim et al. (2022). Our method outperforms all existing methods with F1.\\n\\n# B VISUAL RESULTS\\n\\nWe visualize the prediction results of AnomalyBERT and present the 2D projections of the original data and latent features in the model. Similar to Figure 1, we select three abnormal windows in each of SWaT, WADI, and SMAP test sets and fetch the corresponding anomaly scores after the score prediction process. We then project the original data points and the last latent features from the abnormal windows into 2D planes using t-SNE. As shown in Figure 5, our method distinguishes anomalies successfully and separates abnormal data points from normal points well.\\n\\n# SWaT\\n\\n# WADI\\n\\n# SMAP\\n\\n# Figure 5: Visualization of anomaly score predictions on abnormal sequences in SWaT, WADI, and SMAP datasets.\\n\\n# 4.2.2 Anomaly Diagnosis\\n\\nWe use commonly used metrics to measure the diagnosis performance of all models [62]. HitRate@P% is the measure of how many ground truth dimensions have been included in the top candidates predicted by the model [45]. P% is the percentage of the ground truth dimensions for each timestamp, which we use to consider the top predicted candidates. For instance, if at timestamp t, if 2 dimensions are labeled anomalous in the ground truth, HitRate@100% would consider top 2 dimensions and HitRate@150% would consider 3 dimensions (100 and 150 are chosen based on prior work [62]). We also measure the Normalized Discounted Cumulative Gain (NDCG) [24]. NDCG@P% considers the same number of top predicted candidates as HitRate@P%.\\n\\n# 4.3 Results\\n\\n# Evaluation\\n\\nAnomaly scores (Eq. 2) exceeding a threshold Œ¥ are identified as anomalies. Performance is evaluated using the F1-score for threshold-dependent evaluation. Here, we follow [17] by setting Œ¥ to label a predefined proportion of data points as anomalies (0.5% for SMD, 1% for others). For threshold-independent analysis, we use AUC-ROC, AUC-PR, VUS-ROC, and VUS-PR metrics [21]. We exclude the point-adjustment method [22] due to its overestimation [23]. Our model is compared against baselines and state-of-the-arts models: LSTM [9], vanilla Transformer [17], USAD [13], PatchAD [12], AnomalyTrans [17], and DCdetector [10].\\n\\n1 https://github.com/Raminghorbanii/RESTAD\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Table 1: Performance metrics of baselines and RESTAD on test sets.\\n\\nInitialization methods are denoted as (R) for Random and (K) for K-means. For all measures, a higher value indicates better anomaly detection performance.\\n\\n# 4 Experiments\\n\\nWe compare TranAD with state-of-the-art models for multivariate time-series anomaly detection, including MERLIN, LSTM-NDT (with autoencoder implementation from openGauss), DAGMM, OmniAnomaly, MSCRED, MAD-GAN, USAD, MTAD-GAT, CAE-M and GDN (with graph embedding implementation from GraphAn). For more details refer Section 2.1. We also tested the Isolation Forest method, but due to its low F1 scores, do not include the corresponding results in our discussion. Other classical methods have been omitted as well.\\n\\nQuestion:\\nWhat is the evaluation metric used to compare the performance of AnomalyBERT with previous works, as described in Section 4.2?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Figure 1: The TranAD Model.\\n\\n# 3.3 Transformer Model\\n\\nTransformers are popular deep learning models that have been used in various natural language and vision processing tasks [51]. However, we use insightful refactoring of the transformer architecture for the task of anomaly detection in time-series data. Just like other encoder-decoder models, in a transformer, an input sequence undergoes several attention-based transformations. Figure 1 shows the architecture of the neural network used in TranAD. The encoder encodes the complete sequence until the current timestamp ùê∂ with a focus score (more details later). The window encoder uses this to create an encoded representation of the input window ùëä, which is then passed to two decoders to create its reconstruction.\\n\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\nThe Transformer model enables us to predict the reconstruction of each input time-series window. It does this by acting as an encoder-decoder network at each timestamp. However, traditional encoder-decoder models often are unable to capture short-term trends and tend to miss anomalies if the deviations are too small. To tackle this challenge, we develop an auto-regressive inference style that predicts the reconstructed window in two-phases. In the first phase, the model aims to generate an approximate reconstruction of the input window.\\n\\n# Phase 2 - Focused Input Reconstruction\\n\\nIn the second phase, we use the reconstruction loss for the first decoder as a focus score. Having the focus matrix for the second phase ùêπ = ùêø1, we rerun model inference to obtain the output of the second decoder as ùëÇÀÜ2.\\n\\nFor the future, we propose to extend the method with other transformer models like bidirectional neural networks to allow model generalization to diverse temporal trends in data. We also wish to explore the direction of applying cost-benefit analysis for each model component based on the deployment setting to avoid expensive computation.\\n\\n# Software Availability\\n\\nThe code and relevant training scripts are made publicly available on GitHub under BSD-3 licence at https://github.com/imperial-qore/TranAD.\\n\\n# Acknowledgments\\n\\nShreshth Tuli is supported by the President‚Äôs PhD scholarship at Imperial College London. We thank Kate Highnam for constructive comments on improving the manuscript writing. We thank the providers of all datasets used in this work.\\n\\n# A MERLIN Implementation\\n\\nRecent models such as USAD, MTAD-GAT and GDN use attention mechanisms to focus on specific modes of the data. Moreover, these models try to capture the long-term trends by adjusting the weights of their neural network and only use a local window as an.\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Table 2: Performance comparison of TranAD with baseline methods on the complete dataset.\\n\\nP: Precision, R: Recall, AUC: Area under the ROC curve, F1: F1 score with complete training data. The best F1 and AUC scores are highlighted in bold.\\n\\n# 5.2 Overhead Analysis\\n\\nTable 5 shows the average training times for all models on every dataset in seconds per epoch. For comparison, we report the training time for MERLIN as the time it takes to discover discords in the test data. The training time of TranAD is 75% ‚àí 99% lower than those of the baseline methods. This clearly shows the advantage of having a transformer model with positional encoding to push the complete sequence as an input instead of sequentially inferring over local windows.\\n\\n# 5.3 Sensitivity Analysis\\n\\nQuestion:\\nWhat is the proposed model in the paper titled \\'TranAD\\' and what are its key features?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Anomaly Diagnosis\\n\\nThe anomaly diagnosis results in Table 4 where H and N correspond to HitRate and NDCG (with complete data used for model testing). We only present results on the multivariate SMD and MSDS datasets for the sake of brevity (TranAD yields better scores for others as well). We also ignore models that do not explicitly output anomaly class outputs for each dimension individually. Multi-head attention in TranAD allows it to attend to multiple modes simultaneously, making it more suitable for more inter-correlated anomalies.\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Table 4: Diagnosis Performance.\\n\\nThe rest of the paper is organized as follows. Section 2 overviews related work. Section 3 outlines the working of the TranAD model for multivariate anomaly detection and diagnosis. A performance evaluation of the proposed method is shown in Section 4. Section 5 presents additional analysis. Finally, Section 6 concludes.\\n\\n# 2 Related Work\\n\\nTime series anomaly detection is a long-studied problem in the VLDB community. The prior literature works on two types of time-series data: univariate and multivariate. For the former, various methods analyze and detect anomalies in time-series data with a single data source [34], while for the latter multiple time-series together [14, 45, 62].\\n\\n# Classical methods\\n\\nThe recently proposed HitAnomaly [19] method uses vanilla transformers as encoder-decoder networks, but is only applicable to natural-language log data and not appropriate for generic continuous time-series data as inputs. In our experiments, we compare TranAD against the state-of-the-art methods MERLIN, LSTM-NDT, DAGMM, OmniAnomaly, MSCRED, MAD-GAN, USAD, MTAD-GAT, CAE-M and GDN. These methods have shown superiority in anomaly detection and diagnosis, but complement one another in terms of performance across different time-series datasets. Out of these, only USAD aims to reduce training times, but does this to a limited extent. Just like reconstruction based prior work [4, 29, 45, 60, 61], we develop a TranAD model that learns broad level trends using training data to find anomalies in test data. We specifically improve anomaly detection and diagnosis performance with also reducing the training times in this work.\\n\\n# 3 Methodology\\n\\n# 3.1 Problem Formulation\\n\\nThis work uses various tools, including Transformer neural networks and model-agnostic meta learning, as building blocks. However, each of these different technologies cannot be directly used and need necessary adaptations to create a generalizable model for anomaly detection. Specifically, we propose a transformer-based anomaly detection model (TranAD), that uses self-conditioning and an adversarial training process. Its architecture makes it fast for training and testing while maintaining stability with large input sequences. Simple transformer-based encoder-decoder networks tend to miss anomalies if the deviation is too small, i.e., it is relatively close to normal data. One of our contributions is to show that this can be alleviated by an adversarial training procedure that can amplify reconstruction errors. Further, using self-conditioning for robust multi-modal feature extraction can help gain training stability and allow generalization [32]\\n\\nEfficient anomaly detection and diagnosis in multivariate time-series data is of great importance for modern industrial applications. However, building a system that is able to quickly and accurately pinpoint anomalous observations is a challenging problem. This is due to the lack of anomaly labels, high data volatility and the demands of ultra-low inference times in modern applications. Despite the recent developments of deep learning approaches for anomaly detection, only a few of them can address all of these challenges. In this paper, we propose TranAD, a deep transformer network based anomaly detection and diagnosis model which uses attention-based sequence encoders to swiftly perform inference with the knowledge of the broader temporal trends in the data. TranAD uses focus score-based self-conditioning to enable robust multi-modal feature extraction and adversarial training to gain stability\\n\\nQuestion:\\nWhat is the problem formulation for anomaly detection and diagnosis in the paper \\'TranAD\\'?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Figure 1: The TranAD Model.\\n\\n# 3.3 Transformer Model\\n\\nTransformers are popular deep learning models that have been used in various natural language and vision processing tasks [51]. However, we use insightful refactoring of the transformer architecture for the task of anomaly detection in time-series data. Just like other encoder-decoder models, in a transformer, an input sequence undergoes several attention-based transformations. Figure 1 shows the architecture of the neural network used in TranAD. The encoder encodes the complete sequence until the current timestamp ùê∂ with a focus score (more details later). The window encoder uses this to create an encoded representation of the input window ùëä, which is then passed to two decoders to create its reconstruction.\\n\\n# Figure 2: An overview of our framework.\\n\\nWe design a Transformer-based model and a self-supervised training strategy. In the training stage, a portion of an input window is randomly replaced and the model is directed to classify the degraded part. The main Transformer body is composed of Transformer layers with 1D relative position bias.\\n\\n# 3 METHOD\\n\\n# 3.1 OVERALL ARCHITECTURE\\n\\nWe now provide details on the working of TranAD. A multivariate sequence like ùëä or ùê∂ is transformed first into a matrix form with modality ùëö. We define scaled-dot product attention [51] of three matrices ùëÑ (query), ùêæ (key) and ùëâ (value):\\n\\nAttention(ùëÑ, ùêæ, ùëâ) = softmax(ùëÑùêæT / ‚àöùëö) ùëâ. (2)\\n\\nHere, the softmax forms the convex combination weights for the values in ùëâ, allowing us to compress the matrix ùëâ into a smaller representative embedding for simplified inference in the downstream neural network operations. Unlike traditional attention operation, the scaled-dot product attention scales the weights by a ‚àöùëö term to reduce the variance of the weights, facilitating stable training [51]. For input matrices ùëÑ, ùêæ and ùëâ, we apply Multi-Head Self Attention [51] by first passing it through ‚Ñé (number of heads) feed-forward layers to get ùëÑùëñ, ùêæùëñ and ùëâùëñ for ùëñ ‚àà {1, . . . , ‚Ñé}, and then applying scaled-dot product attention as:\\n\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\nThe Transformer model enables us to predict the reconstruction of each input time-series window. It does this by acting as an encoder-decoder network at each timestamp. However, traditional encoder-decoder models often are unable to capture short-term trends and tend to miss anomalies if the deviations are too small. To tackle this challenge, we develop an auto-regressive inference style that predicts the reconstructed window in two-phases. In the first phase, the model aims to generate an approximate reconstruction of the input window.\\n\\n# Phase 2 - Focused Input Reconstruction\\n\\nIn the second phase, we use the reconstruction loss for the first decoder as a focus score. Having the focus matrix for the second phase ùêπ = ùêø1, we rerun model inference to obtain the output of the second decoder as ùëÇÀÜ2.\\n\\nùëÇùëñ = Sigmoid(FeedForward(ùêº23)), (6)\\n\\nwhere ùëñ ‚àà {1, 2} for the first and second decoder respectively. The Sigmoid activation is used to generate an output in the range [0, 1], to match the normalized input window. Thus, the TranAD model takes the inputs ùê∂ and ùëä to generate two outputs ùëÇ1 and ùëÇ2.\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Algorithm 1 The TranAD training algorithm\\n\\nRequire:\\n\\n- Encoder ùê∏, Decoders ùê∑1 and ùê∑2\\n- Dataset used for training W\\n- Evolutionary hyperparameter ùúñ\\n- Iteration limit ùëÅ\\n\\n1. Initialize weights ùê∏, ùê∑1, ùê∑2\\n2. ùëõ ‚Üê 0\\n3. do\\n\\nwhile ùëõ < ùëÅ\\n\\n# 3.4 Offline Two-Phase Adversarial Training\\n\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\nQuestion:\\nWhat is the architecture of the neural network used in TranAD, as shown in Figure 1?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\nThe Transformer model enables us to predict the reconstruction of each input time-series window. It does this by acting as an encoder-decoder network at each timestamp. However, traditional encoder-decoder models often are unable to capture short-term trends and tend to miss anomalies if the deviations are too small. To tackle this challenge, we develop an auto-regressive inference style that predicts the reconstructed window in two-phases. In the first phase, the model aims to generate an approximate reconstruction of the input window.\\n\\n# Phase 2 - Focused Input Reconstruction\\n\\nIn the second phase, we use the reconstruction loss for the first decoder as a focus score. Having the focus matrix for the second phase ùêπ = ùêø1, we rerun model inference to obtain the output of the second decoder as ùëÇÀÜ2.\\n\\nùëÇùëñ = Sigmoid(FeedForward(ùêº23)), (6)\\n\\nwhere ùëñ ‚àà {1, 2} for the first and second decoder respectively. The Sigmoid activation is used to generate an output in the range [0, 1], to match the normalized input window. Thus, the TranAD model takes the inputs ùê∂ and ùëä to generate two outputs ùëÇ1 and ùëÇ2.\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Algorithm 1 The TranAD training algorithm\\n\\nRequire:\\n\\n- Encoder ùê∏, Decoders ùê∑1 and ùê∑2\\n- Dataset used for training W\\n- Evolutionary hyperparameter ùúñ\\n- Iteration limit ùëÅ\\n\\n1. Initialize weights ùê∏, ùê∑1, ùê∑2\\n2. ùëõ ‚Üê 0\\n3. do\\n\\nwhile ùëõ < ùëÅ\\n\\n# 3.4 Offline Two-Phase Adversarial Training\\n\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\nThe focus score generated in the first phase indicates the deviations of the reconstructed output from the given input. This acts as a prior to modify the attention weights in the second phase and gives higher neural network activation to specific input sub-sequences to extract short-term temporal trends. We refer to this approach as ‚Äúself-conditioning‚Äù in the rest of the paper.\\n\\nThis two-phase auto-regressive inference style has a three-fold benefit. First, it amplifies the deviations, as the reconstruction error acts as an activation in the attention part of the Encoder to generate an anomaly.\\n# Algorithm 2 The TranAD testing algorithm\\n\\nRequire:\\n\\n- Trained Encoder ùê∏, Decoders ùê∑1 and ùê∑2\\n- Test Dataset ÀÜùëáÀÜW\\n\\n1. for(ùë° = 1 to )\\n2. ùëÇ1, ùëÇ2 ‚Üê ùê∑1 ÀÜùë° , ‚à•ùëÇ1 ‚àí ùëä ‚à•2ÀÜ ‚à•2ùë° , 0\\n\\nùëÇ2 ‚Üê ùê∑1 (ùê∏ ( (ùê∏ ( ÀÜùë° ,¬Æ)), ùê∑2 (ùê∏ ( 1‚à•ùëÇ1 ‚àí ùëäùëä 1‚à• ÀÜ2 ‚àí )), ùê∑2 (¬Æ))ÀÜùë° , ‚à•ùëÇ1 ‚àí ùëä ‚à•2))\\n3. ùë† = 2 ùëäÀÜ ‚à•2 + 2 ùëÇ ùëä\\n4. ùë¶ùëñ = 1(ùë†ùëñ ‚â• POT(ùë†ùëñ))\\n5. ùë¶ = ‚à®ùë¶ùëñùëñ\\n\\nùúñ‚àíùëõ in the training process with a small positive constant parameter ùúñ.\\n\\n# Figure 2: An overview of our framework.\\n\\nWe design a Transformer-based model and a self-supervised training strategy. In the training stage, a portion of an input window is randomly replaced and the model is directed to classify the degraded part. The main Transformer body is composed of Transformer layers with 1D relative position bias.\\n\\n# 3 METHOD\\n\\n# 3.1 OVERALL ARCHITECTURE\\n\\n# 5.2 Overhead Analysis\\n\\nTable 5 shows the average training times for all models on every dataset in seconds per epoch. For comparison, we report the training time for MERLIN as the time it takes to discover discords in the test data. The training time of TranAD is 75% ‚àí 99% lower than those of the baseline methods. This clearly shows the advantage of having a transformer model with positional encoding to push the complete sequence as an input instead of sequentially inferring over local windows.\\n\\n# 5.3 Sensitivity Analysis\\n\\nQuestion:\\nWhat is the two-phase adversarial training process in TranAD, as described in Algorithm 1?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Analyses\\n\\n# Ablation Analysis\\n\\nTo study the relative importance of each component of the model, we exclude every major one and observe how it affects the performance in terms of the F1 scores for each dataset. First, we consider the TranAD model without the transformer-based encoder-decoder architecture but instead with a feed-forward network. Second, we consider the model without the self-conditioning, i.e., we fix the focus score ùêπ = 0¬Æ in phase 2. Third, we study the model without the adversarial loss, i.e., a single-phase inference and only the reconstruction loss for model training. Finally, we consider the model without meta-learning. The results are summarized in Table 6 and provide the following findings:\\n\\n# Table 17: Full ablations on ETTh1 and ETTm1 in predicting 96 and 192 steps ahead (MSE reported).\\n\\n# Anomaly Diagnosis\\n\\nThe anomaly diagnosis results in Table 4 where H and N correspond to HitRate and NDCG (with complete data used for model testing). We only present results on the multivariate SMD and MSDS datasets for the sake of brevity (TranAD yields better scores for others as well). We also ignore models that do not explicitly output anomaly class outputs for each dimension individually. Multi-head attention in TranAD allows it to attend to multiple modes simultaneously, making it more suitable for more inter-correlated anomalies.\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Table 4: Diagnosis Performance.\\n\\n# Figure 5: Predicted and Ground Truth labels for the MSDS test set using the TranAD model.\\n\\noutperforms it when compared across all datasets and also when given the complete WADI dataset.\\n\\nWe perform critical difference analysis to assess the significance of the differences among the performance of the models. Figure 4 depicts the critical difference diagrams for the F1 and AUC scores based on the Wilcoxon pair-wised signed-rank test (with ùõº = 0.05) after rejecting the null hypothesis using the Friedman test on all datasets [22]. TranAD achieves the best rank across all models with a significant statistical difference.\\n\\n# Anomaly Diagnosis\\n\\n# 4 Results\\n\\n# 4.1 Ablation Analysis\\n\\nOur empirical results, as detailed in Table 1, highlight the effectiveness of the RESTAD for anomaly detection. The ablation experiments are based on the RBF layer with random initialization. This decision is based on our findings that random initialization is as effective as the K-means strategy (see Table 1), while offering greater simplicity and computational efficiency. While there are slight performance differences between initialization methods, these variations are not significant enough to establish the superiority of one method over another.\\n\\nQuestion:\\nWhat are the results of the ablation study in Table 6, and what do they indicate about the importance of each component of the TranAD model?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model_answers = []\n",
    "for question in all_questions:\n",
    "    answer = process_question(question)\n",
    "    model_answers.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "total_time_local = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvataggio risposte in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {\n",
    "    \"questions\": model_answers\n",
    "}\n",
    "\n",
    "with open('Naive_local_responses.json', 'w') as outfile:\n",
    "    json.dump(output_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misurazione tempi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tempi per risposte globali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"What are the main topics covered by the data in the set of time-series papers?\"\n",
    "query2 = \"How does RestAD leverage both statistical methods and machine learning to achieve robust anomaly detection in noisy time-series data?\"\n",
    "query3 = \"How does TimeGPT approach time-series forecasting?\"\n",
    "query4 = \"What are the key features and benefits of RestAD in anomaly detection for time-series data?\"\n",
    "query5 = \"How does TimeLLM differ from other models in time-series forecasting?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esporto i tempi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "time_data = [\n",
    "    [\"Type of question\", \"Time (seconds)\", \"Number of questions\"],\n",
    "    [\"Local\", total_time_local, 45],\n",
    "    [\"Global\", total_time_global, 37]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempi salvati in naive_times.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = 'naive_times.csv'\n",
    "with open(output_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(time_data)\n",
    "\n",
    "print(f\"Tempi salvati in {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../DatasetCreation/Local/Chronos_qa.json\n",
      "Processing file: ../DatasetCreation/Local/TimeGPT_qa.json\n",
      "Processing file: ../DatasetCreation/Local/Timesfm_qa.json\n",
      "Processing file: ../DatasetCreation/Local/LagLLama_qa.json\n",
      "Processing file: ../DatasetCreation/Local/Foundation_Models_for_Time_Series_Analysis_qa.json\n",
      "Processing file: ../DatasetCreation/Local/TimeLLM_qa.json\n",
      "Processing file: ../DatasetCreation/Local/RESTAD_qa.json\n",
      "Processing file: ../DatasetCreation/Local/AnomalyBERT_qa.json\n",
      "Processing file: ../DatasetCreation/Local/TranAD_qa.json\n",
      "Successfully saved 45 question-answer pairs to all_questions_answers.json\n"
     ]
    }
   ],
   "source": [
    "def extract_questions_and_answers(file_path):\n",
    "    \"\"\"\n",
    "    Extracts questions and their corresponding answers from a single JSON file.\n",
    "    \n",
    "    :param file_path: Path to the JSON file.\n",
    "    :return: List of dictionaries with 'question' and 'answer' keys.\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            examples = data.get('examples', [])\n",
    "            for example in examples:\n",
    "                question = example.get('question')\n",
    "                answer = example.get('answer')\n",
    "                if question and answer:\n",
    "                    qa_pairs.append({\n",
    "                        \"question\": question.strip(),\n",
    "                        \"answer\": answer.strip()\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Missing question or answer in file {file_path}, example: {example}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from file {file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error reading file {file_path}: {e}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def extract_all_questions_and_answers(directory_path):\n",
    "    \"\"\"\n",
    "    Scans all JSON files in a directory and extracts all question-answer pairs.\n",
    "    \n",
    "    :param directory_path: Path to the directory containing JSON files.\n",
    "    :return: List of all question-answer pairs.\n",
    "    \"\"\"\n",
    "    all_qa_pairs = []\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return all_qa_pairs\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith(\".json\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            qa_pairs = extract_questions_and_answers(file_path)\n",
    "            all_qa_pairs.extend(qa_pairs)\n",
    "    \n",
    "    return all_qa_pairs\n",
    "\n",
    "def save_questions_and_answers(qa_pairs, output_file):\n",
    "    \"\"\"\n",
    "    Saves the list of question-answer pairs into a JSON file.\n",
    "    \n",
    "    :param qa_pairs: List of dictionaries with 'question' and 'answer' keys.\n",
    "    :param output_file: Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    data = {\"questions\": qa_pairs}\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Successfully saved {len(qa_pairs)} question-answer pairs to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to file {output_file}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Define the directory containing the JSON files\n",
    "    directory_path = '../DatasetCreation/Local'  # Update this path as needed\n",
    "    \n",
    "    # Extract all questions and answers\n",
    "    all_qa_pairs = extract_all_questions_and_answers(directory_path)\n",
    "    \n",
    "    if not all_qa_pairs:\n",
    "        print(\"No question-answer pairs were extracted.\")\n",
    "        return\n",
    "    \n",
    "    # Define the output JSON file path\n",
    "    output_file = 'all_questions_answers.json'  # You can change the output file name and path as needed\n",
    "    \n",
    "    # Save the extracted data to the output file\n",
    "    save_questions_and_answers(all_qa_pairs, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.1 (Python 3.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
