<html>
    <head>
        <meta charset="utf-8">
        
            <script>function neighbourhoodHighlight(params) {
  // console.log("in nieghbourhoodhighlight");
  allNodes = nodes.get({ returnType: "Object" });
  // originalNodes = JSON.parse(JSON.stringify(allNodes));
  // if something is selected:
  if (params.nodes.length > 0) {
    highlightActive = true;
    var i, j;
    var selectedNode = params.nodes[0];
    var degrees = 2;

    // mark all nodes as hard to read.
    for (let nodeId in allNodes) {
      // nodeColors[nodeId] = allNodes[nodeId].color;
      allNodes[nodeId].color = "rgba(200,200,200,0.5)";
      if (allNodes[nodeId].hiddenLabel === undefined) {
        allNodes[nodeId].hiddenLabel = allNodes[nodeId].label;
        allNodes[nodeId].label = undefined;
      }
    }
    var connectedNodes = network.getConnectedNodes(selectedNode);
    var allConnectedNodes = [];

    // get the second degree nodes
    for (i = 1; i < degrees; i++) {
      for (j = 0; j < connectedNodes.length; j++) {
        allConnectedNodes = allConnectedNodes.concat(
          network.getConnectedNodes(connectedNodes[j])
        );
      }
    }

    // all second degree nodes get a different color and their label back
    for (i = 0; i < allConnectedNodes.length; i++) {
      // allNodes[allConnectedNodes[i]].color = "pink";
      allNodes[allConnectedNodes[i]].color = "rgba(150,150,150,0.75)";
      if (allNodes[allConnectedNodes[i]].hiddenLabel !== undefined) {
        allNodes[allConnectedNodes[i]].label =
          allNodes[allConnectedNodes[i]].hiddenLabel;
        allNodes[allConnectedNodes[i]].hiddenLabel = undefined;
      }
    }

    // all first degree nodes get their own color and their label back
    for (i = 0; i < connectedNodes.length; i++) {
      // allNodes[connectedNodes[i]].color = undefined;
      allNodes[connectedNodes[i]].color = nodeColors[connectedNodes[i]];
      if (allNodes[connectedNodes[i]].hiddenLabel !== undefined) {
        allNodes[connectedNodes[i]].label =
          allNodes[connectedNodes[i]].hiddenLabel;
        allNodes[connectedNodes[i]].hiddenLabel = undefined;
      }
    }

    // the main node gets its own color and its label back.
    // allNodes[selectedNode].color = undefined;
    allNodes[selectedNode].color = nodeColors[selectedNode];
    if (allNodes[selectedNode].hiddenLabel !== undefined) {
      allNodes[selectedNode].label = allNodes[selectedNode].hiddenLabel;
      allNodes[selectedNode].hiddenLabel = undefined;
    }
  } else if (highlightActive === true) {
    // console.log("highlightActive was true");
    // reset all nodes
    for (let nodeId in allNodes) {
      // allNodes[nodeId].color = "purple";
      allNodes[nodeId].color = nodeColors[nodeId];
      // delete allNodes[nodeId].color;
      if (allNodes[nodeId].hiddenLabel !== undefined) {
        allNodes[nodeId].label = allNodes[nodeId].hiddenLabel;
        allNodes[nodeId].hiddenLabel = undefined;
      }
    }
    highlightActive = false;
  }

  // transform the object into an array
  var updateArray = [];
  if (params.nodes.length > 0) {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        // console.log(allNodes[nodeId]);
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  } else {
    // console.log("Nothing was selected");
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        // console.log(allNodes[nodeId]);
        // allNodes[nodeId].color = {};
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  }
}

function filterHighlight(params) {
  allNodes = nodes.get({ returnType: "Object" });
  // if something is selected:
  if (params.nodes.length > 0) {
    filterActive = true;
    let selectedNodes = params.nodes;

    // hiding all nodes and saving the label
    for (let nodeId in allNodes) {
      allNodes[nodeId].hidden = true;
      if (allNodes[nodeId].savedLabel === undefined) {
        allNodes[nodeId].savedLabel = allNodes[nodeId].label;
        allNodes[nodeId].label = undefined;
      }
    }

    for (let i=0; i < selectedNodes.length; i++) {
      allNodes[selectedNodes[i]].hidden = false;
      if (allNodes[selectedNodes[i]].savedLabel !== undefined) {
        allNodes[selectedNodes[i]].label = allNodes[selectedNodes[i]].savedLabel;
        allNodes[selectedNodes[i]].savedLabel = undefined;
      }
    }

  } else if (filterActive === true) {
    // reset all nodes
    for (let nodeId in allNodes) {
      allNodes[nodeId].hidden = false;
      if (allNodes[nodeId].savedLabel !== undefined) {
        allNodes[nodeId].label = allNodes[nodeId].savedLabel;
        allNodes[nodeId].savedLabel = undefined;
      }
    }
    filterActive = false;
  }

  // transform the object into an array
  var updateArray = [];
  if (params.nodes.length > 0) {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  } else {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  }
}

function selectNode(nodes) {
  network.selectNodes(nodes);
  neighbourhoodHighlight({ nodes: nodes });
  return nodes;
}

function selectNodes(nodes) {
  network.selectNodes(nodes);
  filterHighlight({nodes: nodes});
  return nodes;
}

function highlightFilter(filter) {
  let selectedNodes = []
  let selectedProp = filter['property']
  if (filter['item'] === 'node') {
    let allNodes = nodes.get({ returnType: "Object" });
    for (let nodeId in allNodes) {
      if (allNodes[nodeId][selectedProp] && filter['value'].includes((allNodes[nodeId][selectedProp]).toString())) {
        selectedNodes.push(nodeId)
      }
    }
  }
  else if (filter['item'] === 'edge'){
    let allEdges = edges.get({returnType: 'object'});
    // check if the selected property exists for selected edge and select the nodes connected to the edge
    for (let edge in allEdges) {
      if (allEdges[edge][selectedProp] && filter['value'].includes((allEdges[edge][selectedProp]).toString())) {
        selectedNodes.push(allEdges[edge]['from'])
        selectedNodes.push(allEdges[edge]['to'])
      }
    }
  }
  selectNodes(selectedNodes)
}</script>
            <style>.vis-overlay{bottom:0;left:0;position:absolute;right:0;top:0;z-index:10}.vis-active{box-shadow:0 0 10px #86d5f8}.vis [class*=span]{min-height:0;width:auto}div.vis-color-picker{background-color:#fff;border-radius:15px;box-shadow:0 0 10px 0 rgba(0,0,0,.5);display:none;height:444px;left:30px;margin-left:30px;margin-top:-140px;padding:10px;position:absolute;top:0;width:310px;z-index:1}div.vis-color-picker div.vis-arrow{left:5px;position:absolute;top:147px}div.vis-color-picker div.vis-arrow:after,div.vis-color-picker div.vis-arrow:before{border:solid transparent;content:" ";height:0;pointer-events:none;position:absolute;right:100%;top:50%;width:0}div.vis-color-picker div.vis-arrow:after{border-color:hsla(0,0%,100%,0) #fff hsla(0,0%,100%,0) hsla(0,0%,100%,0);border-width:30px;margin-top:-30px}div.vis-color-picker div.vis-color{cursor:pointer;height:289px;position:absolute;width:289px}div.vis-color-picker div.vis-brightness{position:absolute;top:313px}div.vis-color-picker div.vis-opacity{position:absolute;top:350px}div.vis-color-picker div.vis-selector{background:#4c4c4c;background:-moz-linear-gradient(top,#4c4c4c 0,#595959 12%,#666 25%,#474747 39%,#2c2c2c 50%,#000 51%,#111 60%,#2b2b2b 76%,#1c1c1c 91%,#131313 100%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,#4c4c4c),color-stop(12%,#595959),color-stop(25%,#666),color-stop(39%,#474747),color-stop(50%,#2c2c2c),color-stop(51%,#000),color-stop(60%,#111),color-stop(76%,#2b2b2b),color-stop(91%,#1c1c1c),color-stop(100%,#131313));background:-webkit-linear-gradient(top,#4c4c4c,#595959 12%,#666 25%,#474747 39%,#2c2c2c 50%,#000 51%,#111 60%,#2b2b2b 76%,#1c1c1c 91%,#131313);background:-o-linear-gradient(top,#4c4c4c 0,#595959 12%,#666 25%,#474747 39%,#2c2c2c 50%,#000 51%,#111 60%,#2b2b2b 76%,#1c1c1c 91%,#131313 100%);background:-ms-linear-gradient(top,#4c4c4c 0,#595959 12%,#666 25%,#474747 39%,#2c2c2c 50%,#000 51%,#111 60%,#2b2b2b 76%,#1c1c1c 91%,#131313 100%);background:linear-gradient(180deg,#4c4c4c 0,#595959 12%,#666 25%,#474747 39%,#2c2c2c 50%,#000 51%,#111 60%,#2b2b2b 76%,#1c1c1c 91%,#131313);border:1px solid #fff;border-radius:15px;filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#4c4c4c",endColorstr="#131313",GradientType=0);height:15px;left:137px;position:absolute;top:137px;width:15px}div.vis-color-picker div.vis-new-color{left:159px;padding-right:2px;text-align:right}div.vis-color-picker div.vis-initial-color,div.vis-color-picker div.vis-new-color{border:1px solid rgba(0,0,0,.1);border-radius:5px;color:rgba(0,0,0,.4);font-size:10px;height:20px;line-height:20px;position:absolute;top:380px;vertical-align:middle;width:140px}div.vis-color-picker div.vis-initial-color{left:10px;padding-left:2px;text-align:left}div.vis-color-picker div.vis-label{left:10px;position:absolute;width:300px}div.vis-color-picker div.vis-label.vis-brightness{top:300px}div.vis-color-picker div.vis-label.vis-opacity{top:338px}div.vis-color-picker div.vis-button{background-color:#f7f7f7;border:2px solid #d9d9d9;border-radius:10px;cursor:pointer;height:25px;line-height:25px;position:absolute;text-align:center;top:410px;vertical-align:middle;width:68px}div.vis-color-picker div.vis-button.vis-cancel{left:5px}div.vis-color-picker div.vis-button.vis-load{left:82px}div.vis-color-picker div.vis-button.vis-apply{left:159px}div.vis-color-picker div.vis-button.vis-save{left:236px}div.vis-color-picker input.vis-range{height:20px;width:290px}div.vis-configuration{display:block;float:left;font-size:12px;position:relative}div.vis-configuration-wrapper{display:block;width:700px}div.vis-configuration-wrapper:after{clear:both;content:"";display:block}div.vis-configuration.vis-config-option-container{background-color:#fff;border:2px solid #f7f8fa;border-radius:4px;display:block;left:10px;margin-top:20px;padding-left:5px;width:495px}div.vis-configuration.vis-config-button{background-color:#f7f8fa;border:2px solid #ceced0;border-radius:4px;cursor:pointer;display:block;height:25px;left:10px;line-height:25px;margin-bottom:30px;margin-top:20px;padding-left:5px;vertical-align:middle;width:495px}div.vis-configuration.vis-config-button.hover{background-color:#4588e6;border:2px solid #214373;color:#fff}div.vis-configuration.vis-config-item{display:block;float:left;height:25px;line-height:25px;vertical-align:middle;width:495px}div.vis-configuration.vis-config-item.vis-config-s2{background-color:#f7f8fa;border-radius:3px;left:10px;padding-left:5px}div.vis-configuration.vis-config-item.vis-config-s3{background-color:#e4e9f0;border-radius:3px;left:20px;padding-left:5px}div.vis-configuration.vis-config-item.vis-config-s4{background-color:#cfd8e6;border-radius:3px;left:30px;padding-left:5px}div.vis-configuration.vis-config-header{font-size:18px;font-weight:700}div.vis-configuration.vis-config-label{height:25px;line-height:25px;width:120px}div.vis-configuration.vis-config-label.vis-config-s3{width:110px}div.vis-configuration.vis-config-label.vis-config-s4{width:100px}div.vis-configuration.vis-config-colorBlock{border:1px solid #444;border-radius:2px;cursor:pointer;height:19px;margin:0;padding:0;top:1px;width:30px}input.vis-configuration.vis-config-checkbox{left:-5px}input.vis-configuration.vis-config-rangeinput{margin:0;padding:1px;pointer-events:none;position:relative;top:-5px;width:60px}input.vis-configuration.vis-config-range{-webkit-appearance:none;background-color:transparent;border:0 solid #fff;height:20px;width:300px}input.vis-configuration.vis-config-range::-webkit-slider-runnable-track{background:#dedede;background:-moz-linear-gradient(top,#dedede 0,#c8c8c8 99%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,#dedede),color-stop(99%,#c8c8c8));background:-webkit-linear-gradient(top,#dedede,#c8c8c8 99%);background:-o-linear-gradient(top,#dedede 0,#c8c8c8 99%);background:-ms-linear-gradient(top,#dedede 0,#c8c8c8 99%);background:linear-gradient(180deg,#dedede 0,#c8c8c8 99%);border:1px solid #999;border-radius:3px;box-shadow:0 0 3px 0 #aaa;filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#dedede",endColorstr="#c8c8c8",GradientType=0);height:5px;width:300px}input.vis-configuration.vis-config-range::-webkit-slider-thumb{-webkit-appearance:none;background:#3876c2;background:-moz-linear-gradient(top,#3876c2 0,#385380 100%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,#3876c2),color-stop(100%,#385380));background:-webkit-linear-gradient(top,#3876c2,#385380);background:-o-linear-gradient(top,#3876c2 0,#385380 100%);background:-ms-linear-gradient(top,#3876c2 0,#385380 100%);background:linear-gradient(180deg,#3876c2 0,#385380);border:1px solid #14334b;border-radius:50%;box-shadow:0 0 1px 0 #111927;filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#3876c2",endColorstr="#385380",GradientType=0);height:17px;margin-top:-7px;width:17px}input.vis-configuration.vis-config-range:focus{outline:none}input.vis-configuration.vis-config-range:focus::-webkit-slider-runnable-track{background:#9d9d9d;background:-moz-linear-gradient(top,#9d9d9d 0,#c8c8c8 99%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,#9d9d9d),color-stop(99%,#c8c8c8));background:-webkit-linear-gradient(top,#9d9d9d,#c8c8c8 99%);background:-o-linear-gradient(top,#9d9d9d 0,#c8c8c8 99%);background:-ms-linear-gradient(top,#9d9d9d 0,#c8c8c8 99%);background:linear-gradient(180deg,#9d9d9d 0,#c8c8c8 99%);filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#9d9d9d",endColorstr="#c8c8c8",GradientType=0)}input.vis-configuration.vis-config-range::-moz-range-track{background:#dedede;background:-moz-linear-gradient(top,#dedede 0,#c8c8c8 99%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,#dedede),color-stop(99%,#c8c8c8));background:-webkit-linear-gradient(top,#dedede,#c8c8c8 99%);background:-o-linear-gradient(top,#dedede 0,#c8c8c8 99%);background:-ms-linear-gradient(top,#dedede 0,#c8c8c8 99%);background:linear-gradient(180deg,#dedede 0,#c8c8c8 99%);border:1px solid #999;border-radius:3px;box-shadow:0 0 3px 0 #aaa;filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#dedede",endColorstr="#c8c8c8",GradientType=0);height:10px;width:300px}input.vis-configuration.vis-config-range::-moz-range-thumb{background:#385380;border:none;border-radius:50%;height:16px;width:16px}input.vis-configuration.vis-config-range:-moz-focusring{outline:1px solid #fff;outline-offset:-1px}input.vis-configuration.vis-config-range::-ms-track{background:transparent;border-color:transparent;border-width:6px 0;color:transparent;height:5px;width:300px}input.vis-configuration.vis-config-range::-ms-fill-lower{background:#777;border-radius:10px}input.vis-configuration.vis-config-range::-ms-fill-upper{background:#ddd;border-radius:10px}input.vis-configuration.vis-config-range::-ms-thumb{background:#385380;border:none;border-radius:50%;height:16px;width:16px}input.vis-configuration.vis-config-range:focus::-ms-fill-lower{background:#888}input.vis-configuration.vis-config-range:focus::-ms-fill-upper{background:#ccc}.vis-configuration-popup{background:rgba(57,76,89,.85);border:2px solid #f2faff;border-radius:4px;color:#fff;font-size:14px;height:30px;line-height:30px;position:absolute;text-align:center;-webkit-transition:opacity .3s ease-in-out;-moz-transition:opacity .3s ease-in-out;transition:opacity .3s ease-in-out;width:150px}.vis-configuration-popup:after,.vis-configuration-popup:before{border:solid transparent;content:" ";height:0;left:100%;pointer-events:none;position:absolute;top:50%;width:0}.vis-configuration-popup:after{border-color:rgba(136,183,213,0) rgba(136,183,213,0) rgba(136,183,213,0) rgba(57,76,89,.85);border-width:8px;margin-top:-8px}.vis-configuration-popup:before{border-color:rgba(194,225,245,0) rgba(194,225,245,0) rgba(194,225,245,0) #f2faff;border-width:12px;margin-top:-12px}div.vis-tooltip{background-color:#f5f4ed;border:1px solid #808074;-moz-border-radius:3px;-webkit-border-radius:3px;border-radius:3px;box-shadow:3px 3px 10px rgba(0,0,0,.2);color:#000;font-family:verdana;font-size:14px;padding:5px;pointer-events:none;position:absolute;visibility:hidden;white-space:nowrap;z-index:5}div.vis-network div.vis-navigation div.vis-button{-webkit-touch-callout:none;background-position:2px 2px;background-repeat:no-repeat;-moz-border-radius:17px;border-radius:17px;cursor:pointer;display:inline-block;height:34px;position:absolute;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:34px}div.vis-network div.vis-navigation div.vis-button:hover{box-shadow:0 0 3px 3px rgba(56,207,21,.3)}div.vis-network div.vis-navigation div.vis-button:active{box-shadow:0 0 1px 3px rgba(56,207,21,.95)}div.vis-network div.vis-navigation div.vis-button.vis-up{background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKT2lDQ1BQaG90b3Nob3AgSUNDIHByb2ZpbGUAAHjanVNnVFPpFj333vRCS4iAlEtvUhUIIFJCi4AUkSYqIQkQSoghodkVUcERRUUEG8igiAOOjoCMFVEsDIoK2AfkIaKOg6OIisr74Xuja9a89+bN/rXXPues852zzwfACAyWSDNRNYAMqUIeEeCDx8TG4eQuQIEKJHAAEAizZCFz/SMBAPh+PDwrIsAHvgABeNMLCADATZvAMByH/w/qQplcAYCEAcB0kThLCIAUAEB6jkKmAEBGAYCdmCZTAKAEAGDLY2LjAFAtAGAnf+bTAICd+Jl7AQBblCEVAaCRACATZYhEAGg7AKzPVopFAFgwABRmS8Q5ANgtADBJV2ZIALC3AMDOEAuyAAgMADBRiIUpAAR7AGDIIyN4AISZABRG8lc88SuuEOcqAAB4mbI8uSQ5RYFbCC1xB1dXLh4ozkkXKxQ2YQJhmkAuwnmZGTKBNA/g88wAAKCRFRHgg/P9eM4Ors7ONo62Dl8t6r8G/yJiYuP+5c+rcEAAAOF0ftH+LC+zGoA7BoBt/qIl7gRoXgugdfeLZrIPQLUAoOnaV/Nw+H48PEWhkLnZ2eXk5NhKxEJbYcpXff5nwl/AV/1s+X48/Pf14L7iJIEyXYFHBPjgwsz0TKUcz5IJhGLc5o9H/LcL//wd0yLESWK5WCoU41EScY5EmozzMqUiiUKSKcUl0v9k4t8s+wM+3zUAsGo+AXuRLahdYwP2SycQWHTA4vcAAPK7b8HUKAgDgGiD4c93/+8//UegJQCAZkmScQAAXkQkLlTKsz/HCAAARKCBKrBBG/TBGCzABhzBBdzBC/xgNoRCJMTCQhBCCmSAHHJgKayCQiiGzbAdKmAv1EAdNMBRaIaTcA4uwlW4Dj1wD/phCJ7BKLyBCQRByAgTYSHaiAFiilgjjggXmYX4IcFIBBKLJCDJiBRRIkuRNUgxUopUIFVIHfI9cgI5h1xGupE7yAAygvyGvEcxlIGyUT3UDLVDuag3GoRGogvQZHQxmo8WoJvQcrQaPYw2oefQq2gP2o8+Q8cwwOgYBzPEbDAuxsNCsTgsCZNjy7EirAyrxhqwVqwDu4n1Y8+xdwQSgUXACTYEd0IgYR5BSFhMWE7YSKggHCQ0EdoJNwkDhFHCJyKTqEu0JroR+cQYYjIxh1hILCPWEo8TLxB7iEPENyQSiUMyJ7mQAkmxpFTSEtJG0m5SI+ksqZs0SBojk8naZGuyBzmULCAryIXkneTD5DPkG+Qh8lsKnWJAcaT4U+IoUspqShnlEOU05QZlmDJBVaOaUt2ooVQRNY9aQq2htlKvUYeoEzR1mjnNgxZJS6WtopXTGmgXaPdpr+h0uhHdlR5Ol9BX0svpR+iX6AP0dwwNhhWDx4hnKBmbGAcYZxl3GK+YTKYZ04sZx1QwNzHrmOeZD5lvVVgqtip8FZHKCpVKlSaVGyovVKmqpqreqgtV81XLVI+pXlN9rkZVM1PjqQnUlqtVqp1Q61MbU2epO6iHqmeob1Q/pH5Z/YkGWcNMw09DpFGgsV/jvMYgC2MZs3gsIWsNq4Z1gTXEJrHN2Xx2KruY/R27iz2qqaE5QzNKM1ezUvOUZj8H45hx+Jx0TgnnKKeX836K3hTvKeIpG6Y0TLkxZVxrqpaXllirSKtRq0frvTau7aedpr1Fu1n7gQ5Bx0onXCdHZ4/OBZ3nU9lT3acKpxZNPTr1ri6qa6UbobtEd79up+6Ynr5egJ5Mb6feeb3n+hx9L/1U/W36p/VHDFgGswwkBtsMzhg8xTVxbzwdL8fb8VFDXcNAQ6VhlWGX4YSRudE8o9VGjUYPjGnGXOMk423GbcajJgYmISZLTepN7ppSTbmmKaY7TDtMx83MzaLN1pk1mz0x1zLnm+eb15vft2BaeFostqi2uGVJsuRaplnutrxuhVo5WaVYVVpds0atna0l1rutu6cRp7lOk06rntZnw7Dxtsm2qbcZsOXYBtuutm22fWFnYhdnt8Wuw+6TvZN9un2N/T0HDYfZDqsdWh1+c7RyFDpWOt6azpzuP33F9JbpL2dYzxDP2DPjthPLKcRpnVOb00dnF2e5c4PziIuJS4LLLpc+Lpsbxt3IveRKdPVxXeF60vWdm7Obwu2o26/uNu5p7ofcn8w0nymeWTNz0MPIQ+BR5dE/C5+VMGvfrH5PQ0+BZ7XnIy9jL5FXrdewt6V3qvdh7xc+9j5yn+M+4zw33jLeWV/MN8C3yLfLT8Nvnl+F30N/I/9k/3r/0QCngCUBZwOJgUGBWwL7+Hp8Ib+OPzrbZfay2e1BjKC5QRVBj4KtguXBrSFoyOyQrSH355jOkc5pDoVQfujW0Adh5mGLw34MJ4WHhVeGP45wiFga0TGXNXfR3ENz30T6RJZE3ptnMU85ry1KNSo+qi5qPNo3ujS6P8YuZlnM1VidWElsSxw5LiquNm5svt/87fOH4p3iC+N7F5gvyF1weaHOwvSFpxapLhIsOpZATIhOOJTwQRAqqBaMJfITdyWOCnnCHcJnIi/RNtGI2ENcKh5O8kgqTXqS7JG8NXkkxTOlLOW5hCepkLxMDUzdmzqeFpp2IG0yPTq9MYOSkZBxQqohTZO2Z+pn5mZ2y6xlhbL+xW6Lty8elQfJa7OQrAVZLQq2QqboVFoo1yoHsmdlV2a/zYnKOZarnivN7cyzytuQN5zvn//tEsIS4ZK2pYZLVy0dWOa9rGo5sjxxedsK4xUFK4ZWBqw8uIq2Km3VT6vtV5eufr0mek1rgV7ByoLBtQFr6wtVCuWFfevc1+1dT1gvWd+1YfqGnRs+FYmKrhTbF5cVf9go3HjlG4dvyr+Z3JS0qavEuWTPZtJm6ebeLZ5bDpaql+aXDm4N2dq0Dd9WtO319kXbL5fNKNu7g7ZDuaO/PLi8ZafJzs07P1SkVPRU+lQ27tLdtWHX+G7R7ht7vPY07NXbW7z3/T7JvttVAVVN1WbVZftJ+7P3P66Jqun4lvttXa1ObXHtxwPSA/0HIw6217nU1R3SPVRSj9Yr60cOxx++/p3vdy0NNg1VjZzG4iNwRHnk6fcJ3/ceDTradox7rOEH0x92HWcdL2pCmvKaRptTmvtbYlu6T8w+0dbq3nr8R9sfD5w0PFl5SvNUyWna6YLTk2fyz4ydlZ19fi753GDborZ752PO32oPb++6EHTh0kX/i+c7vDvOXPK4dPKy2+UTV7hXmq86X23qdOo8/pPTT8e7nLuarrlca7nuer21e2b36RueN87d9L158Rb/1tWeOT3dvfN6b/fF9/XfFt1+cif9zsu72Xcn7q28T7xf9EDtQdlD3YfVP1v+3Njv3H9qwHeg89HcR/cGhYPP/pH1jw9DBY+Zj8uGDYbrnjg+OTniP3L96fynQ89kzyaeF/6i/suuFxYvfvjV69fO0ZjRoZfyl5O/bXyl/erA6xmv28bCxh6+yXgzMV70VvvtwXfcdx3vo98PT+R8IH8o/2j5sfVT0Kf7kxmTk/8EA5jz/GMzLdsAAAAgY0hSTQAAeiUAAICDAAD5/wAAgOkAAHUwAADqYAAAOpgAABdvkl/FRgAABphJREFUeNqcV2twU9cR/nbPlVTHxpKRbNnBLyEbPyJisLEcPwgwUMKQtjNJAzNJZkgNNJOmJaZAaDKlxaXDTIBAcJtOOzSYKSkdiimhAdIMjyT4bYgBYxA2BgcUQPLrCiGDR4qt2x+yXTASFt1/957d7zt3z3d39xDCMQWUfgAz/RI/T4pSTAJpAGL8rECAXX7QFQGq9wOHOxYO1oCgjAdJj1wtB095Giv9TFuZAIWHAziATMPhTAwiHgUkYPXFJu92lMP/2MTpB1AKUCVEgNAcleUo1M+2F8TO6crSTncb1QleAOj2OTSX3Ge1p+Va42m5JrnzbnsCE8Ov+EHgpa0LPLvCJjZ/whuIlN8wAcXG+e1LUn9hm238QU84p1Ld83nsXvuO7Lq+LzKYGAT6/dn58m/HJTYf4O3EShkT8Irpzab1Uz9sGevT5+tWn+j6NB4A5hp/5NSr43xjfd5rW5tT9e3OAhCBiCua5/WsDEls/hdvYklZSwDefmrT8eXmtzuDkb5YZ33p9ndylICAVjWxf39xw/5g5Luv/9H84ZWNcwNEypZT87rXjqyJB85UYDMJYN3U7UdLJ6/6JlgqV517teRqf9uTlug8e1zEk27HgD22o98WsTBh8fWxvjm6ApdONbGvse8LM5NUPOm1Cfabuz3nACAgxX0QEFTJAnjNvLJ+Sepb14KRHnN+Ev+1XJOhZs3Qu1mbG97J2NQgsXroa1dtxrGuf8cHi1mUtPTay0lv1DMJSCRVLtoX+FgGgDQNysBAcez89l9nbbsQSji7rlXkEhjPxb/QatHOcFu0M9zz419oFSRhj/3PuaHiyqasv1Con9NGxHAYUsoCxAqImbYSgCWmFbZQwdsur7N0eC4m6tT6/jUZ750Zeb82c+OZGLWh/2p/W+Kfrmy0hIp/aVKpTSIJEqu2QgFx2iE8CwDp0RbH7Ljng/4yXr+XT3QdyhYsodS0slGr0g2OrEUK7eCrKW82SqzCVz3/yfb6vRwM4xn9rN7JkRkOQRLmfJn2LBPxQjDBqp9lD7XbX7X8pKTP160zR2bdeiX5jYeU/nLSTztNkem3XL5eXbltRUkonBxdgZ2IIUmahUxERQSCVT+rK5hzQ89xQ6P8VaaK1f5VmRvqQ4G+lba+nlnlb5brMhvlk7FBiaPzuwQEmEQhg5BOxMjWTncHc2501cQLkjDTsMCWpyuRQxFP0xXIJfp5FyVW4Zy7KajC06ItbiIGg6ZITBxDxIgbrr1jTSM0fibGIHz8O9sKK0GAibEua9spANh4aY2VmcEg+DEkiBgR/L2hYFgGtcErkQQAMVJgBxyy9hboZzv32v+Kpr7qbEECTAIMAoaJa3qPTmNiiAAgJAjk6J5xhu6HDAIgQYGLmI29PocmMcI8MNYvT1ckfzD9H/ub5br4e4Me9WfOKqtyX6Ud2cwC449PRamifDm6Auc0rTXokci+Xo1EAgBckiDuYGLjpTvntcGIA+SFcp6uUAaAI879VhWrRteYAqn/edq758brXJ1327QMhgJcZjA3EBjNrgZjOG1PkAjyTGENMjZPq5ECQ0MDE9ERBqFZrk0OJ3i4x/7vyIjBxGERt3takgVJEAp9xq3f769WiPDNvSsJdT3HDOEASPelmoBRYT3Kzt5uMtwauJEgSOCpwrk1DIJCoNUMwj9v7MweP9XSQ8/hJPp496fZTAICvLqcyv2B7nRbrgCA03JN5h8ub7A8VqpB437xHvsOy3l3cyaB4L2uqxhti1WLMcSgZQCw7+bOooO3Pk4JBZIYYXISMV5sKH59UePM10GESRGpIf/bE92HU452HywSJIGIllctrhp6YAK5+fHds0lLtJFMXNwkV6fFqA29mROefqiMJj1h6um4a5vY/92dKGaBxIhU5zJTWW2cJmEgGOmeb3c8FxAfb9mdf2RzyGGv5MvU7QwuEySwKHFp/c/M71zA/2F7b1RajnYdLAqMukMVu2YcfmDYE2MD7H+7/Xlq6cRIJqm4zXM+qd3TGjVBir43KSLlXjiELe5TsX+3/yW/ST45PaAHbKmccWh12AP93JNZywj0kSABIobpiXRHjtZ6faout2tyZMadGLXBCxBcvl6NfaAz+tKdFmObpzWl2+tIIBACYy0t/yj34M7HvsKUK+CGassvicX7alYDwwq+vykIEqPVa+Q9gdYk5+V+UE7lj3+FGbuBM/X5JUT8QwIVSSSZiTgmoFR2MfiqYFFPfjpkyrfWPopwxP47AP1pK1g9/dqeAAAAAElFTkSuQmCC");bottom:50px;left:55px}div.vis-network div.vis-navigation div.vis-button.vis-down{background-image:url("data:image/png;base64,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");bottom:10px;left:55px}div.vis-network div.vis-navigation div.vis-button.vis-left{background-image:url("data:image/png;base64,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");bottom:10px;left:15px}div.vis-network div.vis-navigation div.vis-button.vis-right{background-image:url("data:image/png;base64,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");bottom:10px;left:95px}div.vis-network div.vis-navigation div.vis-button.vis-zoomIn{background-image:url("data:image/png;base64,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");bottom:10px;right:15px}div.vis-network div.vis-navigation div.vis-button.vis-zoomOut{background-image:url("data:image/png;base64,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");bottom:10px;right:55px}div.vis-network div.vis-navigation div.vis-button.vis-zoomExtends{background-image:url("data:image/png;base64,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");bottom:50px;right:15px}div.vis-network div.vis-manipulation{background:#fff;background:-moz-linear-gradient(top,#fff 0,#fcfcfc 48%,#fafafa 50%,#fcfcfc 100%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,#fff),color-stop(48%,#fcfcfc),color-stop(50%,#fafafa),color-stop(100%,#fcfcfc));background:-webkit-linear-gradient(top,#fff,#fcfcfc 48%,#fafafa 50%,#fcfcfc);background:-o-linear-gradient(top,#fff 0,#fcfcfc 48%,#fafafa 50%,#fcfcfc 100%);background:-ms-linear-gradient(top,#fff 0,#fcfcfc 48%,#fafafa 50%,#fcfcfc 100%);background:linear-gradient(180deg,#fff 0,#fcfcfc 48%,#fafafa 50%,#fcfcfc);border:0 solid #d6d9d8;border-bottom:1px;box-sizing:content-box;filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#ffffff",endColorstr="#fcfcfc",GradientType=0);height:28px;left:0;padding-top:4px;position:absolute;top:0;width:100%}div.vis-network button.vis-edit-mode,div.vis-network div.vis-edit-mode{height:30px;left:0;position:absolute;top:5px}div.vis-network button.vis-close{-webkit-touch-callout:none;background-color:transparent;background-image:url("data:image/png;base64,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");background-position:20px 3px;background-repeat:no-repeat;border:none;cursor:pointer;height:30px;position:absolute;right:0;top:0;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:30px}div.vis-network button.vis-close:hover{opacity:.6}div.vis-network div.vis-edit-mode button.vis-button,div.vis-network div.vis-manipulation button.vis-button{-webkit-touch-callout:none;background-color:transparent;background-position:0 0;background-repeat:no-repeat;border:none;-moz-border-radius:15px;border-radius:15px;box-sizing:content-box;cursor:pointer;float:left;font-family:verdana;font-size:12px;height:24px;margin-left:10px;padding:0 8px;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}div.vis-network div.vis-manipulation button.vis-button:hover{box-shadow:1px 1px 8px rgba(0,0,0,.2)}div.vis-network div.vis-manipulation button.vis-button:active{box-shadow:1px 1px 8px rgba(0,0,0,.5)}div.vis-network div.vis-manipulation button.vis-button.vis-back{background-image:url("data:image/png;base64,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")}div.vis-network div.vis-manipulation div.vis-none:hover{box-shadow:1px 1px 8px transparent;cursor:default}div.vis-network div.vis-manipulation div.vis-none:active{box-shadow:1px 1px 8px transparent}div.vis-network div.vis-manipulation div.vis-none{line-height:23px;padding:0}div.vis-network div.vis-manipulation div.notification{font-weight:700;margin:2px}div.vis-network div.vis-manipulation button.vis-button.vis-add{background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAACXBIWXMAAAsTAAALEwEAmpwYAAAKT2lDQ1BQaG90b3Nob3AgSUNDIHByb2ZpbGUAAHjanVNnVFPpFj333vRCS4iAlEtvUhUIIFJCi4AUkSYqIQkQSoghodkVUcERRUUEG8igiAOOjoCMFVEsDIoK2AfkIaKOg6OIisr74Xuja9a89+bN/rXXPues852zzwfACAyWSDNRNYAMqUIeEeCDx8TG4eQuQIEKJHAAEAizZCFz/SMBAPh+PDwrIsAHvgABeNMLCADATZvAMByH/w/qQplcAYCEAcB0kThLCIAUAEB6jkKmAEBGAYCdmCZTAKAEAGDLY2LjAFAtAGAnf+bTAICd+Jl7AQBblCEVAaCRACATZYhEAGg7AKzPVopFAFgwABRmS8Q5ANgtADBJV2ZIALC3AMDOEAuyAAgMADBRiIUpAAR7AGDIIyN4AISZABRG8lc88SuuEOcqAAB4mbI8uSQ5RYFbCC1xB1dXLh4ozkkXKxQ2YQJhmkAuwnmZGTKBNA/g88wAAKCRFRHgg/P9eM4Ors7ONo62Dl8t6r8G/yJiYuP+5c+rcEAAAOF0ftH+LC+zGoA7BoBt/qIl7gRoXgugdfeLZrIPQLUAoOnaV/Nw+H48PEWhkLnZ2eXk5NhKxEJbYcpXff5nwl/AV/1s+X48/Pf14L7iJIEyXYFHBPjgwsz0TKUcz5IJhGLc5o9H/LcL//wd0yLESWK5WCoU41EScY5EmozzMqUiiUKSKcUl0v9k4t8s+wM+3zUAsGo+AXuRLahdYwP2SycQWHTA4vcAAPK7b8HUKAgDgGiD4c93/+8//UegJQCAZkmScQAAXkQkLlTKsz/HCAAARKCBKrBBG/TBGCzABhzBBdzBC/xgNoRCJMTCQhBCCmSAHHJgKayCQiiGzbAdKmAv1EAdNMBRaIaTcA4uwlW4Dj1wD/phCJ7BKLyBCQRByAgTYSHaiAFiilgjjggXmYX4IcFIBBKLJCDJiBRRIkuRNUgxUopUIFVIHfI9cgI5h1xGupE7yAAygvyGvEcxlIGyUT3UDLVDuag3GoRGogvQZHQxmo8WoJvQcrQaPYw2oefQq2gP2o8+Q8cwwOgYBzPEbDAuxsNCsTgsCZNjy7EirAyrxhqwVqwDu4n1Y8+xdwQSgUXACTYEd0IgYR5BSFhMWE7YSKggHCQ0EdoJNwkDhFHCJyKTqEu0JroR+cQYYjIxh1hILCPWEo8TLxB7iEPENyQSiUMyJ7mQAkmxpFTSEtJG0m5SI+ksqZs0SBojk8naZGuyBzmULCAryIXkneTD5DPkG+Qh8lsKnWJAcaT4U+IoUspqShnlEOU05QZlmDJBVaOaUt2ooVQRNY9aQq2htlKvUYeoEzR1mjnNgxZJS6WtopXTGmgXaPdpr+h0uhHdlR5Ol9BX0svpR+iX6AP0dwwNhhWDx4hnKBmbGAcYZxl3GK+YTKYZ04sZx1QwNzHrmOeZD5lvVVgqtip8FZHKCpVKlSaVGyovVKmqpqreqgtV81XLVI+pXlN9rkZVM1PjqQnUlqtVqp1Q61MbU2epO6iHqmeob1Q/pH5Z/YkGWcNMw09DpFGgsV/jvMYgC2MZs3gsIWsNq4Z1gTXEJrHN2Xx2KruY/R27iz2qqaE5QzNKM1ezUvOUZj8H45hx+Jx0TgnnKKeX836K3hTvKeIpG6Y0TLkxZVxrqpaXllirSKtRq0frvTau7aedpr1Fu1n7gQ5Bx0onXCdHZ4/OBZ3nU9lT3acKpxZNPTr1ri6qa6UbobtEd79up+6Ynr5egJ5Mb6feeb3n+hx9L/1U/W36p/VHDFgGswwkBtsMzhg8xTVxbzwdL8fb8VFDXcNAQ6VhlWGX4YSRudE8o9VGjUYPjGnGXOMk423GbcajJgYmISZLTepN7ppSTbmmKaY7TDtMx83MzaLN1pk1mz0x1zLnm+eb15vft2BaeFostqi2uGVJsuRaplnutrxuhVo5WaVYVVpds0atna0l1rutu6cRp7lOk06rntZnw7Dxtsm2qbcZsOXYBtuutm22fWFnYhdnt8Wuw+6TvZN9un2N/T0HDYfZDqsdWh1+c7RyFDpWOt6azpzuP33F9JbpL2dYzxDP2DPjthPLKcRpnVOb00dnF2e5c4PziIuJS4LLLpc+Lpsbxt3IveRKdPVxXeF60vWdm7Obwu2o26/uNu5p7ofcn8w0nymeWTNz0MPIQ+BR5dE/C5+VMGvfrH5PQ0+BZ7XnIy9jL5FXrdewt6V3qvdh7xc+9j5yn+M+4zw33jLeWV/MN8C3yLfLT8Nvnl+F30N/I/9k/3r/0QCngCUBZwOJgUGBWwL7+Hp8Ib+OPzrbZfay2e1BjKC5QRVBj4KtguXBrSFoyOyQrSH355jOkc5pDoVQfujW0Adh5mGLw34MJ4WHhVeGP45wiFga0TGXNXfR3ENz30T6RJZE3ptnMU85ry1KNSo+qi5qPNo3ujS6P8YuZlnM1VidWElsSxw5LiquNm5svt/87fOH4p3iC+N7F5gvyF1weaHOwvSFpxapLhIsOpZATIhOOJTwQRAqqBaMJfITdyWOCnnCHcJnIi/RNtGI2ENcKh5O8kgqTXqS7JG8NXkkxTOlLOW5hCepkLxMDUzdmzqeFpp2IG0yPTq9MYOSkZBxQqohTZO2Z+pn5mZ2y6xlhbL+xW6Lty8elQfJa7OQrAVZLQq2QqboVFoo1yoHsmdlV2a/zYnKOZarnivN7cyzytuQN5zvn//tEsIS4ZK2pYZLVy0dWOa9rGo5sjxxedsK4xUFK4ZWBqw8uIq2Km3VT6vtV5eufr0mek1rgV7ByoLBtQFr6wtVCuWFfevc1+1dT1gvWd+1YfqGnRs+FYmKrhTbF5cVf9go3HjlG4dvyr+Z3JS0qavEuWTPZtJm6ebeLZ5bDpaql+aXDm4N2dq0Dd9WtO319kXbL5fNKNu7g7ZDuaO/PLi8ZafJzs07P1SkVPRU+lQ27tLdtWHX+G7R7ht7vPY07NXbW7z3/T7JvttVAVVN1WbVZftJ+7P3P66Jqun4lvttXa1ObXHtxwPSA/0HIw6217nU1R3SPVRSj9Yr60cOxx++/p3vdy0NNg1VjZzG4iNwRHnk6fcJ3/ceDTradox7rOEH0x92HWcdL2pCmvKaRptTmvtbYlu6T8w+0dbq3nr8R9sfD5w0PFl5SvNUyWna6YLTk2fyz4ydlZ19fi753GDborZ752PO32oPb++6EHTh0kX/i+c7vDvOXPK4dPKy2+UTV7hXmq86X23qdOo8/pPTT8e7nLuarrlca7nuer21e2b36RueN87d9L158Rb/1tWeOT3dvfN6b/fF9/XfFt1+cif9zsu72Xcn7q28T7xf9EDtQdlD3YfVP1v+3Njv3H9qwHeg89HcR/cGhYPP/pH1jw9DBY+Zj8uGDYbrnjg+OTniP3L96fynQ89kzyaeF/6i/suuFxYvfvjV69fO0ZjRoZfyl5O/bXyl/erA6xmv28bCxh6+yXgzMV70VvvtwXfcdx3vo98PT+R8IH8o/2j5sfVT0Kf7kxmTk/8EA5jz/GMzLdsAAEEOaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8P3hwYWNrZXQgYmVnaW49Iu+7vyIgaWQ9Ilc1TTBNcENlaGlIenJlU3pOVGN6a2M5ZCI/Pgo8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJBZG9iZSBYTVAgQ29yZSA1LjUtYzAyMSA3OS4xNTQ5MTEsIDIwMTMvMTAvMjktMTE6NDc6MTYgICAgICAgICI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyIKICAgICAgICAgICAgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIgogICAgICAgICAgICB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIKICAgICAgICAgICAgeG1sbnM6c3RFdnQ9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZUV2ZW50IyIKICAgICAgICAgICAgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiCiAgICAgICAgICAgIHhtbG5zOnBob3Rvc2hvcD0iaHR0cDovL25zLmFkb2JlLmNvbS9waG90b3Nob3AvMS4wLyIKICAgICAgICAgICAgeG1sbnM6dGlmZj0iaHR0cDovL25zLmFkb2JlLmNvbS90aWZmLzEuMC8iCiAgICAgICAgICAgIHhtbG5zOmV4aWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vZXhpZi8xLjAvIj4KICAgICAgICAgPHhtcDpDcmVhdG9yVG9vbD5BZG9iZSBQaG90b3Nob3AgQ0MgKFdpbmRvd3MpPC94bXA6Q3JlYXRvclRvb2w+CiAgICAgICAgIDx4bXA6Q3JlYXRlRGF0ZT4yMDE0LTAxLTIyVDE5OjI0OjUxKzAxOjAwPC94bXA6Q3JlYXRlRGF0ZT4KICAgICAgICAgPHhtcDpNZXRhZGF0YURhdGU+MjAxNC0wMi0wNFQxNDo0MDoyOSswMTowMDwveG1wOk1ldGFkYXRhRGF0ZT4KICAgICAgICAgPHhtcDpNb2RpZnlEYXRlPjIwMTQtMDItMDRUMTQ6NDA6MjkrMDE6MDA8L3htcDpNb2RpZnlEYXRlPgogICAgICAgICA8ZGM6Zm9ybWF0PmltYWdlL3BuZzwvZGM6Zm9ybWF0PgogICAgICAgICA8eG1wTU06SW5zdGFuY2VJRD54bXAuaWlkOjVkNWIwNmQwLTVmMjAtOGE0NC1hMzIwLWZmMTEzMzQwNDc0YjwveG1wTU06SW5zdGFuY2VJRD4KICAgICAgICAgPHhtcE1NOkRvY3VtZW50SUQ+eG1wLmRpZDpFQTc2MkY5Njc0ODNFMzExOTQ4QkQxM0UyQkU3OTlBMTwveG1wTU06RG9jdW1lbnRJRD4KICAgICAgICAgPHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD54bXAuZGlkOjczQjYyQUFEOTE4M0UzMTE5NDhCRDEzRTJCRTc5OUExPC94bXBNTTpPcmlnaW5hbERvY3VtZW50SUQ+CiAgICAgICAgIDx4bXBNTTpIaXN0b3J5PgogICAgICAgICAgICA8cmRmOlNlcT4KICAgICAgICAgICAgICAgPHJkZjpsaSByZGY6cGFyc2VUeXBlPSJSZXNvdXJjZSI+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDphY3Rpb24+Y3JlYXRlZDwvc3RFdnQ6YWN0aW9uPgogICAgICAgICAgICAgICAgICA8c3RFdnQ6aW5zdGFuY2VJRD54bXAuaWlkOjczQjYyQUFEOTE4M0UzMTE5NDhCRDEzRTJCRTc5OUExPC9zdEV2dDppbnN0YW5jZUlEPgogICAgICAgICAgICAgICAgICA8c3RFdnQ6d2hlbj4yMDE0LTAxLTIyVDE5OjI0OjUxKzAxOjAwPC9zdEV2dDp3aGVuPgogICAgICAgICAgICAgICAgICA8c3RFdnQ6c29mdHdhcmVBZ2VudD5BZG9iZSBQaG90b3Nob3AgQ1M2IChXaW5kb3dzKTwvc3RFdnQ6c29mdHdhcmVBZ2VudD4KICAgICAgICAgICAgICAgPC9yZGY6bGk+CiAgICAgICAgICAgICAgIDxyZGY6bGkgcmRmOnBhcnNlVHlwZT0iUmVzb3VyY2UiPgogICAgICAgICAgICAgICAgICA8c3RFdnQ6YWN0aW9uPnNhdmVkPC9zdEV2dDphY3Rpb24+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDppbnN0YW5jZUlEPnhtcC5paWQ6RUE2MEEyNEUxOTg0RTMxMUFEQUZFRkU2RUMzMzNFMDM8L3N0RXZ0Omluc3RhbmNlSUQ+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDp3aGVuPjIwMTQtMDEtMjNUMTk6MTg6MDcrMDE6MDA8L3N0RXZ0OndoZW4+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDpzb2Z0d2FyZUFnZW50PkFkb2JlIFBob3Rvc2hvcCBDUzYgKFdpbmRvd3MpPC9zdEV2dDpzb2Z0d2FyZUFnZW50PgogICAgICAgICAgICAgICAgICA8c3RFdnQ6Y2hhbmdlZD4vPC9zdEV2dDpjaGFuZ2VkPgogICAgICAgICAgICAgICA8L3JkZjpsaT4KICAgICAgICAgICAgICAgPHJkZjpsaSByZGY6cGFyc2VUeXBlPSJSZXNvdXJjZSI+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDphY3Rpb24+c2F2ZWQ8L3N0RXZ0OmFjdGlvbj4KICAgICAgICAgICAgICAgICAgPHN0RXZ0Omluc3RhbmNlSUQ+eG1wLmlpZDo2OWVmYWE1NS01ZTI5LTIzNGUtYTUzMy0xNDkxYjM1NDNmYmE8L3N0RXZ0Omluc3RhbmNlSUQ+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDp3aGVuPjIwMTQtMDItMDRUMTQ6NDA6MjkrMDE6MDA8L3N0RXZ0OndoZW4+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDpzb2Z0d2FyZUFnZW50PkFkb2JlIFBob3Rvc2hvcCBDQyAoV2luZG93cyk8L3N0RXZ0OnNvZnR3YXJlQWdlbnQ+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDpjaGFuZ2VkPi88L3N0RXZ0OmNoYW5nZWQ+CiAgICAgICAgICAgICAgIDwvcmRmOmxpPgogICAgICAgICAgICAgICA8cmRmOmxpIHJkZjpwYXJzZVR5cGU9IlJlc291cmNlIj4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OmFjdGlvbj5jb252ZXJ0ZWQ8L3N0RXZ0OmFjdGlvbj4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OnBhcmFtZXRlcnM+ZnJvbSBhcHBsaWNhdGlvbi92bmQuYWRvYmUucGhvdG9zaG9wIHRvIGltYWdlL3BuZzwvc3RFdnQ6cGFyYW1ldGVycz4KICAgICAgICAgICAgICAgPC9yZGY6bGk+CiAgICAgICAgICAgICAgIDxyZGY6bGkgcmRmOnBhcnNlVHlwZT0iUmVzb3VyY2UiPgogICAgICAgICAgICAgICAgICA8c3RFdnQ6YWN0aW9uPmRlcml2ZWQ8L3N0RXZ0OmFjdGlvbj4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OnBhcmFtZXRlcnM+Y29udmVydGVkIGZyb20gYXBwbGljYXRpb24vdm5kLmFkb2JlLnBob3Rvc2hvcCB0byBpbWFnZS9wbmc8L3N0RXZ0OnBhcmFtZXRlcnM+CiAgICAgICAgICAgICAgIDwvcmRmOmxpPgogICAgICAgICAgICAgICA8cmRmOmxpIHJkZjpwYXJzZVR5cGU9IlJlc291cmNlIj4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OmFjdGlvbj5zYXZlZDwvc3RFdnQ6YWN0aW9uPgogICAgICAgICAgICAgICAgICA8c3RFdnQ6aW5zdGFuY2VJRD54bXAuaWlkOjVkNWIwNmQwLTVmMjAtOGE0NC1hMzIwLWZmMTEzMzQwNDc0Yjwvc3RFdnQ6aW5zdGFuY2VJRD4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OndoZW4+MjAxNC0wMi0wNFQxNDo0MDoyOSswMTowMDwvc3RFdnQ6d2hlbj4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OnNvZnR3YXJlQWdlbnQ+QWRvYmUgUGhvdG9zaG9wIENDIChXaW5kb3dzKTwvc3RFdnQ6c29mdHdhcmVBZ2VudD4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OmNoYW5nZWQ+Lzwvc3RFdnQ6Y2hhbmdlZD4KICAgICAgICAgICAgICAgPC9yZGY6bGk+CiAgICAgICAgICAgIDwvcmRmOlNlcT4KICAgICAgICAgPC94bXBNTTpIaXN0b3J5PgogICAgICAgICA8eG1wTU06RGVyaXZlZEZyb20gcmRmOnBhcnNlVHlwZT0iUmVzb3VyY2UiPgogICAgICAgICAgICA8c3RSZWY6aW5zdGFuY2VJRD54bXAuaWlkOjY5ZWZhYTU1LTVlMjktMjM0ZS1hNTMzLTE0OTFiMzU0M2ZiYTwvc3RSZWY6aW5zdGFuY2VJRD4KICAgICAgICAgICAgPHN0UmVmOmRvY3VtZW50SUQ+eG1wLmRpZDpFQTc2MkY5Njc0ODNFMzExOTQ4QkQxM0UyQkU3OTlBMTwvc3RSZWY6ZG9jdW1lbnRJRD4KICAgICAgICAgICAgPHN0UmVmOm9yaWdpbmFsRG9jdW1lbnRJRD54bXAuZGlkOjczQjYyQUFEOTE4M0UzMTE5NDhCRDEzRTJCRTc5OUExPC9zdFJlZjpvcmlnaW5hbERvY3VtZW50SUQ+CiAgICAgICAgIDwveG1wTU06RGVyaXZlZEZyb20+CiAgICAgICAgIDxwaG90b3Nob3A6Q29sb3JNb2RlPjM8L3Bob3Rvc2hvcDpDb2xvck1vZGU+CiAgICAgICAgIDxwaG90b3Nob3A6SUNDUHJvZmlsZT5zUkdCIElFQzYxOTY2LTIuMTwvcGhvdG9zaG9wOklDQ1Byb2ZpbGU+CiAgICAgICAgIDx0aWZmOk9yaWVudGF0aW9uPjE8L3RpZmY6T3JpZW50YXRpb24+CiAgICAgICAgIDx0aWZmOlhSZXNvbHV0aW9uPjcyMDA5MC8xMDAwMDwvdGlmZjpYUmVzb2x1dGlvbj4KICAgICAgICAgPHRpZmY6WVJlc29sdXRpb24+NzIwMDkwLzEwMDAwPC90aWZmOllSZXNvbHV0aW9uPgogICAgICAgICA8dGlmZjpSZXNvbHV0aW9uVW5pdD4yPC90aWZmOlJlc29sdXRpb25Vbml0PgogICAgICAgICA8ZXhpZjpDb2xvclNwYWNlPjE8L2V4aWY6Q29sb3JTcGFjZT4KICAgICAgICAgPGV4aWY6UGl4ZWxYRGltZW5zaW9uPjI0PC9leGlmOlBpeGVsWERpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6UGl4ZWxZRGltZW5zaW9uPjI0PC9leGlmOlBpeGVsWURpbWVuc2lvbj4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgCjw/eHBhY2tldCBlbmQ9InciPz5WKqp9AAAAIGNIUk0AAHolAACAgwAA+f8AAIDpAAB1MAAA6mAAADqYAAAXb5JfxUYAAAYXSURBVHjafFZtUFTXGX7e9z27sveuMCwYV8ElrA7YSFYHtJUPkaaI0aRqG8wP00zUzljDINNSA/2ROtpO24SxnahlxjYd7SSjmUkymcxYlDhQPzHGisEVp8HwYWCVVVgEsrsuLnL74+5uqTF9Z+7cO/d8PO95zvO851BlZSV0XQcAMDOYGQBARDhX3JRmMDYZwLPMWAzGHACYIgwS46oBNBNwtOL8CwE8EkSEUCgE2rJlC2w2G4go8Zwo/bMDgnoG6gxLfAAAYvPDMCCszKTAMIAGAhrWnf15AAAMwwARIRKJgDZv3gy73Q4iAjPjxIr9VVOMRhbAYKB8zvrO0llrfEsdKwLZek6YAPSFvtSu3GtLawu0ZJ6625SHGBQB1T88t6MxvopgMAjaunUrdF0HM+P4yv27DMYeJmB1RqW3Jnf3tQX2p0L4P9EXuqEd7PmDp+XuMU9sRbvXnnt1TxxACgoKYLVacbzsQDUJGkSATe6qi28uPtzusM6Kxie6NHLGUX3lxVUNX9StPHnn4wy3njuUYcu6n2pNi66avcEXnByP/nv8aiaIyrqz2gO5A9+9FI1GIfn5+WhZdTAdjFMkwMvZOy7uWnTAOz3L4Yk71m3t69fdfTDoUGTBeHTUfiHQ6lo7Z2OXJvpDAChKe+aOCdKRKWxZ2+1qb3yyd3GYmRkQ7GQBVs99wfv6on3eR2k4PdTkDEbH7IuS8/svld/561PJS/pDk1/bzwx94pze7xc5v/H+YPY6r5BAkdrJzODTK46lE6PeYEJt7u+8j+OZwCBiEAgAoNgKJoEQf6PvNvdrXgtZoNhSf7q0KZ3B2AQmVMze0Jmt54S/DcDCVig2NcvEUGxJAE4Pl+YOr0iv6BRSIPAmBeBZAmHlE2sH4p1uhrq1s0MnnEQMBsf8wRASAICQQCCITN1X7/sOuc0kgOVp3/fPs2WHv+coG7gQOJUnLGsUCTxEjPzUohEA+NfIWUdtx0+efzA1kSSkIGyBAQNCKgHAEBAJ3u79U7kiAcWoem/gb5Fd33nrH3kp+SMWtuAB+GllMJxMjCx9QRgA3uiqL5kwHiTlpxb3smlfMDGYGPP1hcMAkJvs8ScpfdJspdj+MK6Pf+5+u29vyb4lR4+BGEziVESAkEpw6Av1OhUpHCz4qOXbzFWz4Ncdj/v/o08Lt92ODDgZDCEFJYoUGH4mzugP92puPTf0pD3H7wvfdFZdqSxnMtWjoGAAmG9fOLxjwesdjT2/XzIQ7ks3sycYMSEwGHNtWf5bkX5NkYCJBxUBXiGV0XHvosOt54Zey33j/K+8P33++vjnbiGJbbLE+J9SANAb6nJ2B79wcUwETAwQQ7fMjPzMvfP8ja87HUIKMOiaAqMZhrGmLdAy78eZrwwsTS0eObTs+IdtgVanxBUExqGbb5VzrIISGIoUXsmqbgEhJldCQWqRf27SvPAn/o8XmgLhZsUkR4ll37mhk3n94Z4OlzY/7NLcYZfm7o1z2zT4vsvUNSXqprBCkmiTFbPX90/fh8GIT2sf+zTPdDMf4dVnNg4z+E0ixsGeBs9jd5ViSgLHjCb/peaR+MD3d4/ZJg2llyuG2Vwy7QWAs8PNnn1f7vkGSGxAzE6mk+kxkx/p/4unffSCR0hAoL1EBCYiPNdWNcwkNQTCR7feWX6g+7f/A7I8rcw/U6UEe0Ndrhc/W7mtL9ztmqlSgstSS/zTJ28dalpOpkRryrwbhwBACgsLMWPGDOT4ll3qyeqAkJTdCF7P/CrUY/GkLL1rE+2hTbSH8+0Lb/WEuhzhyaA905blf9Vd/895WnZwLHrPevir/cvOB1oLYpTtLrm6oYGIMDExAaqtrUVKSgqYGSKCk0WHq5ikkWEWtNL0imv5qUW+RclLRjJsrhBAuH1/QL8R7HR4xy5nescuP23E6hOA6mLv+sb4uTw6Ogqqq6uDpmkQkcStorX4XRcM1FjZ+kvFFjCJKU1WpkNJJUqIMtX1RyLeX3JtQ0JRhmGYZ/L27duRnJycuFGISOJ9pqh5lrB6iYgqGOxRrOaa54DcZmKvkJxk8JHC9rKh+KVhOsD4+Dj+MwADIf8n5m4xGwAAAABJRU5ErkJggg==")}div.vis-network div.vis-edit-mode button.vis-button.vis-edit,div.vis-network div.vis-manipulation button.vis-button.vis-edit{background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAACXBIWXMAAAsTAAALEwEAmpwYAAAKT2lDQ1BQaG90b3Nob3AgSUNDIHByb2ZpbGUAAHjanVNnVFPpFj333vRCS4iAlEtvUhUIIFJCi4AUkSYqIQkQSoghodkVUcERRUUEG8igiAOOjoCMFVEsDIoK2AfkIaKOg6OIisr74Xuja9a89+bN/rXXPues852zzwfACAyWSDNRNYAMqUIeEeCDx8TG4eQuQIEKJHAAEAizZCFz/SMBAPh+PDwrIsAHvgABeNMLCADATZvAMByH/w/qQplcAYCEAcB0kThLCIAUAEB6jkKmAEBGAYCdmCZTAKAEAGDLY2LjAFAtAGAnf+bTAICd+Jl7AQBblCEVAaCRACATZYhEAGg7AKzPVopFAFgwABRmS8Q5ANgtADBJV2ZIALC3AMDOEAuyAAgMADBRiIUpAAR7AGDIIyN4AISZABRG8lc88SuuEOcqAAB4mbI8uSQ5RYFbCC1xB1dXLh4ozkkXKxQ2YQJhmkAuwnmZGTKBNA/g88wAAKCRFRHgg/P9eM4Ors7ONo62Dl8t6r8G/yJiYuP+5c+rcEAAAOF0ftH+LC+zGoA7BoBt/qIl7gRoXgugdfeLZrIPQLUAoOnaV/Nw+H48PEWhkLnZ2eXk5NhKxEJbYcpXff5nwl/AV/1s+X48/Pf14L7iJIEyXYFHBPjgwsz0TKUcz5IJhGLc5o9H/LcL//wd0yLESWK5WCoU41EScY5EmozzMqUiiUKSKcUl0v9k4t8s+wM+3zUAsGo+AXuRLahdYwP2SycQWHTA4vcAAPK7b8HUKAgDgGiD4c93/+8//UegJQCAZkmScQAAXkQkLlTKsz/HCAAARKCBKrBBG/TBGCzABhzBBdzBC/xgNoRCJMTCQhBCCmSAHHJgKayCQiiGzbAdKmAv1EAdNMBRaIaTcA4uwlW4Dj1wD/phCJ7BKLyBCQRByAgTYSHaiAFiilgjjggXmYX4IcFIBBKLJCDJiBRRIkuRNUgxUopUIFVIHfI9cgI5h1xGupE7yAAygvyGvEcxlIGyUT3UDLVDuag3GoRGogvQZHQxmo8WoJvQcrQaPYw2oefQq2gP2o8+Q8cwwOgYBzPEbDAuxsNCsTgsCZNjy7EirAyrxhqwVqwDu4n1Y8+xdwQSgUXACTYEd0IgYR5BSFhMWE7YSKggHCQ0EdoJNwkDhFHCJyKTqEu0JroR+cQYYjIxh1hILCPWEo8TLxB7iEPENyQSiUMyJ7mQAkmxpFTSEtJG0m5SI+ksqZs0SBojk8naZGuyBzmULCAryIXkneTD5DPkG+Qh8lsKnWJAcaT4U+IoUspqShnlEOU05QZlmDJBVaOaUt2ooVQRNY9aQq2htlKvUYeoEzR1mjnNgxZJS6WtopXTGmgXaPdpr+h0uhHdlR5Ol9BX0svpR+iX6AP0dwwNhhWDx4hnKBmbGAcYZxl3GK+YTKYZ04sZx1QwNzHrmOeZD5lvVVgqtip8FZHKCpVKlSaVGyovVKmqpqreqgtV81XLVI+pXlN9rkZVM1PjqQnUlqtVqp1Q61MbU2epO6iHqmeob1Q/pH5Z/YkGWcNMw09DpFGgsV/jvMYgC2MZs3gsIWsNq4Z1gTXEJrHN2Xx2KruY/R27iz2qqaE5QzNKM1ezUvOUZj8H45hx+Jx0TgnnKKeX836K3hTvKeIpG6Y0TLkxZVxrqpaXllirSKtRq0frvTau7aedpr1Fu1n7gQ5Bx0onXCdHZ4/OBZ3nU9lT3acKpxZNPTr1ri6qa6UbobtEd79up+6Ynr5egJ5Mb6feeb3n+hx9L/1U/W36p/VHDFgGswwkBtsMzhg8xTVxbzwdL8fb8VFDXcNAQ6VhlWGX4YSRudE8o9VGjUYPjGnGXOMk423GbcajJgYmISZLTepN7ppSTbmmKaY7TDtMx83MzaLN1pk1mz0x1zLnm+eb15vft2BaeFostqi2uGVJsuRaplnutrxuhVo5WaVYVVpds0atna0l1rutu6cRp7lOk06rntZnw7Dxtsm2qbcZsOXYBtuutm22fWFnYhdnt8Wuw+6TvZN9un2N/T0HDYfZDqsdWh1+c7RyFDpWOt6azpzuP33F9JbpL2dYzxDP2DPjthPLKcRpnVOb00dnF2e5c4PziIuJS4LLLpc+Lpsbxt3IveRKdPVxXeF60vWdm7Obwu2o26/uNu5p7ofcn8w0nymeWTNz0MPIQ+BR5dE/C5+VMGvfrH5PQ0+BZ7XnIy9jL5FXrdewt6V3qvdh7xc+9j5yn+M+4zw33jLeWV/MN8C3yLfLT8Nvnl+F30N/I/9k/3r/0QCngCUBZwOJgUGBWwL7+Hp8Ib+OPzrbZfay2e1BjKC5QRVBj4KtguXBrSFoyOyQrSH355jOkc5pDoVQfujW0Adh5mGLw34MJ4WHhVeGP45wiFga0TGXNXfR3ENz30T6RJZE3ptnMU85ry1KNSo+qi5qPNo3ujS6P8YuZlnM1VidWElsSxw5LiquNm5svt/87fOH4p3iC+N7F5gvyF1weaHOwvSFpxapLhIsOpZATIhOOJTwQRAqqBaMJfITdyWOCnnCHcJnIi/RNtGI2ENcKh5O8kgqTXqS7JG8NXkkxTOlLOW5hCepkLxMDUzdmzqeFpp2IG0yPTq9MYOSkZBxQqohTZO2Z+pn5mZ2y6xlhbL+xW6Lty8elQfJa7OQrAVZLQq2QqboVFoo1yoHsmdlV2a/zYnKOZarnivN7cyzytuQN5zvn//tEsIS4ZK2pYZLVy0dWOa9rGo5sjxxedsK4xUFK4ZWBqw8uIq2Km3VT6vtV5eufr0mek1rgV7ByoLBtQFr6wtVCuWFfevc1+1dT1gvWd+1YfqGnRs+FYmKrhTbF5cVf9go3HjlG4dvyr+Z3JS0qavEuWTPZtJm6ebeLZ5bDpaql+aXDm4N2dq0Dd9WtO319kXbL5fNKNu7g7ZDuaO/PLi8ZafJzs07P1SkVPRU+lQ27tLdtWHX+G7R7ht7vPY07NXbW7z3/T7JvttVAVVN1WbVZftJ+7P3P66Jqun4lvttXa1ObXHtxwPSA/0HIw6217nU1R3SPVRSj9Yr60cOxx++/p3vdy0NNg1VjZzG4iNwRHnk6fcJ3/ceDTradox7rOEH0x92HWcdL2pCmvKaRptTmvtbYlu6T8w+0dbq3nr8R9sfD5w0PFl5SvNUyWna6YLTk2fyz4ydlZ19fi753GDborZ752PO32oPb++6EHTh0kX/i+c7vDvOXPK4dPKy2+UTV7hXmq86X23qdOo8/pPTT8e7nLuarrlca7nuer21e2b36RueN87d9L158Rb/1tWeOT3dvfN6b/fF9/XfFt1+cif9zsu72Xcn7q28T7xf9EDtQdlD3YfVP1v+3Njv3H9qwHeg89HcR/cGhYPP/pH1jw9DBY+Zj8uGDYbrnjg+OTniP3L96fynQ89kzyaeF/6i/suuFxYvfvjV69fO0ZjRoZfyl5O/bXyl/erA6xmv28bCxh6+yXgzMV70VvvtwXfcdx3vo98PT+R8IH8o/2j5sfVT0Kf7kxmTk/8EA5jz/GMzLdsAAEEOaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8P3hwYWNrZXQgYmVnaW49Iu+7vyIgaWQ9Ilc1TTBNcENlaGlIenJlU3pOVGN6a2M5ZCI/Pgo8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJBZG9iZSBYTVAgQ29yZSA1LjUtYzAyMSA3OS4xNTQ5MTEsIDIwMTMvMTAvMjktMTE6NDc6MTYgICAgICAgICI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyIKICAgICAgICAgICAgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIgogICAgICAgICAgICB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIKICAgICAgICAgICAgeG1sbnM6c3RFdnQ9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZUV2ZW50IyIKICAgICAgICAgICAgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiCiAgICAgICAgICAgIHhtbG5zOnBob3Rvc2hvcD0iaHR0cDovL25zLmFkb2JlLmNvbS9waG90b3Nob3AvMS4wLyIKICAgICAgICAgICAgeG1sbnM6dGlmZj0iaHR0cDovL25zLmFkb2JlLmNvbS90aWZmLzEuMC8iCiAgICAgICAgICAgIHhtbG5zOmV4aWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vZXhpZi8xLjAvIj4KICAgICAgICAgPHhtcDpDcmVhdG9yVG9vbD5BZG9iZSBQaG90b3Nob3AgQ0MgKFdpbmRvd3MpPC94bXA6Q3JlYXRvclRvb2w+CiAgICAgICAgIDx4bXA6Q3JlYXRlRGF0ZT4yMDE0LTAxLTIyVDE5OjI0OjUxKzAxOjAwPC94bXA6Q3JlYXRlRGF0ZT4KICAgICAgICAgPHhtcDpNZXRhZGF0YURhdGU+MjAxNC0wMi0wNVQxNDoxMjoyNSswMTowMDwveG1wOk1ldGFkYXRhRGF0ZT4KICAgICAgICAgPHhtcDpNb2RpZnlEYXRlPjIwMTQtMDItMDVUMTQ6MTI6MjUrMDE6MDA8L3htcDpNb2RpZnlEYXRlPgogICAgICAgICA8ZGM6Zm9ybWF0PmltYWdlL3BuZzwvZGM6Zm9ybWF0PgogICAgICAgICA8eG1wTU06SW5zdGFuY2VJRD54bXAuaWlkOjY5OTM3ZGZjLTJjNzQtYTU0YS05OTIzLTQyMmZhNDNkMjljNDwveG1wTU06SW5zdGFuY2VJRD4KICAgICAgICAgPHhtcE1NOkRvY3VtZW50SUQ+eG1wLmRpZDpFQTc2MkY5Njc0ODNFMzExOTQ4QkQxM0UyQkU3OTlBMTwveG1wTU06RG9jdW1lbnRJRD4KICAgICAgICAgPHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD54bXAuZGlkOjczQjYyQUFEOTE4M0UzMTE5NDhCRDEzRTJCRTc5OUExPC94bXBNTTpPcmlnaW5hbERvY3VtZW50SUQ+CiAgICAgICAgIDx4bXBNTTpIaXN0b3J5PgogICAgICAgICAgICA8cmRmOlNlcT4KICAgICAgICAgICAgICAgPHJkZjpsaSByZGY6cGFyc2VUeXBlPSJSZXNvdXJjZSI+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDphY3Rpb24+Y3JlYXRlZDwvc3RFdnQ6YWN0aW9uPgogICAgICAgICAgICAgICAgICA8c3RFdnQ6aW5zdGFuY2VJRD54bXAuaWlkOjczQjYyQUFEOTE4M0UzMTE5NDhCRDEzRTJCRTc5OUExPC9zdEV2dDppbnN0YW5jZUlEPgogICAgICAgICAgICAgICAgICA8c3RFdnQ6d2hlbj4yMDE0LTAxLTIyVDE5OjI0OjUxKzAxOjAwPC9zdEV2dDp3aGVuPgogICAgICAgICAgICAgICAgICA8c3RFdnQ6c29mdHdhcmVBZ2VudD5BZG9iZSBQaG90b3Nob3AgQ1M2IChXaW5kb3dzKTwvc3RFdnQ6c29mdHdhcmVBZ2VudD4KICAgICAgICAgICAgICAgPC9yZGY6bGk+CiAgICAgICAgICAgICAgIDxyZGY6bGkgcmRmOnBhcnNlVHlwZT0iUmVzb3VyY2UiPgogICAgICAgICAgICAgICAgICA8c3RFdnQ6YWN0aW9uPnNhdmVkPC9zdEV2dDphY3Rpb24+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDppbnN0YW5jZUlEPnhtcC5paWQ6RUE2MEEyNEUxOTg0RTMxMUFEQUZFRkU2RUMzMzNFMDM8L3N0RXZ0Omluc3RhbmNlSUQ+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDp3aGVuPjIwMTQtMDEtMjNUMTk6MTg6MDcrMDE6MDA8L3N0RXZ0OndoZW4+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDpzb2Z0d2FyZUFnZW50PkFkb2JlIFBob3Rvc2hvcCBDUzYgKFdpbmRvd3MpPC9zdEV2dDpzb2Z0d2FyZUFnZW50PgogICAgICAgICAgICAgICAgICA8c3RFdnQ6Y2hhbmdlZD4vPC9zdEV2dDpjaGFuZ2VkPgogICAgICAgICAgICAgICA8L3JkZjpsaT4KICAgICAgICAgICAgICAgPHJkZjpsaSByZGY6cGFyc2VUeXBlPSJSZXNvdXJjZSI+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDphY3Rpb24+c2F2ZWQ8L3N0RXZ0OmFjdGlvbj4KICAgICAgICAgICAgICAgICAgPHN0RXZ0Omluc3RhbmNlSUQ+eG1wLmlpZDozOWNhNzE5ZC03YzNlLTUyNGEtYmY1NS03NGVmMmM1MzE0YTc8L3N0RXZ0Omluc3RhbmNlSUQ+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDp3aGVuPjIwMTQtMDItMDVUMTQ6MTI6MjUrMDE6MDA8L3N0RXZ0OndoZW4+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDpzb2Z0d2FyZUFnZW50PkFkb2JlIFBob3Rvc2hvcCBDQyAoV2luZG93cyk8L3N0RXZ0OnNvZnR3YXJlQWdlbnQ+CiAgICAgICAgICAgICAgICAgIDxzdEV2dDpjaGFuZ2VkPi88L3N0RXZ0OmNoYW5nZWQ+CiAgICAgICAgICAgICAgIDwvcmRmOmxpPgogICAgICAgICAgICAgICA8cmRmOmxpIHJkZjpwYXJzZVR5cGU9IlJlc291cmNlIj4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OmFjdGlvbj5jb252ZXJ0ZWQ8L3N0RXZ0OmFjdGlvbj4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OnBhcmFtZXRlcnM+ZnJvbSBhcHBsaWNhdGlvbi92bmQuYWRvYmUucGhvdG9zaG9wIHRvIGltYWdlL3BuZzwvc3RFdnQ6cGFyYW1ldGVycz4KICAgICAgICAgICAgICAgPC9yZGY6bGk+CiAgICAgICAgICAgICAgIDxyZGY6bGkgcmRmOnBhcnNlVHlwZT0iUmVzb3VyY2UiPgogICAgICAgICAgICAgICAgICA8c3RFdnQ6YWN0aW9uPmRlcml2ZWQ8L3N0RXZ0OmFjdGlvbj4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OnBhcmFtZXRlcnM+Y29udmVydGVkIGZyb20gYXBwbGljYXRpb24vdm5kLmFkb2JlLnBob3Rvc2hvcCB0byBpbWFnZS9wbmc8L3N0RXZ0OnBhcmFtZXRlcnM+CiAgICAgICAgICAgICAgIDwvcmRmOmxpPgogICAgICAgICAgICAgICA8cmRmOmxpIHJkZjpwYXJzZVR5cGU9IlJlc291cmNlIj4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OmFjdGlvbj5zYXZlZDwvc3RFdnQ6YWN0aW9uPgogICAgICAgICAgICAgICAgICA8c3RFdnQ6aW5zdGFuY2VJRD54bXAuaWlkOjY5OTM3ZGZjLTJjNzQtYTU0YS05OTIzLTQyMmZhNDNkMjljNDwvc3RFdnQ6aW5zdGFuY2VJRD4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OndoZW4+MjAxNC0wMi0wNVQxNDoxMjoyNSswMTowMDwvc3RFdnQ6d2hlbj4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OnNvZnR3YXJlQWdlbnQ+QWRvYmUgUGhvdG9zaG9wIENDIChXaW5kb3dzKTwvc3RFdnQ6c29mdHdhcmVBZ2VudD4KICAgICAgICAgICAgICAgICAgPHN0RXZ0OmNoYW5nZWQ+Lzwvc3RFdnQ6Y2hhbmdlZD4KICAgICAgICAgICAgICAgPC9yZGY6bGk+CiAgICAgICAgICAgIDwvcmRmOlNlcT4KICAgICAgICAgPC94bXBNTTpIaXN0b3J5PgogICAgICAgICA8eG1wTU06RGVyaXZlZEZyb20gcmRmOnBhcnNlVHlwZT0iUmVzb3VyY2UiPgogICAgICAgICAgICA8c3RSZWY6aW5zdGFuY2VJRD54bXAuaWlkOjM5Y2E3MTlkLTdjM2UtNTI0YS1iZjU1LTc0ZWYyYzUzMTRhNzwvc3RSZWY6aW5zdGFuY2VJRD4KICAgICAgICAgICAgPHN0UmVmOmRvY3VtZW50SUQ+eG1wLmRpZDpFQTc2MkY5Njc0ODNFMzExOTQ4QkQxM0UyQkU3OTlBMTwvc3RSZWY6ZG9jdW1lbnRJRD4KICAgICAgICAgICAgPHN0UmVmOm9yaWdpbmFsRG9jdW1lbnRJRD54bXAuZGlkOjczQjYyQUFEOTE4M0UzMTE5NDhCRDEzRTJCRTc5OUExPC9zdFJlZjpvcmlnaW5hbERvY3VtZW50SUQ+CiAgICAgICAgIDwveG1wTU06RGVyaXZlZEZyb20+CiAgICAgICAgIDxwaG90b3Nob3A6Q29sb3JNb2RlPjM8L3Bob3Rvc2hvcDpDb2xvck1vZGU+CiAgICAgICAgIDxwaG90b3Nob3A6SUNDUHJvZmlsZT5zUkdCIElFQzYxOTY2LTIuMTwvcGhvdG9zaG9wOklDQ1Byb2ZpbGU+CiAgICAgICAgIDx0aWZmOk9yaWVudGF0aW9uPjE8L3RpZmY6T3JpZW50YXRpb24+CiAgICAgICAgIDx0aWZmOlhSZXNvbHV0aW9uPjcyMDA5MC8xMDAwMDwvdGlmZjpYUmVzb2x1dGlvbj4KICAgICAgICAgPHRpZmY6WVJlc29sdXRpb24+NzIwMDkwLzEwMDAwPC90aWZmOllSZXNvbHV0aW9uPgogICAgICAgICA8dGlmZjpSZXNvbHV0aW9uVW5pdD4yPC90aWZmOlJlc29sdXRpb25Vbml0PgogICAgICAgICA8ZXhpZjpDb2xvclNwYWNlPjE8L2V4aWY6Q29sb3JTcGFjZT4KICAgICAgICAgPGV4aWY6UGl4ZWxYRGltZW5zaW9uPjI0PC9leGlmOlBpeGVsWERpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6UGl4ZWxZRGltZW5zaW9uPjI0PC9leGlmOlBpeGVsWURpbWVuc2lvbj4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgCjw/eHBhY2tldCBlbmQ9InciPz4ykninAAAAIGNIUk0AAHolAACAgwAA+f8AAIDpAAB1MAAA6mAAADqYAAAXb5JfxUYAAAYpSURBVHjafFZtTFvnFX7Oea+NudiY2Hwam4CBlgQwXdKREDKUoYg0jbRJ29RJ2VZ1mjRFUxSpA3VTfkzJfkQbS7spU6rtx5Z2UtppScjaHxvLuiatWi2jLEoMIUDCh23g2gbj7+tPuPvhOurawPl1dc99n+c55z33fV46ceIEZFkGADAziAgAQERoe/9ZK4GPM/AcgbsIXAcABCgMvkfAqAa89eDoJyF8LogIqqqChoaGYDAYHr8kItS8uc8iIH6iAa9IkAo5EAQX8pqmgUVBCBggYFgDhv0/GAsBgKZpICJkMhnQ4OAgZFkGEYGZUXmp+0cS+CKBwWA0DVRPOg5Zl2q6zaHyJlnVAMQXVTkwHrUqH0Xsvn+tdQAAMQDgpPLS2MViFY8rkGUZzIzaS/t/xqCzGggtz9e697zsnKhoLUtim4jOq/LE6x7X0nsh16dEZ5a/O3a2SCAOHjwInU6Hujd6ThJ4mCDQ+b2G232v7v6vwarPbQn8MGlMr+X0kpE3Wr5Zt5hL5HPhqYSdQIfKJ+yhxDPKWC6Xg+jt7UXD5b5KBt1kCHS85Ljd8/On3NupfnhFaZj4rWff1B98B1R/hnUmKd36bdtCNl4g0en4edNE/cXwLq8qMTMIPAQwmo/WuHvObA8+9c58k/dKtD0TyZWXN5YGA7ej7epKxspM//7SoNOdWc/Jyq2wiwhDzPxT8cP0jys3VMM7OmL0/77zn4Ydui3b8uiK0jD7RrA77c9Wd57cefPpF+2T6bWsFPWkaiPTCWvTsZpHFU+XrS+8G3AR08F6X+1FJvBxQQzHQOWk2SmrW4FPX/U2LVwPuDZj+fJKl2khPpeyAqA9rzR/YqwuiWXX8taN/CabGkrVuq9YJlkQQDjOAJ5jAhz9Vt9W4N5/rNp8I+vtMV/aZm4zLnUNNt0urdYnF68HWoJj4Wo1mLGUNRr8LEgDgNqeCh8xQIKOsgC7iAjVe83rT9zQa8uNM28u70kspessu8q8zq/V3NcZpVzb9+0zmVhOvvvrhaMVzrJg0zeq7xMVCCwdpnWSGBqjUyJwLTFgbvxie3w31uoWR1Y74r60rdxZqrR8q85t2W2MGCp12bm/KC3hyaSTiMhxuGrKcahqpbjOaDOoEhOEoFqJQCCJvqA85I6bfTdDjQlf2lbxVNlS6wt19yy7jRHZZlDnrinNj/6sHMhnNw2Ogco7O79e5fm/xQywRBBCEAuwn4gQ96bkYj4Vyuq9N1Z3Bj4Od5bs0MXt/dZZ21ctiqFan174q985P+Lfp+U1g7XDON/1ctP458WlVjLyJhOISZE0wM0S1QfuRC3lTjkJAKKEtNC9eIOhSh9xHLZOJRZTFuXDsEoStLkR/768ummsaJG9Pb9oe+9J+xaeSVokiQDSJphAo5uaBuWjiKP4QTqS1cUWU7ayesN66wu22frD1vmVW6GW6T8u9eVjGyZzs+w78Nqu0a2mbvVu1KEJQAgeZRL0liQYyx+GOmKeQpu0rMYsAJPNEFGD2dLodLIy6c9Ys7G8yeSUl3tf2/X3rcBVJSOv34l3sCBogi7z1LH/rBHjl4IJ93/ncQFAnjeImJD0Z8zuCwu9q3djDXqTlAKID5xv+9t2R8n8VcUFBljQ8Gyfe40BYBM4DwDLt8Kue79ZcFkbzfEdbUbv+oN4c9KTtsfm1MbYQqqh+2zrVZYKs/7Ef+byimt1POYiJhDhPBFBIiIEXhxfs7/dfYoIF+auBfYTE/pebx/V8hqBP2ODvD34yvuh/WCAmU75Bx6sIgaI/v5+6PV6JLqUsYr7dpDAoehs0h73pHTWrvKgThYbRSt9UmSjef3MpaUvBz4O72UmADgTOPJguGiZor+/HyUlJWBmJFz+D8xTtlUiOpbwpmrmrweeSXrT+g11k4SBN3RGKUcAVCVdFhyP1nreDbY//NPyEXUlU/Pp4XYycGT6V0Ux2WwWdO7cOZSWlkII8diX7SPPNgDaKdbxoNAxwATBAEkEEgSWCEQAqPAMwqvMdCEwMO0tVqZpWsGTT58+DaPR+PhGIYQAAAgh0P7B3ioW/B0iGiCGiwXbCuOHFSJys6AbYFye2T+xWhT3WYJEIoH/DQBMw3kes8OJPgAAAABJRU5ErkJggg==")}div.vis-network div.vis-edit-mode button.vis-button.vis-edit.vis-edit-mode{background-color:#fcfcfc;border:1px solid #ccc}div.vis-network div.vis-manipulation button.vis-button.vis-connect{background-image:url("data:image/png;base64,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")}div.vis-network div.vis-manipulation button.vis-button.vis-delete{background-image:url("data:image/png;base64,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")}div.vis-network div.vis-edit-mode div.vis-label,div.vis-network div.vis-manipulation div.vis-label{line-height:25px;margin:0 0 0 23px}div.vis-network div.vis-manipulation div.vis-separator-line{background-color:#bdbdbd;display:inline-block;float:left;height:21px;margin:0 7px 0 15px;width:1px}</style>
            <script>/**
 * vis-network
 * https://visjs.github.io/vis-network/
 *
 * A dynamic, browser-based visualization library.
 *
 * @version 9.1.2
 * @date    2022-03-28T20:17:35.342Z
 *
 * @copyright (c) 2011-2017 Almende B.V, http://almende.com
 * @copyright (c) 2017-2019 visjs contributors, https://github.com/visjs
 *
 * @license
 * vis.js is dual licensed under both
 *
 *   1. The Apache 2.0 License
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *   and
 *
 *   2. The MIT License
 *      http://opensource.org/licenses/MIT
 *
 * vis.js may be distributed under either license.
 */
!function(t,e){"object"==typeof exports&&"undefined"!=typeof module?e(exports):"function"==typeof define&&define.amd?define(["exports"],e):e((t="undefined"!=typeof globalThis?globalThis:t||self).vis=t.vis||{})}(this,(function(t){"use strict";var e="undefined"!=typeof globalThis?globalThis:"undefined"!=typeof window?window:"undefined"!=typeof global?global:"undefined"!=typeof self?self:{},i=function(t){return t&&t.Math==Math&&t},n=i("object"==typeof globalThis&&globalThis)||i("object"==typeof window&&window)||i("object"==typeof self&&self)||i("object"==typeof e&&e)||function(){return this}()||Function("return this")(),o=function(t){try{return!!t()}catch(t){return!0}},r=!o((function(){var t=function(){}.bind();return"function"!=typeof t||t.hasOwnProperty("prototype")})),s=r,a=Function.prototype,h=a.apply,l=a.call,d="object"==typeof Reflect&&Reflect.apply||(s?l.bind(h):function(){return l.apply(h,arguments)}),c=r,u=Function.prototype,f=u.bind,p=u.call,v=c&&f.bind(p,p),g=c?function(t){return t&&v(t)}:function(t){return t&&function(){return p.apply(t,arguments)}},y=function(t){return"function"==typeof t},m={},b=!o((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]})),w=r,k=Function.prototype.call,_=w?k.bind(k):function(){return k.apply(k,arguments)},x={},E={}.propertyIsEnumerable,O=Object.getOwnPropertyDescriptor,C=O&&!E.call({1:2},1);x.f=C?function(t){var e=O(this,t);return!!e&&e.enumerable}:E;var S,T,M=function(t,e){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:e}},P=g,D=P({}.toString),I=P("".slice),B=function(t){return I(D(t),8,-1)},z=g,N=o,F=B,A=n.Object,j=z("".split),R=N((function(){return!A("z").propertyIsEnumerable(0)}))?function(t){return"String"==F(t)?j(t,""):A(t)}:A,L=n.TypeError,H=function(t){if(null==t)throw L("Can't call method on "+t);return t},W=R,q=H,V=function(t){return W(q(t))},U=y,Y=function(t){return"object"==typeof t?null!==t:U(t)},X={},G=X,K=n,$=y,Z=function(t){return $(t)?t:void 0},Q=function(t,e){return arguments.length<2?Z(G[t])||Z(K[t]):G[t]&&G[t][e]||K[t]&&K[t][e]},J=g({}.isPrototypeOf),tt=Q("navigator","userAgent")||"",et=n,it=tt,nt=et.process,ot=et.Deno,rt=nt&&nt.versions||ot&&ot.version,st=rt&&rt.v8;st&&(T=(S=st.split("."))[0]>0&&S[0]<4?1:+(S[0]+S[1])),!T&&it&&(!(S=it.match(/Edge\/(\d+)/))||S[1]>=74)&&(S=it.match(/Chrome\/(\d+)/))&&(T=+S[1]);var at=T,ht=at,lt=o,dt=!!Object.getOwnPropertySymbols&&!lt((function(){var t=Symbol();return!String(t)||!(Object(t)instanceof Symbol)||!Symbol.sham&&ht&&ht<41})),ct=dt&&!Symbol.sham&&"symbol"==typeof Symbol.iterator,ut=Q,ft=y,pt=J,vt=ct,gt=n.Object,yt=vt?function(t){return"symbol"==typeof t}:function(t){var e=ut("Symbol");return ft(e)&&pt(e.prototype,gt(t))},mt=n.String,bt=function(t){try{return mt(t)}catch(t){return"Object"}},wt=y,kt=bt,_t=n.TypeError,xt=function(t){if(wt(t))return t;throw _t(kt(t)+" is not a function")},Et=xt,Ot=function(t,e){var i=t[e];return null==i?void 0:Et(i)},Ct=_,St=y,Tt=Y,Mt=n.TypeError,Pt={exports:{}},Dt=n,It=Object.defineProperty,Bt=function(t,e){try{It(Dt,t,{value:e,configurable:!0,writable:!0})}catch(i){Dt[t]=e}return e},zt="__core-js_shared__",Nt=n[zt]||Bt(zt,{}),Ft=Nt;(Pt.exports=function(t,e){return Ft[t]||(Ft[t]=void 0!==e?e:{})})("versions",[]).push({version:"3.21.1",mode:"pure",copyright:"© 2014-2022 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.21.1/LICENSE",source:"https://github.com/zloirock/core-js"});var At=H,jt=n.Object,Rt=function(t){return jt(At(t))},Lt=Rt,Ht=g({}.hasOwnProperty),Wt=Object.hasOwn||function(t,e){return Ht(Lt(t),e)},qt=g,Vt=0,Ut=Math.random(),Yt=qt(1..toString),Xt=function(t){return"Symbol("+(void 0===t?"":t)+")_"+Yt(++Vt+Ut,36)},Gt=n,Kt=Pt.exports,$t=Wt,Zt=Xt,Qt=dt,Jt=ct,te=Kt("wks"),ee=Gt.Symbol,ie=ee&&ee.for,ne=Jt?ee:ee&&ee.withoutSetter||Zt,oe=function(t){if(!$t(te,t)||!Qt&&"string"!=typeof te[t]){var e="Symbol."+t;Qt&&$t(ee,t)?te[t]=ee[t]:te[t]=Jt&&ie?ie(e):ne(e)}return te[t]},re=_,se=Y,ae=yt,he=Ot,le=function(t,e){var i,n;if("string"===e&&St(i=t.toString)&&!Tt(n=Ct(i,t)))return n;if(St(i=t.valueOf)&&!Tt(n=Ct(i,t)))return n;if("string"!==e&&St(i=t.toString)&&!Tt(n=Ct(i,t)))return n;throw Mt("Can't convert object to primitive value")},de=oe,ce=n.TypeError,ue=de("toPrimitive"),fe=function(t,e){if(!se(t)||ae(t))return t;var i,n=he(t,ue);if(n){if(void 0===e&&(e="default"),i=re(n,t,e),!se(i)||ae(i))return i;throw ce("Can't convert object to primitive value")}return void 0===e&&(e="number"),le(t,e)},pe=yt,ve=function(t){var e=fe(t,"string");return pe(e)?e:e+""},ge=Y,ye=n.document,me=ge(ye)&&ge(ye.createElement),be=function(t){return me?ye.createElement(t):{}},we=be,ke=!b&&!o((function(){return 7!=Object.defineProperty(we("div"),"a",{get:function(){return 7}}).a})),_e=b,xe=_,Ee=x,Oe=M,Ce=V,Se=ve,Te=Wt,Me=ke,Pe=Object.getOwnPropertyDescriptor;m.f=_e?Pe:function(t,e){if(t=Ce(t),e=Se(e),Me)try{return Pe(t,e)}catch(t){}if(Te(t,e))return Oe(!xe(Ee.f,t,e),t[e])};var De=o,Ie=y,Be=/#|\.prototype\./,ze=function(t,e){var i=Fe[Ne(t)];return i==je||i!=Ae&&(Ie(e)?De(e):!!e)},Ne=ze.normalize=function(t){return String(t).replace(Be,".").toLowerCase()},Fe=ze.data={},Ae=ze.NATIVE="N",je=ze.POLYFILL="P",Re=ze,Le=xt,He=r,We=g(g.bind),qe=function(t,e){return Le(t),void 0===e?t:He?We(t,e):function(){return t.apply(e,arguments)}},Ve={},Ue=b&&o((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype})),Ye=n,Xe=Y,Ge=Ye.String,Ke=Ye.TypeError,$e=function(t){if(Xe(t))return t;throw Ke(Ge(t)+" is not an object")},Ze=b,Qe=ke,Je=Ue,ti=$e,ei=ve,ii=n.TypeError,ni=Object.defineProperty,oi=Object.getOwnPropertyDescriptor,ri="enumerable",si="configurable",ai="writable";Ve.f=Ze?Je?function(t,e,i){if(ti(t),e=ei(e),ti(i),"function"==typeof t&&"prototype"===e&&"value"in i&&ai in i&&!i.writable){var n=oi(t,e);n&&n.writable&&(t[e]=i.value,i={configurable:si in i?i.configurable:n.configurable,enumerable:ri in i?i.enumerable:n.enumerable,writable:!1})}return ni(t,e,i)}:ni:function(t,e,i){if(ti(t),e=ei(e),ti(i),Qe)try{return ni(t,e,i)}catch(t){}if("get"in i||"set"in i)throw ii("Accessors not supported");return"value"in i&&(t[e]=i.value),t};var hi=Ve,li=M,di=b?function(t,e,i){return hi.f(t,e,li(1,i))}:function(t,e,i){return t[e]=i,t},ci=n,ui=d,fi=g,pi=y,vi=m.f,gi=Re,yi=X,mi=qe,bi=di,wi=Wt,ki=function(t){var e=function(i,n,o){if(this instanceof e){switch(arguments.length){case 0:return new t;case 1:return new t(i);case 2:return new t(i,n)}return new t(i,n,o)}return ui(t,this,arguments)};return e.prototype=t.prototype,e},_i=function(t,e){var i,n,o,r,s,a,h,l,d=t.target,c=t.global,u=t.stat,f=t.proto,p=c?ci:u?ci[d]:(ci[d]||{}).prototype,v=c?yi:yi[d]||bi(yi,d,{})[d],g=v.prototype;for(o in e)i=!gi(c?o:d+(u?".":"#")+o,t.forced)&&p&&wi(p,o),s=v[o],i&&(a=t.noTargetGet?(l=vi(p,o))&&l.value:p[o]),r=i&&a?a:e[o],i&&typeof s==typeof r||(h=t.bind&&i?mi(r,ci):t.wrap&&i?ki(r):f&&pi(r)?fi(r):r,(t.sham||r&&r.sham||s&&s.sham)&&bi(h,"sham",!0),bi(v,o,h),f&&(wi(yi,n=d+"Prototype")||bi(yi,n,{}),bi(yi[n],o,r),t.real&&g&&!g[o]&&bi(g,o,r)))},xi=Math.ceil,Ei=Math.floor,Oi=function(t){var e=+t;return e!=e||0===e?0:(e>0?Ei:xi)(e)},Ci=Oi,Si=Math.max,Ti=Math.min,Mi=function(t,e){var i=Ci(t);return i<0?Si(i+e,0):Ti(i,e)},Pi=Oi,Di=Math.min,Ii=function(t){return t>0?Di(Pi(t),9007199254740991):0},Bi=function(t){return Ii(t.length)},zi=V,Ni=Mi,Fi=Bi,Ai=function(t){return function(e,i,n){var o,r=zi(e),s=Fi(r),a=Ni(n,s);if(t&&i!=i){for(;s>a;)if((o=r[a++])!=o)return!0}else for(;s>a;a++)if((t||a in r)&&r[a]===i)return t||a||0;return!t&&-1}},ji={includes:Ai(!0),indexOf:Ai(!1)},Ri={},Li=Wt,Hi=V,Wi=ji.indexOf,qi=Ri,Vi=g([].push),Ui=function(t,e){var i,n=Hi(t),o=0,r=[];for(i in n)!Li(qi,i)&&Li(n,i)&&Vi(r,i);for(;e.length>o;)Li(n,i=e[o++])&&(~Wi(r,i)||Vi(r,i));return r},Yi=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"],Xi=Ui,Gi=Yi,Ki=Object.keys||function(t){return Xi(t,Gi)},$i={};$i.f=Object.getOwnPropertySymbols;var Zi=b,Qi=g,Ji=_,tn=o,en=Ki,nn=$i,on=x,rn=Rt,sn=R,an=Object.assign,hn=Object.defineProperty,ln=Qi([].concat),dn=!an||tn((function(){if(Zi&&1!==an({b:1},an(hn({},"a",{enumerable:!0,get:function(){hn(this,"b",{value:3,enumerable:!1})}}),{b:2})).b)return!0;var t={},e={},i=Symbol(),n="abcdefghijklmnopqrst";return t[i]=7,n.split("").forEach((function(t){e[t]=t})),7!=an({},t)[i]||en(an({},e)).join("")!=n}))?function(t,e){for(var i=rn(t),n=arguments.length,o=1,r=nn.f,s=on.f;n>o;)for(var a,h=sn(arguments[o++]),l=r?ln(en(h),r(h)):en(h),d=l.length,c=0;d>c;)a=l[c++],Zi&&!Ji(s,h,a)||(i[a]=h[a]);return i}:an,cn=dn;_i({target:"Object",stat:!0,forced:Object.assign!==cn},{assign:cn});var un=X.Object.assign,fn=g([].slice),pn=g,vn=xt,gn=Y,yn=Wt,mn=fn,bn=r,wn=n.Function,kn=pn([].concat),_n=pn([].join),xn={},En=function(t,e,i){if(!yn(xn,e)){for(var n=[],o=0;o<e;o++)n[o]="a["+o+"]";xn[e]=wn("C,a","return new C("+_n(n,",")+")")}return xn[e](t,i)},On=bn?wn.bind:function(t){var e=vn(this),i=e.prototype,n=mn(arguments,1),o=function(){var i=kn(n,mn(arguments));return this instanceof o?En(e,i.length,i):e.apply(t,i)};return gn(i)&&(o.prototype=i),o},Cn=On;_i({target:"Function",proto:!0,forced:Function.bind!==Cn},{bind:Cn});var Sn=X,Tn=function(t){return Sn[t+"Prototype"]},Mn=Tn("Function").bind,Pn=J,Dn=Mn,In=Function.prototype,Bn=function(t){var e=t.bind;return t===In||Pn(In,t)&&e===In.bind?Dn:e},zn=Bn;function Nn(t,e,i,n){t.beginPath(),t.arc(e,i,n,0,2*Math.PI,!1),t.closePath()}function Fn(t,e,i,n,o,r){var s=Math.PI/180;n-2*r<0&&(r=n/2),o-2*r<0&&(r=o/2),t.beginPath(),t.moveTo(e+r,i),t.lineTo(e+n-r,i),t.arc(e+n-r,i+r,r,270*s,360*s,!1),t.lineTo(e+n,i+o-r),t.arc(e+n-r,i+o-r,r,0,90*s,!1),t.lineTo(e+r,i+o),t.arc(e+r,i+o-r,r,90*s,180*s,!1),t.lineTo(e,i+r),t.arc(e+r,i+r,r,180*s,270*s,!1),t.closePath()}function An(t,e,i,n,o){var r=.5522848,s=n/2*r,a=o/2*r,h=e+n,l=i+o,d=e+n/2,c=i+o/2;t.beginPath(),t.moveTo(e,c),t.bezierCurveTo(e,c-a,d-s,i,d,i),t.bezierCurveTo(d+s,i,h,c-a,h,c),t.bezierCurveTo(h,c+a,d+s,l,d,l),t.bezierCurveTo(d-s,l,e,c+a,e,c),t.closePath()}function jn(t,e,i,n,o){var r=o*(1/3),s=.5522848,a=n/2*s,h=r/2*s,l=e+n,d=i+r,c=e+n/2,u=i+r/2,f=i+(o-r/2),p=i+o;t.beginPath(),t.moveTo(l,u),t.bezierCurveTo(l,u+h,c+a,d,c,d),t.bezierCurveTo(c-a,d,e,u+h,e,u),t.bezierCurveTo(e,u-h,c-a,i,c,i),t.bezierCurveTo(c+a,i,l,u-h,l,u),t.lineTo(l,f),t.bezierCurveTo(l,f+h,c+a,p,c,p),t.bezierCurveTo(c-a,p,e,f+h,e,f),t.lineTo(e,u)}function Rn(t,e,i,n,o,r){t.beginPath(),t.moveTo(e,i);for(var s=r.length,a=n-e,h=o-i,l=h/a,d=Math.sqrt(a*a+h*h),c=0,u=!0,f=0,p=+r[0];d>=.1;)(p=+r[c++%s])>d&&(p=d),f=Math.sqrt(p*p/(1+l*l)),e+=f=a<0?-f:f,i+=l*f,!0===u?t.lineTo(e,i):t.moveTo(e,i),d-=p,u=!u}var Ln={circle:Nn,dashedLine:Rn,database:jn,diamond:function(t,e,i,n){t.beginPath(),t.lineTo(e,i+n),t.lineTo(e+n,i),t.lineTo(e,i-n),t.lineTo(e-n,i),t.closePath()},ellipse:An,ellipse_vis:An,hexagon:function(t,e,i,n){t.beginPath();var o=2*Math.PI/6;t.moveTo(e+n,i);for(var r=1;r<6;r++)t.lineTo(e+n*Math.cos(o*r),i+n*Math.sin(o*r));t.closePath()},roundRect:Fn,square:function(t,e,i,n){t.beginPath(),t.rect(e-n,i-n,2*n,2*n),t.closePath()},star:function(t,e,i,n){t.beginPath(),i+=.1*(n*=.82);for(var o=0;o<10;o++){var r=o%2==0?1.3*n:.5*n;t.lineTo(e+r*Math.sin(2*o*Math.PI/10),i-r*Math.cos(2*o*Math.PI/10))}t.closePath()},triangle:function(t,e,i,n){t.beginPath(),i+=.275*(n*=1.15);var o=2*n,r=o/2,s=Math.sqrt(3)/6*o,a=Math.sqrt(o*o-r*r);t.moveTo(e,i-(a-s)),t.lineTo(e+r,i+s),t.lineTo(e-r,i+s),t.lineTo(e,i-(a-s)),t.closePath()},triangleDown:function(t,e,i,n){t.beginPath(),i-=.275*(n*=1.15);var o=2*n,r=o/2,s=Math.sqrt(3)/6*o,a=Math.sqrt(o*o-r*r);t.moveTo(e,i+(a-s)),t.lineTo(e+r,i-s),t.lineTo(e-r,i-s),t.lineTo(e,i+(a-s)),t.closePath()}};var Hn={exports:{}};!function(t){function e(t){if(t)return function(t){for(var i in e.prototype)t[i]=e.prototype[i];return t}(t)}t.exports=e,e.prototype.on=e.prototype.addEventListener=function(t,e){return this._callbacks=this._callbacks||{},(this._callbacks["$"+t]=this._callbacks["$"+t]||[]).push(e),this},e.prototype.once=function(t,e){function i(){this.off(t,i),e.apply(this,arguments)}return i.fn=e,this.on(t,i),this},e.prototype.off=e.prototype.removeListener=e.prototype.removeAllListeners=e.prototype.removeEventListener=function(t,e){if(this._callbacks=this._callbacks||{},0==arguments.length)return this._callbacks={},this;var i,n=this._callbacks["$"+t];if(!n)return this;if(1==arguments.length)return delete this._callbacks["$"+t],this;for(var o=0;o<n.length;o++)if((i=n[o])===e||i.fn===e){n.splice(o,1);break}return 0===n.length&&delete this._callbacks["$"+t],this},e.prototype.emit=function(t){this._callbacks=this._callbacks||{};for(var e=new Array(arguments.length-1),i=this._callbacks["$"+t],n=1;n<arguments.length;n++)e[n-1]=arguments[n];if(i){n=0;for(var o=(i=i.slice(0)).length;n<o;++n)i[n].apply(this,e)}return this},e.prototype.listeners=function(t){return this._callbacks=this._callbacks||{},this._callbacks["$"+t]||[]},e.prototype.hasListeners=function(t){return!!this.listeners(t).length}}(Hn);var Wn=Hn.exports,qn={};qn[oe("toStringTag")]="z";var Vn="[object z]"===String(qn),Un=n,Yn=Vn,Xn=y,Gn=B,Kn=oe("toStringTag"),$n=Un.Object,Zn="Arguments"==Gn(function(){return arguments}()),Qn=Yn?Gn:function(t){var e,i,n;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(i=function(t,e){try{return t[e]}catch(t){}}(e=$n(t),Kn))?i:Zn?Gn(e):"Object"==(n=Gn(e))&&Xn(e.callee)?"Arguments":n},Jn=Qn,to=n.String,eo=function(t){if("Symbol"===Jn(t))throw TypeError("Cannot convert a Symbol value to a string");return to(t)},io=g,no=Oi,oo=eo,ro=H,so=io("".charAt),ao=io("".charCodeAt),ho=io("".slice),lo=function(t){return function(e,i){var n,o,r=oo(ro(e)),s=no(i),a=r.length;return s<0||s>=a?t?"":void 0:(n=ao(r,s))<55296||n>56319||s+1===a||(o=ao(r,s+1))<56320||o>57343?t?so(r,s):n:t?ho(r,s,s+2):o-56320+(n-55296<<10)+65536}},co={codeAt:lo(!1),charAt:lo(!0)},uo=y,fo=Nt,po=g(Function.toString);uo(fo.inspectSource)||(fo.inspectSource=function(t){return po(t)});var vo,go,yo,mo=fo.inspectSource,bo=y,wo=mo,ko=n.WeakMap,_o=bo(ko)&&/native code/.test(wo(ko)),xo=Pt.exports,Eo=Xt,Oo=xo("keys"),Co=function(t){return Oo[t]||(Oo[t]=Eo(t))},So=_o,To=n,Mo=g,Po=Y,Do=di,Io=Wt,Bo=Nt,zo=Co,No=Ri,Fo="Object already initialized",Ao=To.TypeError,jo=To.WeakMap;if(So||Bo.state){var Ro=Bo.state||(Bo.state=new jo),Lo=Mo(Ro.get),Ho=Mo(Ro.has),Wo=Mo(Ro.set);vo=function(t,e){if(Ho(Ro,t))throw new Ao(Fo);return e.facade=t,Wo(Ro,t,e),e},go=function(t){return Lo(Ro,t)||{}},yo=function(t){return Ho(Ro,t)}}else{var qo=zo("state");No[qo]=!0,vo=function(t,e){if(Io(t,qo))throw new Ao(Fo);return e.facade=t,Do(t,qo,e),e},go=function(t){return Io(t,qo)?t[qo]:{}},yo=function(t){return Io(t,qo)}}var Vo={set:vo,get:go,has:yo,enforce:function(t){return yo(t)?go(t):vo(t,{})},getterFor:function(t){return function(e){var i;if(!Po(e)||(i=go(e)).type!==t)throw Ao("Incompatible receiver, "+t+" required");return i}}},Uo=b,Yo=Wt,Xo=Function.prototype,Go=Uo&&Object.getOwnPropertyDescriptor,Ko=Yo(Xo,"name"),$o={EXISTS:Ko,PROPER:Ko&&"something"===function(){}.name,CONFIGURABLE:Ko&&(!Uo||Uo&&Go(Xo,"name").configurable)},Zo={},Qo=b,Jo=Ue,tr=Ve,er=$e,ir=V,nr=Ki;Zo.f=Qo&&!Jo?Object.defineProperties:function(t,e){er(t);for(var i,n=ir(e),o=nr(e),r=o.length,s=0;r>s;)tr.f(t,i=o[s++],n[i]);return t};var or,rr=Q("document","documentElement"),sr=$e,ar=Zo,hr=Yi,lr=Ri,dr=rr,cr=be,ur=Co("IE_PROTO"),fr=function(){},pr=function(t){return"<script>"+t+"</"+"script>"},vr=function(t){t.write(pr("")),t.close();var e=t.parentWindow.Object;return t=null,e},gr=function(){try{or=new ActiveXObject("htmlfile")}catch(t){}var t,e;gr="undefined"!=typeof document?document.domain&&or?vr(or):((e=cr("iframe")).style.display="none",dr.appendChild(e),e.src=String("javascript:"),(t=e.contentWindow.document).open(),t.write(pr("document.F=Object")),t.close(),t.F):vr(or);for(var i=hr.length;i--;)delete gr.prototype[hr[i]];return gr()};lr[ur]=!0;var yr,mr,br,wr=Object.create||function(t,e){var i;return null!==t?(fr.prototype=sr(t),i=new fr,fr.prototype=null,i[ur]=t):i=gr(),void 0===e?i:ar.f(i,e)},kr=!o((function(){function t(){}return t.prototype.constructor=null,Object.getPrototypeOf(new t)!==t.prototype})),_r=n,xr=Wt,Er=y,Or=Rt,Cr=kr,Sr=Co("IE_PROTO"),Tr=_r.Object,Mr=Tr.prototype,Pr=Cr?Tr.getPrototypeOf:function(t){var e=Or(t);if(xr(e,Sr))return e[Sr];var i=e.constructor;return Er(i)&&e instanceof i?i.prototype:e instanceof Tr?Mr:null},Dr=di,Ir=function(t,e,i,n){n&&n.enumerable?t[e]=i:Dr(t,e,i)},Br=o,zr=y,Nr=wr,Fr=Pr,Ar=Ir,jr=oe("iterator"),Rr=!1;[].keys&&("next"in(br=[].keys())?(mr=Fr(Fr(br)))!==Object.prototype&&(yr=mr):Rr=!0);var Lr=null==yr||Br((function(){var t={};return yr[jr].call(t)!==t}));zr((yr=Lr?{}:Nr(yr))[jr])||Ar(yr,jr,(function(){return this}));var Hr={IteratorPrototype:yr,BUGGY_SAFARI_ITERATORS:Rr},Wr=Qn,qr=Vn?{}.toString:function(){return"[object "+Wr(this)+"]"},Vr=Vn,Ur=Ve.f,Yr=di,Xr=Wt,Gr=qr,Kr=oe("toStringTag"),$r=function(t,e,i,n){if(t){var o=i?t:t.prototype;Xr(o,Kr)||Ur(o,Kr,{configurable:!0,value:e}),n&&!Vr&&Yr(o,"toString",Gr)}},Zr={},Qr=Hr.IteratorPrototype,Jr=wr,ts=M,es=$r,is=Zr,ns=function(){return this},os=n,rs=y,ss=os.String,as=os.TypeError,hs=g,ls=$e,ds=function(t){if("object"==typeof t||rs(t))return t;throw as("Can't set "+ss(t)+" as a prototype")},cs=Object.setPrototypeOf||("__proto__"in{}?function(){var t,e=!1,i={};try{(t=hs(Object.getOwnPropertyDescriptor(Object.prototype,"__proto__").set))(i,[]),e=i instanceof Array}catch(t){}return function(i,n){return ls(i),ds(n),e?t(i,n):i.__proto__=n,i}}():void 0),us=_i,fs=_,ps=function(t,e,i,n){var o=e+" Iterator";return t.prototype=Jr(Qr,{next:ts(+!n,i)}),es(t,o,!1,!0),is[o]=ns,t},vs=Pr,gs=$r,ys=Ir,ms=Zr,bs=$o.PROPER,ws=Hr.BUGGY_SAFARI_ITERATORS,ks=oe("iterator"),_s="keys",xs="values",Es="entries",Os=function(){return this},Cs=function(t,e,i,n,o,r,s){ps(i,e,n);var a,h,l,d=function(t){if(t===o&&v)return v;if(!ws&&t in f)return f[t];switch(t){case _s:case xs:case Es:return function(){return new i(this,t)}}return function(){return new i(this)}},c=e+" Iterator",u=!1,f=t.prototype,p=f[ks]||f["@@iterator"]||o&&f[o],v=!ws&&p||d(o),g="Array"==e&&f.entries||p;if(g&&(a=vs(g.call(new t)))!==Object.prototype&&a.next&&(gs(a,c,!0,!0),ms[c]=Os),bs&&o==xs&&p&&p.name!==xs&&(u=!0,v=function(){return fs(p,this)}),o)if(h={values:d(xs),keys:r?v:d(_s),entries:d(Es)},s)for(l in h)(ws||u||!(l in f))&&ys(f,l,h[l]);else us({target:e,proto:!0,forced:ws||u},h);return s&&f[ks]!==v&&ys(f,ks,v,{name:o}),ms[e]=v,h},Ss=co.charAt,Ts=eo,Ms=Vo,Ps=Cs,Ds="String Iterator",Is=Ms.set,Bs=Ms.getterFor(Ds);Ps(String,"String",(function(t){Is(this,{type:Ds,string:Ts(t),index:0})}),(function(){var t,e=Bs(this),i=e.string,n=e.index;return n>=i.length?{value:void 0,done:!0}:(t=Ss(i,n),e.index+=t.length,{value:t,done:!1})}));var zs=_,Ns=$e,Fs=Ot,As=function(t,e,i){var n,o;Ns(t);try{if(!(n=Fs(t,"return"))){if("throw"===e)throw i;return i}n=zs(n,t)}catch(t){o=!0,n=t}if("throw"===e)throw i;if(o)throw n;return Ns(n),i},js=$e,Rs=As,Ls=Zr,Hs=oe("iterator"),Ws=Array.prototype,qs=function(t){return void 0!==t&&(Ls.Array===t||Ws[Hs]===t)},Vs=g,Us=o,Ys=y,Xs=Qn,Gs=mo,Ks=function(){},$s=[],Zs=Q("Reflect","construct"),Qs=/^\s*(?:class|function)\b/,Js=Vs(Qs.exec),ta=!Qs.exec(Ks),ea=function(t){if(!Ys(t))return!1;try{return Zs(Ks,$s,t),!0}catch(t){return!1}},ia=function(t){if(!Ys(t))return!1;switch(Xs(t)){case"AsyncFunction":case"GeneratorFunction":case"AsyncGeneratorFunction":return!1}try{return ta||!!Js(Qs,Gs(t))}catch(t){return!0}};ia.sham=!0;var na=!Zs||Us((function(){var t;return ea(ea.call)||!ea(Object)||!ea((function(){t=!0}))||t}))?ia:ea,oa=ve,ra=Ve,sa=M,aa=function(t,e,i){var n=oa(e);n in t?ra.f(t,n,sa(0,i)):t[n]=i},ha=Qn,la=Ot,da=Zr,ca=oe("iterator"),ua=function(t){if(null!=t)return la(t,ca)||la(t,"@@iterator")||da[ha(t)]},fa=_,pa=xt,va=$e,ga=bt,ya=ua,ma=n.TypeError,ba=function(t,e){var i=arguments.length<2?ya(t):e;if(pa(i))return va(fa(i,t));throw ma(ga(t)+" is not iterable")},wa=qe,ka=_,_a=Rt,xa=function(t,e,i,n){try{return n?e(js(i)[0],i[1]):e(i)}catch(e){Rs(t,"throw",e)}},Ea=qs,Oa=na,Ca=Bi,Sa=aa,Ta=ba,Ma=ua,Pa=n.Array,Da=oe("iterator"),Ia=!1;try{var Ba=0,za={next:function(){return{done:!!Ba++}},return:function(){Ia=!0}};za[Da]=function(){return this},Array.from(za,(function(){throw 2}))}catch(t){}var Na=function(t){var e=_a(t),i=Oa(this),n=arguments.length,o=n>1?arguments[1]:void 0,r=void 0!==o;r&&(o=wa(o,n>2?arguments[2]:void 0));var s,a,h,l,d,c,u=Ma(e),f=0;if(!u||this==Pa&&Ea(u))for(s=Ca(e),a=i?new this(s):Pa(s);s>f;f++)c=r?o(e[f],f):e[f],Sa(a,f,c);else for(d=(l=Ta(e,u)).next,a=i?new this:[];!(h=ka(d,l)).done;f++)c=r?xa(l,o,[h.value,f],!0):h.value,Sa(a,f,c);return a.length=f,a},Fa=function(t,e){if(!e&&!Ia)return!1;var i=!1;try{var n={};n[Da]=function(){return{next:function(){return{done:i=!0}}}},t(n)}catch(t){}return i};_i({target:"Array",stat:!0,forced:!Fa((function(t){Array.from(t)}))},{from:Na});var Aa=X.Array.from,ja=Aa,Ra=V,La=Zr,Ha=Vo;Ve.f;var Wa=Cs,qa="Array Iterator",Va=Ha.set,Ua=Ha.getterFor(qa);Wa(Array,"Array",(function(t,e){Va(this,{type:qa,target:Ra(t),index:0,kind:e})}),(function(){var t=Ua(this),e=t.target,i=t.kind,n=t.index++;return!e||n>=e.length?(t.target=void 0,{value:void 0,done:!0}):"keys"==i?{value:n,done:!1}:"values"==i?{value:e[n],done:!1}:{value:[n,e[n]],done:!1}}),"values"),La.Arguments=La.Array;var Ya=ua,Xa={CSSRuleList:0,CSSStyleDeclaration:0,CSSValueList:0,ClientRectList:0,DOMRectList:0,DOMStringList:0,DOMTokenList:1,DataTransferItemList:0,FileList:0,HTMLAllCollection:0,HTMLCollection:0,HTMLFormElement:0,HTMLSelectElement:0,MediaList:0,MimeTypeArray:0,NamedNodeMap:0,NodeList:1,PaintRequestList:0,Plugin:0,PluginArray:0,SVGLengthList:0,SVGNumberList:0,SVGPathSegList:0,SVGPointList:0,SVGStringList:0,SVGTransformList:0,SourceBufferList:0,StyleSheetList:0,TextTrackCueList:0,TextTrackList:0,TouchList:0},Ga=n,Ka=Qn,$a=di,Za=Zr,Qa=oe("toStringTag");for(var Ja in Xa){var th=Ga[Ja],eh=th&&th.prototype;eh&&Ka(eh)!==Qa&&$a(eh,Qa,Ja),Za[Ja]=Za.Array}var ih=Ya,nh=B,oh=Array.isArray||function(t){return"Array"==nh(t)},rh={},sh=Ui,ah=Yi.concat("length","prototype");rh.f=Object.getOwnPropertyNames||function(t){return sh(t,ah)};var hh={},lh=Mi,dh=Bi,ch=aa,uh=n.Array,fh=Math.max,ph=function(t,e,i){for(var n=dh(t),o=lh(e,n),r=lh(void 0===i?n:i,n),s=uh(fh(r-o,0)),a=0;o<r;o++,a++)ch(s,a,t[o]);return s.length=a,s},vh=B,gh=V,yh=rh.f,mh=ph,bh="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];hh.f=function(t){return bh&&"Window"==vh(t)?function(t){try{return yh(t)}catch(t){return mh(bh)}}(t):yh(gh(t))};var wh={},kh=oe;wh.f=kh;var _h=X,xh=Wt,Eh=wh,Oh=Ve.f,Ch=function(t){var e=_h.Symbol||(_h.Symbol={});xh(e,t)||Oh(e,t,{value:Eh.f(t)})},Sh=n,Th=oh,Mh=na,Ph=Y,Dh=oe("species"),Ih=Sh.Array,Bh=function(t){var e;return Th(t)&&(e=t.constructor,(Mh(e)&&(e===Ih||Th(e.prototype))||Ph(e)&&null===(e=e[Dh]))&&(e=void 0)),void 0===e?Ih:e},zh=function(t,e){return new(Bh(t))(0===e?0:e)},Nh=qe,Fh=R,Ah=Rt,jh=Bi,Rh=zh,Lh=g([].push),Hh=function(t){var e=1==t,i=2==t,n=3==t,o=4==t,r=6==t,s=7==t,a=5==t||r;return function(h,l,d,c){for(var u,f,p=Ah(h),v=Fh(p),g=Nh(l,d),y=jh(v),m=0,b=c||Rh,w=e?b(h,y):i||s?b(h,0):void 0;y>m;m++)if((a||m in v)&&(f=g(u=v[m],m,p),t))if(e)w[m]=f;else if(f)switch(t){case 3:return!0;case 5:return u;case 6:return m;case 2:Lh(w,u)}else switch(t){case 4:return!1;case 7:Lh(w,u)}return r?-1:n||o?o:w}},Wh={forEach:Hh(0),map:Hh(1),filter:Hh(2),some:Hh(3),every:Hh(4),find:Hh(5),findIndex:Hh(6),filterReject:Hh(7)},qh=_i,Vh=n,Uh=Q,Yh=d,Xh=_,Gh=g,Kh=b,$h=dt,Zh=o,Qh=Wt,Jh=oh,tl=y,el=Y,il=J,nl=yt,ol=$e,rl=Rt,sl=V,al=ve,hl=eo,ll=M,dl=wr,cl=Ki,ul=rh,fl=hh,pl=$i,vl=m,gl=Ve,yl=Zo,ml=x,bl=fn,wl=Ir,kl=Pt.exports,_l=Ri,xl=Xt,El=oe,Ol=wh,Cl=Ch,Sl=$r,Tl=Vo,Ml=Wh.forEach,Pl=Co("hidden"),Dl="Symbol",Il=El("toPrimitive"),Bl=Tl.set,zl=Tl.getterFor(Dl),Nl=Object.prototype,Fl=Vh.Symbol,Al=Fl&&Fl.prototype,jl=Vh.TypeError,Rl=Vh.QObject,Ll=Uh("JSON","stringify"),Hl=vl.f,Wl=gl.f,ql=fl.f,Vl=ml.f,Ul=Gh([].push),Yl=kl("symbols"),Xl=kl("op-symbols"),Gl=kl("string-to-symbol-registry"),Kl=kl("symbol-to-string-registry"),$l=kl("wks"),Zl=!Rl||!Rl.prototype||!Rl.prototype.findChild,Ql=Kh&&Zh((function(){return 7!=dl(Wl({},"a",{get:function(){return Wl(this,"a",{value:7}).a}})).a}))?function(t,e,i){var n=Hl(Nl,e);n&&delete Nl[e],Wl(t,e,i),n&&t!==Nl&&Wl(Nl,e,n)}:Wl,Jl=function(t,e){var i=Yl[t]=dl(Al);return Bl(i,{type:Dl,tag:t,description:e}),Kh||(i.description=e),i},td=function(t,e,i){t===Nl&&td(Xl,e,i),ol(t);var n=al(e);return ol(i),Qh(Yl,n)?(i.enumerable?(Qh(t,Pl)&&t[Pl][n]&&(t[Pl][n]=!1),i=dl(i,{enumerable:ll(0,!1)})):(Qh(t,Pl)||Wl(t,Pl,ll(1,{})),t[Pl][n]=!0),Ql(t,n,i)):Wl(t,n,i)},ed=function(t,e){ol(t);var i=sl(e),n=cl(i).concat(rd(i));return Ml(n,(function(e){Kh&&!Xh(id,i,e)||td(t,e,i[e])})),t},id=function(t){var e=al(t),i=Xh(Vl,this,e);return!(this===Nl&&Qh(Yl,e)&&!Qh(Xl,e))&&(!(i||!Qh(this,e)||!Qh(Yl,e)||Qh(this,Pl)&&this[Pl][e])||i)},nd=function(t,e){var i=sl(t),n=al(e);if(i!==Nl||!Qh(Yl,n)||Qh(Xl,n)){var o=Hl(i,n);return!o||!Qh(Yl,n)||Qh(i,Pl)&&i[Pl][n]||(o.enumerable=!0),o}},od=function(t){var e=ql(sl(t)),i=[];return Ml(e,(function(t){Qh(Yl,t)||Qh(_l,t)||Ul(i,t)})),i},rd=function(t){var e=t===Nl,i=ql(e?Xl:sl(t)),n=[];return Ml(i,(function(t){!Qh(Yl,t)||e&&!Qh(Nl,t)||Ul(n,Yl[t])})),n};if($h||(Fl=function(){if(il(Al,this))throw jl("Symbol is not a constructor");var t=arguments.length&&void 0!==arguments[0]?hl(arguments[0]):void 0,e=xl(t),i=function(t){this===Nl&&Xh(i,Xl,t),Qh(this,Pl)&&Qh(this[Pl],e)&&(this[Pl][e]=!1),Ql(this,e,ll(1,t))};return Kh&&Zl&&Ql(Nl,e,{configurable:!0,set:i}),Jl(e,t)},wl(Al=Fl.prototype,"toString",(function(){return zl(this).tag})),wl(Fl,"withoutSetter",(function(t){return Jl(xl(t),t)})),ml.f=id,gl.f=td,yl.f=ed,vl.f=nd,ul.f=fl.f=od,pl.f=rd,Ol.f=function(t){return Jl(El(t),t)},Kh&&Wl(Al,"description",{configurable:!0,get:function(){return zl(this).description}})),qh({global:!0,wrap:!0,forced:!$h,sham:!$h},{Symbol:Fl}),Ml(cl($l),(function(t){Cl(t)})),qh({target:Dl,stat:!0,forced:!$h},{for:function(t){var e=hl(t);if(Qh(Gl,e))return Gl[e];var i=Fl(e);return Gl[e]=i,Kl[i]=e,i},keyFor:function(t){if(!nl(t))throw jl(t+" is not a symbol");if(Qh(Kl,t))return Kl[t]},useSetter:function(){Zl=!0},useSimple:function(){Zl=!1}}),qh({target:"Object",stat:!0,forced:!$h,sham:!Kh},{create:function(t,e){return void 0===e?dl(t):ed(dl(t),e)},defineProperty:td,defineProperties:ed,getOwnPropertyDescriptor:nd}),qh({target:"Object",stat:!0,forced:!$h},{getOwnPropertyNames:od,getOwnPropertySymbols:rd}),qh({target:"Object",stat:!0,forced:Zh((function(){pl.f(1)}))},{getOwnPropertySymbols:function(t){return pl.f(rl(t))}}),Ll){var sd=!$h||Zh((function(){var t=Fl();return"[null]"!=Ll([t])||"{}"!=Ll({a:t})||"{}"!=Ll(Object(t))}));qh({target:"JSON",stat:!0,forced:sd},{stringify:function(t,e,i){var n=bl(arguments),o=e;if((el(e)||void 0!==t)&&!nl(t))return Jh(e)||(e=function(t,e){if(tl(o)&&(e=Xh(o,this,t,e)),!nl(e))return e}),n[1]=e,Yh(Ll,null,n)}})}if(!Al[Il]){var ad=Al.valueOf;wl(Al,Il,(function(t){return Xh(ad,this)}))}Sl(Fl,Dl),_l[Pl]=!0;var hd=X.Object.getOwnPropertySymbols,ld={exports:{}},dd=_i,cd=o,ud=V,fd=m.f,pd=b,vd=cd((function(){fd(1)}));dd({target:"Object",stat:!0,forced:!pd||vd,sham:!pd},{getOwnPropertyDescriptor:function(t,e){return fd(ud(t),e)}});var gd=X.Object,yd=ld.exports=function(t,e){return gd.getOwnPropertyDescriptor(t,e)};gd.getOwnPropertyDescriptor.sham&&(yd.sham=!0);var md=ld.exports,bd=md,wd=Q,kd=rh,_d=$i,xd=$e,Ed=g([].concat),Od=wd("Reflect","ownKeys")||function(t){var e=kd.f(xd(t)),i=_d.f;return i?Ed(e,i(t)):e},Cd=Od,Sd=V,Td=m,Md=aa;_i({target:"Object",stat:!0,sham:!b},{getOwnPropertyDescriptors:function(t){for(var e,i,n=Sd(t),o=Td.f,r=Cd(n),s={},a=0;r.length>a;)void 0!==(i=o(n,e=r[a++]))&&Md(s,e,i);return s}});var Pd=X.Object.getOwnPropertyDescriptors,Dd={exports:{}},Id=_i,Bd=b,zd=Zo.f;Id({target:"Object",stat:!0,forced:Object.defineProperties!==zd,sham:!Bd},{defineProperties:zd});var Nd=X.Object,Fd=Dd.exports=function(t,e){return Nd.defineProperties(t,e)};Nd.defineProperties.sham&&(Fd.sham=!0);var Ad=Dd.exports,jd={exports:{}},Rd=_i,Ld=b,Hd=Ve.f;Rd({target:"Object",stat:!0,forced:Object.defineProperty!==Hd,sham:!Ld},{defineProperty:Hd});var Wd=X.Object,qd=jd.exports=function(t,e,i){return Wd.defineProperty(t,e,i)};Wd.defineProperty.sham&&(qd.sham=!0);var Vd=jd.exports,Ud=Vd;function Yd(t,e){if(!(t instanceof e))throw new TypeError("Cannot call a class as a function")}var Xd=Vd;function Gd(t,e){for(var i=0;i<e.length;i++){var n=e[i];n.enumerable=n.enumerable||!1,n.configurable=!0,"value"in n&&(n.writable=!0),Xd(t,n.key,n)}}function Kd(t,e,i){return e&&Gd(t.prototype,e),i&&Gd(t,i),Xd(t,"prototype",{writable:!1}),t}function $d(t,e,i){return e in t?Xd(t,e,{value:i,enumerable:!0,configurable:!0,writable:!0}):t[e]=i,t}_i({target:"Array",stat:!0},{isArray:oh});var Zd=X.Array.isArray,Qd=Zd;var Jd=o,tc=at,ec=oe("species"),ic=function(t){return tc>=51||!Jd((function(){var e=[];return(e.constructor={})[ec]=function(){return{foo:1}},1!==e[t](Boolean).foo}))},nc=_i,oc=n,rc=o,sc=oh,ac=Y,hc=Rt,lc=Bi,dc=aa,cc=zh,uc=ic,fc=at,pc=oe("isConcatSpreadable"),vc=9007199254740991,gc="Maximum allowed index exceeded",yc=oc.TypeError,mc=fc>=51||!rc((function(){var t=[];return t[pc]=!1,t.concat()[0]!==t})),bc=uc("concat"),wc=function(t){if(!ac(t))return!1;var e=t[pc];return void 0!==e?!!e:sc(t)};nc({target:"Array",proto:!0,forced:!mc||!bc},{concat:function(t){var e,i,n,o,r,s=hc(this),a=cc(s,0),h=0;for(e=-1,n=arguments.length;e<n;e++)if(wc(r=-1===e?s:arguments[e])){if(h+(o=lc(r))>vc)throw yc(gc);for(i=0;i<o;i++,h++)i in r&&dc(a,h,r[i])}else{if(h>=vc)throw yc(gc);dc(a,h++,r)}return a.length=h,a}}),Ch("asyncIterator"),Ch("hasInstance"),Ch("isConcatSpreadable"),Ch("iterator"),Ch("match"),Ch("matchAll"),Ch("replace"),Ch("search"),Ch("species"),Ch("split"),Ch("toPrimitive"),Ch("toStringTag"),Ch("unscopables"),$r(n.JSON,"JSON",!0);var kc=X.Symbol,_c=kc;Ch("asyncDispose"),Ch("dispose"),Ch("matcher"),Ch("metadata"),Ch("observable"),Ch("patternMatch"),Ch("replaceAll");var xc=_c;var Ec=_i,Oc=n,Cc=oh,Sc=na,Tc=Y,Mc=Mi,Pc=Bi,Dc=V,Ic=aa,Bc=oe,zc=fn,Nc=ic("slice"),Fc=Bc("species"),Ac=Oc.Array,jc=Math.max;Ec({target:"Array",proto:!0,forced:!Nc},{slice:function(t,e){var i,n,o,r=Dc(this),s=Pc(r),a=Mc(t,s),h=Mc(void 0===e?s:e,s);if(Cc(r)&&(i=r.constructor,(Sc(i)&&(i===Ac||Cc(i.prototype))||Tc(i)&&null===(i=i[Fc]))&&(i=void 0),i===Ac||void 0===i))return zc(r,a,h);for(n=new(void 0===i?Ac:i)(jc(h-a,0)),o=0;a<h;a++,o++)a in r&&Ic(n,o,r[a]);return n.length=o,n}});var Rc=Tn("Array").slice,Lc=J,Hc=Rc,Wc=Array.prototype,qc=function(t){var e=t.slice;return t===Wc||Lc(Wc,t)&&e===Wc.slice?Hc:e},Vc=qc,Uc=Vc,Yc=Aa;function Xc(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}function Gc(t,e){var i;if(t){if("string"==typeof t)return Xc(t,e);var n=Uc(i=Object.prototype.toString.call(t)).call(i,8,-1);return"Object"===n&&t.constructor&&(n=t.constructor.name),"Map"===n||"Set"===n?Yc(t):"Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)?Xc(t,e):void 0}}function Kc(t,e){return function(t){if(Qd(t))return t}(t)||function(t,e){var i=null==t?null:void 0!==xc&&ih(t)||t["@@iterator"];if(null!=i){var n,o,r=[],s=!0,a=!1;try{for(i=i.call(t);!(s=(n=i.next()).done)&&(r.push(n.value),!e||r.length!==e);s=!0);}catch(t){a=!0,o=t}finally{try{s||null==i.return||i.return()}finally{if(a)throw o}}return r}}(t,e)||Gc(t,e)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}var $c=wh.f("iterator"),Zc=$c;function Qc(t){return Qc="function"==typeof xc&&"symbol"==typeof Zc?function(t){return typeof t}:function(t){return t&&"function"==typeof xc&&t.constructor===xc&&t!==xc.prototype?"symbol":typeof t},Qc(t)}function Jc(t){return function(t){if(Qd(t))return Xc(t)}(t)||function(t){if(void 0!==xc&&null!=ih(t)||null!=t["@@iterator"])return Yc(t)}(t)||Gc(t)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}var tu=kc,eu=Tn("Array").concat,iu=J,nu=eu,ou=Array.prototype,ru=function(t){var e=t.concat;return t===ou||iu(ou,t)&&e===ou.concat?nu:e},su=ru,au=Vc;_i({target:"Reflect",stat:!0},{ownKeys:Od});var hu=X.Reflect.ownKeys,lu=Zd,du=Wh.map;_i({target:"Array",proto:!0,forced:!ic("map")},{map:function(t){return du(this,t,arguments.length>1?arguments[1]:void 0)}});var cu=Tn("Array").map,uu=J,fu=cu,pu=Array.prototype,vu=function(t){var e=t.map;return t===pu||uu(pu,t)&&e===pu.map?fu:e},gu=vu,yu=Rt,mu=Ki;_i({target:"Object",stat:!0,forced:o((function(){mu(1)}))},{keys:function(t){return mu(yu(t))}});var bu=X.Object.keys,wu=_i,ku=g,_u=n.Date,xu=ku(_u.prototype.getTime);wu({target:"Date",stat:!0},{now:function(){return xu(new _u)}});var Eu=X.Date.now,Ou=o,Cu=function(t,e){var i=[][t];return!!i&&Ou((function(){i.call(null,e||function(){return 1},1)}))},Su=Wh.forEach,Tu=Cu("forEach")?[].forEach:function(t){return Su(this,t,arguments.length>1?arguments[1]:void 0)};_i({target:"Array",proto:!0,forced:[].forEach!=Tu},{forEach:Tu});var Mu=Tn("Array").forEach,Pu=Qn,Du=Wt,Iu=J,Bu=Mu,zu=Array.prototype,Nu={DOMTokenList:!0,NodeList:!0},Fu=function(t){var e=t.forEach;return t===zu||Iu(zu,t)&&e===zu.forEach||Du(Nu,Pu(t))?Bu:e},Au=_i,ju=oh,Ru=g([].reverse),Lu=[1,2];Au({target:"Array",proto:!0,forced:String(Lu)===String(Lu.reverse())},{reverse:function(){return ju(this)&&(this.length=this.length),Ru(this)}});var Hu=Tn("Array").reverse,Wu=J,qu=Hu,Vu=Array.prototype,Uu=function(t){var e=t.reverse;return t===Vu||Wu(Vu,t)&&e===Vu.reverse?qu:e},Yu=Uu,Xu=_i,Gu=n,Ku=Mi,$u=Oi,Zu=Bi,Qu=Rt,Ju=zh,tf=aa,ef=ic("splice"),nf=Gu.TypeError,of=Math.max,rf=Math.min,sf=9007199254740991,af="Maximum allowed length exceeded";Xu({target:"Array",proto:!0,forced:!ef},{splice:function(t,e){var i,n,o,r,s,a,h=Qu(this),l=Zu(h),d=Ku(t,l),c=arguments.length;if(0===c?i=n=0:1===c?(i=0,n=l-d):(i=c-2,n=rf(of($u(e),0),l-d)),l+i-n>sf)throw nf(af);for(o=Ju(h,n),r=0;r<n;r++)(s=d+r)in h&&tf(o,r,h[s]);if(o.length=n,i<n){for(r=d;r<l-n;r++)a=r+i,(s=r+n)in h?h[a]=h[s]:delete h[a];for(r=l;r>l-n+i;r--)delete h[r-1]}else if(i>n)for(r=l-n;r>d;r--)a=r+i-1,(s=r+n-1)in h?h[a]=h[s]:delete h[a];for(r=0;r<i;r++)h[r+d]=arguments[r+2];return h.length=l-n+i,o}});var hf=Tn("Array").splice,lf=J,df=hf,cf=Array.prototype,uf=function(t){var e=t.splice;return t===cf||lf(cf,t)&&e===cf.splice?df:e},ff=uf,pf=ji.includes;_i({target:"Array",proto:!0},{includes:function(t){return pf(this,t,arguments.length>1?arguments[1]:void 0)}});var vf=Tn("Array").includes,gf=Y,yf=B,mf=oe("match"),bf=function(t){var e;return gf(t)&&(void 0!==(e=t[mf])?!!e:"RegExp"==yf(t))},wf=n.TypeError,kf=oe("match"),_f=_i,xf=function(t){if(bf(t))throw wf("The method doesn't accept regular expressions");return t},Ef=H,Of=eo,Cf=function(t){var e=/./;try{"/./"[t](e)}catch(i){try{return e[kf]=!1,"/./"[t](e)}catch(t){}}return!1},Sf=g("".indexOf);_f({target:"String",proto:!0,forced:!Cf("includes")},{includes:function(t){return!!~Sf(Of(Ef(this)),Of(xf(t)),arguments.length>1?arguments[1]:void 0)}});var Tf=Tn("String").includes,Mf=J,Pf=vf,Df=Tf,If=Array.prototype,Bf=String.prototype,zf=function(t){var e=t.includes;return t===If||Mf(If,t)&&e===If.includes?Pf:"string"==typeof t||t===Bf||Mf(Bf,t)&&e===Bf.includes?Df:e},Nf=zf,Ff=Rt,Af=Pr,jf=kr;_i({target:"Object",stat:!0,forced:o((function(){Af(1)})),sham:!jf},{getPrototypeOf:function(t){return Af(Ff(t))}});var Rf=X.Object.getPrototypeOf,Lf=Rf,Hf=Wh.filter;_i({target:"Array",proto:!0,forced:!ic("filter")},{filter:function(t){return Hf(this,t,arguments.length>1?arguments[1]:void 0)}});var Wf=Tn("Array").filter,qf=J,Vf=Wf,Uf=Array.prototype,Yf=function(t){var e=t.filter;return t===Uf||qf(Uf,t)&&e===Uf.filter?Vf:e},Xf=Yf,Gf=b,Kf=g,$f=Ki,Zf=V,Qf=Kf(x.f),Jf=Kf([].push),tp=function(t){return function(e){for(var i,n=Zf(e),o=$f(n),r=o.length,s=0,a=[];r>s;)i=o[s++],Gf&&!Qf(n,i)||Jf(a,t?[i,n[i]]:n[i]);return a}},ep={entries:tp(!0),values:tp(!1)}.values;_i({target:"Object",stat:!0},{values:function(t){return ep(t)}});var ip=X.Object.values,np="\t\n\v\f\r                　\u2028\u2029\ufeff",op=H,rp=eo,sp=g("".replace),ap="[\t\n\v\f\r                　\u2028\u2029\ufeff]",hp=RegExp("^"+ap+ap+"*"),lp=RegExp(ap+ap+"*$"),dp=function(t){return function(e){var i=rp(op(e));return 1&t&&(i=sp(i,hp,"")),2&t&&(i=sp(i,lp,"")),i}},cp={start:dp(1),end:dp(2),trim:dp(3)},up=n,fp=o,pp=g,vp=eo,gp=cp.trim,yp=np,mp=up.parseInt,bp=up.Symbol,wp=bp&&bp.iterator,kp=/^[+-]?0x/i,_p=pp(kp.exec),xp=8!==mp(yp+"08")||22!==mp(yp+"0x16")||wp&&!fp((function(){mp(Object(wp))}))?function(t,e){var i=gp(vp(t));return mp(i,e>>>0||(_p(kp,i)?16:10))}:mp;_i({global:!0,forced:parseInt!=xp},{parseInt:xp});var Ep=X.parseInt,Op=_i,Cp=ji.indexOf,Sp=Cu,Tp=g([].indexOf),Mp=!!Tp&&1/Tp([1],1,-0)<0,Pp=Sp("indexOf");Op({target:"Array",proto:!0,forced:Mp||!Pp},{indexOf:function(t){var e=arguments.length>1?arguments[1]:void 0;return Mp?Tp(this,t,e)||0:Cp(this,t,e)}});var Dp=Tn("Array").indexOf,Ip=J,Bp=Dp,zp=Array.prototype,Np=function(t){var e=t.indexOf;return t===zp||Ip(zp,t)&&e===zp.indexOf?Bp:e},Fp=Np,Ap=$o.PROPER,jp=o,Rp=np,Lp=cp.trim;_i({target:"String",proto:!0,forced:function(t){return jp((function(){return!!Rp[t]()||"​᠎"!=="​᠎"[t]()||Ap&&Rp[t].name!==t}))}("trim")},{trim:function(){return Lp(this)}});var Hp=Tn("String").trim,Wp=J,qp=Hp,Vp=String.prototype,Up=function(t){var e=t.trim;return"string"==typeof t||t===Vp||Wp(Vp,t)&&e===Vp.trim?qp:e},Yp=Up;_i({target:"Object",stat:!0,sham:!b},{create:wr});var Xp=X.Object,Gp=function(t,e){return Xp.create(t,e)},Kp=Gp,$p=_i,Zp=Q,Qp=d,Jp=g,tv=o,ev=n.Array,iv=Zp("JSON","stringify"),nv=Jp(/./.exec),ov=Jp("".charAt),rv=Jp("".charCodeAt),sv=Jp("".replace),av=Jp(1..toString),hv=/[\uD800-\uDFFF]/g,lv=/^[\uD800-\uDBFF]$/,dv=/^[\uDC00-\uDFFF]$/,cv=function(t,e,i){var n=ov(i,e-1),o=ov(i,e+1);return nv(lv,t)&&!nv(dv,o)||nv(dv,t)&&!nv(lv,n)?"\\u"+av(rv(t,0),16):t},uv=tv((function(){return'"\\udf06\\ud834"'!==iv("\udf06\ud834")||'"\\udead"'!==iv("\udead")}));iv&&$p({target:"JSON",stat:!0,forced:uv},{stringify:function(t,e,i){for(var n=0,o=arguments.length,r=ev(o);n<o;n++)r[n]=arguments[n];var s=Qp(iv,null,r);return"string"==typeof s?sv(s,hv,cv):s}});var fv=X,pv=d;fv.JSON||(fv.JSON={stringify:JSON.stringify});var vv=function(t,e,i){return pv(fv.JSON.stringify,null,arguments)},gv=vv,yv=n.TypeError,mv=_i,bv=n,wv=d,kv=y,_v=fn,xv=function(t,e){if(t<e)throw yv("Not enough arguments");return t},Ev=/MSIE .\./.test(tt),Ov=bv.Function,Cv=function(t){return function(e,i){var n=xv(arguments.length,1)>2,o=kv(e)?e:Ov(e),r=n?_v(arguments,2):void 0;return t(n?function(){wv(o,this,r)}:o,i)}};mv({global:!0,bind:!0,forced:Ev},{setTimeout:Cv(bv.setTimeout),setInterval:Cv(bv.setInterval)});var Sv=X.setTimeout,Tv=Rt,Mv=Mi,Pv=Bi,Dv=function(t){for(var e=Tv(this),i=Pv(e),n=arguments.length,o=Mv(n>1?arguments[1]:void 0,i),r=n>2?arguments[2]:void 0,s=void 0===r?i:Mv(r,i);s>o;)e[o++]=t;return e};_i({target:"Array",proto:!0},{fill:Dv});var Iv,Bv=Tn("Array").fill,zv=J,Nv=Bv,Fv=Array.prototype,Av=function(t){var e=t.fill;return t===Fv||zv(Fv,t)&&e===Fv.fill?Nv:e},jv=Av;function Rv(){return Rv=Object.assign||function(t){for(var e=1;e<arguments.length;e++){var i=arguments[e];for(var n in i)Object.prototype.hasOwnProperty.call(i,n)&&(t[n]=i[n])}return t},Rv.apply(this,arguments)}function Lv(t,e){t.prototype=Object.create(e.prototype),t.prototype.constructor=t,t.__proto__=e}function Hv(t){if(void 0===t)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return t}Iv="function"!=typeof Object.assign?function(t){if(null==t)throw new TypeError("Cannot convert undefined or null to object");for(var e=Object(t),i=1;i<arguments.length;i++){var n=arguments[i];if(null!=n)for(var o in n)n.hasOwnProperty(o)&&(e[o]=n[o])}return e}:Object.assign;var Wv,qv=Iv,Vv=["","webkit","Moz","MS","ms","o"],Uv="undefined"==typeof document?{style:{}}:document.createElement("div"),Yv=Math.round,Xv=Math.abs,Gv=Date.now;function Kv(t,e){for(var i,n,o=e[0].toUpperCase()+e.slice(1),r=0;r<Vv.length;){if((n=(i=Vv[r])?i+o:e)in t)return n;r++}}Wv="undefined"==typeof window?{}:window;var $v=Kv(Uv.style,"touchAction"),Zv=void 0!==$v;var Qv="compute",Jv="auto",tg="manipulation",eg="none",ig="pan-x",ng="pan-y",og=function(){if(!Zv)return!1;var t={},e=Wv.CSS&&Wv.CSS.supports;return["auto","manipulation","pan-y","pan-x","pan-x pan-y","none"].forEach((function(i){return t[i]=!e||Wv.CSS.supports("touch-action",i)})),t}(),rg="ontouchstart"in Wv,sg=void 0!==Kv(Wv,"PointerEvent"),ag=rg&&/mobile|tablet|ip(ad|hone|od)|android/i.test(navigator.userAgent),hg="touch",lg="mouse",dg=16,cg=24,ug=["x","y"],fg=["clientX","clientY"];function pg(t,e,i){var n;if(t)if(t.forEach)t.forEach(e,i);else if(void 0!==t.length)for(n=0;n<t.length;)e.call(i,t[n],n,t),n++;else for(n in t)t.hasOwnProperty(n)&&e.call(i,t[n],n,t)}function vg(t,e){return"function"==typeof t?t.apply(e&&e[0]||void 0,e):t}function gg(t,e){return t.indexOf(e)>-1}var yg=function(){function t(t,e){this.manager=t,this.set(e)}var e=t.prototype;return e.set=function(t){t===Qv&&(t=this.compute()),Zv&&this.manager.element.style&&og[t]&&(this.manager.element.style[$v]=t),this.actions=t.toLowerCase().trim()},e.update=function(){this.set(this.manager.options.touchAction)},e.compute=function(){var t=[];return pg(this.manager.recognizers,(function(e){vg(e.options.enable,[e])&&(t=t.concat(e.getTouchAction()))})),function(t){if(gg(t,eg))return eg;var e=gg(t,ig),i=gg(t,ng);return e&&i?eg:e||i?e?ig:ng:gg(t,tg)?tg:Jv}(t.join(" "))},e.preventDefaults=function(t){var e=t.srcEvent,i=t.offsetDirection;if(this.manager.session.prevented)e.preventDefault();else{var n=this.actions,o=gg(n,eg)&&!og.none,r=gg(n,ng)&&!og["pan-y"],s=gg(n,ig)&&!og["pan-x"];if(o){var a=1===t.pointers.length,h=t.distance<2,l=t.deltaTime<250;if(a&&h&&l)return}if(!s||!r)return o||r&&6&i||s&&i&cg?this.preventSrc(e):void 0}},e.preventSrc=function(t){this.manager.session.prevented=!0,t.preventDefault()},t}();function mg(t,e){for(;t;){if(t===e)return!0;t=t.parentNode}return!1}function bg(t){var e=t.length;if(1===e)return{x:Yv(t[0].clientX),y:Yv(t[0].clientY)};for(var i=0,n=0,o=0;o<e;)i+=t[o].clientX,n+=t[o].clientY,o++;return{x:Yv(i/e),y:Yv(n/e)}}function wg(t){for(var e=[],i=0;i<t.pointers.length;)e[i]={clientX:Yv(t.pointers[i].clientX),clientY:Yv(t.pointers[i].clientY)},i++;return{timeStamp:Gv(),pointers:e,center:bg(e),deltaX:t.deltaX,deltaY:t.deltaY}}function kg(t,e,i){i||(i=ug);var n=e[i[0]]-t[i[0]],o=e[i[1]]-t[i[1]];return Math.sqrt(n*n+o*o)}function _g(t,e,i){i||(i=ug);var n=e[i[0]]-t[i[0]],o=e[i[1]]-t[i[1]];return 180*Math.atan2(o,n)/Math.PI}function xg(t,e){return t===e?1:Xv(t)>=Xv(e)?t<0?2:4:e<0?8:dg}function Eg(t,e,i){return{x:e/t||0,y:i/t||0}}function Og(t,e){var i=t.session,n=e.pointers,o=n.length;i.firstInput||(i.firstInput=wg(e)),o>1&&!i.firstMultiple?i.firstMultiple=wg(e):1===o&&(i.firstMultiple=!1);var r=i.firstInput,s=i.firstMultiple,a=s?s.center:r.center,h=e.center=bg(n);e.timeStamp=Gv(),e.deltaTime=e.timeStamp-r.timeStamp,e.angle=_g(a,h),e.distance=kg(a,h),function(t,e){var i=e.center,n=t.offsetDelta||{},o=t.prevDelta||{},r=t.prevInput||{};1!==e.eventType&&4!==r.eventType||(o=t.prevDelta={x:r.deltaX||0,y:r.deltaY||0},n=t.offsetDelta={x:i.x,y:i.y}),e.deltaX=o.x+(i.x-n.x),e.deltaY=o.y+(i.y-n.y)}(i,e),e.offsetDirection=xg(e.deltaX,e.deltaY);var l,d,c=Eg(e.deltaTime,e.deltaX,e.deltaY);e.overallVelocityX=c.x,e.overallVelocityY=c.y,e.overallVelocity=Xv(c.x)>Xv(c.y)?c.x:c.y,e.scale=s?(l=s.pointers,kg((d=n)[0],d[1],fg)/kg(l[0],l[1],fg)):1,e.rotation=s?function(t,e){return _g(e[1],e[0],fg)+_g(t[1],t[0],fg)}(s.pointers,n):0,e.maxPointers=i.prevInput?e.pointers.length>i.prevInput.maxPointers?e.pointers.length:i.prevInput.maxPointers:e.pointers.length,function(t,e){var i,n,o,r,s=t.lastInterval||e,a=e.timeStamp-s.timeStamp;if(8!==e.eventType&&(a>25||void 0===s.velocity)){var h=e.deltaX-s.deltaX,l=e.deltaY-s.deltaY,d=Eg(a,h,l);n=d.x,o=d.y,i=Xv(d.x)>Xv(d.y)?d.x:d.y,r=xg(h,l),t.lastInterval=e}else i=s.velocity,n=s.velocityX,o=s.velocityY,r=s.direction;e.velocity=i,e.velocityX=n,e.velocityY=o,e.direction=r}(i,e);var u,f=t.element,p=e.srcEvent;mg(u=p.composedPath?p.composedPath()[0]:p.path?p.path[0]:p.target,f)&&(f=u),e.target=f}function Cg(t,e,i){var n=i.pointers.length,o=i.changedPointers.length,r=1&e&&n-o==0,s=12&e&&n-o==0;i.isFirst=!!r,i.isFinal=!!s,r&&(t.session={}),i.eventType=e,Og(t,i),t.emit("hammer.input",i),t.recognize(i),t.session.prevInput=i}function Sg(t){return t.trim().split(/\s+/g)}function Tg(t,e,i){pg(Sg(e),(function(e){t.addEventListener(e,i,!1)}))}function Mg(t,e,i){pg(Sg(e),(function(e){t.removeEventListener(e,i,!1)}))}function Pg(t){var e=t.ownerDocument||t;return e.defaultView||e.parentWindow||window}var Dg=function(){function t(t,e){var i=this;this.manager=t,this.callback=e,this.element=t.element,this.target=t.options.inputTarget,this.domHandler=function(e){vg(t.options.enable,[t])&&i.handler(e)},this.init()}var e=t.prototype;return e.handler=function(){},e.init=function(){this.evEl&&Tg(this.element,this.evEl,this.domHandler),this.evTarget&&Tg(this.target,this.evTarget,this.domHandler),this.evWin&&Tg(Pg(this.element),this.evWin,this.domHandler)},e.destroy=function(){this.evEl&&Mg(this.element,this.evEl,this.domHandler),this.evTarget&&Mg(this.target,this.evTarget,this.domHandler),this.evWin&&Mg(Pg(this.element),this.evWin,this.domHandler)},t}();function Ig(t,e,i){if(t.indexOf&&!i)return t.indexOf(e);for(var n=0;n<t.length;){if(i&&t[n][i]==e||!i&&t[n]===e)return n;n++}return-1}var Bg={pointerdown:1,pointermove:2,pointerup:4,pointercancel:8,pointerout:8},zg={2:hg,3:"pen",4:lg,5:"kinect"},Ng="pointerdown",Fg="pointermove pointerup pointercancel";Wv.MSPointerEvent&&!Wv.PointerEvent&&(Ng="MSPointerDown",Fg="MSPointerMove MSPointerUp MSPointerCancel");var Ag=function(t){function e(){var i,n=e.prototype;return n.evEl=Ng,n.evWin=Fg,(i=t.apply(this,arguments)||this).store=i.manager.session.pointerEvents=[],i}return Lv(e,t),e.prototype.handler=function(t){var e=this.store,i=!1,n=t.type.toLowerCase().replace("ms",""),o=Bg[n],r=zg[t.pointerType]||t.pointerType,s=r===hg,a=Ig(e,t.pointerId,"pointerId");1&o&&(0===t.button||s)?a<0&&(e.push(t),a=e.length-1):12&o&&(i=!0),a<0||(e[a]=t,this.callback(this.manager,o,{pointers:e,changedPointers:[t],pointerType:r,srcEvent:t}),i&&e.splice(a,1))},e}(Dg);function jg(t){return Array.prototype.slice.call(t,0)}function Rg(t,e,i){for(var n=[],o=[],r=0;r<t.length;){var s=e?t[r][e]:t[r];Ig(o,s)<0&&n.push(t[r]),o[r]=s,r++}return i&&(n=e?n.sort((function(t,i){return t[e]>i[e]})):n.sort()),n}var Lg={touchstart:1,touchmove:2,touchend:4,touchcancel:8},Hg="touchstart touchmove touchend touchcancel",Wg=function(t){function e(){var i;return e.prototype.evTarget=Hg,(i=t.apply(this,arguments)||this).targetIds={},i}return Lv(e,t),e.prototype.handler=function(t){var e=Lg[t.type],i=qg.call(this,t,e);i&&this.callback(this.manager,e,{pointers:i[0],changedPointers:i[1],pointerType:hg,srcEvent:t})},e}(Dg);function qg(t,e){var i,n,o=jg(t.touches),r=this.targetIds;if(3&e&&1===o.length)return r[o[0].identifier]=!0,[o,o];var s=jg(t.changedTouches),a=[],h=this.target;if(n=o.filter((function(t){return mg(t.target,h)})),1===e)for(i=0;i<n.length;)r[n[i].identifier]=!0,i++;for(i=0;i<s.length;)r[s[i].identifier]&&a.push(s[i]),12&e&&delete r[s[i].identifier],i++;return a.length?[Rg(n.concat(a),"identifier",!0),a]:void 0}var Vg={mousedown:1,mousemove:2,mouseup:4},Ug="mousedown",Yg="mousemove mouseup",Xg=function(t){function e(){var i,n=e.prototype;return n.evEl=Ug,n.evWin=Yg,(i=t.apply(this,arguments)||this).pressed=!1,i}return Lv(e,t),e.prototype.handler=function(t){var e=Vg[t.type];1&e&&0===t.button&&(this.pressed=!0),2&e&&1!==t.which&&(e=4),this.pressed&&(4&e&&(this.pressed=!1),this.callback(this.manager,e,{pointers:[t],changedPointers:[t],pointerType:lg,srcEvent:t}))},e}(Dg);function Gg(t){var e=t.changedPointers[0];if(e.identifier===this.primaryTouch){var i={x:e.clientX,y:e.clientY},n=this.lastTouches;this.lastTouches.push(i);setTimeout((function(){var t=n.indexOf(i);t>-1&&n.splice(t,1)}),2500)}}function Kg(t,e){1&t?(this.primaryTouch=e.changedPointers[0].identifier,Gg.call(this,e)):12&t&&Gg.call(this,e)}function $g(t){for(var e=t.srcEvent.clientX,i=t.srcEvent.clientY,n=0;n<this.lastTouches.length;n++){var o=this.lastTouches[n],r=Math.abs(e-o.x),s=Math.abs(i-o.y);if(r<=25&&s<=25)return!0}return!1}var Zg=function(){return function(t){function e(e,i){var n;return(n=t.call(this,e,i)||this).handler=function(t,e,i){var o=i.pointerType===hg,r=i.pointerType===lg;if(!(r&&i.sourceCapabilities&&i.sourceCapabilities.firesTouchEvents)){if(o)Kg.call(Hv(Hv(n)),e,i);else if(r&&$g.call(Hv(Hv(n)),i))return;n.callback(t,e,i)}},n.touch=new Wg(n.manager,n.handler),n.mouse=new Xg(n.manager,n.handler),n.primaryTouch=null,n.lastTouches=[],n}return Lv(e,t),e.prototype.destroy=function(){this.touch.destroy(),this.mouse.destroy()},e}(Dg)}();function Qg(t,e,i){return!!Array.isArray(t)&&(pg(t,i[e],i),!0)}var Jg=32,ty=1;function ey(t,e){var i=e.manager;return i?i.get(t):t}function iy(t){return 16&t?"cancel":8&t?"end":4&t?"move":2&t?"start":""}var ny=function(){function t(t){void 0===t&&(t={}),this.options=Rv({enable:!0},t),this.id=ty++,this.manager=null,this.state=1,this.simultaneous={},this.requireFail=[]}var e=t.prototype;return e.set=function(t){return qv(this.options,t),this.manager&&this.manager.touchAction.update(),this},e.recognizeWith=function(t){if(Qg(t,"recognizeWith",this))return this;var e=this.simultaneous;return e[(t=ey(t,this)).id]||(e[t.id]=t,t.recognizeWith(this)),this},e.dropRecognizeWith=function(t){return Qg(t,"dropRecognizeWith",this)||(t=ey(t,this),delete this.simultaneous[t.id]),this},e.requireFailure=function(t){if(Qg(t,"requireFailure",this))return this;var e=this.requireFail;return-1===Ig(e,t=ey(t,this))&&(e.push(t),t.requireFailure(this)),this},e.dropRequireFailure=function(t){if(Qg(t,"dropRequireFailure",this))return this;t=ey(t,this);var e=Ig(this.requireFail,t);return e>-1&&this.requireFail.splice(e,1),this},e.hasRequireFailures=function(){return this.requireFail.length>0},e.canRecognizeWith=function(t){return!!this.simultaneous[t.id]},e.emit=function(t){var e=this,i=this.state;function n(i){e.manager.emit(i,t)}i<8&&n(e.options.event+iy(i)),n(e.options.event),t.additionalEvent&&n(t.additionalEvent),i>=8&&n(e.options.event+iy(i))},e.tryEmit=function(t){if(this.canEmit())return this.emit(t);this.state=Jg},e.canEmit=function(){for(var t=0;t<this.requireFail.length;){if(!(33&this.requireFail[t].state))return!1;t++}return!0},e.recognize=function(t){var e=qv({},t);if(!vg(this.options.enable,[this,e]))return this.reset(),void(this.state=Jg);56&this.state&&(this.state=1),this.state=this.process(e),30&this.state&&this.tryEmit(e)},e.process=function(t){},e.getTouchAction=function(){},e.reset=function(){},t}(),oy=function(t){function e(e){var i;return void 0===e&&(e={}),(i=t.call(this,Rv({event:"tap",pointers:1,taps:1,interval:300,time:250,threshold:9,posThreshold:10},e))||this).pTime=!1,i.pCenter=!1,i._timer=null,i._input=null,i.count=0,i}Lv(e,t);var i=e.prototype;return i.getTouchAction=function(){return[tg]},i.process=function(t){var e=this,i=this.options,n=t.pointers.length===i.pointers,o=t.distance<i.threshold,r=t.deltaTime<i.time;if(this.reset(),1&t.eventType&&0===this.count)return this.failTimeout();if(o&&r&&n){if(4!==t.eventType)return this.failTimeout();var s=!this.pTime||t.timeStamp-this.pTime<i.interval,a=!this.pCenter||kg(this.pCenter,t.center)<i.posThreshold;if(this.pTime=t.timeStamp,this.pCenter=t.center,a&&s?this.count+=1:this.count=1,this._input=t,0===this.count%i.taps)return this.hasRequireFailures()?(this._timer=setTimeout((function(){e.state=8,e.tryEmit()}),i.interval),2):8}return Jg},i.failTimeout=function(){var t=this;return this._timer=setTimeout((function(){t.state=Jg}),this.options.interval),Jg},i.reset=function(){clearTimeout(this._timer)},i.emit=function(){8===this.state&&(this._input.tapCount=this.count,this.manager.emit(this.options.event,this._input))},e}(ny),ry=function(t){function e(e){return void 0===e&&(e={}),t.call(this,Rv({pointers:1},e))||this}Lv(e,t);var i=e.prototype;return i.attrTest=function(t){var e=this.options.pointers;return 0===e||t.pointers.length===e},i.process=function(t){var e=this.state,i=t.eventType,n=6&e,o=this.attrTest(t);return n&&(8&i||!o)?16|e:n||o?4&i?8|e:2&e?4|e:2:Jg},e}(ny);function sy(t){return t===dg?"down":8===t?"up":2===t?"left":4===t?"right":""}var ay=function(t){function e(e){var i;return void 0===e&&(e={}),(i=t.call(this,Rv({event:"pan",threshold:10,pointers:1,direction:30},e))||this).pX=null,i.pY=null,i}Lv(e,t);var i=e.prototype;return i.getTouchAction=function(){var t=this.options.direction,e=[];return 6&t&&e.push(ng),t&cg&&e.push(ig),e},i.directionTest=function(t){var e=this.options,i=!0,n=t.distance,o=t.direction,r=t.deltaX,s=t.deltaY;return o&e.direction||(6&e.direction?(o=0===r?1:r<0?2:4,i=r!==this.pX,n=Math.abs(t.deltaX)):(o=0===s?1:s<0?8:dg,i=s!==this.pY,n=Math.abs(t.deltaY))),t.direction=o,i&&n>e.threshold&&o&e.direction},i.attrTest=function(t){return ry.prototype.attrTest.call(this,t)&&(2&this.state||!(2&this.state)&&this.directionTest(t))},i.emit=function(e){this.pX=e.deltaX,this.pY=e.deltaY;var i=sy(e.direction);i&&(e.additionalEvent=this.options.event+i),t.prototype.emit.call(this,e)},e}(ry),hy=function(t){function e(e){return void 0===e&&(e={}),t.call(this,Rv({event:"swipe",threshold:10,velocity:.3,direction:30,pointers:1},e))||this}Lv(e,t);var i=e.prototype;return i.getTouchAction=function(){return ay.prototype.getTouchAction.call(this)},i.attrTest=function(e){var i,n=this.options.direction;return 30&n?i=e.overallVelocity:6&n?i=e.overallVelocityX:n&cg&&(i=e.overallVelocityY),t.prototype.attrTest.call(this,e)&&n&e.offsetDirection&&e.distance>this.options.threshold&&e.maxPointers===this.options.pointers&&Xv(i)>this.options.velocity&&4&e.eventType},i.emit=function(t){var e=sy(t.offsetDirection);e&&this.manager.emit(this.options.event+e,t),this.manager.emit(this.options.event,t)},e}(ry),ly=function(t){function e(e){return void 0===e&&(e={}),t.call(this,Rv({event:"pinch",threshold:0,pointers:2},e))||this}Lv(e,t);var i=e.prototype;return i.getTouchAction=function(){return[eg]},i.attrTest=function(e){return t.prototype.attrTest.call(this,e)&&(Math.abs(e.scale-1)>this.options.threshold||2&this.state)},i.emit=function(e){if(1!==e.scale){var i=e.scale<1?"in":"out";e.additionalEvent=this.options.event+i}t.prototype.emit.call(this,e)},e}(ry),dy=function(t){function e(e){return void 0===e&&(e={}),t.call(this,Rv({event:"rotate",threshold:0,pointers:2},e))||this}Lv(e,t);var i=e.prototype;return i.getTouchAction=function(){return[eg]},i.attrTest=function(e){return t.prototype.attrTest.call(this,e)&&(Math.abs(e.rotation)>this.options.threshold||2&this.state)},e}(ry),cy=function(t){function e(e){var i;return void 0===e&&(e={}),(i=t.call(this,Rv({event:"press",pointers:1,time:251,threshold:9},e))||this)._timer=null,i._input=null,i}Lv(e,t);var i=e.prototype;return i.getTouchAction=function(){return[Jv]},i.process=function(t){var e=this,i=this.options,n=t.pointers.length===i.pointers,o=t.distance<i.threshold,r=t.deltaTime>i.time;if(this._input=t,!o||!n||12&t.eventType&&!r)this.reset();else if(1&t.eventType)this.reset(),this._timer=setTimeout((function(){e.state=8,e.tryEmit()}),i.time);else if(4&t.eventType)return 8;return Jg},i.reset=function(){clearTimeout(this._timer)},i.emit=function(t){8===this.state&&(t&&4&t.eventType?this.manager.emit(this.options.event+"up",t):(this._input.timeStamp=Gv(),this.manager.emit(this.options.event,this._input)))},e}(ny),uy={domEvents:!1,touchAction:Qv,enable:!0,inputTarget:null,inputClass:null,cssProps:{userSelect:"none",touchSelect:"none",touchCallout:"none",contentZooming:"none",userDrag:"none",tapHighlightColor:"rgba(0,0,0,0)"}},fy=[[dy,{enable:!1}],[ly,{enable:!1},["rotate"]],[hy,{direction:6}],[ay,{direction:6},["swipe"]],[oy],[oy,{event:"doubletap",taps:2},["tap"]],[cy]];function py(t,e){var i,n=t.element;n.style&&(pg(t.options.cssProps,(function(o,r){i=Kv(n.style,r),e?(t.oldCssProps[i]=n.style[i],n.style[i]=o):n.style[i]=t.oldCssProps[i]||""})),e||(t.oldCssProps={}))}var vy=function(){function t(t,e){var i,n=this;this.options=qv({},uy,e||{}),this.options.inputTarget=this.options.inputTarget||t,this.handlers={},this.session={},this.recognizers=[],this.oldCssProps={},this.element=t,this.input=new((i=this).options.inputClass||(sg?Ag:ag?Wg:rg?Zg:Xg))(i,Cg),this.touchAction=new yg(this,this.options.touchAction),py(this,!0),pg(this.options.recognizers,(function(t){var e=n.add(new t[0](t[1]));t[2]&&e.recognizeWith(t[2]),t[3]&&e.requireFailure(t[3])}),this)}var e=t.prototype;return e.set=function(t){return qv(this.options,t),t.touchAction&&this.touchAction.update(),t.inputTarget&&(this.input.destroy(),this.input.target=t.inputTarget,this.input.init()),this},e.stop=function(t){this.session.stopped=t?2:1},e.recognize=function(t){var e=this.session;if(!e.stopped){var i;this.touchAction.preventDefaults(t);var n=this.recognizers,o=e.curRecognizer;(!o||o&&8&o.state)&&(e.curRecognizer=null,o=null);for(var r=0;r<n.length;)i=n[r],2===e.stopped||o&&i!==o&&!i.canRecognizeWith(o)?i.reset():i.recognize(t),!o&&14&i.state&&(e.curRecognizer=i,o=i),r++}},e.get=function(t){if(t instanceof ny)return t;for(var e=this.recognizers,i=0;i<e.length;i++)if(e[i].options.event===t)return e[i];return null},e.add=function(t){if(Qg(t,"add",this))return this;var e=this.get(t.options.event);return e&&this.remove(e),this.recognizers.push(t),t.manager=this,this.touchAction.update(),t},e.remove=function(t){if(Qg(t,"remove",this))return this;var e=this.get(t);if(t){var i=this.recognizers,n=Ig(i,e);-1!==n&&(i.splice(n,1),this.touchAction.update())}return this},e.on=function(t,e){if(void 0===t||void 0===e)return this;var i=this.handlers;return pg(Sg(t),(function(t){i[t]=i[t]||[],i[t].push(e)})),this},e.off=function(t,e){if(void 0===t)return this;var i=this.handlers;return pg(Sg(t),(function(t){e?i[t]&&i[t].splice(Ig(i[t],e),1):delete i[t]})),this},e.emit=function(t,e){this.options.domEvents&&function(t,e){var i=document.createEvent("Event");i.initEvent(t,!0,!0),i.gesture=e,e.target.dispatchEvent(i)}(t,e);var i=this.handlers[t]&&this.handlers[t].slice();if(i&&i.length){e.type=t,e.preventDefault=function(){e.srcEvent.preventDefault()};for(var n=0;n<i.length;)i[n](e),n++}},e.destroy=function(){this.element&&py(this,!1),this.handlers={},this.session={},this.input.destroy(),this.element=null},t}(),gy={touchstart:1,touchmove:2,touchend:4,touchcancel:8},yy="touchstart",my="touchstart touchmove touchend touchcancel",by=function(t){function e(){var i,n=e.prototype;return n.evTarget=yy,n.evWin=my,(i=t.apply(this,arguments)||this).started=!1,i}return Lv(e,t),e.prototype.handler=function(t){var e=gy[t.type];if(1===e&&(this.started=!0),this.started){var i=wy.call(this,t,e);12&e&&i[0].length-i[1].length==0&&(this.started=!1),this.callback(this.manager,e,{pointers:i[0],changedPointers:i[1],pointerType:hg,srcEvent:t})}},e}(Dg);function wy(t,e){var i=jg(t.touches),n=jg(t.changedTouches);return 12&e&&(i=Rg(i.concat(n),"identifier",!0)),[i,n]}function ky(t,e,i){var n="DEPRECATED METHOD: "+e+"\n"+i+" AT \n";return function(){var e=new Error("get-stack-trace"),i=e&&e.stack?e.stack.replace(/^[^\(]+?[\n$]/gm,"").replace(/^\s+at\s+/gm,"").replace(/^Object.<anonymous>\s*\(/gm,"{anonymous}()@"):"Unknown Stack Trace",o=window.console&&(window.console.warn||window.console.log);return o&&o.call(window.console,n,i),t.apply(this,arguments)}}var _y=ky((function(t,e,i){for(var n=Object.keys(e),o=0;o<n.length;)(!i||i&&void 0===t[n[o]])&&(t[n[o]]=e[n[o]]),o++;return t}),"extend","Use `assign`."),xy=ky((function(t,e){return _y(t,e,!0)}),"merge","Use `assign`.");function Ey(t,e,i){var n,o=e.prototype;(n=t.prototype=Object.create(o)).constructor=t,n._super=o,i&&qv(n,i)}function Oy(t,e){return function(){return t.apply(e,arguments)}}var Cy=function(){var t=function(t,e){return void 0===e&&(e={}),new vy(t,Rv({recognizers:fy.concat()},e))};return t.VERSION="2.0.17-rc",t.DIRECTION_ALL=30,t.DIRECTION_DOWN=dg,t.DIRECTION_LEFT=2,t.DIRECTION_RIGHT=4,t.DIRECTION_UP=8,t.DIRECTION_HORIZONTAL=6,t.DIRECTION_VERTICAL=cg,t.DIRECTION_NONE=1,t.DIRECTION_DOWN=dg,t.INPUT_START=1,t.INPUT_MOVE=2,t.INPUT_END=4,t.INPUT_CANCEL=8,t.STATE_POSSIBLE=1,t.STATE_BEGAN=2,t.STATE_CHANGED=4,t.STATE_ENDED=8,t.STATE_RECOGNIZED=8,t.STATE_CANCELLED=16,t.STATE_FAILED=Jg,t.Manager=vy,t.Input=Dg,t.TouchAction=yg,t.TouchInput=Wg,t.MouseInput=Xg,t.PointerEventInput=Ag,t.TouchMouseInput=Zg,t.SingleTouchInput=by,t.Recognizer=ny,t.AttrRecognizer=ry,t.Tap=oy,t.Pan=ay,t.Swipe=hy,t.Pinch=ly,t.Rotate=dy,t.Press=cy,t.on=Tg,t.off=Mg,t.each=pg,t.merge=xy,t.extend=_y,t.bindFn=Oy,t.assign=qv,t.inherit=Ey,t.bindFn=Oy,t.prefixed=Kv,t.toArray=jg,t.inArray=Ig,t.uniqueArray=Rg,t.splitStr=Sg,t.boolOrFn=vg,t.hasParent=mg,t.addEventListeners=Tg,t.removeEventListeners=Mg,t.defaults=qv({},uy,{preset:fy}),t}(),Sy=Cy;function Ty(t,e){var i=bu(t);if(hd){var n=hd(t);e&&(n=Xf(n).call(n,(function(e){return bd(t,e).enumerable}))),i.push.apply(i,n)}return i}function My(t){for(var e=1;e<arguments.length;e++){var i,n,o=null!=arguments[e]?arguments[e]:{};e%2?Fu(i=Ty(Object(o),!0)).call(i,(function(e){$d(t,e,o[e])})):Pd?Ad(t,Pd(o)):Fu(n=Ty(Object(o))).call(n,(function(e){Ud(t,e,bd(o,e))}))}return t}function Py(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return Dy(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return Dy(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function Dy(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}var Iy=tu("DELETE");function By(t){for(var e,i=arguments.length,n=new Array(i>1?i-1:0),o=1;o<i;o++)n[o-1]=arguments[o];return zy.apply(void 0,su(e=[{},t]).call(e,n))}function zy(){var t=Ny.apply(void 0,arguments);return Ay(t),t}function Ny(){for(var t=arguments.length,e=new Array(t),i=0;i<t;i++)e[i]=arguments[i];if(e.length<2)return e[0];var n;if(e.length>2)return Ny.apply(void 0,su(n=[zy(e[0],e[1])]).call(n,Jc(au(e).call(e,2))));var o,r=e[0],s=e[1],a=Py(hu(s));try{for(a.s();!(o=a.n()).done;){var h=o.value;Object.prototype.propertyIsEnumerable.call(s,h)&&(s[h]===Iy?delete r[h]:null===r[h]||null===s[h]||"object"!==Qc(r[h])||"object"!==Qc(s[h])||lu(r[h])||lu(s[h])?r[h]=Fy(s[h]):r[h]=Ny(r[h],s[h]))}}catch(t){a.e(t)}finally{a.f()}return r}function Fy(t){return lu(t)?gu(t).call(t,(function(t){return Fy(t)})):"object"===Qc(t)&&null!==t?Ny({},t):t}function Ay(t){for(var e=0,i=bu(t);e<i.length;e++){var n=i[e];t[n]===Iy?delete t[n]:"object"===Qc(t[n])&&null!==t[n]&&Ay(t[n])}}function jy(){for(var t=arguments.length,e=new Array(t),i=0;i<t;i++)e[i]=arguments[i];return Ry(e.length?e:[Eu()])}function Ry(t){var e=function(){for(var t=Ly(),e=t(" "),i=t(" "),n=t(" "),o=0;o<arguments.length;o++)(e-=t(o<0||arguments.length<=o?void 0:arguments[o]))<0&&(e+=1),(i-=t(o<0||arguments.length<=o?void 0:arguments[o]))<0&&(i+=1),(n-=t(o<0||arguments.length<=o?void 0:arguments[o]))<0&&(n+=1);return[e,i,n]}(t),i=Kc(e,3),n=i[0],o=i[1],r=i[2],s=1,a=function(){var t=2091639*n+2.3283064365386963e-10*s;return n=o,o=r,r=t-(s=0|t)};return a.uint32=function(){return 4294967296*a()},a.fract53=function(){return a()+11102230246251565e-32*(2097152*a()|0)},a.algorithm="Alea",a.seed=t,a.version="0.9",a}function Ly(){var t=4022871197;return function(e){for(var i=e.toString(),n=0;n<i.length;n++){var o=.02519603282416938*(t+=i.charCodeAt(n));o-=t=o>>>0,t=(o*=t)>>>0,t+=4294967296*(o-=t)}return 2.3283064365386963e-10*(t>>>0)}}var Hy="undefined"!=typeof window?window.Hammer||Sy:function(){return function(){var t=function(){};return{on:t,off:t,destroy:t,emit:t,get:function(){return{set:t}}}}()};function Wy(t){var e,i=this;this._cleanupQueue=[],this.active=!1,this._dom={container:t,overlay:document.createElement("div")},this._dom.overlay.classList.add("vis-overlay"),this._dom.container.appendChild(this._dom.overlay),this._cleanupQueue.push((function(){i._dom.overlay.parentNode.removeChild(i._dom.overlay)}));var n=Hy(this._dom.overlay);n.on("tap",zn(e=this._onTapOverlay).call(e,this)),this._cleanupQueue.push((function(){n.destroy()}));var o=["tap","doubletap","press","pinch","pan","panstart","panmove","panend"];Fu(o).call(o,(function(t){n.on(t,(function(t){t.srcEvent.stopPropagation()}))})),document&&document.body&&(this._onClick=function(e){(function(t,e){for(;t;){if(t===e)return!0;t=t.parentNode}return!1})(e.target,t)||i.deactivate()},document.body.addEventListener("click",this._onClick),this._cleanupQueue.push((function(){document.body.removeEventListener("click",i._onClick)}))),this._escListener=function(t){("key"in t?"Escape"===t.key:27===t.keyCode)&&i.deactivate()}}Wn(Wy.prototype),Wy.current=null,Wy.prototype.destroy=function(){var t,e;this.deactivate();var i,n=Py(Yu(t=ff(e=this._cleanupQueue).call(e,0)).call(t));try{for(n.s();!(i=n.n()).done;){(0,i.value)()}}catch(t){n.e(t)}finally{n.f()}},Wy.prototype.activate=function(){Wy.current&&Wy.current.deactivate(),Wy.current=this,this.active=!0,this._dom.overlay.style.display="none",this._dom.container.classList.add("vis-active"),this.emit("change"),this.emit("activate"),document.body.addEventListener("keydown",this._escListener)},Wy.prototype.deactivate=function(){this.active=!1,this._dom.overlay.style.display="block",this._dom.container.classList.remove("vis-active"),document.body.removeEventListener("keydown",this._escListener),this.emit("change"),this.emit("deactivate")},Wy.prototype._onTapOverlay=function(t){this.activate(),t.srcEvent.stopPropagation()};var qy=/^\/?Date\((-?\d+)/i,Vy=/^#?([a-f\d]{2})([a-f\d]{2})([a-f\d]{2})$/i,Uy=/^#?([a-f\d])([a-f\d])([a-f\d])$/i,Yy=/^rgb\( *(1?\d{1,2}|2[0-4]\d|25[0-5]) *, *(1?\d{1,2}|2[0-4]\d|25[0-5]) *, *(1?\d{1,2}|2[0-4]\d|25[0-5]) *\)$/i,Xy=/^rgba\( *(1?\d{1,2}|2[0-4]\d|25[0-5]) *, *(1?\d{1,2}|2[0-4]\d|25[0-5]) *, *(1?\d{1,2}|2[0-4]\d|25[0-5]) *, *([01]|0?\.\d+) *\)$/i;function Gy(t){return t instanceof Number||"number"==typeof t}function Ky(t){if(t)for(;!0===t.hasChildNodes();){var e=t.firstChild;e&&(Ky(e),t.removeChild(e))}}function $y(t){return t instanceof String||"string"==typeof t}function Zy(t){return"object"===Qc(t)&&null!==t}function Qy(t,e,i,n){var o=!1;!0===n&&(o=null===e[i]&&void 0!==t[i]),o?delete t[i]:t[i]=e[i]}function Jy(t,e){var i=arguments.length>2&&void 0!==arguments[2]&&arguments[2];for(var n in t)if(void 0!==e[n])if(null===e[n]||"object"!==Qc(e[n]))Qy(t,e,n,i);else{var o=t[n],r=e[n];Zy(o)&&Zy(r)&&Jy(o,r,i)}}var tm=un;function em(t,e,i){var n=arguments.length>3&&void 0!==arguments[3]&&arguments[3];if(lu(i))throw new TypeError("Arrays are not supported by deepExtend");for(var o=0;o<t.length;o++){var r=t[o];if(Object.prototype.hasOwnProperty.call(i,r))if(i[r]&&i[r].constructor===Object)void 0===e[r]&&(e[r]={}),e[r].constructor===Object?nm(e[r],i[r],!1,n):Qy(e,i,r,n);else{if(lu(i[r]))throw new TypeError("Arrays are not supported by deepExtend");Qy(e,i,r,n)}}return e}function im(t,e,i){var n=arguments.length>3&&void 0!==arguments[3]&&arguments[3];if(lu(i))throw new TypeError("Arrays are not supported by deepExtend");for(var o in i)if(Object.prototype.hasOwnProperty.call(i,o)&&!Nf(t).call(t,o))if(i[o]&&i[o].constructor===Object)void 0===e[o]&&(e[o]={}),e[o].constructor===Object?nm(e[o],i[o]):Qy(e,i,o,n);else if(lu(i[o])){e[o]=[];for(var r=0;r<i[o].length;r++)e[o].push(i[o][r])}else Qy(e,i,o,n);return e}function nm(t,e){var i=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=arguments.length>3&&void 0!==arguments[3]&&arguments[3];for(var o in e)if(Object.prototype.hasOwnProperty.call(e,o)||!0===i)if("object"===Qc(e[o])&&null!==e[o]&&Lf(e[o])===Object.prototype)void 0===t[o]?t[o]=nm({},e[o],i):"object"===Qc(t[o])&&null!==t[o]&&Lf(t[o])===Object.prototype?nm(t[o],e[o],i):Qy(t,e,o,n);else if(lu(e[o])){var r;t[o]=au(r=e[o]).call(r)}else Qy(t,e,o,n);return t}function om(t,e){var i;return su(i=[]).call(i,Jc(t),[e])}function rm(t){return au(t).call(t)}function sm(t){return t.getBoundingClientRect().left}function am(t){return t.getBoundingClientRect().top}function hm(t,e){if(lu(t))for(var i=t.length,n=0;n<i;n++)e(t[n],n,t);else for(var o in t)Object.prototype.hasOwnProperty.call(t,o)&&e(t[o],o,t)}var lm=ip;function dm(t,e,i,n){var o;t.addEventListener?(void 0===n&&(n=!1),"mousewheel"===e&&Nf(o=navigator.userAgent).call(o,"Firefox")&&(e="DOMMouseScroll"),t.addEventListener(e,i,n)):t.attachEvent("on"+e,i)}function cm(t,e,i,n){var o;t.removeEventListener?(void 0===n&&(n=!1),"mousewheel"===e&&Nf(o=navigator.userAgent).call(o,"Firefox")&&(e="DOMMouseScroll"),t.removeEventListener(e,i,n)):t.detachEvent("on"+e,i)}var um={asBoolean:function(t,e){return"function"==typeof t&&(t=t()),null!=t?0!=t:e||null},asNumber:function(t,e){return"function"==typeof t&&(t=t()),null!=t?Number(t)||e||null:e||null},asString:function(t,e){return"function"==typeof t&&(t=t()),null!=t?String(t):e||null},asSize:function(t,e){return"function"==typeof t&&(t=t()),$y(t)?t:Gy(t)?t+"px":e||null},asElement:function(t,e){return"function"==typeof t&&(t=t()),t||e||null}};function fm(t){var e;switch(t.length){case 3:case 4:return(e=Uy.exec(t))?{r:Ep(e[1]+e[1],16),g:Ep(e[2]+e[2],16),b:Ep(e[3]+e[3],16)}:null;case 6:case 7:return(e=Vy.exec(t))?{r:Ep(e[1],16),g:Ep(e[2],16),b:Ep(e[3],16)}:null;default:return null}}function pm(t,e){if(Nf(t).call(t,"rgba"))return t;if(Nf(t).call(t,"rgb")){var i=t.substr(Fp(t).call(t,"(")+1).replace(")","").split(",");return"rgba("+i[0]+","+i[1]+","+i[2]+","+e+")"}var n=fm(t);return null==n?t:"rgba("+n.r+","+n.g+","+n.b+","+e+")"}function vm(t,e,i){var n;return"#"+au(n=((1<<24)+(t<<16)+(e<<8)+i).toString(16)).call(n,1)}function gm(t,e){if($y(t)){var i=t;if(Em(i)){var n,o=gu(n=i.substr(4).substr(0,i.length-5).split(",")).call(n,(function(t){return Ep(t)}));i=vm(o[0],o[1],o[2])}if(!0===xm(i)){var r=_m(i),s={h:r.h,s:.8*r.s,v:Math.min(1,1.02*r.v)},a={h:r.h,s:Math.min(1,1.25*r.s),v:.8*r.v},h=km(a.h,a.s,a.v),l=km(s.h,s.s,s.v);return{background:i,border:h,highlight:{background:l,border:h},hover:{background:l,border:h}}}return{background:i,border:i,highlight:{background:i,border:i},hover:{background:i,border:i}}}return e?{background:t.background||e.background,border:t.border||e.border,highlight:$y(t.highlight)?{border:t.highlight,background:t.highlight}:{background:t.highlight&&t.highlight.background||e.highlight.background,border:t.highlight&&t.highlight.border||e.highlight.border},hover:$y(t.hover)?{border:t.hover,background:t.hover}:{border:t.hover&&t.hover.border||e.hover.border,background:t.hover&&t.hover.background||e.hover.background}}:{background:t.background||void 0,border:t.border||void 0,highlight:$y(t.highlight)?{border:t.highlight,background:t.highlight}:{background:t.highlight&&t.highlight.background||void 0,border:t.highlight&&t.highlight.border||void 0},hover:$y(t.hover)?{border:t.hover,background:t.hover}:{border:t.hover&&t.hover.border||void 0,background:t.hover&&t.hover.background||void 0}}}function ym(t,e,i){t/=255,e/=255,i/=255;var n=Math.min(t,Math.min(e,i)),o=Math.max(t,Math.max(e,i));return n===o?{h:0,s:0,v:n}:{h:60*((t===n?3:i===n?1:5)-(t===n?e-i:i===n?t-e:i-t)/(o-n))/360,s:(o-n)/o,v:o}}var mm=function(t){var e,i={};return Fu(e=t.split(";")).call(e,(function(t){if(""!=Yp(t).call(t)){var e,n,o=t.split(":"),r=Yp(e=o[0]).call(e),s=Yp(n=o[1]).call(n);i[r]=s}})),i},bm=function(t){var e;return gu(e=bu(t)).call(e,(function(e){return e+": "+t[e]})).join("; ")};function wm(t,e,i){var n,o,r,s=Math.floor(6*t),a=6*t-s,h=i*(1-e),l=i*(1-a*e),d=i*(1-(1-a)*e);switch(s%6){case 0:n=i,o=d,r=h;break;case 1:n=l,o=i,r=h;break;case 2:n=h,o=i,r=d;break;case 3:n=h,o=l,r=i;break;case 4:n=d,o=h,r=i;break;case 5:n=i,o=h,r=l}return{r:Math.floor(255*n),g:Math.floor(255*o),b:Math.floor(255*r)}}function km(t,e,i){var n=wm(t,e,i);return vm(n.r,n.g,n.b)}function _m(t){var e=fm(t);if(!e)throw new TypeError("'".concat(t,"' is not a valid color."));return ym(e.r,e.g,e.b)}function xm(t){return/(^#[0-9A-F]{6}$)|(^#[0-9A-F]{3}$)/i.test(t)}function Em(t){return Yy.test(t)}function Om(t){return Xy.test(t)}function Cm(t){if(null===t||"object"!==Qc(t))return null;if(t instanceof Element)return t;var e=Kp(t);for(var i in t)Object.prototype.hasOwnProperty.call(t,i)&&"object"==Qc(t[i])&&(e[i]=Cm(t[i]));return e}function Sm(t,e,i){var n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:{},o=function(t){return null!=t},r=function(t){return null!==t&&"object"===Qc(t)},s=function(t){for(var e in t)if(Object.prototype.hasOwnProperty.call(t,e))return!1;return!0};if(!r(t))throw new Error("Parameter mergeTarget must be an object");if(!r(e))throw new Error("Parameter options must be an object");if(!o(i))throw new Error("Parameter option must have a value");if(!r(n))throw new Error("Parameter globalOptions must be an object");var a=function(t,e,i){r(t[i])||(t[i]={});var n=e[i],o=t[i];for(var s in n)Object.prototype.hasOwnProperty.call(n,s)&&(o[s]=n[s])},h=e[i],l=r(n)&&!s(n),d=l?n[i]:void 0,c=d?d.enabled:void 0;if(void 0!==h){if("boolean"==typeof h)return r(t[i])||(t[i]={}),void(t[i].enabled=h);if(null===h&&!r(t[i])){if(!o(d))return;t[i]=Kp(d)}if(r(h)){var u=!0;void 0!==h.enabled?u=h.enabled:void 0!==c&&(u=d.enabled),a(t,e,i),t[i].enabled=u}}}var Tm={linear:function(t){return t},easeInQuad:function(t){return t*t},easeOutQuad:function(t){return t*(2-t)},easeInOutQuad:function(t){return t<.5?2*t*t:(4-2*t)*t-1},easeInCubic:function(t){return t*t*t},easeOutCubic:function(t){return--t*t*t+1},easeInOutCubic:function(t){return t<.5?4*t*t*t:(t-1)*(2*t-2)*(2*t-2)+1},easeInQuart:function(t){return t*t*t*t},easeOutQuart:function(t){return 1- --t*t*t*t},easeInOutQuart:function(t){return t<.5?8*t*t*t*t:1-8*--t*t*t*t},easeInQuint:function(t){return t*t*t*t*t},easeOutQuint:function(t){return 1+--t*t*t*t*t},easeInOutQuint:function(t){return t<.5?16*t*t*t*t*t:1+16*--t*t*t*t*t}};function Mm(t,e){var i;lu(e)||(e=[e]);var n,o=Py(t);try{for(o.s();!(n=o.n()).done;){var r=n.value;if(r){i=r[e[0]];for(var s=1;s<e.length;s++)i&&(i=i[e[s]]);if(void 0!==i)break}}}catch(t){o.e(t)}finally{o.f()}return i}var Pm={black:"#000000",navy:"#000080",darkblue:"#00008B",mediumblue:"#0000CD",blue:"#0000FF",darkgreen:"#006400",green:"#008000",teal:"#008080",darkcyan:"#008B8B",deepskyblue:"#00BFFF",darkturquoise:"#00CED1",mediumspringgreen:"#00FA9A",lime:"#00FF00",springgreen:"#00FF7F",aqua:"#00FFFF",cyan:"#00FFFF",midnightblue:"#191970",dodgerblue:"#1E90FF",lightseagreen:"#20B2AA",forestgreen:"#228B22",seagreen:"#2E8B57",darkslategray:"#2F4F4F",limegreen:"#32CD32",mediumseagreen:"#3CB371",turquoise:"#40E0D0",royalblue:"#4169E1",steelblue:"#4682B4",darkslateblue:"#483D8B",mediumturquoise:"#48D1CC",indigo:"#4B0082",darkolivegreen:"#556B2F",cadetblue:"#5F9EA0",cornflowerblue:"#6495ED",mediumaquamarine:"#66CDAA",dimgray:"#696969",slateblue:"#6A5ACD",olivedrab:"#6B8E23",slategray:"#708090",lightslategray:"#778899",mediumslateblue:"#7B68EE",lawngreen:"#7CFC00",chartreuse:"#7FFF00",aquamarine:"#7FFFD4",maroon:"#800000",purple:"#800080",olive:"#808000",gray:"#808080",skyblue:"#87CEEB",lightskyblue:"#87CEFA",blueviolet:"#8A2BE2",darkred:"#8B0000",darkmagenta:"#8B008B",saddlebrown:"#8B4513",darkseagreen:"#8FBC8F",lightgreen:"#90EE90",mediumpurple:"#9370D8",darkviolet:"#9400D3",palegreen:"#98FB98",darkorchid:"#9932CC",yellowgreen:"#9ACD32",sienna:"#A0522D",brown:"#A52A2A",darkgray:"#A9A9A9",lightblue:"#ADD8E6",greenyellow:"#ADFF2F",paleturquoise:"#AFEEEE",lightsteelblue:"#B0C4DE",powderblue:"#B0E0E6",firebrick:"#B22222",darkgoldenrod:"#B8860B",mediumorchid:"#BA55D3",rosybrown:"#BC8F8F",darkkhaki:"#BDB76B",silver:"#C0C0C0",mediumvioletred:"#C71585",indianred:"#CD5C5C",peru:"#CD853F",chocolate:"#D2691E",tan:"#D2B48C",lightgrey:"#D3D3D3",palevioletred:"#D87093",thistle:"#D8BFD8",orchid:"#DA70D6",goldenrod:"#DAA520",crimson:"#DC143C",gainsboro:"#DCDCDC",plum:"#DDA0DD",burlywood:"#DEB887",lightcyan:"#E0FFFF",lavender:"#E6E6FA",darksalmon:"#E9967A",violet:"#EE82EE",palegoldenrod:"#EEE8AA",lightcoral:"#F08080",khaki:"#F0E68C",aliceblue:"#F0F8FF",honeydew:"#F0FFF0",azure:"#F0FFFF",sandybrown:"#F4A460",wheat:"#F5DEB3",beige:"#F5F5DC",whitesmoke:"#F5F5F5",mintcream:"#F5FFFA",ghostwhite:"#F8F8FF",salmon:"#FA8072",antiquewhite:"#FAEBD7",linen:"#FAF0E6",lightgoldenrodyellow:"#FAFAD2",oldlace:"#FDF5E6",red:"#FF0000",fuchsia:"#FF00FF",magenta:"#FF00FF",deeppink:"#FF1493",orangered:"#FF4500",tomato:"#FF6347",hotpink:"#FF69B4",coral:"#FF7F50",darkorange:"#FF8C00",lightsalmon:"#FFA07A",orange:"#FFA500",lightpink:"#FFB6C1",pink:"#FFC0CB",gold:"#FFD700",peachpuff:"#FFDAB9",navajowhite:"#FFDEAD",moccasin:"#FFE4B5",bisque:"#FFE4C4",mistyrose:"#FFE4E1",blanchedalmond:"#FFEBCD",papayawhip:"#FFEFD5",lavenderblush:"#FFF0F5",seashell:"#FFF5EE",cornsilk:"#FFF8DC",lemonchiffon:"#FFFACD",floralwhite:"#FFFAF0",snow:"#FFFAFA",yellow:"#FFFF00",lightyellow:"#FFFFE0",ivory:"#FFFFF0",white:"#FFFFFF"},Dm=function(){function t(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:1;Yd(this,t),this.pixelRatio=e,this.generated=!1,this.centerCoordinates={x:144.5,y:144.5},this.r=289*.49,this.color={r:255,g:255,b:255,a:1},this.hueCircle=void 0,this.initialColor={r:255,g:255,b:255,a:1},this.previousColor=void 0,this.applied=!1,this.updateCallback=function(){},this.closeCallback=function(){},this._create()}return Kd(t,[{key:"insertTo",value:function(t){void 0!==this.hammer&&(this.hammer.destroy(),this.hammer=void 0),this.container=t,this.container.appendChild(this.frame),this._bindHammer(),this._setSize()}},{key:"setUpdateCallback",value:function(t){if("function"!=typeof t)throw new Error("Function attempted to set as colorPicker update callback is not a function.");this.updateCallback=t}},{key:"setCloseCallback",value:function(t){if("function"!=typeof t)throw new Error("Function attempted to set as colorPicker closing callback is not a function.");this.closeCallback=t}},{key:"_isColorString",value:function(t){if("string"==typeof t)return Pm[t]}},{key:"setColor",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];if("none"!==t){var i,n=this._isColorString(t);if(void 0!==n&&(t=n),!0===$y(t)){if(!0===Em(t)){var o=t.substr(4).substr(0,t.length-5).split(",");i={r:o[0],g:o[1],b:o[2],a:1}}else if(!0===Om(t)){var r=t.substr(5).substr(0,t.length-6).split(",");i={r:r[0],g:r[1],b:r[2],a:r[3]}}else if(!0===xm(t)){var s=fm(t);i={r:s.r,g:s.g,b:s.b,a:1}}}else if(t instanceof Object&&void 0!==t.r&&void 0!==t.g&&void 0!==t.b){var a=void 0!==t.a?t.a:"1.0";i={r:t.r,g:t.g,b:t.b,a:a}}if(void 0===i)throw new Error("Unknown color passed to the colorPicker. Supported are strings: rgb, hex, rgba. Object: rgb ({r:r,g:g,b:b,[a:a]}). Supplied: "+gv(t));this._setColor(i,e)}}},{key:"show",value:function(){void 0!==this.closeCallback&&(this.closeCallback(),this.closeCallback=void 0),this.applied=!1,this.frame.style.display="block",this._generateHueCircle()}},{key:"_hide",value:function(){var t=this,e=!(arguments.length>0&&void 0!==arguments[0])||arguments[0];!0===e&&(this.previousColor=un({},this.color)),!0===this.applied&&this.updateCallback(this.initialColor),this.frame.style.display="none",Sv((function(){void 0!==t.closeCallback&&(t.closeCallback(),t.closeCallback=void 0)}),0)}},{key:"_save",value:function(){this.updateCallback(this.color),this.applied=!1,this._hide()}},{key:"_apply",value:function(){this.applied=!0,this.updateCallback(this.color),this._updatePicker(this.color)}},{key:"_loadLast",value:function(){void 0!==this.previousColor?this.setColor(this.previousColor,!1):alert("There is no last color to load...")}},{key:"_setColor",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];!0===e&&(this.initialColor=un({},t)),this.color=t;var i=ym(t.r,t.g,t.b),n=2*Math.PI,o=this.r*i.s,r=this.centerCoordinates.x+o*Math.sin(n*i.h),s=this.centerCoordinates.y+o*Math.cos(n*i.h);this.colorPickerSelector.style.left=r-.5*this.colorPickerSelector.clientWidth+"px",this.colorPickerSelector.style.top=s-.5*this.colorPickerSelector.clientHeight+"px",this._updatePicker(t)}},{key:"_setOpacity",value:function(t){this.color.a=t/100,this._updatePicker(this.color)}},{key:"_setBrightness",value:function(t){var e=ym(this.color.r,this.color.g,this.color.b);e.v=t/100;var i=wm(e.h,e.s,e.v);i.a=this.color.a,this.color=i,this._updatePicker()}},{key:"_updatePicker",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.color,e=ym(t.r,t.g,t.b),i=this.colorPickerCanvas.getContext("2d");void 0===this.pixelRation&&(this.pixelRatio=(window.devicePixelRatio||1)/(i.webkitBackingStorePixelRatio||i.mozBackingStorePixelRatio||i.msBackingStorePixelRatio||i.oBackingStorePixelRatio||i.backingStorePixelRatio||1)),i.setTransform(this.pixelRatio,0,0,this.pixelRatio,0,0);var n=this.colorPickerCanvas.clientWidth,o=this.colorPickerCanvas.clientHeight;i.clearRect(0,0,n,o),i.putImageData(this.hueCircle,0,0),i.fillStyle="rgba(0,0,0,"+(1-e.v)+")",i.circle(this.centerCoordinates.x,this.centerCoordinates.y,this.r),jv(i).call(i),this.brightnessRange.value=100*e.v,this.opacityRange.value=100*t.a,this.initialColorDiv.style.backgroundColor="rgba("+this.initialColor.r+","+this.initialColor.g+","+this.initialColor.b+","+this.initialColor.a+")",this.newColorDiv.style.backgroundColor="rgba("+this.color.r+","+this.color.g+","+this.color.b+","+this.color.a+")"}},{key:"_setSize",value:function(){this.colorPickerCanvas.style.width="100%",this.colorPickerCanvas.style.height="100%",this.colorPickerCanvas.width=289*this.pixelRatio,this.colorPickerCanvas.height=289*this.pixelRatio}},{key:"_create",value:function(){var t,e,i,n;if(this.frame=document.createElement("div"),this.frame.className="vis-color-picker",this.colorPickerDiv=document.createElement("div"),this.colorPickerSelector=document.createElement("div"),this.colorPickerSelector.className="vis-selector",this.colorPickerDiv.appendChild(this.colorPickerSelector),this.colorPickerCanvas=document.createElement("canvas"),this.colorPickerDiv.appendChild(this.colorPickerCanvas),this.colorPickerCanvas.getContext){var o=this.colorPickerCanvas.getContext("2d");this.pixelRatio=(window.devicePixelRatio||1)/(o.webkitBackingStorePixelRatio||o.mozBackingStorePixelRatio||o.msBackingStorePixelRatio||o.oBackingStorePixelRatio||o.backingStorePixelRatio||1),this.colorPickerCanvas.getContext("2d").setTransform(this.pixelRatio,0,0,this.pixelRatio,0,0)}else{var r=document.createElement("DIV");r.style.color="red",r.style.fontWeight="bold",r.style.padding="10px",r.innerText="Error: your browser does not support HTML canvas",this.colorPickerCanvas.appendChild(r)}this.colorPickerDiv.className="vis-color",this.opacityDiv=document.createElement("div"),this.opacityDiv.className="vis-opacity",this.brightnessDiv=document.createElement("div"),this.brightnessDiv.className="vis-brightness",this.arrowDiv=document.createElement("div"),this.arrowDiv.className="vis-arrow",this.opacityRange=document.createElement("input");try{this.opacityRange.type="range",this.opacityRange.min="0",this.opacityRange.max="100"}catch(t){}this.opacityRange.value="100",this.opacityRange.className="vis-range",this.brightnessRange=document.createElement("input");try{this.brightnessRange.type="range",this.brightnessRange.min="0",this.brightnessRange.max="100"}catch(t){}this.brightnessRange.value="100",this.brightnessRange.className="vis-range",this.opacityDiv.appendChild(this.opacityRange),this.brightnessDiv.appendChild(this.brightnessRange);var s=this;this.opacityRange.onchange=function(){s._setOpacity(this.value)},this.opacityRange.oninput=function(){s._setOpacity(this.value)},this.brightnessRange.onchange=function(){s._setBrightness(this.value)},this.brightnessRange.oninput=function(){s._setBrightness(this.value)},this.brightnessLabel=document.createElement("div"),this.brightnessLabel.className="vis-label vis-brightness",this.brightnessLabel.innerText="brightness:",this.opacityLabel=document.createElement("div"),this.opacityLabel.className="vis-label vis-opacity",this.opacityLabel.innerText="opacity:",this.newColorDiv=document.createElement("div"),this.newColorDiv.className="vis-new-color",this.newColorDiv.innerText="new",this.initialColorDiv=document.createElement("div"),this.initialColorDiv.className="vis-initial-color",this.initialColorDiv.innerText="initial",this.cancelButton=document.createElement("div"),this.cancelButton.className="vis-button vis-cancel",this.cancelButton.innerText="cancel",this.cancelButton.onclick=zn(t=this._hide).call(t,this,!1),this.applyButton=document.createElement("div"),this.applyButton.className="vis-button vis-apply",this.applyButton.innerText="apply",this.applyButton.onclick=zn(e=this._apply).call(e,this),this.saveButton=document.createElement("div"),this.saveButton.className="vis-button vis-save",this.saveButton.innerText="save",this.saveButton.onclick=zn(i=this._save).call(i,this),this.loadButton=document.createElement("div"),this.loadButton.className="vis-button vis-load",this.loadButton.innerText="load last",this.loadButton.onclick=zn(n=this._loadLast).call(n,this),this.frame.appendChild(this.colorPickerDiv),this.frame.appendChild(this.arrowDiv),this.frame.appendChild(this.brightnessLabel),this.frame.appendChild(this.brightnessDiv),this.frame.appendChild(this.opacityLabel),this.frame.appendChild(this.opacityDiv),this.frame.appendChild(this.newColorDiv),this.frame.appendChild(this.initialColorDiv),this.frame.appendChild(this.cancelButton),this.frame.appendChild(this.applyButton),this.frame.appendChild(this.saveButton),this.frame.appendChild(this.loadButton)}},{key:"_bindHammer",value:function(){var t=this;this.drag={},this.pinch={},this.hammer=new Hy(this.colorPickerCanvas),this.hammer.get("pinch").set({enable:!0}),this.hammer.on("hammer.input",(function(e){e.isFirst&&t._moveSelector(e)})),this.hammer.on("tap",(function(e){t._moveSelector(e)})),this.hammer.on("panstart",(function(e){t._moveSelector(e)})),this.hammer.on("panmove",(function(e){t._moveSelector(e)})),this.hammer.on("panend",(function(e){t._moveSelector(e)}))}},{key:"_generateHueCircle",value:function(){if(!1===this.generated){var t=this.colorPickerCanvas.getContext("2d");void 0===this.pixelRation&&(this.pixelRatio=(window.devicePixelRatio||1)/(t.webkitBackingStorePixelRatio||t.mozBackingStorePixelRatio||t.msBackingStorePixelRatio||t.oBackingStorePixelRatio||t.backingStorePixelRatio||1)),t.setTransform(this.pixelRatio,0,0,this.pixelRatio,0,0);var e,i,n,o,r=this.colorPickerCanvas.clientWidth,s=this.colorPickerCanvas.clientHeight;t.clearRect(0,0,r,s),this.centerCoordinates={x:.5*r,y:.5*s},this.r=.49*r;var a,h=2*Math.PI/360,l=1/this.r;for(n=0;n<360;n++)for(o=0;o<this.r;o++)e=this.centerCoordinates.x+o*Math.sin(h*n),i=this.centerCoordinates.y+o*Math.cos(h*n),a=wm(.002777777777777778*n,o*l,1),t.fillStyle="rgb("+a.r+","+a.g+","+a.b+")",t.fillRect(e-.5,i-.5,2,2);t.strokeStyle="rgba(0,0,0,1)",t.circle(this.centerCoordinates.x,this.centerCoordinates.y,this.r),t.stroke(),this.hueCircle=t.getImageData(0,0,r,s)}this.generated=!0}},{key:"_moveSelector",value:function(t){var e=this.colorPickerDiv.getBoundingClientRect(),i=t.center.x-e.left,n=t.center.y-e.top,o=.5*this.colorPickerDiv.clientHeight,r=.5*this.colorPickerDiv.clientWidth,s=i-r,a=n-o,h=Math.atan2(s,a),l=.98*Math.min(Math.sqrt(s*s+a*a),r),d=Math.cos(h)*l+o,c=Math.sin(h)*l+r;this.colorPickerSelector.style.top=d-.5*this.colorPickerSelector.clientHeight+"px",this.colorPickerSelector.style.left=c-.5*this.colorPickerSelector.clientWidth+"px";var u=h/(2*Math.PI);u=u<0?u+1:u;var f=l/this.r,p=ym(this.color.r,this.color.g,this.color.b);p.h=u,p.s=f;var v=wm(p.h,p.s,p.v);v.a=this.color.a,this.color=v,this.initialColorDiv.style.backgroundColor="rgba("+this.initialColor.r+","+this.initialColor.g+","+this.initialColor.b+","+this.initialColor.a+")",this.newColorDiv.style.backgroundColor="rgba("+this.color.r+","+this.color.g+","+this.color.b+","+this.color.a+")"}}]),t}();function Im(){for(var t=arguments.length,e=new Array(t),i=0;i<t;i++)e[i]=arguments[i];if(e.length<1)throw new TypeError("Invalid arguments.");if(1===e.length)return document.createTextNode(e[0]);var n=document.createElement(e[0]);return n.appendChild(Im.apply(void 0,Jc(au(e).call(e,1)))),n}var Bm,zm=function(){function t(e,i,n){var o=arguments.length>3&&void 0!==arguments[3]?arguments[3]:1,r=arguments.length>4&&void 0!==arguments[4]?arguments[4]:function(){return!1};Yd(this,t),this.parent=e,this.changedOptions=[],this.container=i,this.allowCreation=!1,this.hideOption=r,this.options={},this.initialized=!1,this.popupCounter=0,this.defaultOptions={enabled:!1,filter:!0,container:void 0,showButton:!0},un(this.options,this.defaultOptions),this.configureOptions=n,this.moduleOptions={},this.domElements=[],this.popupDiv={},this.popupLimit=5,this.popupHistory={},this.colorPicker=new Dm(o),this.wrapper=void 0}return Kd(t,[{key:"setOptions",value:function(t){if(void 0!==t){this.popupHistory={},this._removePopup();var e=!0;if("string"==typeof t)this.options.filter=t;else if(lu(t))this.options.filter=t.join();else if("object"===Qc(t)){if(null==t)throw new TypeError("options cannot be null");void 0!==t.container&&(this.options.container=t.container),void 0!==Xf(t)&&(this.options.filter=Xf(t)),void 0!==t.showButton&&(this.options.showButton=t.showButton),void 0!==t.enabled&&(e=t.enabled)}else"boolean"==typeof t?(this.options.filter=!0,e=t):"function"==typeof t&&(this.options.filter=t,e=!0);!1===Xf(this.options)&&(e=!1),this.options.enabled=e}this._clean()}},{key:"setModuleOptions",value:function(t){this.moduleOptions=t,!0===this.options.enabled&&(this._clean(),void 0!==this.options.container&&(this.container=this.options.container),this._create())}},{key:"_create",value:function(){this._clean(),this.changedOptions=[];var t=Xf(this.options),e=0,i=!1;for(var n in this.configureOptions)Object.prototype.hasOwnProperty.call(this.configureOptions,n)&&(this.allowCreation=!1,i=!1,"function"==typeof t?i=(i=t(n,[]))||this._handleObject(this.configureOptions[n],[n],!0):!0!==t&&-1===Fp(t).call(t,n)||(i=!0),!1!==i&&(this.allowCreation=!0,e>0&&this._makeItem([]),this._makeHeader(n),this._handleObject(this.configureOptions[n],[n])),e++);this._makeButton(),this._push()}},{key:"_push",value:function(){this.wrapper=document.createElement("div"),this.wrapper.className="vis-configuration-wrapper",this.container.appendChild(this.wrapper);for(var t=0;t<this.domElements.length;t++)this.wrapper.appendChild(this.domElements[t]);this._showPopupIfNeeded()}},{key:"_clean",value:function(){for(var t=0;t<this.domElements.length;t++)this.wrapper.removeChild(this.domElements[t]);void 0!==this.wrapper&&(this.container.removeChild(this.wrapper),this.wrapper=void 0),this.domElements=[],this._removePopup()}},{key:"_getValue",value:function(t){for(var e=this.moduleOptions,i=0;i<t.length;i++){if(void 0===e[t[i]]){e=void 0;break}e=e[t[i]]}return e}},{key:"_makeItem",value:function(t){if(!0===this.allowCreation){var e=document.createElement("div");e.className="vis-configuration vis-config-item vis-config-s"+t.length;for(var i=arguments.length,n=new Array(i>1?i-1:0),o=1;o<i;o++)n[o-1]=arguments[o];return Fu(n).call(n,(function(t){e.appendChild(t)})),this.domElements.push(e),this.domElements.length}return 0}},{key:"_makeHeader",value:function(t){var e=document.createElement("div");e.className="vis-configuration vis-config-header",e.innerText=t,this._makeItem([],e)}},{key:"_makeLabel",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=document.createElement("div");if(n.className="vis-configuration vis-config-label vis-config-s"+e.length,!0===i){for(;n.firstChild;)n.removeChild(n.firstChild);n.appendChild(Im("i","b",t))}else n.innerText=t+":";return n}},{key:"_makeDropdown",value:function(t,e,i){var n=document.createElement("select");n.className="vis-configuration vis-config-select";var o=0;void 0!==e&&-1!==Fp(t).call(t,e)&&(o=Fp(t).call(t,e));for(var r=0;r<t.length;r++){var s=document.createElement("option");s.value=t[r],r===o&&(s.selected="selected"),s.innerText=t[r],n.appendChild(s)}var a=this;n.onchange=function(){a._update(this.value,i)};var h=this._makeLabel(i[i.length-1],i);this._makeItem(i,h,n)}},{key:"_makeRange",value:function(t,e,i){var n=t[0],o=t[1],r=t[2],s=t[3],a=document.createElement("input");a.className="vis-configuration vis-config-range";try{a.type="range",a.min=o,a.max=r}catch(t){}a.step=s;var h="",l=0;if(void 0!==e){var d=1.2;e<0&&e*d<o?(a.min=Math.ceil(e*d),l=a.min,h="range increased"):e/d<o&&(a.min=Math.ceil(e/d),l=a.min,h="range increased"),e*d>r&&1!==r&&(a.max=Math.ceil(e*d),l=a.max,h="range increased"),a.value=e}else a.value=n;var c=document.createElement("input");c.className="vis-configuration vis-config-rangeinput",c.value=a.value;var u=this;a.onchange=function(){c.value=this.value,u._update(Number(this.value),i)},a.oninput=function(){c.value=this.value};var f=this._makeLabel(i[i.length-1],i),p=this._makeItem(i,f,a,c);""!==h&&this.popupHistory[p]!==l&&(this.popupHistory[p]=l,this._setupPopup(h,p))}},{key:"_makeButton",value:function(){var t=this;if(!0===this.options.showButton){var e=document.createElement("div");e.className="vis-configuration vis-config-button",e.innerText="generate options",e.onclick=function(){t._printOptions()},e.onmouseover=function(){e.className="vis-configuration vis-config-button hover"},e.onmouseout=function(){e.className="vis-configuration vis-config-button"},this.optionsContainer=document.createElement("div"),this.optionsContainer.className="vis-configuration vis-config-option-container",this.domElements.push(this.optionsContainer),this.domElements.push(e)}}},{key:"_setupPopup",value:function(t,e){var i=this;if(!0===this.initialized&&!0===this.allowCreation&&this.popupCounter<this.popupLimit){var n=document.createElement("div");n.id="vis-configuration-popup",n.className="vis-configuration-popup",n.innerText=t,n.onclick=function(){i._removePopup()},this.popupCounter+=1,this.popupDiv={html:n,index:e}}}},{key:"_removePopup",value:function(){void 0!==this.popupDiv.html&&(this.popupDiv.html.parentNode.removeChild(this.popupDiv.html),clearTimeout(this.popupDiv.hideTimeout),clearTimeout(this.popupDiv.deleteTimeout),this.popupDiv={})}},{key:"_showPopupIfNeeded",value:function(){var t=this;if(void 0!==this.popupDiv.html){var e=this.domElements[this.popupDiv.index].getBoundingClientRect();this.popupDiv.html.style.left=e.left+"px",this.popupDiv.html.style.top=e.top-30+"px",document.body.appendChild(this.popupDiv.html),this.popupDiv.hideTimeout=Sv((function(){t.popupDiv.html.style.opacity=0}),1500),this.popupDiv.deleteTimeout=Sv((function(){t._removePopup()}),1800)}}},{key:"_makeCheckbox",value:function(t,e,i){var n=document.createElement("input");n.type="checkbox",n.className="vis-configuration vis-config-checkbox",n.checked=t,void 0!==e&&(n.checked=e,e!==t&&("object"===Qc(t)?e!==t.enabled&&this.changedOptions.push({path:i,value:e}):this.changedOptions.push({path:i,value:e})));var o=this;n.onchange=function(){o._update(this.checked,i)};var r=this._makeLabel(i[i.length-1],i);this._makeItem(i,r,n)}},{key:"_makeTextInput",value:function(t,e,i){var n=document.createElement("input");n.type="text",n.className="vis-configuration vis-config-text",n.value=e,e!==t&&this.changedOptions.push({path:i,value:e});var o=this;n.onchange=function(){o._update(this.value,i)};var r=this._makeLabel(i[i.length-1],i);this._makeItem(i,r,n)}},{key:"_makeColorField",value:function(t,e,i){var n=this,o=t[1],r=document.createElement("div");"none"!==(e=void 0===e?o:e)?(r.className="vis-configuration vis-config-colorBlock",r.style.backgroundColor=e):r.className="vis-configuration vis-config-colorBlock none",e=void 0===e?o:e,r.onclick=function(){n._showColorPicker(e,r,i)};var s=this._makeLabel(i[i.length-1],i);this._makeItem(i,s,r)}},{key:"_showColorPicker",value:function(t,e,i){var n=this;e.onclick=function(){},this.colorPicker.insertTo(e),this.colorPicker.show(),this.colorPicker.setColor(t),this.colorPicker.setUpdateCallback((function(t){var o="rgba("+t.r+","+t.g+","+t.b+","+t.a+")";e.style.backgroundColor=o,n._update(o,i)})),this.colorPicker.setCloseCallback((function(){e.onclick=function(){n._showColorPicker(t,e,i)}}))}},{key:"_handleObject",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:[],i=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=!1,o=Xf(this.options),r=!1;for(var s in t)if(Object.prototype.hasOwnProperty.call(t,s)){n=!0;var a=t[s],h=om(e,s);if("function"==typeof o&&!1===(n=o(s,e))&&!lu(a)&&"string"!=typeof a&&"boolean"!=typeof a&&a instanceof Object&&(this.allowCreation=!1,n=this._handleObject(a,h,!0),this.allowCreation=!1===i),!1!==n){r=!0;var l=this._getValue(h);if(lu(a))this._handleArray(a,l,h);else if("string"==typeof a)this._makeTextInput(a,l,h);else if("boolean"==typeof a)this._makeCheckbox(a,l,h);else if(a instanceof Object){if(!this.hideOption(e,s,this.moduleOptions))if(void 0!==a.enabled){var d=om(h,"enabled"),c=this._getValue(d);if(!0===c){var u=this._makeLabel(s,h,!0);this._makeItem(h,u),r=this._handleObject(a,h)||r}else this._makeCheckbox(a,c,h)}else{var f=this._makeLabel(s,h,!0);this._makeItem(h,f),r=this._handleObject(a,h)||r}}else console.error("dont know how to handle",a,s,h)}}return r}},{key:"_handleArray",value:function(t,e,i){"string"==typeof t[0]&&"color"===t[0]?(this._makeColorField(t,e,i),t[1]!==e&&this.changedOptions.push({path:i,value:e})):"string"==typeof t[0]?(this._makeDropdown(t,e,i),t[0]!==e&&this.changedOptions.push({path:i,value:e})):"number"==typeof t[0]&&(this._makeRange(t,e,i),t[0]!==e&&this.changedOptions.push({path:i,value:Number(e)}))}},{key:"_update",value:function(t,e){var i=this._constructOptions(t,e);this.parent.body&&this.parent.body.emitter&&this.parent.body.emitter.emit&&this.parent.body.emitter.emit("configChange",i),this.initialized=!0,this.parent.setOptions(i)}},{key:"_constructOptions",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{},n=i;t="false"!==(t="true"===t||t)&&t;for(var o=0;o<e.length;o++)"global"!==e[o]&&(void 0===n[e[o]]&&(n[e[o]]={}),o!==e.length-1?n=n[e[o]]:n[e[o]]=t);return i}},{key:"_printOptions",value:function(){for(var t=this.getOptions();this.optionsContainer.firstChild;)this.optionsContainer.removeChild(this.optionsContainer.firstChild);this.optionsContainer.appendChild(Im("pre","const options = "+gv(t,null,2)))}},{key:"getOptions",value:function(){for(var t={},e=0;e<this.changedOptions.length;e++)this._constructOptions(this.changedOptions[e].value,this.changedOptions[e].path,t);return t}}]),t}(),Nm=function(){function t(e,i){Yd(this,t),this.container=e,this.overflowMethod=i||"cap",this.x=0,this.y=0,this.padding=5,this.hidden=!1,this.frame=document.createElement("div"),this.frame.className="vis-tooltip",this.container.appendChild(this.frame)}return Kd(t,[{key:"setPosition",value:function(t,e){this.x=Ep(t),this.y=Ep(e)}},{key:"setText",value:function(t){if(t instanceof Element){for(;this.frame.firstChild;)this.frame.removeChild(this.frame.firstChild);this.frame.appendChild(t)}else this.frame.innerText=t}},{key:"show",value:function(t){if(void 0===t&&(t=!0),!0===t){var e=this.frame.clientHeight,i=this.frame.clientWidth,n=this.frame.parentNode.clientHeight,o=this.frame.parentNode.clientWidth,r=0,s=0;if("flip"==this.overflowMethod){var a=!1,h=!0;this.y-e<this.padding&&(h=!1),this.x+i>o-this.padding&&(a=!0),r=a?this.x-i:this.x,s=h?this.y-e:this.y}else(s=this.y-e)+e+this.padding>n&&(s=n-e-this.padding),s<this.padding&&(s=this.padding),(r=this.x)+i+this.padding>o&&(r=o-i-this.padding),r<this.padding&&(r=this.padding);this.frame.style.left=r+"px",this.frame.style.top=s+"px",this.frame.style.visibility="visible",this.hidden=!1}else this.hide()}},{key:"hide",value:function(){this.hidden=!0,this.frame.style.left="0",this.frame.style.top="0",this.frame.style.visibility="hidden"}},{key:"destroy",value:function(){this.frame.parentNode.removeChild(this.frame)}}]),t}(),Fm=!1,Am="background: #FFeeee; color: #dd0000",jm=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"validate",value:function(e,i,n){Fm=!1,Bm=i;var o=i;return void 0!==n&&(o=i[n]),t.parse(e,o,[]),Fm}},{key:"parse",value:function(e,i,n){for(var o in e)Object.prototype.hasOwnProperty.call(e,o)&&t.check(o,e,i,n)}},{key:"check",value:function(e,i,n,o){if(void 0!==n[e]||void 0!==n.__any__){var r=e,s=!0;void 0===n[e]&&void 0!==n.__any__&&(r="__any__",s="object"===t.getType(i[e]));var a=n[r];s&&void 0!==a.__type__&&(a=a.__type__),t.checkFields(e,i,n,r,a,o)}else t.getSuggestion(e,n,o)}},{key:"checkFields",value:function(e,i,n,o,r,s){var a=function(i){console.error("%c"+i+t.printLocation(s,e),Am)},h=t.getType(i[e]),l=r[h];void 0!==l?"array"===t.getType(l)&&-1===Fp(l).call(l,i[e])?(a('Invalid option detected in "'+e+'". Allowed values are:'+t.print(l)+' not "'+i[e]+'". '),Fm=!0):"object"===h&&"__any__"!==o&&(s=om(s,e),t.parse(i[e],n[o],s)):void 0===r.any&&(a('Invalid type received for "'+e+'". Expected: '+t.print(bu(r))+". Received ["+h+'] "'+i[e]+'"'),Fm=!0)}},{key:"getType",value:function(t){var e=Qc(t);return"object"===e?null===t?"null":t instanceof Boolean?"boolean":t instanceof Number?"number":t instanceof String?"string":lu(t)?"array":t instanceof Date?"date":void 0!==t.nodeType?"dom":!0===t._isAMomentObject?"moment":"object":"number"===e?"number":"boolean"===e?"boolean":"string"===e?"string":void 0===e?"undefined":e}},{key:"getSuggestion",value:function(e,i,n){var o,r=t.findInOptions(e,i,n,!1),s=t.findInOptions(e,Bm,[],!0);o=void 0!==r.indexMatch?" in "+t.printLocation(r.path,e,"")+'Perhaps it was incomplete? Did you mean: "'+r.indexMatch+'"?\n\n':s.distance<=4&&r.distance>s.distance?" in "+t.printLocation(r.path,e,"")+"Perhaps it was misplaced? Matching option found at: "+t.printLocation(s.path,s.closestMatch,""):r.distance<=8?'. Did you mean "'+r.closestMatch+'"?'+t.printLocation(r.path,e):". Did you mean one of these: "+t.print(bu(i))+t.printLocation(n,e),console.error('%cUnknown option detected: "'+e+'"'+o,Am),Fm=!0}},{key:"findInOptions",value:function(e,i,n){var o=arguments.length>3&&void 0!==arguments[3]&&arguments[3],r=1e9,s="",a=[],h=e.toLowerCase(),l=void 0;for(var d in i){var c=void 0;if(void 0!==i[d].__type__&&!0===o){var u=t.findInOptions(e,i[d],om(n,d));r>u.distance&&(s=u.closestMatch,a=u.path,r=u.distance,l=u.indexMatch)}else{var f;-1!==Fp(f=d.toLowerCase()).call(f,h)&&(l=d),r>(c=t.levenshteinDistance(e,d))&&(s=d,a=rm(n),r=c)}}return{closestMatch:s,path:a,distance:r,indexMatch:l}}},{key:"printLocation",value:function(t,e){for(var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:"Problem value found at: \n",n="\n\n"+i+"options = {\n",o=0;o<t.length;o++){for(var r=0;r<o+1;r++)n+="  ";n+=t[o]+": {\n"}for(var s=0;s<t.length+1;s++)n+="  ";n+=e+"\n";for(var a=0;a<t.length+1;a++){for(var h=0;h<t.length-a;h++)n+="  ";n+="}\n"}return n+"\n\n"}},{key:"print",value:function(t){return gv(t).replace(/(")|(\[)|(\])|(,"__type__")/g,"").replace(/(,)/g,", ")}},{key:"levenshteinDistance",value:function(t,e){if(0===t.length)return e.length;if(0===e.length)return t.length;var i,n,o=[];for(i=0;i<=e.length;i++)o[i]=[i];for(n=0;n<=t.length;n++)o[0][n]=n;for(i=1;i<=e.length;i++)for(n=1;n<=t.length;n++)e.charAt(i-1)==t.charAt(n-1)?o[i][n]=o[i-1][n-1]:o[i][n]=Math.min(o[i-1][n-1]+1,Math.min(o[i][n-1]+1,o[i-1][n]+1));return o[e.length][t.length]}}]),t}(),Rm=Wy,Lm=Dm,Hm=zm,Wm=Hy,qm=Nm,Vm=Am,Um=jm,Ym=Object.freeze({__proto__:null,Activator:Rm,Alea:jy,ColorPicker:Lm,Configurator:Hm,DELETE:Iy,HSVToHex:km,HSVToRGB:wm,Hammer:Wm,Popup:qm,RGBToHSV:ym,RGBToHex:vm,VALIDATOR_PRINT_STYLE:Vm,Validator:Um,addClassName:function(t,e){var i=t.className.split(" "),n=e.split(" ");i=su(i).call(i,Xf(n).call(n,(function(t){return!Nf(i).call(i,t)}))),t.className=i.join(" ")},addCssText:function(t,e){var i=mm(t.style.cssText),n=mm(e),o=My(My({},i),n);t.style.cssText=bm(o)},addEventListener:dm,binarySearchCustom:function(t,e,i,n){for(var o=0,r=0,s=t.length-1;r<=s&&o<1e4;){var a=Math.floor((r+s)/2),h=t[a],l=e(void 0===n?h[i]:h[i][n]);if(0==l)return a;-1==l?r=a+1:s=a-1,o++}return-1},binarySearchValue:function(t,e,i,n,o){var r,s,a,h,l=0,d=0,c=t.length-1;for(o=null!=o?o:function(t,e){return t==e?0:t<e?-1:1};d<=c&&l<1e4;){if(h=Math.floor(.5*(c+d)),r=t[Math.max(0,h-1)][i],s=t[h][i],a=t[Math.min(t.length-1,h+1)][i],0==o(s,e))return h;if(o(r,e)<0&&o(s,e)>0)return"before"==n?Math.max(0,h-1):h;if(o(s,e)<0&&o(a,e)>0)return"before"==n?h:Math.min(t.length-1,h+1);o(s,e)<0?d=h+1:c=h-1,l++}return-1},bridgeObject:Cm,copyAndExtendArray:om,copyArray:rm,deepExtend:nm,deepObjectAssign:zy,easingFunctions:Tm,equalArray:function(t,e){if(t.length!==e.length)return!1;for(var i=0,n=t.length;i<n;i++)if(t[i]!=e[i])return!1;return!0},extend:tm,fillIfDefined:Jy,forEach:hm,getAbsoluteLeft:sm,getAbsoluteRight:function(t){return t.getBoundingClientRect().right},getAbsoluteTop:am,getScrollBarWidth:function(){var t=document.createElement("p");t.style.width="100%",t.style.height="200px";var e=document.createElement("div");e.style.position="absolute",e.style.top="0px",e.style.left="0px",e.style.visibility="hidden",e.style.width="200px",e.style.height="150px",e.style.overflow="hidden",e.appendChild(t),document.body.appendChild(e);var i=t.offsetWidth;e.style.overflow="scroll";var n=t.offsetWidth;return i==n&&(n=e.clientWidth),document.body.removeChild(e),i-n},getTarget:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:window.event,e=null;return t&&(t.target?e=t.target:t.srcElement&&(e=t.srcElement)),e instanceof Element&&(null==e.nodeType||3!=e.nodeType||(e=e.parentNode)instanceof Element)?e:null},getType:function(t){var e=Qc(t);return"object"===e?null===t?"null":t instanceof Boolean?"Boolean":t instanceof Number?"Number":t instanceof String?"String":lu(t)?"Array":t instanceof Date?"Date":"Object":"number"===e?"Number":"boolean"===e?"Boolean":"string"===e?"String":void 0===e?"undefined":e},hasParent:function(t,e){for(var i=t;i;){if(i===e)return!0;if(!i.parentNode)return!1;i=i.parentNode}return!1},hexToHSV:_m,hexToRGB:fm,insertSort:function(t,e){for(var i=0;i<t.length;i++){var n=t[i],o=void 0;for(o=i;o>0&&e(n,t[o-1])<0;o--)t[o]=t[o-1];t[o]=n}return t},isDate:function(t){if(t instanceof Date)return!0;if($y(t)){if(qy.exec(t))return!0;if(!isNaN(Date.parse(t)))return!0}return!1},isNumber:Gy,isObject:Zy,isString:$y,isValidHex:xm,isValidRGB:Em,isValidRGBA:Om,mergeOptions:Sm,option:um,overrideOpacity:pm,parseColor:gm,preventDefault:function(t){t||(t=window.event),t&&(t.preventDefault?t.preventDefault():t.returnValue=!1)},pureDeepObjectAssign:By,recursiveDOMDelete:Ky,removeClassName:function(t,e){var i=t.className.split(" "),n=e.split(" ");i=Xf(i).call(i,(function(t){return!Nf(n).call(n,t)})),t.className=i.join(" ")},removeCssText:function(t,e){var i=mm(t.style.cssText),n=mm(e);for(var o in n)Object.prototype.hasOwnProperty.call(n,o)&&delete i[o];t.style.cssText=bm(i)},removeEventListener:cm,selectiveBridgeObject:function(t,e){if(null!==e&&"object"===Qc(e)){for(var i=Kp(e),n=0;n<t.length;n++)Object.prototype.hasOwnProperty.call(e,t[n])&&"object"==Qc(e[t[n]])&&(i[t[n]]=Cm(e[t[n]]));return i}return null},selectiveDeepExtend:em,selectiveExtend:function(t,e){if(!lu(t))throw new Error("Array with property names expected as first argument");for(var i=arguments.length,n=new Array(i>2?i-2:0),o=2;o<i;o++)n[o-2]=arguments[o];for(var r=0,s=n;r<s.length;r++)for(var a=s[r],h=0;h<t.length;h++){var l=t[h];a&&Object.prototype.hasOwnProperty.call(a,l)&&(e[l]=a[l])}return e},selectiveNotDeepExtend:im,throttle:function(t){var e=!1;return function(){e||(e=!0,requestAnimationFrame((function(){e=!1,t()})))}},toArray:lm,topMost:Mm,updateProperty:function(t,e,i){return t[e]!==i&&(t[e]=i,!0)}});function Xm(t){return eb=t,function(){var t={};ib=0,void(nb=eb.charAt(0)),pb(),"strict"===ob&&(t.strict=!0,pb());"graph"!==ob&&"digraph"!==ob||(t.type=ob,pb());rb===Qm&&(t.id=ob,pb());if("{"!=ob)throw wb("Angle bracket { expected");if(pb(),vb(t),"}"!=ob)throw wb("Angle bracket } expected");if(pb(),""!==ob)throw wb("End of file expected");return pb(),delete t.node,delete t.edge,delete t.graph,t}()}var Gm={fontsize:"font.size",fontcolor:"font.color",labelfontcolor:"font.color",fontname:"font.face",color:["color.border","color.background"],fillcolor:"color.background",tooltip:"title",labeltooltip:"title"},Km=Kp(Gm);Km.color="color.color",Km.style="dashes";var $m=0,Zm=1,Qm=2,Jm=3,tb={"{":!0,"}":!0,"[":!0,"]":!0,";":!0,"=":!0,",":!0,"->":!0,"--":!0},eb="",ib=0,nb="",ob="",rb=$m;function sb(){ib++,nb=eb.charAt(ib)}function ab(){return eb.charAt(ib+1)}function hb(t){var e=t.charCodeAt(0);return e<47?35===e||46===e:e<59?e>47:e<91?e>64:e<96?95===e:e<123&&e>96}function lb(t,e){if(t||(t={}),e)for(var i in e)e.hasOwnProperty(i)&&(t[i]=e[i]);return t}function db(t,e,i){for(var n=e.split("."),o=t;n.length;){var r=n.shift();n.length?(o[r]||(o[r]={}),o=o[r]):o[r]=i}}function cb(t,e){for(var i,n,o=null,r=[t],s=t;s.parent;)r.push(s.parent),s=s.parent;if(s.nodes)for(i=0,n=s.nodes.length;i<n;i++)if(e.id===s.nodes[i].id){o=s.nodes[i];break}for(o||(o={id:e.id},t.node&&(o.attr=lb(o.attr,t.node))),i=r.length-1;i>=0;i--){var a,h=r[i];h.nodes||(h.nodes=[]),-1===Fp(a=h.nodes).call(a,o)&&h.nodes.push(o)}e.attr&&(o.attr=lb(o.attr,e.attr))}function ub(t,e){if(t.edges||(t.edges=[]),t.edges.push(e),t.edge){var i=lb({},t.edge);e.attr=lb(i,e.attr)}}function fb(t,e,i,n,o){var r={from:e,to:i,type:n};return t.edge&&(r.attr=lb({},t.edge)),r.attr=lb(r.attr||{},o),null!=o&&o.hasOwnProperty("arrows")&&null!=o.arrows&&(r.arrows={to:{enabled:!0,type:o.arrows.type}},o.arrows=null),r}function pb(){for(rb=$m,ob="";" "===nb||"\t"===nb||"\n"===nb||"\r"===nb;)sb();do{var t=!1;if("#"===nb){for(var e=ib-1;" "===eb.charAt(e)||"\t"===eb.charAt(e);)e--;if("\n"===eb.charAt(e)||""===eb.charAt(e)){for(;""!=nb&&"\n"!=nb;)sb();t=!0}}if("/"===nb&&"/"===ab()){for(;""!=nb&&"\n"!=nb;)sb();t=!0}if("/"===nb&&"*"===ab()){for(;""!=nb;){if("*"===nb&&"/"===ab()){sb(),sb();break}sb()}t=!0}for(;" "===nb||"\t"===nb||"\n"===nb||"\r"===nb;)sb()}while(t);if(""!==nb){var i=nb+ab();if(tb[i])return rb=Zm,ob=i,sb(),void sb();if(tb[nb])return rb=Zm,ob=nb,void sb();if(hb(nb)||"-"===nb){for(ob+=nb,sb();hb(nb);)ob+=nb,sb();return"false"===ob?ob=!1:"true"===ob?ob=!0:isNaN(Number(ob))||(ob=Number(ob)),void(rb=Qm)}if('"'===nb){for(sb();""!=nb&&('"'!=nb||'"'===nb&&'"'===ab());)'"'===nb?(ob+=nb,sb()):"\\"===nb&&"n"===ab()?(ob+="\n",sb()):ob+=nb,sb();if('"'!=nb)throw wb('End of string " expected');return sb(),void(rb=Qm)}for(rb=Jm;""!=nb;)ob+=nb,sb();throw new SyntaxError('Syntax error in part "'+kb(ob,30)+'"')}rb=Zm}function vb(t){for(;""!==ob&&"}"!=ob;)gb(t),";"===ob&&pb()}function gb(t){var e=yb(t);if(e)mb(t,e);else{var i=function(t){if("node"===ob)return pb(),t.node=bb(),"node";if("edge"===ob)return pb(),t.edge=bb(),"edge";if("graph"===ob)return pb(),t.graph=bb(),"graph";return null}(t);if(!i){if(rb!=Qm)throw wb("Identifier expected");var n=ob;if(pb(),"="===ob){if(pb(),rb!=Qm)throw wb("Identifier expected");t[n]=ob,pb()}else!function(t,e){var i={id:e},n=bb();n&&(i.attr=n);cb(t,i),mb(t,e)}(t,n)}}}function yb(t){var e=null;if("subgraph"===ob&&((e={}).type="subgraph",pb(),rb===Qm&&(e.id=ob,pb())),"{"===ob){if(pb(),e||(e={}),e.parent=t,e.node=t.node,e.edge=t.edge,e.graph=t.graph,vb(e),"}"!=ob)throw wb("Angle bracket } expected");pb(),delete e.node,delete e.edge,delete e.graph,delete e.parent,t.subgraphs||(t.subgraphs=[]),t.subgraphs.push(e)}return e}function mb(t,e){for(;"->"===ob||"--"===ob;){var i,n=ob;pb();var o=yb(t);if(o)i=o;else{if(rb!=Qm)throw wb("Identifier or subgraph expected");cb(t,{id:i=ob}),pb()}ub(t,fb(t,e,i,n,bb())),e=i}}function bb(){for(var t,e,i=null,n={dashed:!0,solid:!1,dotted:[1,5]},o={dot:"circle",box:"box",crow:"crow",curve:"curve",icurve:"inv_curve",normal:"triangle",inv:"inv_triangle",diamond:"diamond",tee:"bar",vee:"vee"},r=new Array,s=new Array;"["===ob;){for(pb(),i={};""!==ob&&"]"!=ob;){if(rb!=Qm)throw wb("Attribute name expected");var a=ob;if(pb(),"="!=ob)throw wb("Equal sign = expected");if(pb(),rb!=Qm)throw wb("Attribute value expected");var h=ob;"style"===a&&(h=n[h]),"arrowhead"===a&&(a="arrows",h={to:{enabled:!0,type:o[h]}}),"arrowtail"===a&&(a="arrows",h={from:{enabled:!0,type:o[h]}}),r.push({attr:i,name:a,value:h}),s.push(a),pb(),","==ob&&pb()}if("]"!=ob)throw wb("Bracket ] expected");pb()}if(Nf(s).call(s,"dir")){var l={arrows:{}};for(t=0;t<r.length;t++)if("arrows"===r[t].name)if(null!=r[t].value.to)l.arrows.to=t;else{if(null==r[t].value.from)throw wb("Invalid value of arrows");l.arrows.from=t}else"dir"===r[t].name&&(l.dir=t);var d,c,u=r[l.dir].value;if(!Nf(s).call(s,"arrows"))if("both"===u)r.push({attr:r[l.dir].attr,name:"arrows",value:{to:{enabled:!0}}}),l.arrows.to=r.length-1,r.push({attr:r[l.dir].attr,name:"arrows",value:{from:{enabled:!0}}}),l.arrows.from=r.length-1;else if("forward"===u)r.push({attr:r[l.dir].attr,name:"arrows",value:{to:{enabled:!0}}}),l.arrows.to=r.length-1;else if("back"===u)r.push({attr:r[l.dir].attr,name:"arrows",value:{from:{enabled:!0}}}),l.arrows.from=r.length-1;else{if("none"!==u)throw wb('Invalid dir type "'+u+'"');r.push({attr:r[l.dir].attr,name:"arrows",value:""}),l.arrows.to=r.length-1}if("both"===u)l.arrows.to&&l.arrows.from?(c=r[l.arrows.to].value.to.type,d=r[l.arrows.from].value.from.type,r[l.arrows.to]={attr:r[l.arrows.to].attr,name:r[l.arrows.to].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}},ff(r).call(r,l.arrows.from,1)):l.arrows.to?(c=r[l.arrows.to].value.to.type,d="arrow",r[l.arrows.to]={attr:r[l.arrows.to].attr,name:r[l.arrows.to].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}}):l.arrows.from&&(c="arrow",d=r[l.arrows.from].value.from.type,r[l.arrows.from]={attr:r[l.arrows.from].attr,name:r[l.arrows.from].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}});else if("back"===u)l.arrows.to&&l.arrows.from?(c="",d=r[l.arrows.from].value.from.type,r[l.arrows.from]={attr:r[l.arrows.from].attr,name:r[l.arrows.from].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}}):l.arrows.to?(c="",d="arrow",l.arrows.from=l.arrows.to,r[l.arrows.from]={attr:r[l.arrows.from].attr,name:r[l.arrows.from].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}}):l.arrows.from&&(c="",d=r[l.arrows.from].value.from.type,r[l.arrows.to]={attr:r[l.arrows.from].attr,name:r[l.arrows.from].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}}),r[l.arrows.from]={attr:r[l.arrows.from].attr,name:r[l.arrows.from].name,value:{from:{enabled:!0,type:r[l.arrows.from].value.from.type}}};else if("none"===u){var f;r[f=l.arrows.to?l.arrows.to:l.arrows.from]={attr:r[f].attr,name:r[f].name,value:""}}else{if("forward"!==u)throw wb('Invalid dir type "'+u+'"');l.arrows.to&&l.arrows.from||l.arrows.to?(c=r[l.arrows.to].value.to.type,d="",r[l.arrows.to]={attr:r[l.arrows.to].attr,name:r[l.arrows.to].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}}):l.arrows.from&&(c="arrow",d="",l.arrows.to=l.arrows.from,r[l.arrows.to]={attr:r[l.arrows.to].attr,name:r[l.arrows.to].name,value:{to:{enabled:!0,type:c},from:{enabled:!0,type:d}}}),r[l.arrows.to]={attr:r[l.arrows.to].attr,name:r[l.arrows.to].name,value:{to:{enabled:!0,type:r[l.arrows.to].value.to.type}}}}ff(r).call(r,l.dir,1)}if(Nf(s).call(s,"penwidth")){var p=[];for(e=r.length,t=0;t<e;t++)"width"!==r[t].name&&("penwidth"===r[t].name&&(r[t].name="width"),p.push(r[t]));r=p}for(e=r.length,t=0;t<e;t++)db(r[t].attr,r[t].name,r[t].value);return i}function wb(t){return new SyntaxError(t+', got "'+kb(ob,30)+'" (char '+ib+")")}function kb(t,e){return t.length<=e?t:t.substr(0,27)+"..."}function _b(t,e,i){for(var n=e.split("."),o=n.pop(),r=t,s=0;s<n.length;s++){var a=n[s];a in r||(r[a]={}),r=r[a]}return r[o]=i,t}function xb(t,e){var i={};for(var n in t)if(t.hasOwnProperty(n)){var o=e[n];lu(o)?Fu(o).call(o,(function(e){_b(i,e,t[n])})):_b(i,"string"==typeof o?o:n,t[n])}return i}function Eb(t){var e,i=Xm(t),n={nodes:[],edges:[],options:{}};i.nodes&&Fu(e=i.nodes).call(e,(function(t){var e={id:t.id,label:String(t.label||t.id)};lb(e,xb(t.attr,Gm)),e.image&&(e.shape="image"),n.nodes.push(e)}));if(i.edges){var o,r=function(t){var e={from:t.from,to:t.to};return lb(e,xb(t.attr,Km)),null==e.arrows&&"->"===t.type&&(e.arrows="to"),e};Fu(o=i.edges).call(o,(function(t){var e,i,o,s,a,h,l;(e=t.from instanceof Object?t.from.nodes:{id:t.from},i=t.to instanceof Object?t.to.nodes:{id:t.to},t.from instanceof Object&&t.from.edges)&&Fu(o=t.from.edges).call(o,(function(t){var e=r(t);n.edges.push(e)}));(a=i,h=function(e,i){var o=fb(n,e.id,i.id,t.type,t.attr),s=r(o);n.edges.push(s)},lu(s=e)?Fu(s).call(s,(function(t){lu(a)?Fu(a).call(a,(function(e){h(t,e)})):h(t,a)})):lu(a)?Fu(a).call(a,(function(t){h(s,t)})):h(s,a),t.to instanceof Object&&t.to.edges)&&Fu(l=t.to.edges).call(l,(function(t){var e=r(t);n.edges.push(e)}))}))}return i.attr&&(n.options=i.attr),n}var Ob=Object.freeze({__proto__:null,parseDOT:Xm,DOTToGraph:Eb});function Cb(t,e){var i,n={edges:{inheritColor:!1},nodes:{fixed:!1,parseColor:!1}};null!=e&&(null!=e.fixed&&(n.nodes.fixed=e.fixed),null!=e.parseColor&&(n.nodes.parseColor=e.parseColor),null!=e.inheritColor&&(n.edges.inheritColor=e.inheritColor));var o=t.edges,r=gu(o).call(o,(function(t){var e={from:t.source,id:t.id,to:t.target};return null!=t.attributes&&(e.attributes=t.attributes),null!=t.label&&(e.label=t.label),null!=t.attributes&&null!=t.attributes.title&&(e.title=t.attributes.title),"Directed"===t.type&&(e.arrows="to"),t.color&&!1===n.edges.inheritColor&&(e.color=t.color),e}));return{nodes:gu(i=t.nodes).call(i,(function(t){var e={id:t.id,fixed:n.nodes.fixed&&null!=t.x&&null!=t.y};return null!=t.attributes&&(e.attributes=t.attributes),null!=t.label&&(e.label=t.label),null!=t.size&&(e.size=t.size),null!=t.attributes&&null!=t.attributes.title&&(e.title=t.attributes.title),null!=t.title&&(e.title=t.title),null!=t.x&&(e.x=t.x),null!=t.y&&(e.y=t.y),null!=t.color&&(!0===n.nodes.parseColor?e.color=t.color:e.color={background:t.color,border:t.color,highlight:{background:t.color,border:t.color},hover:{background:t.color,border:t.color}}),e})),edges:r}}var Sb=Object.freeze({__proto__:null,parseGephi:Cb}),Tb=Object.freeze({__proto__:null,en:{addDescription:"Click in an empty space to place a new node.",addEdge:"Add Edge",addNode:"Add Node",back:"Back",close:"Close",createEdgeError:"Cannot link edges to a cluster.",del:"Delete selected",deleteClusterError:"Clusters cannot be deleted.",edgeDescription:"Click on a node and drag the edge to another node to connect them.",edit:"Edit",editClusterError:"Clusters cannot be edited.",editEdge:"Edit Edge",editEdgeDescription:"Click on the control points and drag them to a node to connect to it.",editNode:"Edit Node"},de:{addDescription:"Klicke auf eine freie Stelle, um einen neuen Knoten zu plazieren.",addEdge:"Kante hinzufügen",addNode:"Knoten hinzufügen",back:"Zurück",close:"Schließen",createEdgeError:"Es ist nicht möglich, Kanten mit Clustern zu verbinden.",del:"Lösche Auswahl",deleteClusterError:"Cluster können nicht gelöscht werden.",edgeDescription:"Klicke auf einen Knoten und ziehe die Kante zu einem anderen Knoten, um diese zu verbinden.",edit:"Editieren",editClusterError:"Cluster können nicht editiert werden.",editEdge:"Kante editieren",editEdgeDescription:"Klicke auf die Verbindungspunkte und ziehe diese auf einen Knoten, um sie zu verbinden.",editNode:"Knoten editieren"},es:{addDescription:"Haga clic en un lugar vacío para colocar un nuevo nodo.",addEdge:"Añadir arista",addNode:"Añadir nodo",back:"Atrás",close:"Cerrar",createEdgeError:"No se puede conectar una arista a un grupo.",del:"Eliminar selección",deleteClusterError:"No es posible eliminar grupos.",edgeDescription:"Haga clic en un nodo y arrastre la arista hacia otro nodo para conectarlos.",edit:"Editar",editClusterError:"No es posible editar grupos.",editEdge:"Editar arista",editEdgeDescription:"Haga clic en un punto de control y arrastrelo a un nodo para conectarlo.",editNode:"Editar nodo"},it:{addDescription:"Clicca per aggiungere un nuovo nodo",addEdge:"Aggiungi un vertice",addNode:"Aggiungi un nodo",back:"Indietro",close:"Chiudere",createEdgeError:"Non si possono collegare vertici ad un cluster",del:"Cancella la selezione",deleteClusterError:"I cluster non possono essere cancellati",edgeDescription:"Clicca su un nodo e trascinalo ad un altro nodo per connetterli.",edit:"Modifica",editClusterError:"I clusters non possono essere modificati.",editEdge:"Modifica il vertice",editEdgeDescription:"Clicca sui Punti di controllo e trascinali ad un nodo per connetterli.",editNode:"Modifica il nodo"},nl:{addDescription:"Klik op een leeg gebied om een nieuwe node te maken.",addEdge:"Link toevoegen",addNode:"Node toevoegen",back:"Terug",close:"Sluiten",createEdgeError:"Kan geen link maken naar een cluster.",del:"Selectie verwijderen",deleteClusterError:"Clusters kunnen niet worden verwijderd.",edgeDescription:"Klik op een node en sleep de link naar een andere node om ze te verbinden.",edit:"Wijzigen",editClusterError:"Clusters kunnen niet worden aangepast.",editEdge:"Link wijzigen",editEdgeDescription:"Klik op de verbindingspunten en sleep ze naar een node om daarmee te verbinden.",editNode:"Node wijzigen"},pt:{addDescription:"Clique em um espaço em branco para adicionar um novo nó",addEdge:"Adicionar aresta",addNode:"Adicionar nó",back:"Voltar",close:"Fechar",createEdgeError:"Não foi possível linkar arestas a um cluster.",del:"Remover selecionado",deleteClusterError:"Clusters não puderam ser removidos.",edgeDescription:"Clique em um nó e arraste a aresta até outro nó para conectá-los",edit:"Editar",editClusterError:"Clusters não puderam ser editados.",editEdge:"Editar aresta",editEdgeDescription:"Clique nos pontos de controle e os arraste para um nó para conectá-los",editNode:"Editar nó"},ru:{addDescription:"Кликните в свободное место, чтобы добавить новый узел.",addEdge:"Добавить ребро",addNode:"Добавить узел",back:"Назад",close:"Закрывать",createEdgeError:"Невозможно соединить ребра в кластер.",del:"Удалить выбранное",deleteClusterError:"Кластеры не могут быть удалены",edgeDescription:"Кликните на узел и протяните ребро к другому узлу, чтобы соединить их.",edit:"Редактировать",editClusterError:"Кластеры недоступны для редактирования.",editEdge:"Редактировать ребро",editEdgeDescription:"Кликните на контрольные точки и перетащите их в узел, чтобы подключиться к нему.",editNode:"Редактировать узел"},cn:{addDescription:"单击空白处放置新节点。",addEdge:"添加连接线",addNode:"添加节点",back:"返回",close:"關閉",createEdgeError:"无法将连接线连接到群集。",del:"删除选定",deleteClusterError:"无法删除群集。",edgeDescription:"单击某个节点并将该连接线拖动到另一个节点以连接它们。",edit:"编辑",editClusterError:"无法编辑群集。",editEdge:"编辑连接线",editEdgeDescription:"单击控制节点并将它们拖到节点上连接。",editNode:"编辑节点"},uk:{addDescription:"Kлікніть на вільне місце, щоб додати новий вузол.",addEdge:"Додати край",addNode:"Додати вузол",back:"Назад",close:"Закрити",createEdgeError:"Не можливо об'єднати краї в групу.",del:"Видалити обране",deleteClusterError:"Групи не можуть бути видалені.",edgeDescription:"Клікніть на вузол і перетягніть край до іншого вузла, щоб їх з'єднати.",edit:"Редагувати",editClusterError:"Групи недоступні для редагування.",editEdge:"Редагувати край",editEdgeDescription:"Клікніть на контрольні точки і перетягніть їх у вузол, щоб підключитися до нього.",editNode:"Редагувати вузол"},fr:{addDescription:"Cliquez dans un endroit vide pour placer un nœud.",addEdge:"Ajouter un lien",addNode:"Ajouter un nœud",back:"Retour",close:"Fermer",createEdgeError:"Impossible de créer un lien vers un cluster.",del:"Effacer la sélection",deleteClusterError:"Les clusters ne peuvent pas être effacés.",edgeDescription:"Cliquez sur un nœud et glissez le lien vers un autre nœud pour les connecter.",edit:"Éditer",editClusterError:"Les clusters ne peuvent pas être édités.",editEdge:"Éditer le lien",editEdgeDescription:"Cliquez sur les points de contrôle et glissez-les pour connecter un nœud.",editNode:"Éditer le nœud"},cs:{addDescription:"Kluknutím do prázdného prostoru můžete přidat nový vrchol.",addEdge:"Přidat hranu",addNode:"Přidat vrchol",back:"Zpět",close:"Zavřít",createEdgeError:"Nelze připojit hranu ke shluku.",del:"Smazat výběr",deleteClusterError:"Nelze mazat shluky.",edgeDescription:"Přetažením z jednoho vrcholu do druhého můžete spojit tyto vrcholy novou hranou.",edit:"Upravit",editClusterError:"Nelze upravovat shluky.",editEdge:"Upravit hranu",editEdgeDescription:"Přetažením kontrolního vrcholu hrany ji můžete připojit k jinému vrcholu.",editNode:"Upravit vrchol"}});var Mb=function(){function t(){Yd(this,t),this.NUM_ITERATIONS=4,this.image=new Image,this.canvas=document.createElement("canvas")}return Kd(t,[{key:"init",value:function(){if(!this.initialized()){this.src=this.image.src;var t=this.image.width,e=this.image.height;this.width=t,this.height=e;var i=Math.floor(e/2),n=Math.floor(e/4),o=Math.floor(e/8),r=Math.floor(e/16),s=Math.floor(t/2),a=Math.floor(t/4),h=Math.floor(t/8),l=Math.floor(t/16);this.canvas.width=3*a,this.canvas.height=i,this.coordinates=[[0,0,s,i],[s,0,a,n],[s,n,h,o],[5*h,n,l,r]],this._fillMipMap()}}},{key:"initialized",value:function(){return void 0!==this.coordinates}},{key:"_fillMipMap",value:function(){var t=this.canvas.getContext("2d"),e=this.coordinates[0];t.drawImage(this.image,e[0],e[1],e[2],e[3]);for(var i=1;i<this.NUM_ITERATIONS;i++){var n=this.coordinates[i-1],o=this.coordinates[i];t.drawImage(this.canvas,n[0],n[1],n[2],n[3],o[0],o[1],o[2],o[3])}}},{key:"drawImageAtPosition",value:function(t,e,i,n,o,r){if(this.initialized())if(e>2){e*=.5;for(var s=0;e>2&&s<this.NUM_ITERATIONS;)e*=.5,s+=1;s>=this.NUM_ITERATIONS&&(s=this.NUM_ITERATIONS-1);var a=this.coordinates[s];t.drawImage(this.canvas,a[0],a[1],a[2],a[3],i,n,o,r)}else t.drawImage(this.image,i,n,o,r)}}]),t}(),Pb=function(){function t(e){Yd(this,t),this.images={},this.imageBroken={},this.callback=e}return Kd(t,[{key:"_tryloadBrokenUrl",value:function(t,e,i){void 0!==t&&void 0!==i&&(void 0!==e?(i.image.onerror=function(){console.error("Could not load brokenImage:",e)},i.image.src=e):console.warn("No broken url image defined"))}},{key:"_redrawWithImage",value:function(t){this.callback&&this.callback(t)}},{key:"load",value:function(t,e){var i=this,n=this.images[t];if(n)return n;var o=new Mb;return this.images[t]=o,o.image.onload=function(){i._fixImageCoordinates(o.image),o.init(),i._redrawWithImage(o)},o.image.onerror=function(){console.error("Could not load image:",t),i._tryloadBrokenUrl(t,e,o)},o.image.src=t,o}},{key:"_fixImageCoordinates",value:function(t){0===t.width&&(document.body.appendChild(t),t.width=t.offsetWidth,t.height=t.offsetHeight,document.body.removeChild(t))}}]),t}(),Db={exports:{}},Ib=o((function(){if("function"==typeof ArrayBuffer){var t=new ArrayBuffer(8);Object.isExtensible(t)&&Object.defineProperty(t,"a",{value:8})}})),Bb=o,zb=Y,Nb=B,Fb=Ib,Ab=Object.isExtensible,jb=Bb((function(){Ab(1)}))||Fb?function(t){return!!zb(t)&&((!Fb||"ArrayBuffer"!=Nb(t))&&(!Ab||Ab(t)))}:Ab,Rb=!o((function(){return Object.isExtensible(Object.preventExtensions({}))})),Lb=_i,Hb=g,Wb=Ri,qb=Y,Vb=Wt,Ub=Ve.f,Yb=rh,Xb=hh,Gb=jb,Kb=Rb,$b=!1,Zb=Xt("meta"),Qb=0,Jb=function(t){Ub(t,Zb,{value:{objectID:"O"+Qb++,weakData:{}}})},tw=Db.exports={enable:function(){tw.enable=function(){},$b=!0;var t=Yb.f,e=Hb([].splice),i={};i[Zb]=1,t(i).length&&(Yb.f=function(i){for(var n=t(i),o=0,r=n.length;o<r;o++)if(n[o]===Zb){e(n,o,1);break}return n},Lb({target:"Object",stat:!0,forced:!0},{getOwnPropertyNames:Xb.f}))},fastKey:function(t,e){if(!qb(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!Vb(t,Zb)){if(!Gb(t))return"F";if(!e)return"E";Jb(t)}return t[Zb].objectID},getWeakData:function(t,e){if(!Vb(t,Zb)){if(!Gb(t))return!0;if(!e)return!1;Jb(t)}return t[Zb].weakData},onFreeze:function(t){return Kb&&$b&&Gb(t)&&!Vb(t,Zb)&&Jb(t),t}};Wb[Zb]=!0;var ew=qe,iw=_,nw=$e,ow=bt,rw=qs,sw=Bi,aw=J,hw=ba,lw=ua,dw=As,cw=n.TypeError,uw=function(t,e){this.stopped=t,this.result=e},fw=uw.prototype,pw=function(t,e,i){var n,o,r,s,a,h,l,d=i&&i.that,c=!(!i||!i.AS_ENTRIES),u=!(!i||!i.IS_ITERATOR),f=!(!i||!i.INTERRUPTED),p=ew(e,d),v=function(t){return n&&dw(n,"normal",t),new uw(!0,t)},g=function(t){return c?(nw(t),f?p(t[0],t[1],v):p(t[0],t[1])):f?p(t,v):p(t)};if(u)n=t;else{if(!(o=lw(t)))throw cw(ow(t)+" is not iterable");if(rw(o)){for(r=0,s=sw(t);s>r;r++)if((a=g(t[r]))&&aw(fw,a))return a;return new uw(!1)}n=hw(t,o)}for(h=n.next;!(l=iw(h,n)).done;){try{a=g(l.value)}catch(t){dw(n,"throw",t)}if("object"==typeof a&&a&&aw(fw,a))return a}return new uw(!1)},vw=J,gw=n.TypeError,yw=function(t,e){if(vw(e,t))return t;throw gw("Incorrect invocation")},mw=_i,bw=n,ww=Db.exports,kw=o,_w=di,xw=pw,Ew=yw,Ow=y,Cw=Y,Sw=$r,Tw=Ve.f,Mw=Wh.forEach,Pw=b,Dw=Vo.set,Iw=Vo.getterFor,Bw=function(t,e,i){var n,o=-1!==t.indexOf("Map"),r=-1!==t.indexOf("Weak"),s=o?"set":"add",a=bw[t],h=a&&a.prototype,l={};if(Pw&&Ow(a)&&(r||h.forEach&&!kw((function(){(new a).entries().next()})))){var d=(n=e((function(e,i){Dw(Ew(e,d),{type:t,collection:new a}),null!=i&&xw(i,e[s],{that:e,AS_ENTRIES:o})}))).prototype,c=Iw(t);Mw(["add","clear","delete","forEach","get","has","set","keys","values","entries"],(function(t){var e="add"==t||"set"==t;!(t in h)||r&&"clear"==t||_w(d,t,(function(i,n){var o=c(this).collection;if(!e&&r&&!Cw(i))return"get"==t&&void 0;var s=o[t](0===i?0:i,n);return e?this:s}))})),r||Tw(d,"size",{configurable:!0,get:function(){return c(this).collection.size}})}else n=i.getConstructor(e,t,o,s),ww.enable();return Sw(n,t,!1,!0),l[t]=n,mw({global:!0,forced:!0},l),r||i.setStrong(n,t,o),n},zw=Ir,Nw=function(t,e,i){for(var n in e)i&&i.unsafe&&t[n]?t[n]=e[n]:zw(t,n,e[n],i);return t},Fw=Q,Aw=Ve,jw=b,Rw=oe("species"),Lw=Ve.f,Hw=wr,Ww=Nw,qw=qe,Vw=yw,Uw=pw,Yw=Cs,Xw=function(t){var e=Fw(t),i=Aw.f;jw&&e&&!e[Rw]&&i(e,Rw,{configurable:!0,get:function(){return this}})},Gw=b,Kw=Db.exports.fastKey,$w=Vo.set,Zw=Vo.getterFor,Qw={getConstructor:function(t,e,i,n){var o=t((function(t,o){Vw(t,r),$w(t,{type:e,index:Hw(null),first:void 0,last:void 0,size:0}),Gw||(t.size=0),null!=o&&Uw(o,t[n],{that:t,AS_ENTRIES:i})})),r=o.prototype,s=Zw(e),a=function(t,e,i){var n,o,r=s(t),a=h(t,e);return a?a.value=i:(r.last=a={index:o=Kw(e,!0),key:e,value:i,previous:n=r.last,next:void 0,removed:!1},r.first||(r.first=a),n&&(n.next=a),Gw?r.size++:t.size++,"F"!==o&&(r.index[o]=a)),t},h=function(t,e){var i,n=s(t),o=Kw(e);if("F"!==o)return n.index[o];for(i=n.first;i;i=i.next)if(i.key==e)return i};return Ww(r,{clear:function(){for(var t=s(this),e=t.index,i=t.first;i;)i.removed=!0,i.previous&&(i.previous=i.previous.next=void 0),delete e[i.index],i=i.next;t.first=t.last=void 0,Gw?t.size=0:this.size=0},delete:function(t){var e=this,i=s(e),n=h(e,t);if(n){var o=n.next,r=n.previous;delete i.index[n.index],n.removed=!0,r&&(r.next=o),o&&(o.previous=r),i.first==n&&(i.first=o),i.last==n&&(i.last=r),Gw?i.size--:e.size--}return!!n},forEach:function(t){for(var e,i=s(this),n=qw(t,arguments.length>1?arguments[1]:void 0);e=e?e.next:i.first;)for(n(e.value,e.key,this);e&&e.removed;)e=e.previous},has:function(t){return!!h(this,t)}}),Ww(r,i?{get:function(t){var e=h(this,t);return e&&e.value},set:function(t,e){return a(this,0===t?0:t,e)}}:{add:function(t){return a(this,t=0===t?0:t,t)}}),Gw&&Lw(r,"size",{get:function(){return s(this).size}}),o},setStrong:function(t,e,i){var n=e+" Iterator",o=Zw(e),r=Zw(n);Yw(t,e,(function(t,e){$w(this,{type:n,target:t,state:o(t),kind:e,last:void 0})}),(function(){for(var t=r(this),e=t.kind,i=t.last;i&&i.removed;)i=i.previous;return t.target&&(t.last=i=i?i.next:t.state.first)?"keys"==e?{value:i.key,done:!1}:"values"==e?{value:i.value,done:!1}:{value:[i.key,i.value],done:!1}:(t.target=void 0,{value:void 0,done:!0})}),i?"entries":"values",!i,!0),Xw(e)}};Bw("Map",(function(t){return function(){return t(this,arguments.length?arguments[0]:void 0)}}),Qw);var Jw=X.Map,tk=function(){function t(){Yd(this,t),this.clear(),this._defaultIndex=0,this._groupIndex=0,this._defaultGroups=[{border:"#2B7CE9",background:"#97C2FC",highlight:{border:"#2B7CE9",background:"#D2E5FF"},hover:{border:"#2B7CE9",background:"#D2E5FF"}},{border:"#FFA500",background:"#FFFF00",highlight:{border:"#FFA500",background:"#FFFFA3"},hover:{border:"#FFA500",background:"#FFFFA3"}},{border:"#FA0A10",background:"#FB7E81",highlight:{border:"#FA0A10",background:"#FFAFB1"},hover:{border:"#FA0A10",background:"#FFAFB1"}},{border:"#41A906",background:"#7BE141",highlight:{border:"#41A906",background:"#A1EC76"},hover:{border:"#41A906",background:"#A1EC76"}},{border:"#E129F0",background:"#EB7DF4",highlight:{border:"#E129F0",background:"#F0B3F5"},hover:{border:"#E129F0",background:"#F0B3F5"}},{border:"#7C29F0",background:"#AD85E4",highlight:{border:"#7C29F0",background:"#D3BDF0"},hover:{border:"#7C29F0",background:"#D3BDF0"}},{border:"#C37F00",background:"#FFA807",highlight:{border:"#C37F00",background:"#FFCA66"},hover:{border:"#C37F00",background:"#FFCA66"}},{border:"#4220FB",background:"#6E6EFD",highlight:{border:"#4220FB",background:"#9B9BFD"},hover:{border:"#4220FB",background:"#9B9BFD"}},{border:"#FD5A77",background:"#FFC0CB",highlight:{border:"#FD5A77",background:"#FFD1D9"},hover:{border:"#FD5A77",background:"#FFD1D9"}},{border:"#4AD63A",background:"#C2FABC",highlight:{border:"#4AD63A",background:"#E6FFE3"},hover:{border:"#4AD63A",background:"#E6FFE3"}},{border:"#990000",background:"#EE0000",highlight:{border:"#BB0000",background:"#FF3333"},hover:{border:"#BB0000",background:"#FF3333"}},{border:"#FF6000",background:"#FF6000",highlight:{border:"#FF6000",background:"#FF6000"},hover:{border:"#FF6000",background:"#FF6000"}},{border:"#97C2FC",background:"#2B7CE9",highlight:{border:"#D2E5FF",background:"#2B7CE9"},hover:{border:"#D2E5FF",background:"#2B7CE9"}},{border:"#399605",background:"#255C03",highlight:{border:"#399605",background:"#255C03"},hover:{border:"#399605",background:"#255C03"}},{border:"#B70054",background:"#FF007E",highlight:{border:"#B70054",background:"#FF007E"},hover:{border:"#B70054",background:"#FF007E"}},{border:"#AD85E4",background:"#7C29F0",highlight:{border:"#D3BDF0",background:"#7C29F0"},hover:{border:"#D3BDF0",background:"#7C29F0"}},{border:"#4557FA",background:"#000EA1",highlight:{border:"#6E6EFD",background:"#000EA1"},hover:{border:"#6E6EFD",background:"#000EA1"}},{border:"#FFC0CB",background:"#FD5A77",highlight:{border:"#FFD1D9",background:"#FD5A77"},hover:{border:"#FFD1D9",background:"#FD5A77"}},{border:"#C2FABC",background:"#74D66A",highlight:{border:"#E6FFE3",background:"#74D66A"},hover:{border:"#E6FFE3",background:"#74D66A"}},{border:"#EE0000",background:"#990000",highlight:{border:"#FF3333",background:"#BB0000"},hover:{border:"#FF3333",background:"#BB0000"}}],this.options={},this.defaultOptions={useDefaultGroups:!0},un(this.options,this.defaultOptions)}return Kd(t,[{key:"setOptions",value:function(t){var e=["useDefaultGroups"];if(void 0!==t)for(var i in t)if(Object.prototype.hasOwnProperty.call(t,i)&&-1===Fp(e).call(e,i)){var n=t[i];this.add(i,n)}}},{key:"clear",value:function(){this._groups=new Jw,this._groupNames=[]}},{key:"get",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1],i=this._groups.get(t);if(void 0===i&&e)if(!1===this.options.useDefaultGroups&&this._groupNames.length>0){var n=this._groupIndex%this._groupNames.length;++this._groupIndex,(i={}).color=this._groups.get(this._groupNames[n]),this._groups.set(t,i)}else{var o=this._defaultIndex%this._defaultGroups.length;this._defaultIndex++,(i={}).color=this._defaultGroups[o],this._groups.set(t,i)}return i}},{key:"add",value:function(t,e){return this._groups.has(t)||this._groupNames.push(t),this._groups.set(t,e),e}}]),t}();_i({target:"Number",stat:!0},{isNaN:function(t){return t!=t}});var ek=X.Number.isNaN,ik=n.isFinite,nk=Number.isFinite||function(t){return"number"==typeof t&&ik(t)};_i({target:"Number",stat:!0},{isFinite:nk});var ok=X.Number.isFinite,rk=Wh.some;_i({target:"Array",proto:!0,forced:!Cu("some")},{some:function(t){return rk(this,t,arguments.length>1?arguments[1]:void 0)}});var sk=Tn("Array").some,ak=J,hk=sk,lk=Array.prototype,dk=function(t){var e=t.some;return t===lk||ak(lk,t)&&e===lk.some?hk:e},ck=dk,uk=na,fk=bt,pk=n.TypeError,vk=_i,gk=d,yk=On,mk=function(t){if(uk(t))return t;throw pk(fk(t)+" is not a constructor")},bk=$e,wk=Y,kk=wr,_k=o,xk=Q("Reflect","construct"),Ek=Object.prototype,Ok=[].push,Ck=_k((function(){function t(){}return!(xk((function(){}),[],t)instanceof t)})),Sk=!_k((function(){xk((function(){}))})),Tk=Ck||Sk;vk({target:"Reflect",stat:!0,forced:Tk,sham:Tk},{construct:function(t,e){mk(t),bk(e);var i=arguments.length<3?t:mk(arguments[2]);if(Sk&&!Ck)return xk(t,e,i);if(t==i){switch(e.length){case 0:return new t;case 1:return new t(e[0]);case 2:return new t(e[0],e[1]);case 3:return new t(e[0],e[1],e[2]);case 4:return new t(e[0],e[1],e[2],e[3])}var n=[null];return gk(Ok,n,e),new(gk(yk,t,n))}var o=i.prototype,r=kk(wk(o)?o:Ek),s=gk(t,r,e);return wk(s)?s:r}});var Mk=X.Reflect.construct;function Pk(t){if(void 0===t)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return t}var Dk=Gp;_i({target:"Object",stat:!0},{setPrototypeOf:cs});var Ik=X.Object.setPrototypeOf;function Bk(t,e){return Bk=Ik||function(t,e){return t.__proto__=e,t},Bk(t,e)}function zk(t,e){if("function"!=typeof e&&null!==e)throw new TypeError("Super expression must either be null or a function");t.prototype=Dk(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),Xd(t,"prototype",{writable:!1}),e&&Bk(t,e)}function Nk(t,e){if(e&&("object"===Qc(e)||"function"==typeof e))return e;if(void 0!==e)throw new TypeError("Derived constructors may only return object or undefined");return Pk(t)}var Fk=Rf;function Ak(t){return Ak=Ik?Fk:function(t){return t.__proto__||Fk(t)},Ak(t)}var jk={exports:{}};!function(t){var e=function(t){var e,i=Object.prototype,n=i.hasOwnProperty,o="function"==typeof Symbol?Symbol:{},r=o.iterator||"@@iterator",s=o.asyncIterator||"@@asyncIterator",a=o.toStringTag||"@@toStringTag";function h(t,e,i){return Object.defineProperty(t,e,{value:i,enumerable:!0,configurable:!0,writable:!0}),t[e]}try{h({},"")}catch(t){h=function(t,e,i){return t[e]=i}}function l(t,e,i,n){var o=e&&e.prototype instanceof g?e:g,r=Object.create(o.prototype),s=new T(n||[]);return r._invoke=function(t,e,i){var n=c;return function(o,r){if(n===f)throw new Error("Generator is already running");if(n===p){if("throw"===o)throw r;return P()}for(i.method=o,i.arg=r;;){var s=i.delegate;if(s){var a=O(s,i);if(a){if(a===v)continue;return a}}if("next"===i.method)i.sent=i._sent=i.arg;else if("throw"===i.method){if(n===c)throw n=p,i.arg;i.dispatchException(i.arg)}else"return"===i.method&&i.abrupt("return",i.arg);n=f;var h=d(t,e,i);if("normal"===h.type){if(n=i.done?p:u,h.arg===v)continue;return{value:h.arg,done:i.done}}"throw"===h.type&&(n=p,i.method="throw",i.arg=h.arg)}}}(t,i,s),r}function d(t,e,i){try{return{type:"normal",arg:t.call(e,i)}}catch(t){return{type:"throw",arg:t}}}t.wrap=l;var c="suspendedStart",u="suspendedYield",f="executing",p="completed",v={};function g(){}function y(){}function m(){}var b={};h(b,r,(function(){return this}));var w=Object.getPrototypeOf,k=w&&w(w(M([])));k&&k!==i&&n.call(k,r)&&(b=k);var _=m.prototype=g.prototype=Object.create(b);function x(t){["next","throw","return"].forEach((function(e){h(t,e,(function(t){return this._invoke(e,t)}))}))}function E(t,e){function i(o,r,s,a){var h=d(t[o],t,r);if("throw"!==h.type){var l=h.arg,c=l.value;return c&&"object"==typeof c&&n.call(c,"__await")?e.resolve(c.__await).then((function(t){i("next",t,s,a)}),(function(t){i("throw",t,s,a)})):e.resolve(c).then((function(t){l.value=t,s(l)}),(function(t){return i("throw",t,s,a)}))}a(h.arg)}var o;this._invoke=function(t,n){function r(){return new e((function(e,o){i(t,n,e,o)}))}return o=o?o.then(r,r):r()}}function O(t,i){var n=t.iterator[i.method];if(n===e){if(i.delegate=null,"throw"===i.method){if(t.iterator.return&&(i.method="return",i.arg=e,O(t,i),"throw"===i.method))return v;i.method="throw",i.arg=new TypeError("The iterator does not provide a 'throw' method")}return v}var o=d(n,t.iterator,i.arg);if("throw"===o.type)return i.method="throw",i.arg=o.arg,i.delegate=null,v;var r=o.arg;return r?r.done?(i[t.resultName]=r.value,i.next=t.nextLoc,"return"!==i.method&&(i.method="next",i.arg=e),i.delegate=null,v):r:(i.method="throw",i.arg=new TypeError("iterator result is not an object"),i.delegate=null,v)}function C(t){var e={tryLoc:t[0]};1 in t&&(e.catchLoc=t[1]),2 in t&&(e.finallyLoc=t[2],e.afterLoc=t[3]),this.tryEntries.push(e)}function S(t){var e=t.completion||{};e.type="normal",delete e.arg,t.completion=e}function T(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(C,this),this.reset(!0)}function M(t){if(t){var i=t[r];if(i)return i.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var o=-1,s=function i(){for(;++o<t.length;)if(n.call(t,o))return i.value=t[o],i.done=!1,i;return i.value=e,i.done=!0,i};return s.next=s}}return{next:P}}function P(){return{value:e,done:!0}}return y.prototype=m,h(_,"constructor",m),h(m,"constructor",y),y.displayName=h(m,a,"GeneratorFunction"),t.isGeneratorFunction=function(t){var e="function"==typeof t&&t.constructor;return!!e&&(e===y||"GeneratorFunction"===(e.displayName||e.name))},t.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,m):(t.__proto__=m,h(t,a,"GeneratorFunction")),t.prototype=Object.create(_),t},t.awrap=function(t){return{__await:t}},x(E.prototype),h(E.prototype,s,(function(){return this})),t.AsyncIterator=E,t.async=function(e,i,n,o,r){void 0===r&&(r=Promise);var s=new E(l(e,i,n,o),r);return t.isGeneratorFunction(i)?s:s.next().then((function(t){return t.done?t.value:s.next()}))},x(_),h(_,a,"Generator"),h(_,r,(function(){return this})),h(_,"toString",(function(){return"[object Generator]"})),t.keys=function(t){var e=[];for(var i in t)e.push(i);return e.reverse(),function i(){for(;e.length;){var n=e.pop();if(n in t)return i.value=n,i.done=!1,i}return i.done=!0,i}},t.values=M,T.prototype={constructor:T,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=e,this.done=!1,this.delegate=null,this.method="next",this.arg=e,this.tryEntries.forEach(S),!t)for(var i in this)"t"===i.charAt(0)&&n.call(this,i)&&!isNaN(+i.slice(1))&&(this[i]=e)},stop:function(){this.done=!0;var t=this.tryEntries[0].completion;if("throw"===t.type)throw t.arg;return this.rval},dispatchException:function(t){if(this.done)throw t;var i=this;function o(n,o){return a.type="throw",a.arg=t,i.next=n,o&&(i.method="next",i.arg=e),!!o}for(var r=this.tryEntries.length-1;r>=0;--r){var s=this.tryEntries[r],a=s.completion;if("root"===s.tryLoc)return o("end");if(s.tryLoc<=this.prev){var h=n.call(s,"catchLoc"),l=n.call(s,"finallyLoc");if(h&&l){if(this.prev<s.catchLoc)return o(s.catchLoc,!0);if(this.prev<s.finallyLoc)return o(s.finallyLoc)}else if(h){if(this.prev<s.catchLoc)return o(s.catchLoc,!0)}else{if(!l)throw new Error("try statement without catch or finally");if(this.prev<s.finallyLoc)return o(s.finallyLoc)}}}},abrupt:function(t,e){for(var i=this.tryEntries.length-1;i>=0;--i){var o=this.tryEntries[i];if(o.tryLoc<=this.prev&&n.call(o,"finallyLoc")&&this.prev<o.finallyLoc){var r=o;break}}r&&("break"===t||"continue"===t)&&r.tryLoc<=e&&e<=r.finallyLoc&&(r=null);var s=r?r.completion:{};return s.type=t,s.arg=e,r?(this.method="next",this.next=r.finallyLoc,v):this.complete(s)},complete:function(t,e){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&e&&(this.next=e),v},finish:function(t){for(var e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e];if(i.finallyLoc===t)return this.complete(i.completion,i.afterLoc),S(i),v}},catch:function(t){for(var e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e];if(i.tryLoc===t){var n=i.completion;if("throw"===n.type){var o=n.arg;S(i)}return o}}throw new Error("illegal catch attempt")},delegateYield:function(t,i,n){return this.delegate={iterator:M(t),resultName:i,nextLoc:n},"next"===this.method&&(this.arg=e),v}},t}(t.exports);try{regeneratorRuntime=e}catch(t){"object"==typeof globalThis?globalThis.regeneratorRuntime=e:Function("r","regeneratorRuntime = r")(e)}}(jk);var Rk=jk.exports,Lk=xt,Hk=Rt,Wk=R,qk=Bi,Vk=n.TypeError,Uk=function(t){return function(e,i,n,o){Lk(i);var r=Hk(e),s=Wk(r),a=qk(r),h=t?a-1:0,l=t?-1:1;if(n<2)for(;;){if(h in s){o=s[h],h+=l;break}if(h+=l,t?h<0:a<=h)throw Vk("Reduce of empty array with no initial value")}for(;t?h>=0:a>h;h+=l)h in s&&(o=i(o,s[h],h,r));return o}},Yk={left:Uk(!1),right:Uk(!0)},Xk="process"==B(n.process),Gk=Yk.left,Kk=at,$k=Xk;_i({target:"Array",proto:!0,forced:!Cu("reduce")||!$k&&Kk>79&&Kk<83},{reduce:function(t){var e=arguments.length;return Gk(this,t,e,e>1?arguments[1]:void 0)}});var Zk=Tn("Array").reduce,Qk=J,Jk=Zk,t_=Array.prototype,e_=function(t){var e=t.reduce;return t===t_||Qk(t_,t)&&e===t_.reduce?Jk:e},i_=e_,n_=oh,o_=Bi,r_=qe,s_=n.TypeError,a_=function(t,e,i,n,o,r,s,a){for(var h,l,d=o,c=0,u=!!s&&r_(s,a);c<n;){if(c in i){if(h=u?u(i[c],c,e):i[c],r>0&&n_(h))l=o_(h),d=a_(t,e,h,l,d,r-1)-1;else{if(d>=9007199254740991)throw s_("Exceed the acceptable array length");t[d]=h}d++}c++}return d},h_=a_,l_=xt,d_=Rt,c_=Bi,u_=zh;_i({target:"Array",proto:!0},{flatMap:function(t){var e,i=d_(this),n=c_(i);return l_(t),(e=u_(i,0)).length=h_(e,i,i,n,0,1,t,arguments.length>1?arguments[1]:void 0),e}});var f_=Tn("Array").flatMap,p_=J,v_=f_,g_=Array.prototype,y_=function(t){var e=t.flatMap;return t===g_||p_(g_,t)&&e===g_.flatMap?v_:e},m_=y_;Bw("Set",(function(t){return function(){return t(this,arguments.length?arguments[0]:void 0)}}),Qw);var b_=X.Set,w_=$c,k_=ba,__=ph,x_=Math.floor,E_=function(t,e){var i=t.length,n=x_(i/2);return i<8?O_(t,e):C_(t,E_(__(t,0,n),e),E_(__(t,n),e),e)},O_=function(t,e){for(var i,n,o=t.length,r=1;r<o;){for(n=r,i=t[r];n&&e(t[n-1],i)>0;)t[n]=t[--n];n!==r++&&(t[n]=i)}return t},C_=function(t,e,i,n){for(var o=e.length,r=i.length,s=0,a=0;s<o||a<r;)t[s+a]=s<o&&a<r?n(e[s],i[a])<=0?e[s++]:i[a++]:s<o?e[s++]:i[a++];return t},S_=E_,T_=tt.match(/firefox\/(\d+)/i),M_=!!T_&&+T_[1],P_=/MSIE|Trident/.test(tt),D_=tt.match(/AppleWebKit\/(\d+)\./),I_=!!D_&&+D_[1],B_=_i,z_=g,N_=xt,F_=Rt,A_=Bi,j_=eo,R_=o,L_=S_,H_=Cu,W_=M_,q_=P_,V_=at,U_=I_,Y_=[],X_=z_(Y_.sort),G_=z_(Y_.push),K_=R_((function(){Y_.sort(void 0)})),$_=R_((function(){Y_.sort(null)})),Z_=H_("sort"),Q_=!R_((function(){if(V_)return V_<70;if(!(W_&&W_>3)){if(q_)return!0;if(U_)return U_<603;var t,e,i,n,o="";for(t=65;t<76;t++){switch(e=String.fromCharCode(t),t){case 66:case 69:case 70:case 72:i=3;break;case 68:case 71:i=4;break;default:i=2}for(n=0;n<47;n++)Y_.push({k:e+n,v:i})}for(Y_.sort((function(t,e){return e.v-t.v})),n=0;n<Y_.length;n++)e=Y_[n].k.charAt(0),o.charAt(o.length-1)!==e&&(o+=e);return"DGBEFHACIJK"!==o}}));B_({target:"Array",proto:!0,forced:K_||!$_||!Z_||!Q_},{sort:function(t){void 0!==t&&N_(t);var e=F_(this);if(Q_)return void 0===t?X_(e):X_(e,t);var i,n,o=[],r=A_(e);for(n=0;n<r;n++)n in e&&G_(o,e[n]);for(L_(o,function(t){return function(e,i){return void 0===i?-1:void 0===e?1:void 0!==t?+t(e,i)||0:j_(e)>j_(i)?1:-1}}(t)),i=o.length,n=0;n<i;)e[n]=o[n++];for(;n<r;)delete e[n++];return e}});var J_,tx=Tn("Array").sort,ex=J,ix=tx,nx=Array.prototype,ox=function(t){var e=t.sort;return t===nx||ex(nx,t)&&e===nx.sort?ix:e},rx=ox,sx=Tn("Array").keys,ax=Qn,hx=Wt,lx=J,dx=sx,cx=Array.prototype,ux={DOMTokenList:!0,NodeList:!0},fx=function(t){var e=t.keys;return t===cx||lx(cx,t)&&e===cx.keys||hx(ux,ax(t))?dx:e},px=Tn("Array").values,vx=Qn,gx=Wt,yx=J,mx=px,bx=Array.prototype,wx={DOMTokenList:!0,NodeList:!0},kx=function(t){var e=t.values;return t===bx||yx(bx,t)&&e===bx.values||gx(wx,vx(t))?mx:e},_x=Tn("Array").entries,xx=Qn,Ex=Wt,Ox=J,Cx=_x,Sx=Array.prototype,Tx={DOMTokenList:!0,NodeList:!0},Mx=function(t){var e=t.entries;return t===Sx||Ox(Sx,t)&&e===Sx.entries||Ex(Tx,xx(t))?Cx:e},Px=new Uint8Array(16);function Dx(){if(!J_&&!(J_="undefined"!=typeof crypto&&crypto.getRandomValues&&crypto.getRandomValues.bind(crypto)||"undefined"!=typeof msCrypto&&"function"==typeof msCrypto.getRandomValues&&msCrypto.getRandomValues.bind(msCrypto)))throw new Error("crypto.getRandomValues() not supported. See https://github.com/uuidjs/uuid#getrandomvalues-not-supported");return J_(Px)}var Ix=/^(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)$/i;function Bx(t){return"string"==typeof t&&Ix.test(t)}for(var zx,Nx=[],Fx=0;Fx<256;++Fx)Nx.push((Fx+256).toString(16).substr(1));function Ax(t,e,i){var n=(t=t||{}).random||(t.rng||Dx)();if(n[6]=15&n[6]|64,n[8]=63&n[8]|128,e){i=i||0;for(var o=0;o<16;++o)e[i+o]=n[o];return e}return function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:0,i=(Nx[t[e+0]]+Nx[t[e+1]]+Nx[t[e+2]]+Nx[t[e+3]]+"-"+Nx[t[e+4]]+Nx[t[e+5]]+"-"+Nx[t[e+6]]+Nx[t[e+7]]+"-"+Nx[t[e+8]]+Nx[t[e+9]]+"-"+Nx[t[e+10]]+Nx[t[e+11]]+Nx[t[e+12]]+Nx[t[e+13]]+Nx[t[e+14]]+Nx[t[e+15]]).toLowerCase();if(!Bx(i))throw TypeError("Stringified UUID is invalid");return i}(n)}function jx(t,e){var i=bu(t);if(hd){var n=hd(t);e&&(n=Xf(n).call(n,(function(e){return bd(t,e).enumerable}))),i.push.apply(i,n)}return i}function Rx(t){for(var e=1;e<arguments.length;e++){var i,n,o=null!=arguments[e]?arguments[e]:{};e%2?Fu(i=jx(Object(o),!0)).call(i,(function(e){$d(t,e,o[e])})):Pd?Ad(t,Pd(o)):Fu(n=jx(Object(o))).call(n,(function(e){Ud(t,e,bd(o,e))}))}return t}function Lx(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}function Hx(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return Wx(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return Wx(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function Wx(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}var qx=function(){function t(e,i,n){var o,r,s;Yd(this,t),$d(this,"_source",void 0),$d(this,"_transformers",void 0),$d(this,"_target",void 0),$d(this,"_listeners",{add:zn(o=this._add).call(o,this),remove:zn(r=this._remove).call(r,this),update:zn(s=this._update).call(s,this)}),this._source=e,this._transformers=i,this._target=n}return Kd(t,[{key:"all",value:function(){return this._target.update(this._transformItems(this._source.get())),this}},{key:"start",value:function(){return this._source.on("add",this._listeners.add),this._source.on("remove",this._listeners.remove),this._source.on("update",this._listeners.update),this}},{key:"stop",value:function(){return this._source.off("add",this._listeners.add),this._source.off("remove",this._listeners.remove),this._source.off("update",this._listeners.update),this}},{key:"_transformItems",value:function(t){var e;return i_(e=this._transformers).call(e,(function(t,e){return e(t)}),t)}},{key:"_add",value:function(t,e){null!=e&&this._target.add(this._transformItems(this._source.get(e.items)))}},{key:"_update",value:function(t,e){null!=e&&this._target.update(this._transformItems(this._source.get(e.items)))}},{key:"_remove",value:function(t,e){null!=e&&this._target.remove(this._transformItems(e.oldData))}}]),t}(),Vx=function(){function t(e){Yd(this,t),$d(this,"_source",void 0),$d(this,"_transformers",[]),this._source=e}return Kd(t,[{key:"filter",value:function(t){return this._transformers.push((function(e){return Xf(e).call(e,t)})),this}},{key:"map",value:function(t){return this._transformers.push((function(e){return gu(e).call(e,t)})),this}},{key:"flatMap",value:function(t){return this._transformers.push((function(e){return m_(e).call(e,t)})),this}},{key:"to",value:function(t){return new qx(this._source,this._transformers,t)}}]),t}();function Ux(t){return"string"==typeof t||"number"==typeof t}var Yx=function(){function t(e){Yd(this,t),$d(this,"delay",void 0),$d(this,"max",void 0),$d(this,"_queue",[]),$d(this,"_timeout",null),$d(this,"_extended",null),this.delay=null,this.max=1/0,this.setOptions(e)}return Kd(t,[{key:"setOptions",value:function(t){t&&void 0!==t.delay&&(this.delay=t.delay),t&&void 0!==t.max&&(this.max=t.max),this._flushIfNeeded()}},{key:"destroy",value:function(){if(this.flush(),this._extended){for(var t=this._extended.object,e=this._extended.methods,i=0;i<e.length;i++){var n=e[i];n.original?t[n.name]=n.original:delete t[n.name]}this._extended=null}}},{key:"replace",value:function(t,e){var i=this,n=t[e];if(!n)throw new Error("Method "+e+" undefined");t[e]=function(){for(var t=arguments.length,e=new Array(t),o=0;o<t;o++)e[o]=arguments[o];i.queue({args:e,fn:n,context:this})}}},{key:"queue",value:function(t){"function"==typeof t?this._queue.push({fn:t}):this._queue.push(t),this._flushIfNeeded()}},{key:"_flushIfNeeded",value:function(){var t=this;this._queue.length>this.max&&this.flush(),null!=this._timeout&&(clearTimeout(this._timeout),this._timeout=null),this.queue.length>0&&"number"==typeof this.delay&&(this._timeout=Sv((function(){t.flush()}),this.delay))}},{key:"flush",value:function(){var t,e;Fu(t=ff(e=this._queue).call(e,0)).call(t,(function(t){t.fn.apply(t.context||t.fn,t.args||[])}))}}],[{key:"extend",value:function(e,i){var n=new t(i);if(void 0!==e.flush)throw new Error("Target object already has a property flush");e.flush=function(){n.flush()};var o=[{name:"flush",original:void 0}];if(i&&i.replace)for(var r=0;r<i.replace.length;r++){var s=i.replace[r];o.push({name:s,original:e[s]}),n.replace(e,s)}return n._extended={object:e,methods:o},n}}]),t}(),Xx=function(){function t(){Yd(this,t),$d(this,"_subscribers",{"*":[],add:[],remove:[],update:[]}),$d(this,"subscribe",t.prototype.on),$d(this,"unsubscribe",t.prototype.off)}return Kd(t,[{key:"_trigger",value:function(t,e,i){var n,o;if("*"===t)throw new Error("Cannot trigger event *");Fu(n=su(o=[]).call(o,Jc(this._subscribers[t]),Jc(this._subscribers["*"]))).call(n,(function(n){n(t,e,null!=i?i:null)}))}},{key:"on",value:function(t,e){"function"==typeof e&&this._subscribers[t].push(e)}},{key:"off",value:function(t,e){var i;this._subscribers[t]=Xf(i=this._subscribers[t]).call(i,(function(t){return t!==e}))}}]),t}();zx=w_;var Gx=function(){function t(e){Yd(this,t),$d(this,"_pairs",void 0),this._pairs=e}return Kd(t,[{key:zx,value:Rk.mark((function t(){var e,i,n,o,r;return Rk.wrap((function(t){for(;;)switch(t.prev=t.next){case 0:e=Hx(this._pairs),t.prev=1,e.s();case 3:if((i=e.n()).done){t.next=9;break}return n=Kc(i.value,2),o=n[0],r=n[1],t.next=7,[o,r];case 7:t.next=3;break;case 9:t.next=14;break;case 11:t.prev=11,t.t0=t.catch(1),e.e(t.t0);case 14:return t.prev=14,e.f(),t.finish(14);case 17:case"end":return t.stop()}}),t,this,[[1,11,14,17]])}))},{key:"entries",value:Rk.mark((function t(){var e,i,n,o,r;return Rk.wrap((function(t){for(;;)switch(t.prev=t.next){case 0:e=Hx(this._pairs),t.prev=1,e.s();case 3:if((i=e.n()).done){t.next=9;break}return n=Kc(i.value,2),o=n[0],r=n[1],t.next=7,[o,r];case 7:t.next=3;break;case 9:t.next=14;break;case 11:t.prev=11,t.t0=t.catch(1),e.e(t.t0);case 14:return t.prev=14,e.f(),t.finish(14);case 17:case"end":return t.stop()}}),t,this,[[1,11,14,17]])}))},{key:"keys",value:Rk.mark((function t(){var e,i,n,o;return Rk.wrap((function(t){for(;;)switch(t.prev=t.next){case 0:e=Hx(this._pairs),t.prev=1,e.s();case 3:if((i=e.n()).done){t.next=9;break}return n=Kc(i.value,1),o=n[0],t.next=7,o;case 7:t.next=3;break;case 9:t.next=14;break;case 11:t.prev=11,t.t0=t.catch(1),e.e(t.t0);case 14:return t.prev=14,e.f(),t.finish(14);case 17:case"end":return t.stop()}}),t,this,[[1,11,14,17]])}))},{key:"values",value:Rk.mark((function t(){var e,i,n,o;return Rk.wrap((function(t){for(;;)switch(t.prev=t.next){case 0:e=Hx(this._pairs),t.prev=1,e.s();case 3:if((i=e.n()).done){t.next=9;break}return n=Kc(i.value,2),o=n[1],t.next=7,o;case 7:t.next=3;break;case 9:t.next=14;break;case 11:t.prev=11,t.t0=t.catch(1),e.e(t.t0);case 14:return t.prev=14,e.f(),t.finish(14);case 17:case"end":return t.stop()}}),t,this,[[1,11,14,17]])}))},{key:"toIdArray",value:function(){var t;return gu(t=Jc(this._pairs)).call(t,(function(t){return t[0]}))}},{key:"toItemArray",value:function(){var t;return gu(t=Jc(this._pairs)).call(t,(function(t){return t[1]}))}},{key:"toEntryArray",value:function(){return Jc(this._pairs)}},{key:"toObjectMap",value:function(){var t,e=Kp(null),i=Hx(this._pairs);try{for(i.s();!(t=i.n()).done;){var n=Kc(t.value,2),o=n[0],r=n[1];e[o]=r}}catch(t){i.e(t)}finally{i.f()}return e}},{key:"toMap",value:function(){return new Jw(this._pairs)}},{key:"toIdSet",value:function(){return new b_(this.toIdArray())}},{key:"toItemSet",value:function(){return new b_(this.toItemArray())}},{key:"cache",value:function(){return new t(Jc(this._pairs))}},{key:"distinct",value:function(t){var e,i=new b_,n=Hx(this._pairs);try{for(n.s();!(e=n.n()).done;){var o=Kc(e.value,2),r=o[0],s=o[1];i.add(t(s,r))}}catch(t){n.e(t)}finally{n.f()}return i}},{key:"filter",value:function(e){var i=this._pairs;return new t($d({},w_,Rk.mark((function t(){var n,o,r,s,a;return Rk.wrap((function(t){for(;;)switch(t.prev=t.next){case 0:n=Hx(i),t.prev=1,n.s();case 3:if((o=n.n()).done){t.next=10;break}if(r=Kc(o.value,2),s=r[0],a=r[1],!e(a,s)){t.next=8;break}return t.next=8,[s,a];case 8:t.next=3;break;case 10:t.next=15;break;case 12:t.prev=12,t.t0=t.catch(1),n.e(t.t0);case 15:return t.prev=15,n.f(),t.finish(15);case 18:case"end":return t.stop()}}),t,null,[[1,12,15,18]])}))))}},{key:"forEach",value:function(t){var e,i=Hx(this._pairs);try{for(i.s();!(e=i.n()).done;){var n=Kc(e.value,2),o=n[0];t(n[1],o)}}catch(t){i.e(t)}finally{i.f()}}},{key:"map",value:function(e){var i=this._pairs;return new t($d({},w_,Rk.mark((function t(){var n,o,r,s,a;return Rk.wrap((function(t){for(;;)switch(t.prev=t.next){case 0:n=Hx(i),t.prev=1,n.s();case 3:if((o=n.n()).done){t.next=9;break}return r=Kc(o.value,2),s=r[0],a=r[1],t.next=7,[s,e(a,s)];case 7:t.next=3;break;case 9:t.next=14;break;case 11:t.prev=11,t.t0=t.catch(1),n.e(t.t0);case 14:return t.prev=14,n.f(),t.finish(14);case 17:case"end":return t.stop()}}),t,null,[[1,11,14,17]])}))))}},{key:"max",value:function(t){var e=k_(this._pairs),i=e.next();if(i.done)return null;for(var n=i.value[1],o=t(i.value[1],i.value[0]);!(i=e.next()).done;){var r=Kc(i.value,2),s=r[0],a=r[1],h=t(a,s);h>o&&(o=h,n=a)}return n}},{key:"min",value:function(t){var e=k_(this._pairs),i=e.next();if(i.done)return null;for(var n=i.value[1],o=t(i.value[1],i.value[0]);!(i=e.next()).done;){var r=Kc(i.value,2),s=r[0],a=r[1],h=t(a,s);h<o&&(o=h,n=a)}return n}},{key:"reduce",value:function(t,e){var i,n=Hx(this._pairs);try{for(n.s();!(i=n.n()).done;){var o=Kc(i.value,2),r=o[0];e=t(e,o[1],r)}}catch(t){n.e(t)}finally{n.f()}return e}},{key:"sort",value:function(e){var i=this;return new t($d({},w_,(function(){var t;return k_(rx(t=Jc(i._pairs)).call(t,(function(t,i){var n=Kc(t,2),o=n[0],r=n[1],s=Kc(i,2),a=s[0],h=s[1];return e(r,h,o,a)})))})))}}]),t}();var Kx=function(t){zk(i,t);var e=Lx(i);function i(t,n){var o;return Yd(this,i),$d(Pk(o=e.call(this)),"flush",void 0),$d(Pk(o),"length",void 0),$d(Pk(o),"_options",void 0),$d(Pk(o),"_data",void 0),$d(Pk(o),"_idProp",void 0),$d(Pk(o),"_queue",null),t&&!lu(t)&&(n=t,t=[]),o._options=n||{},o._data=new Jw,o.length=0,o._idProp=o._options.fieldId||"id",t&&t.length&&o.add(t),o.setOptions(n),o}return Kd(i,[{key:"idProp",get:function(){return this._idProp}},{key:"setOptions",value:function(t){t&&void 0!==t.queue&&(!1===t.queue?this._queue&&(this._queue.destroy(),this._queue=null):(this._queue||(this._queue=Yx.extend(this,{replace:["add","update","remove"]})),t.queue&&"object"===Qc(t.queue)&&this._queue.setOptions(t.queue)))}},{key:"add",value:function(t,e){var i,n=this,o=[];if(lu(t)){var r=gu(t).call(t,(function(t){return t[n._idProp]}));if(ck(r).call(r,(function(t){return n._data.has(t)})))throw new Error("A duplicate id was found in the parameter array.");for(var s=0,a=t.length;s<a;s++)i=this._addItem(t[s]),o.push(i)}else{if(!t||"object"!==Qc(t))throw new Error("Unknown dataType");i=this._addItem(t),o.push(i)}return o.length&&this._trigger("add",{items:o},e),o}},{key:"update",value:function(t,e){var i=this,n=[],o=[],r=[],s=[],a=this._idProp,h=function(t){var e=t[a];if(null!=e&&i._data.has(e)){var h=t,l=un({},i._data.get(e)),d=i._updateItem(h);o.push(d),s.push(h),r.push(l)}else{var c=i._addItem(t);n.push(c)}};if(lu(t))for(var l=0,d=t.length;l<d;l++)t[l]&&"object"===Qc(t[l])?h(t[l]):console.warn("Ignoring input item, which is not an object at index "+l);else{if(!t||"object"!==Qc(t))throw new Error("Unknown dataType");h(t)}if(n.length&&this._trigger("add",{items:n},e),o.length){var c={items:o,oldData:r,data:s};this._trigger("update",c,e)}return su(n).call(n,o)}},{key:"updateOnly",value:function(t,e){var i,n=this;lu(t)||(t=[t]);var o=gu(i=gu(t).call(t,(function(t){var e=n._data.get(t[n._idProp]);if(null==e)throw new Error("Updating non-existent items is not allowed.");return{oldData:e,update:t}}))).call(i,(function(t){var e=t.oldData,i=t.update,o=e[n._idProp],r=By(e,i);return n._data.set(o,r),{id:o,oldData:e,updatedData:r}}));if(o.length){var r={items:gu(o).call(o,(function(t){return t.id})),oldData:gu(o).call(o,(function(t){return t.oldData})),data:gu(o).call(o,(function(t){return t.updatedData}))};return this._trigger("update",r,e),r.items}return[]}},{key:"get",value:function(t,e){var i=void 0,n=void 0,o=void 0;Ux(t)?(i=t,o=e):lu(t)?(n=t,o=e):o=t;var r,s=o&&"Object"===o.returnType?"Object":"Array",a=o&&Xf(o),h=[],l=void 0,d=void 0,c=void 0;if(null!=i)(l=this._data.get(i))&&a&&!a(l)&&(l=void 0);else if(null!=n)for(var u=0,f=n.length;u<f;u++)null==(l=this._data.get(n[u]))||a&&!a(l)||h.push(l);else for(var p,v=0,g=(d=Jc(fx(p=this._data).call(p))).length;v<g;v++)c=d[v],null==(l=this._data.get(c))||a&&!a(l)||h.push(l);if(o&&o.order&&null==i&&this._sort(h,o.order),o&&o.fields){var y=o.fields;if(null!=i&&null!=l)l=this._filterFields(l,y);else for(var m=0,b=h.length;m<b;m++)h[m]=this._filterFields(h[m],y)}if("Object"==s){for(var w={},k=0,_=h.length;k<_;k++){var x=h[k];w[x[this._idProp]]=x}return w}return null!=i?null!==(r=l)&&void 0!==r?r:null:h}},{key:"getIds",value:function(t){var e=this._data,i=t&&Xf(t),n=t&&t.order,o=Jc(fx(e).call(e)),r=[];if(i)if(n){for(var s=[],a=0,h=o.length;a<h;a++){var l=o[a],d=this._data.get(l);null!=d&&i(d)&&s.push(d)}this._sort(s,n);for(var c=0,u=s.length;c<u;c++)r.push(s[c][this._idProp])}else for(var f=0,p=o.length;f<p;f++){var v=o[f],g=this._data.get(v);null!=g&&i(g)&&r.push(g[this._idProp])}else if(n){for(var y=[],m=0,b=o.length;m<b;m++){var w=o[m];y.push(e.get(w))}this._sort(y,n);for(var k=0,_=y.length;k<_;k++)r.push(y[k][this._idProp])}else for(var x=0,E=o.length;x<E;x++){var O=o[x],C=e.get(O);null!=C&&r.push(C[this._idProp])}return r}},{key:"getDataSet",value:function(){return this}},{key:"forEach",value:function(t,e){var i=e&&Xf(e),n=this._data,o=Jc(fx(n).call(n));if(e&&e.order)for(var r=this.get(e),s=0,a=r.length;s<a;s++){var h=r[s];t(h,h[this._idProp])}else for(var l=0,d=o.length;l<d;l++){var c=o[l],u=this._data.get(c);null==u||i&&!i(u)||t(u,c)}}},{key:"map",value:function(t,e){for(var i=e&&Xf(e),n=[],o=this._data,r=Jc(fx(o).call(o)),s=0,a=r.length;s<a;s++){var h=r[s],l=this._data.get(h);null==l||i&&!i(l)||n.push(t(l,h))}return e&&e.order&&this._sort(n,e.order),n}},{key:"_filterFields",value:function(t,e){var i;return t?i_(i=lu(e)?e:bu(e)).call(i,(function(e,i){return e[i]=t[i],e}),{}):t}},{key:"_sort",value:function(t,e){if("string"==typeof e){var i=e;rx(t).call(t,(function(t,e){var n=t[i],o=e[i];return n>o?1:n<o?-1:0}))}else{if("function"!=typeof e)throw new TypeError("Order must be a function or a string");rx(t).call(t,e)}}},{key:"remove",value:function(t,e){for(var i=[],n=[],o=lu(t)?t:[t],r=0,s=o.length;r<s;r++){var a=this._remove(o[r]);if(a){var h=a[this._idProp];null!=h&&(i.push(h),n.push(a))}}return i.length&&this._trigger("remove",{items:i,oldData:n},e),i}},{key:"_remove",value:function(t){var e;if(Ux(t)?e=t:t&&"object"===Qc(t)&&(e=t[this._idProp]),null!=e&&this._data.has(e)){var i=this._data.get(e)||null;return this._data.delete(e),--this.length,i}return null}},{key:"clear",value:function(t){for(var e,i=Jc(fx(e=this._data).call(e)),n=[],o=0,r=i.length;o<r;o++)n.push(this._data.get(i[o]));return this._data.clear(),this.length=0,this._trigger("remove",{items:i,oldData:n},t),i}},{key:"max",value:function(t){var e,i,n=null,o=null,r=Hx(kx(e=this._data).call(e));try{for(r.s();!(i=r.n()).done;){var s=i.value,a=s[t];"number"==typeof a&&(null==o||a>o)&&(n=s,o=a)}}catch(t){r.e(t)}finally{r.f()}return n||null}},{key:"min",value:function(t){var e,i,n=null,o=null,r=Hx(kx(e=this._data).call(e));try{for(r.s();!(i=r.n()).done;){var s=i.value,a=s[t];"number"==typeof a&&(null==o||a<o)&&(n=s,o=a)}}catch(t){r.e(t)}finally{r.f()}return n||null}},{key:"distinct",value:function(t){for(var e=this._data,i=Jc(fx(e).call(e)),n=[],o=0,r=0,s=i.length;r<s;r++){for(var a=i[r],h=e.get(a)[t],l=!1,d=0;d<o;d++)if(n[d]==h){l=!0;break}l||void 0===h||(n[o]=h,o++)}return n}},{key:"_addItem",value:function(t){var e=function(t,e){return null==t[e]&&(t[e]=Ax()),t}(t,this._idProp),i=e[this._idProp];if(this._data.has(i))throw new Error("Cannot add item: item with id "+i+" already exists");return this._data.set(i,e),++this.length,i}},{key:"_updateItem",value:function(t){var e=t[this._idProp];if(null==e)throw new Error("Cannot update item: item has no id (item: "+gv(t)+")");var i=this._data.get(e);if(!i)throw new Error("Cannot update item: no item with id "+e+" found");return this._data.set(e,Rx(Rx({},i),t)),e}},{key:"stream",value:function(t){if(t){var e=this._data;return new Gx($d({},w_,Rk.mark((function i(){var n,o,r,s;return Rk.wrap((function(i){for(;;)switch(i.prev=i.next){case 0:n=Hx(t),i.prev=1,n.s();case 3:if((o=n.n()).done){i.next=11;break}if(r=o.value,null==(s=e.get(r))){i.next=9;break}return i.next=9,[r,s];case 9:i.next=3;break;case 11:i.next=16;break;case 13:i.prev=13,i.t0=i.catch(1),n.e(i.t0);case 16:return i.prev=16,n.f(),i.finish(16);case 19:case"end":return i.stop()}}),i,null,[[1,13,16,19]])}))))}var i;return new Gx($d({},w_,zn(i=Mx(this._data)).call(i,this._data)))}}]),i}(Xx),$x=function(t){zk(i,t);var e=Lx(i);function i(t,n){var o,r;return Yd(this,i),$d(Pk(r=e.call(this)),"length",0),$d(Pk(r),"_listener",void 0),$d(Pk(r),"_data",void 0),$d(Pk(r),"_ids",new b_),$d(Pk(r),"_options",void 0),r._options=n||{},r._listener=zn(o=r._onEvent).call(o,Pk(r)),r.setData(t),r}return Kd(i,[{key:"idProp",get:function(){return this.getDataSet().idProp}},{key:"setData",value:function(t){if(this._data){this._data.off&&this._data.off("*",this._listener);var e=this._data.getIds({filter:Xf(this._options)}),i=this._data.get(e);this._ids.clear(),this.length=0,this._trigger("remove",{items:e,oldData:i})}if(null!=t){this._data=t;for(var n=this._data.getIds({filter:Xf(this._options)}),o=0,r=n.length;o<r;o++){var s=n[o];this._ids.add(s)}this.length=n.length,this._trigger("add",{items:n})}else this._data=new Kx;this._data.on&&this._data.on("*",this._listener)}},{key:"refresh",value:function(){for(var t=this._data.getIds({filter:Xf(this._options)}),e=Jc(this._ids),i={},n=[],o=[],r=[],s=0,a=t.length;s<a;s++){var h=t[s];i[h]=!0,this._ids.has(h)||(n.push(h),this._ids.add(h))}for(var l=0,d=e.length;l<d;l++){var c=e[l],u=this._data.get(c);null==u?console.error("If you see this, report it please."):i[c]||(o.push(c),r.push(u),this._ids.delete(c))}this.length+=n.length-o.length,n.length&&this._trigger("add",{items:n}),o.length&&this._trigger("remove",{items:o,oldData:r})}},{key:"get",value:function(t,e){if(null==this._data)return null;var i,n=null;Ux(t)||lu(t)?(n=t,i=e):i=t;var o=un({},this._options,i),r=Xf(this._options),s=i&&Xf(i);return r&&s&&(o.filter=function(t){return r(t)&&s(t)}),null==n?this._data.get(o):this._data.get(n,o)}},{key:"getIds",value:function(t){if(this._data.length){var e,i=Xf(this._options),n=null!=t?Xf(t):null;return e=n?i?function(t){return i(t)&&n(t)}:n:i,this._data.getIds({filter:e,order:t&&t.order})}return[]}},{key:"forEach",value:function(t,e){if(this._data){var i,n,o=Xf(this._options),r=e&&Xf(e);n=r?o?function(t){return o(t)&&r(t)}:r:o,Fu(i=this._data).call(i,t,{filter:n,order:e&&e.order})}}},{key:"map",value:function(t,e){if(this._data){var i,n,o=Xf(this._options),r=e&&Xf(e);return n=r?o?function(t){return o(t)&&r(t)}:r:o,gu(i=this._data).call(i,t,{filter:n,order:e&&e.order})}return[]}},{key:"getDataSet",value:function(){return this._data.getDataSet()}},{key:"stream",value:function(t){var e;return this._data.stream(t||$d({},w_,zn(e=fx(this._ids)).call(e,this._ids)))}},{key:"dispose",value:function(){var t;null!==(t=this._data)&&void 0!==t&&t.off&&this._data.off("*",this._listener);var e,n="This data view has already been disposed of.",o={get:function(){throw new Error(n)},set:function(){throw new Error(n)},configurable:!1},r=Hx(hu(i.prototype));try{for(r.s();!(e=r.n()).done;){var s=e.value;Ud(this,s,o)}}catch(t){r.e(t)}finally{r.f()}}},{key:"_onEvent",value:function(t,e,i){if(e&&e.items&&this._data){var n=e.items,o=[],r=[],s=[],a=[],h=[],l=[];switch(t){case"add":for(var d=0,c=n.length;d<c;d++){var u=n[d];this.get(u)&&(this._ids.add(u),o.push(u))}break;case"update":for(var f=0,p=n.length;f<p;f++){var v=n[f];this.get(v)?this._ids.has(v)?(r.push(v),h.push(e.data[f]),a.push(e.oldData[f])):(this._ids.add(v),o.push(v)):this._ids.has(v)&&(this._ids.delete(v),s.push(v),l.push(e.oldData[f]))}break;case"remove":for(var g=0,y=n.length;g<y;g++){var m=n[g];this._ids.has(m)&&(this._ids.delete(m),s.push(m),l.push(e.oldData[g]))}}this.length+=o.length-s.length,o.length&&this._trigger("add",{items:o},i),r.length&&this._trigger("update",{items:r,oldData:a,data:h},i),s.length&&this._trigger("remove",{items:s,oldData:l},i)}}}]),i}(Xx);function Zx(t,e){return"object"===Qc(e)&&null!==e&&t===e.idProp&&"function"==typeof e.add&&"function"==typeof e.clear&&"function"==typeof e.distinct&&"function"==typeof Fu(e)&&"function"==typeof e.get&&"function"==typeof e.getDataSet&&"function"==typeof e.getIds&&"number"==typeof e.length&&"function"==typeof gu(e)&&"function"==typeof e.max&&"function"==typeof e.min&&"function"==typeof e.off&&"function"==typeof e.on&&"function"==typeof e.remove&&"function"==typeof e.setOptions&&"function"==typeof e.stream&&"function"==typeof e.update&&"function"==typeof e.updateOnly}function Qx(t,e){return"object"===Qc(e)&&null!==e&&t===e.idProp&&"function"==typeof Fu(e)&&"function"==typeof e.get&&"function"==typeof e.getDataSet&&"function"==typeof e.getIds&&"number"==typeof e.length&&"function"==typeof gu(e)&&"function"==typeof e.off&&"function"==typeof e.on&&"function"==typeof e.stream&&Zx(t,e.getDataSet())}var Jx=Object.freeze({__proto__:null,DELETE:Iy,DataSet:Kx,DataStream:Gx,DataView:$x,Queue:Yx,createNewDataPipeFrom:function(t){return new Vx(t)},isDataSetLike:Zx,isDataViewLike:Qx}),tE=n,eE=o,iE=eo,nE=cp.trim,oE=g("".charAt),rE=tE.parseFloat,sE=tE.Symbol,aE=sE&&sE.iterator,hE=1/rE("\t\n\v\f\r                　\u2028\u2029\ufeff-0")!=-1/0||aE&&!eE((function(){rE(Object(aE))}))?function(t){var e=nE(iE(t)),i=rE(e);return 0===i&&"-"==oE(e,0)?-0:i}:rE;_i({global:!0,forced:parseFloat!=hE},{parseFloat:hE});var lE=X.parseFloat,dE=_i,cE=o,uE=hh.f;dE({target:"Object",stat:!0,forced:cE((function(){return!Object.getOwnPropertyNames(1)}))},{getOwnPropertyNames:uE});var fE=X.Object,pE=function(t){return fE.getOwnPropertyNames(t)},vE=pE;function gE(t,e){var i=["node","edge","label"],n=!0,o=Mm(e,"chosen");if("boolean"==typeof o)n=o;else if("object"===Qc(o)){if(-1===Fp(i).call(i,t))throw new Error("choosify: subOption '"+t+"' should be one of '"+i.join("', '")+"'");var r=Mm(e,["chosen",t]);"boolean"!=typeof r&&"function"!=typeof r||(n=r)}return n}function yE(t,e,i){if(t.width<=0||t.height<=0)return!1;if(void 0!==i){var n={x:e.x-i.x,y:e.y-i.y};if(0!==i.angle){var o=-i.angle;e={x:Math.cos(o)*n.x-Math.sin(o)*n.y,y:Math.sin(o)*n.x+Math.cos(o)*n.y}}else e=n}var r=t.x+t.width,s=t.y+t.width;return t.left<e.x&&r>e.x&&t.top<e.y&&s>e.y}function mE(t){return"string"==typeof t&&""!==t}function bE(t,e,i,n){var o=n.x,r=n.y;if("function"==typeof n.distanceToBorder){var s=n.distanceToBorder(t,e),a=Math.sin(e)*s,h=Math.cos(e)*s;h===s?(o+=s,r=n.y):a===s?(o=n.x,r-=s):(o+=h,r-=a)}else n.shape.width>n.shape.height?(o=n.x+.5*n.shape.width,r=n.y-i):(o=n.x+i,r=n.y-.5*n.shape.height);return{x:o,y:r}}var wE=function(){function t(e){Yd(this,t),this.measureText=e,this.current=0,this.width=0,this.height=0,this.lines=[]}return Kd(t,[{key:"_add",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:"normal";void 0===this.lines[t]&&(this.lines[t]={width:0,height:0,blocks:[]});var n=e;void 0!==e&&""!==e||(n=" ");var o=this.measureText(n,i),r=un({},kx(o));r.text=e,r.width=o.width,r.mod=i,void 0!==e&&""!==e||(r.width=0),this.lines[t].blocks.push(r),this.lines[t].width+=r.width}},{key:"curWidth",value:function(){var t=this.lines[this.current];return void 0===t?0:t.width}},{key:"append",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"normal";this._add(this.current,t,e)}},{key:"newLine",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"normal";this._add(this.current,t,e),this.current++}},{key:"determineLineHeights",value:function(){for(var t=0;t<this.lines.length;t++){var e=this.lines[t],i=0;if(void 0!==e.blocks)for(var n=0;n<e.blocks.length;n++){var o=e.blocks[n];i<o.height&&(i=o.height)}e.height=i}}},{key:"determineLabelSize",value:function(){for(var t=0,e=0,i=0;i<this.lines.length;i++){var n=this.lines[i];n.width>t&&(t=n.width),e+=n.height}this.width=t,this.height=e}},{key:"removeEmptyBlocks",value:function(){for(var t=[],e=0;e<this.lines.length;e++){var i=this.lines[e];if(0!==i.blocks.length&&(e!==this.lines.length-1||0!==i.width)){var n={};un(n,i),n.blocks=[];for(var o=void 0,r=[],s=0;s<i.blocks.length;s++){var a=i.blocks[s];0!==a.width?r.push(a):void 0===o&&(o=a)}0===r.length&&void 0!==o&&r.push(o),n.blocks=r,t.push(n)}}return t}},{key:"finalize",value:function(){this.determineLineHeights(),this.determineLabelSize();var t=this.removeEmptyBlocks();return{width:this.width,height:this.height,lines:t}}}]),t}(),kE={"<b>":/<b>/,"<i>":/<i>/,"<code>":/<code>/,"</b>":/<\/b>/,"</i>":/<\/i>/,"</code>":/<\/code>/,"*":/\*/,_:/_/,"`":/`/,afterBold:/[^*]/,afterItal:/[^_]/,afterMono:/[^`]/},_E=function(){function t(e){Yd(this,t),this.text=e,this.bold=!1,this.ital=!1,this.mono=!1,this.spacing=!1,this.position=0,this.buffer="",this.modStack=[],this.blocks=[]}return Kd(t,[{key:"mod",value:function(){return 0===this.modStack.length?"normal":this.modStack[0]}},{key:"modName",value:function(){return 0===this.modStack.length?"normal":"mono"===this.modStack[0]?"mono":this.bold&&this.ital?"boldital":this.bold?"bold":this.ital?"ital":void 0}},{key:"emitBlock",value:function(){this.spacing&&(this.add(" "),this.spacing=!1),this.buffer.length>0&&(this.blocks.push({text:this.buffer,mod:this.modName()}),this.buffer="")}},{key:"add",value:function(t){" "===t&&(this.spacing=!0),this.spacing&&(this.buffer+=" ",this.spacing=!1)," "!=t&&(this.buffer+=t)}},{key:"parseWS",value:function(t){return!!/[ \t]/.test(t)&&(this.mono?this.add(t):this.spacing=!0,!0)}},{key:"setTag",value:function(t){this.emitBlock(),this[t]=!0,this.modStack.unshift(t)}},{key:"unsetTag",value:function(t){this.emitBlock(),this[t]=!1,this.modStack.shift()}},{key:"parseStartTag",value:function(t,e){return!(this.mono||this[t]||!this.match(e))&&(this.setTag(t),!0)}},{key:"match",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1],i=this.prepareRegExp(t),n=Kc(i,2),o=n[0],r=n[1],s=o.test(this.text.substr(this.position,r));return s&&e&&(this.position+=r-1),s}},{key:"parseEndTag",value:function(t,e,i){var n=this.mod()===t;return!(!(n="mono"===t?n&&this.mono:n&&!this.mono)||!this.match(e))&&(void 0!==i?(this.position===this.text.length-1||this.match(i,!1))&&this.unsetTag(t):this.unsetTag(t),!0)}},{key:"replace",value:function(t,e){return!!this.match(t)&&(this.add(e),this.position+=length-1,!0)}},{key:"prepareRegExp",value:function(t){var e,i;if(t instanceof RegExp)i=t,e=1;else{var n=kE[t];i=void 0!==n?n:new RegExp(t),e=t.length}return[i,e]}}]),t}(),xE=function(){function t(e,i,n,o){var r=this;Yd(this,t),this.ctx=e,this.parent=i,this.selected=n,this.hover=o;this.lines=new wE((function(t,i){if(void 0===t)return 0;var s=r.parent.getFormattingValues(e,n,o,i),a=0;""!==t&&(a=r.ctx.measureText(t).width);return{width:a,values:s}}))}return Kd(t,[{key:"process",value:function(t){if(!mE(t))return this.lines.finalize();var e=this.parent.fontOptions;t=(t=t.replace(/\r\n/g,"\n")).replace(/\r/g,"\n");var i=String(t).split("\n"),n=i.length;if(e.multi)for(var o=0;o<n;o++){var r=this.splitBlocks(i[o],e.multi);if(void 0!==r)if(0!==r.length){if(e.maxWdt>0)for(var s=0;s<r.length;s++){var a=r[s].mod,h=r[s].text;this.splitStringIntoLines(h,a,!0)}else for(var l=0;l<r.length;l++){var d=r[l].mod,c=r[l].text;this.lines.append(c,d)}this.lines.newLine()}else this.lines.newLine("")}else if(e.maxWdt>0)for(var u=0;u<n;u++)this.splitStringIntoLines(i[u]);else for(var f=0;f<n;f++)this.lines.newLine(i[f]);return this.lines.finalize()}},{key:"decodeMarkupSystem",value:function(t){var e="none";return"markdown"===t||"md"===t?e="markdown":!0!==t&&"html"!==t||(e="html"),e}},{key:"splitHtmlBlocks",value:function(t){for(var e=new _E(t),i=function(t){return!!/&/.test(t)&&(e.replace(e.text,"&lt;","<")||e.replace(e.text,"&amp;","&")||e.add("&"),!0)};e.position<e.text.length;){var n=e.text.charAt(e.position);e.parseWS(n)||/</.test(n)&&(e.parseStartTag("bold","<b>")||e.parseStartTag("ital","<i>")||e.parseStartTag("mono","<code>")||e.parseEndTag("bold","</b>")||e.parseEndTag("ital","</i>")||e.parseEndTag("mono","</code>"))||i(n)||e.add(n),e.position++}return e.emitBlock(),e.blocks}},{key:"splitMarkdownBlocks",value:function(t){for(var e=this,i=new _E(t),n=!0,o=function(t){return!!/\\/.test(t)&&(i.position<e.text.length+1&&(i.position++,t=e.text.charAt(i.position),/ \t/.test(t)?i.spacing=!0:(i.add(t),n=!1)),!0)};i.position<i.text.length;){var r=i.text.charAt(i.position);i.parseWS(r)||o(r)||(n||i.spacing)&&(i.parseStartTag("bold","*")||i.parseStartTag("ital","_")||i.parseStartTag("mono","`"))||i.parseEndTag("bold","*","afterBold")||i.parseEndTag("ital","_","afterItal")||i.parseEndTag("mono","`","afterMono")||(i.add(r),n=!1),i.position++}return i.emitBlock(),i.blocks}},{key:"splitBlocks",value:function(t,e){var i=this.decodeMarkupSystem(e);return"none"===i?[{text:t,mod:"normal"}]:"markdown"===i?this.splitMarkdownBlocks(t):"html"===i?this.splitHtmlBlocks(t):void 0}},{key:"overMaxWidth",value:function(t){var e=this.ctx.measureText(t).width;return this.lines.curWidth()+e>this.parent.fontOptions.maxWdt}},{key:"getLongestFit",value:function(t){for(var e="",i=0;i<t.length;){var n=e+(""===e?"":" ")+t[i];if(this.overMaxWidth(n))break;e=n,i++}return i}},{key:"getLongestFitWord",value:function(t){for(var e=0;e<t.length&&!this.overMaxWidth(au(t).call(t,0,e));)e++;return e}},{key:"splitStringIntoLines",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"normal",i=arguments.length>2&&void 0!==arguments[2]&&arguments[2];this.parent.getFormattingValues(this.ctx,this.selected,this.hover,e);for(var n=(t=(t=t.replace(/^( +)/g,"$1\r")).replace(/([^\r][^ ]*)( +)/g,"$1\r$2\r")).split("\r");n.length>0;){var o=this.getLongestFit(n);if(0===o){var r=n[0],s=this.getLongestFitWord(r);this.lines.newLine(au(r).call(r,0,s),e),n[0]=au(r).call(r,s)}else{var a=o;" "===n[o-1]?o--:" "===n[a]&&a++;var h=au(n).call(n,0,o).join("");o==n.length&&i?this.lines.append(h,e):this.lines.newLine(h,e),n=au(n).call(n,a)}}}}]),t}(),EE=["bold","ital","boldital","mono"],OE=function(){function t(e,i){var n=arguments.length>2&&void 0!==arguments[2]&&arguments[2];Yd(this,t),this.body=e,this.pointToSelf=!1,this.baseSize=void 0,this.fontOptions={},this.setOptions(i),this.size={top:0,left:0,width:0,height:0,yLine:0},this.isEdgeLabel=n}return Kd(t,[{key:"setOptions",value:function(t){if(this.elementOptions=t,this.initFontOptions(t.font),mE(t.label)?this.labelDirty=!0:t.label=void 0,void 0!==t.font&&null!==t.font)if("string"==typeof t.font)this.baseSize=this.fontOptions.size;else if("object"===Qc(t.font)){var e=t.font.size;void 0!==e&&(this.baseSize=e)}}},{key:"initFontOptions",value:function(e){var i=this;hm(EE,(function(t){i.fontOptions[t]={}})),t.parseFontString(this.fontOptions,e)?this.fontOptions.vadjust=0:hm(e,(function(t,e){null!=t&&"object"!==Qc(t)&&(i.fontOptions[e]=t)}))}},{key:"constrain",value:function(t){var e={constrainWidth:!1,maxWdt:-1,minWdt:-1,constrainHeight:!1,minHgt:-1,valign:"middle"},i=Mm(t,"widthConstraint");if("number"==typeof i)e.maxWdt=Number(i),e.minWdt=Number(i);else if("object"===Qc(i)){var n=Mm(t,["widthConstraint","maximum"]);"number"==typeof n&&(e.maxWdt=Number(n));var o=Mm(t,["widthConstraint","minimum"]);"number"==typeof o&&(e.minWdt=Number(o))}var r=Mm(t,"heightConstraint");if("number"==typeof r)e.minHgt=Number(r);else if("object"===Qc(r)){var s=Mm(t,["heightConstraint","minimum"]);"number"==typeof s&&(e.minHgt=Number(s));var a=Mm(t,["heightConstraint","valign"]);"string"==typeof a&&("top"!==a&&"bottom"!==a||(e.valign=a))}return e}},{key:"update",value:function(t,e){this.setOptions(t,!0),this.propagateFonts(e),nm(this.fontOptions,this.constrain(e)),this.fontOptions.chooser=gE("label",e)}},{key:"adjustSizes",value:function(t){var e=t?t.right+t.left:0;this.fontOptions.constrainWidth&&(this.fontOptions.maxWdt-=e,this.fontOptions.minWdt-=e);var i=t?t.top+t.bottom:0;this.fontOptions.constrainHeight&&(this.fontOptions.minHgt-=i)}},{key:"addFontOptionsToPile",value:function(t,e){for(var i=0;i<e.length;++i)this.addFontToPile(t,e[i])}},{key:"addFontToPile",value:function(t,e){if(void 0!==e&&void 0!==e.font&&null!==e.font){var i=e.font;t.push(i)}}},{key:"getBasicOptions",value:function(e){for(var i={},n=0;n<e.length;++n){var o=e[n],r={};t.parseFontString(r,o)&&(o=r),hm(o,(function(t,e){void 0!==t&&(Object.prototype.hasOwnProperty.call(i,e)||(-1!==Fp(EE).call(EE,e)?i[e]={}:i[e]=t))}))}return i}},{key:"getFontOption",value:function(e,i,n){for(var o,r=0;r<e.length;++r){var s=e[r];if(Object.prototype.hasOwnProperty.call(s,i)){if(null==(o=s[i]))continue;var a={};if(t.parseFontString(a,o)&&(o=a),Object.prototype.hasOwnProperty.call(o,n))return o[n]}}if(Object.prototype.hasOwnProperty.call(this.fontOptions,n))return this.fontOptions[n];throw new Error("Did not find value for multi-font for property: '"+n+"'")}},{key:"getFontOptions",value:function(t,e){for(var i={},n=["color","size","face","mod","vadjust"],o=0;o<n.length;++o){var r=n[o];i[r]=this.getFontOption(t,e,r)}return i}},{key:"propagateFonts",value:function(t){var e=this,i=[];this.addFontOptionsToPile(i,t),this.fontOptions=this.getBasicOptions(i);for(var n=function(t){var n=EE[t],o=e.fontOptions[n];hm(e.getFontOptions(i,n),(function(t,e){o[e]=t})),o.size=Number(o.size),o.vadjust=Number(o.vadjust)},o=0;o<EE.length;++o)n(o)}},{key:"draw",value:function(t,e,i,n,o){var r=arguments.length>5&&void 0!==arguments[5]?arguments[5]:"middle";if(void 0!==this.elementOptions.label){var s=this.fontOptions.size*this.body.view.scale;this.elementOptions.label&&s<this.elementOptions.scaling.label.drawThreshold-1||(s>=this.elementOptions.scaling.label.maxVisible&&(s=Number(this.elementOptions.scaling.label.maxVisible)/this.body.view.scale),this.calculateLabelSize(t,n,o,e,i,r),this._drawBackground(t),this._drawText(t,e,this.size.yLine,r,s))}}},{key:"_drawBackground",value:function(t){if(void 0!==this.fontOptions.background&&"none"!==this.fontOptions.background){t.fillStyle=this.fontOptions.background;var e=this.getSize();t.fillRect(e.left,e.top,e.width,e.height)}}},{key:"_drawText",value:function(t,e,i){var n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:"middle",o=arguments.length>4?arguments[4]:void 0,r=this._setAlignment(t,e,i,n),s=Kc(r,2);e=s[0],i=s[1],t.textAlign="left",e-=this.size.width/2,this.fontOptions.valign&&this.size.height>this.size.labelHeight&&("top"===this.fontOptions.valign&&(i-=(this.size.height-this.size.labelHeight)/2),"bottom"===this.fontOptions.valign&&(i+=(this.size.height-this.size.labelHeight)/2));for(var a=0;a<this.lineCount;a++){var h=this.lines[a];if(h&&h.blocks){var l=0;this.isEdgeLabel||"center"===this.fontOptions.align?l+=(this.size.width-h.width)/2:"right"===this.fontOptions.align&&(l+=this.size.width-h.width);for(var d=0;d<h.blocks.length;d++){var c=h.blocks[d];t.font=c.font;var u=this._getColor(c.color,o,c.strokeColor),f=Kc(u,2),p=f[0],v=f[1];c.strokeWidth>0&&(t.lineWidth=c.strokeWidth,t.strokeStyle=v,t.lineJoin="round"),t.fillStyle=p,c.strokeWidth>0&&t.strokeText(c.text,e+l,i+c.vadjust),t.fillText(c.text,e+l,i+c.vadjust),l+=c.width}i+=h.height}}}},{key:"_setAlignment",value:function(t,e,i,n){if(this.isEdgeLabel&&"horizontal"!==this.fontOptions.align&&!1===this.pointToSelf){e=0,i=0;"top"===this.fontOptions.align?(t.textBaseline="alphabetic",i-=4):"bottom"===this.fontOptions.align?(t.textBaseline="hanging",i+=4):t.textBaseline="middle"}else t.textBaseline=n;return[e,i]}},{key:"_getColor",value:function(t,e,i){var n=t||"#000000",o=i||"#ffffff";if(e<=this.elementOptions.scaling.label.drawThreshold){var r=Math.max(0,Math.min(1,1-(this.elementOptions.scaling.label.drawThreshold-e)));n=pm(n,r),o=pm(o,r)}return[n,o]}},{key:"getTextSize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]&&arguments[1],i=arguments.length>2&&void 0!==arguments[2]&&arguments[2];return this._processLabel(t,e,i),{width:this.size.width,height:this.size.height,lineCount:this.lineCount}}},{key:"getSize",value:function(){var t=this.size.left,e=this.size.top-1;if(this.isEdgeLabel){var i=.5*-this.size.width;switch(this.fontOptions.align){case"middle":t=i,e=.5*-this.size.height;break;case"top":t=i,e=-(this.size.height+2);break;case"bottom":t=i,e=2}}return{left:t,top:e,width:this.size.width,height:this.size.height}}},{key:"calculateLabelSize",value:function(t,e,i){var n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:0,o=arguments.length>4&&void 0!==arguments[4]?arguments[4]:0,r=arguments.length>5&&void 0!==arguments[5]?arguments[5]:"middle";this._processLabel(t,e,i),this.size.left=n-.5*this.size.width,this.size.top=o-.5*this.size.height,this.size.yLine=o+.5*(1-this.lineCount)*this.fontOptions.size,"hanging"===r&&(this.size.top+=.5*this.fontOptions.size,this.size.top+=4,this.size.yLine+=4)}},{key:"getFormattingValues",value:function(t,e,i,n){var o=function(t,e,i){return"normal"===e?"mod"===i?"":t[i]:void 0!==t[e][i]?t[e][i]:t[i]},r={color:o(this.fontOptions,n,"color"),size:o(this.fontOptions,n,"size"),face:o(this.fontOptions,n,"face"),mod:o(this.fontOptions,n,"mod"),vadjust:o(this.fontOptions,n,"vadjust"),strokeWidth:this.fontOptions.strokeWidth,strokeColor:this.fontOptions.strokeColor};(e||i)&&("normal"===n&&!0===this.fontOptions.chooser&&this.elementOptions.labelHighlightBold?r.mod="bold":"function"==typeof this.fontOptions.chooser&&this.fontOptions.chooser(r,this.elementOptions.id,e,i));var s="";return void 0!==r.mod&&""!==r.mod&&(s+=r.mod+" "),s+=r.size+"px "+r.face,t.font=s.replace(/"/g,""),r.font=t.font,r.height=r.size,r}},{key:"differentState",value:function(t,e){return t!==this.selectedState||e!==this.hoverState}},{key:"_processLabelText",value:function(t,e,i,n){return new xE(t,this,e,i).process(n)}},{key:"_processLabel",value:function(t,e,i){if(!1!==this.labelDirty||this.differentState(e,i)){var n=this._processLabelText(t,e,i,this.elementOptions.label);this.fontOptions.minWdt>0&&n.width<this.fontOptions.minWdt&&(n.width=this.fontOptions.minWdt),this.size.labelHeight=n.height,this.fontOptions.minHgt>0&&n.height<this.fontOptions.minHgt&&(n.height=this.fontOptions.minHgt),this.lines=n.lines,this.lineCount=n.lines.length,this.size.width=n.width,this.size.height=n.height,this.selectedState=e,this.hoverState=i,this.labelDirty=!1}}},{key:"visible",value:function(){return 0!==this.size.width&&0!==this.size.height&&void 0!==this.elementOptions.label&&!(this.fontOptions.size*this.body.view.scale<this.elementOptions.scaling.label.drawThreshold-1)}}],[{key:"parseFontString",value:function(t,e){if(!e||"string"!=typeof e)return!1;var i=e.split(" ");return t.size=+i[0].replace("px",""),t.face=i[1],t.color=i[2],!0}}]),t}(),CE=function(){function t(e,i,n){Yd(this,t),this.body=i,this.labelModule=n,this.setOptions(e),this.top=void 0,this.left=void 0,this.height=void 0,this.width=void 0,this.radius=void 0,this.margin=void 0,this.refreshNeeded=!0,this.boundingBox={top:0,left:0,right:0,bottom:0}}return Kd(t,[{key:"setOptions",value:function(t){this.options=t}},{key:"_setMargins",value:function(t){this.margin={},this.options.margin&&("object"==Qc(this.options.margin)?(this.margin.top=this.options.margin.top,this.margin.right=this.options.margin.right,this.margin.bottom=this.options.margin.bottom,this.margin.left=this.options.margin.left):(this.margin.top=this.options.margin,this.margin.right=this.options.margin,this.margin.bottom=this.options.margin,this.margin.left=this.options.margin)),t.adjustSizes(this.margin)}},{key:"_distanceToBorder",value:function(t,e){var i=this.options.borderWidth;return t&&this.resize(t),Math.min(Math.abs(this.width/2/Math.cos(e)),Math.abs(this.height/2/Math.sin(e)))+i}},{key:"enableShadow",value:function(t,e){e.shadow&&(t.shadowColor=e.shadowColor,t.shadowBlur=e.shadowSize,t.shadowOffsetX=e.shadowX,t.shadowOffsetY=e.shadowY)}},{key:"disableShadow",value:function(t,e){e.shadow&&(t.shadowColor="rgba(0,0,0,0)",t.shadowBlur=0,t.shadowOffsetX=0,t.shadowOffsetY=0)}},{key:"enableBorderDashes",value:function(t,e){if(!1!==e.borderDashes)if(void 0!==t.setLineDash){var i=e.borderDashes;!0===i&&(i=[5,15]),t.setLineDash(i)}else console.warn("setLineDash is not supported in this browser. The dashed borders cannot be used."),this.options.shapeProperties.borderDashes=!1,e.borderDashes=!1}},{key:"disableBorderDashes",value:function(t,e){!1!==e.borderDashes&&(void 0!==t.setLineDash?t.setLineDash([0]):(console.warn("setLineDash is not supported in this browser. The dashed borders cannot be used."),this.options.shapeProperties.borderDashes=!1,e.borderDashes=!1))}},{key:"needsRefresh",value:function(t,e){return!0===this.refreshNeeded?(this.refreshNeeded=!1,!0):void 0===this.width||this.labelModule.differentState(t,e)}},{key:"initContextForDraw",value:function(t,e){var i=e.borderWidth/this.body.view.scale;t.lineWidth=Math.min(this.width,i),t.strokeStyle=e.borderColor,t.fillStyle=e.color}},{key:"performStroke",value:function(t,e){var i=e.borderWidth/this.body.view.scale;t.save(),i>0&&(this.enableBorderDashes(t,e),t.stroke(),this.disableBorderDashes(t,e)),t.restore()}},{key:"performFill",value:function(t,e){t.save(),t.fillStyle=e.color,this.enableShadow(t,e),jv(t).call(t),this.disableShadow(t,e),t.restore(),this.performStroke(t,e)}},{key:"_addBoundingBoxMargin",value:function(t){this.boundingBox.left-=t,this.boundingBox.top-=t,this.boundingBox.bottom+=t,this.boundingBox.right+=t}},{key:"_updateBoundingBox",value:function(t,e,i,n,o){void 0!==i&&this.resize(i,n,o),this.left=t-this.width/2,this.top=e-this.height/2,this.boundingBox.left=this.left,this.boundingBox.top=this.top,this.boundingBox.bottom=this.top+this.height,this.boundingBox.right=this.left+this.width}},{key:"updateBoundingBox",value:function(t,e,i,n,o){this._updateBoundingBox(t,e,i,n,o)}},{key:"getDimensionsFromLabel",value:function(t,e,i){this.textSize=this.labelModule.getTextSize(t,e,i);var n=this.textSize.width,o=this.textSize.height;return 0===n&&(n=14,o=14),{width:n,height:o}}}]),t}();function SE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var TE=function(t){zk(i,t);var e=SE(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o))._setMargins(o),r}return Kd(i,[{key:"resize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.selected,i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.hover;if(this.needsRefresh(e,i)){var n=this.getDimensionsFromLabel(t,e,i);this.width=n.width+this.margin.right+this.margin.left,this.height=n.height+this.margin.top+this.margin.bottom,this.radius=this.width/2}}},{key:"draw",value:function(t,e,i,n,o,r){this.resize(t,n,o),this.left=e-this.width/2,this.top=i-this.height/2,this.initContextForDraw(t,r),Fn(t,this.left,this.top,this.width,this.height,r.borderRadius),this.performFill(t,r),this.updateBoundingBox(e,i,t,n,o),this.labelModule.draw(t,this.left+this.textSize.width/2+this.margin.left,this.top+this.textSize.height/2+this.margin.top,n,o)}},{key:"updateBoundingBox",value:function(t,e,i,n,o){this._updateBoundingBox(t,e,i,n,o);var r=this.options.shapeProperties.borderRadius;this._addBoundingBoxMargin(r)}},{key:"distanceToBorder",value:function(t,e){t&&this.resize(t);var i=this.options.borderWidth;return Math.min(Math.abs(this.width/2/Math.cos(e)),Math.abs(this.height/2/Math.sin(e)))+i}}]),i}(CE);function ME(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var PE=function(t){zk(i,t);var e=ME(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o)).labelOffset=0,r.selected=!1,r}return Kd(i,[{key:"setOptions",value:function(t,e,i){this.options=t,void 0===e&&void 0===i||this.setImages(e,i)}},{key:"setImages",value:function(t,e){e&&this.selected?(this.imageObj=e,this.imageObjAlt=t):(this.imageObj=t,this.imageObjAlt=e)}},{key:"switchImages",value:function(t){var e=t&&!this.selected||!t&&this.selected;if(this.selected=t,void 0!==this.imageObjAlt&&e){var i=this.imageObj;this.imageObj=this.imageObjAlt,this.imageObjAlt=i}}},{key:"_getImagePadding",value:function(){var t={top:0,right:0,bottom:0,left:0};if(this.options.imagePadding){var e=this.options.imagePadding;"object"==Qc(e)?(t.top=e.top,t.right=e.right,t.bottom=e.bottom,t.left=e.left):(t.top=e,t.right=e,t.bottom=e,t.left=e)}return t}},{key:"_resizeImage",value:function(){var t,e;if(!1===this.options.shapeProperties.useImageSize){var i=1,n=1;this.imageObj.width&&this.imageObj.height&&(this.imageObj.width>this.imageObj.height?i=this.imageObj.width/this.imageObj.height:n=this.imageObj.height/this.imageObj.width),t=2*this.options.size*i,e=2*this.options.size*n}else{var o=this._getImagePadding();t=this.imageObj.width+o.left+o.right,e=this.imageObj.height+o.top+o.bottom}this.width=t,this.height=e,this.radius=.5*this.width}},{key:"_drawRawCircle",value:function(t,e,i,n){this.initContextForDraw(t,n),Nn(t,e,i,n.size),this.performFill(t,n)}},{key:"_drawImageAtPosition",value:function(t,e){if(0!=this.imageObj.width){t.globalAlpha=void 0!==e.opacity?e.opacity:1,this.enableShadow(t,e);var i=1;!0===this.options.shapeProperties.interpolation&&(i=this.imageObj.width/this.width/this.body.view.scale);var n=this._getImagePadding(),o=this.left+n.left,r=this.top+n.top,s=this.width-n.left-n.right,a=this.height-n.top-n.bottom;this.imageObj.drawImageAtPosition(t,i,o,r,s,a),this.disableShadow(t,e)}}},{key:"_drawImageLabel",value:function(t,e,i,n,o){var r=0;if(void 0!==this.height){r=.5*this.height;var s=this.labelModule.getTextSize(t,n,o);s.lineCount>=1&&(r+=s.height/2)}var a=i+r;this.options.label&&(this.labelOffset=r),this.labelModule.draw(t,e,a,n,o,"hanging")}}]),i}(CE);function DE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var IE=function(t){zk(i,t);var e=DE(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o))._setMargins(o),r}return Kd(i,[{key:"resize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.selected,i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.hover;if(this.needsRefresh(e,i)){var n=this.getDimensionsFromLabel(t,e,i),o=Math.max(n.width+this.margin.right+this.margin.left,n.height+this.margin.top+this.margin.bottom);this.options.size=o/2,this.width=o,this.height=o,this.radius=this.width/2}}},{key:"draw",value:function(t,e,i,n,o,r){this.resize(t,n,o),this.left=e-this.width/2,this.top=i-this.height/2,this._drawRawCircle(t,e,i,r),this.updateBoundingBox(e,i),this.labelModule.draw(t,this.left+this.textSize.width/2+this.margin.left,i,n,o)}},{key:"updateBoundingBox",value:function(t,e){this.boundingBox.top=e-this.options.size,this.boundingBox.left=t-this.options.size,this.boundingBox.right=t+this.options.size,this.boundingBox.bottom=e+this.options.size}},{key:"distanceToBorder",value:function(t){return t&&this.resize(t),.5*this.width}}]),i}(PE);function BE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var zE=function(t){zk(i,t);var e=BE(i);function i(t,n,o,r,s){var a;return Yd(this,i),(a=e.call(this,t,n,o)).setImages(r,s),a}return Kd(i,[{key:"resize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.selected,i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.hover,n=void 0===this.imageObj.src||void 0===this.imageObj.width||void 0===this.imageObj.height;if(n){var o=2*this.options.size;return this.width=o,this.height=o,void(this.radius=.5*this.width)}this.needsRefresh(e,i)&&this._resizeImage()}},{key:"draw",value:function(t,e,i,n,o,r){this.switchImages(n),this.resize();var s=e,a=i;"top-left"===this.options.shapeProperties.coordinateOrigin?(this.left=e,this.top=i,s+=this.width/2,a+=this.height/2):(this.left=e-this.width/2,this.top=i-this.height/2),this._drawRawCircle(t,s,a,r),t.save(),t.clip(),this._drawImageAtPosition(t,r),t.restore(),this._drawImageLabel(t,s,a,n,o),this.updateBoundingBox(e,i)}},{key:"updateBoundingBox",value:function(t,e){"top-left"===this.options.shapeProperties.coordinateOrigin?(this.boundingBox.top=e,this.boundingBox.left=t,this.boundingBox.right=t+2*this.options.size,this.boundingBox.bottom=e+2*this.options.size):(this.boundingBox.top=e-this.options.size,this.boundingBox.left=t-this.options.size,this.boundingBox.right=t+this.options.size,this.boundingBox.bottom=e+this.options.size),this.boundingBox.left=Math.min(this.boundingBox.left,this.labelModule.size.left),this.boundingBox.right=Math.max(this.boundingBox.right,this.labelModule.size.left+this.labelModule.size.width),this.boundingBox.bottom=Math.max(this.boundingBox.bottom,this.boundingBox.bottom+this.labelOffset)}},{key:"distanceToBorder",value:function(t){return t&&this.resize(t),.5*this.width}}]),i}(PE);function NE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var FE=function(t){zk(i,t);var e=NE(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"resize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.selected,i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.hover,n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:{size:this.options.size};if(this.needsRefresh(e,i)){var o,r;this.labelModule.getTextSize(t,e,i);var s=2*n.size;this.width=null!==(o=this.customSizeWidth)&&void 0!==o?o:s,this.height=null!==(r=this.customSizeHeight)&&void 0!==r?r:s,this.radius=.5*this.width}}},{key:"_drawShape",value:function(t,e,i,n,o,r,s,a){var h,l=this;return this.resize(t,r,s,a),this.left=n-this.width/2,this.top=o-this.height/2,this.initContextForDraw(t,a),(h=e,Object.prototype.hasOwnProperty.call(Ln,h)?Ln[h]:function(t){for(var e=arguments.length,i=new Array(e>1?e-1:0),n=1;n<e;n++)i[n-1]=arguments[n];CanvasRenderingContext2D.prototype[h].call(t,i)})(t,n,o,a.size),this.performFill(t,a),void 0!==this.options.icon&&void 0!==this.options.icon.code&&(t.font=(r?"bold ":"")+this.height/2+"px "+(this.options.icon.face||"FontAwesome"),t.fillStyle=this.options.icon.color||"black",t.textAlign="center",t.textBaseline="middle",t.fillText(this.options.icon.code,n,o)),{drawExternalLabel:function(){if(void 0!==l.options.label){l.labelModule.calculateLabelSize(t,r,s,n,o,"hanging");var e=o+.5*l.height+.5*l.labelModule.size.height;l.labelModule.draw(t,n,e,r,s,"hanging")}l.updateBoundingBox(n,o)}}}},{key:"updateBoundingBox",value:function(t,e){this.boundingBox.top=e-this.options.size,this.boundingBox.left=t-this.options.size,this.boundingBox.right=t+this.options.size,this.boundingBox.bottom=e+this.options.size,void 0!==this.options.label&&this.labelModule.size.width>0&&(this.boundingBox.left=Math.min(this.boundingBox.left,this.labelModule.size.left),this.boundingBox.right=Math.max(this.boundingBox.right,this.labelModule.size.left+this.labelModule.size.width),this.boundingBox.bottom=Math.max(this.boundingBox.bottom,this.boundingBox.bottom+this.labelModule.size.height))}}]),i}(CE);function AE(t,e){var i=bu(t);if(hd){var n=hd(t);e&&(n=Xf(n).call(n,(function(e){return bd(t,e).enumerable}))),i.push.apply(i,n)}return i}function jE(t){for(var e=1;e<arguments.length;e++){var i,n,o=null!=arguments[e]?arguments[e]:{};e%2?Fu(i=AE(Object(o),!0)).call(i,(function(e){$d(t,e,o[e])})):Pd?Ad(t,Pd(o)):Fu(n=AE(Object(o))).call(n,(function(e){Ud(t,e,bd(o,e))}))}return t}function RE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var LE=function(t){zk(i,t);var e=RE(i);function i(t,n,o,r){var s;return Yd(this,i),(s=e.call(this,t,n,o,r)).ctxRenderer=r,s}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){this.resize(t,n,o,r),this.left=e-this.width/2,this.top=i-this.height/2,t.save();var s=this.ctxRenderer({ctx:t,id:this.options.id,x:e,y:i,state:{selected:n,hover:o},style:jE({},r),label:this.options.label});if(null!=s.drawNode&&s.drawNode(),t.restore(),s.drawExternalLabel){var a=s.drawExternalLabel;s.drawExternalLabel=function(){t.save(),a(),t.restore()}}return s.nodeDimensions&&(this.customSizeWidth=s.nodeDimensions.width,this.customSizeHeight=s.nodeDimensions.height),s}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function HE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var WE=function(t){zk(i,t);var e=HE(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o))._setMargins(o),r}return Kd(i,[{key:"resize",value:function(t,e,i){if(this.needsRefresh(e,i)){var n=this.getDimensionsFromLabel(t,e,i).width+this.margin.right+this.margin.left;this.width=n,this.height=n,this.radius=this.width/2}}},{key:"draw",value:function(t,e,i,n,o,r){this.resize(t,n,o),this.left=e-this.width/2,this.top=i-this.height/2,this.initContextForDraw(t,r),jn(t,e-this.width/2,i-this.height/2,this.width,this.height),this.performFill(t,r),this.updateBoundingBox(e,i,t,n,o),this.labelModule.draw(t,this.left+this.textSize.width/2+this.margin.left,this.top+this.textSize.height/2+this.margin.top,n,o)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(CE);function qE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var VE=function(t){zk(i,t);var e=qE(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"diamond",4,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function UE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var YE=function(t){zk(i,t);var e=UE(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"circle",2,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t){return t&&this.resize(t),this.options.size}}]),i}(FE);function XE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var GE=function(t){zk(i,t);var e=XE(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"resize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.selected,i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.hover;if(this.needsRefresh(e,i)){var n=this.getDimensionsFromLabel(t,e,i);this.height=2*n.height,this.width=n.width+n.height,this.radius=.5*this.width}}},{key:"draw",value:function(t,e,i,n,o,r){this.resize(t,n,o),this.left=e-.5*this.width,this.top=i-.5*this.height,this.initContextForDraw(t,r),An(t,this.left,this.top,this.width,this.height),this.performFill(t,r),this.updateBoundingBox(e,i,t,n,o),this.labelModule.draw(t,e,i,n,o)}},{key:"distanceToBorder",value:function(t,e){t&&this.resize(t);var i=.5*this.width,n=.5*this.height,o=Math.sin(e)*i,r=Math.cos(e)*n;return i*n/Math.sqrt(o*o+r*r)}}]),i}(CE);function KE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var $E=function(t){zk(i,t);var e=KE(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o))._setMargins(o),r}return Kd(i,[{key:"resize",value:function(t,e,i){this.needsRefresh(e,i)&&(this.iconSize={width:Number(this.options.icon.size),height:Number(this.options.icon.size)},this.width=this.iconSize.width+this.margin.right+this.margin.left,this.height=this.iconSize.height+this.margin.top+this.margin.bottom,this.radius=.5*this.width)}},{key:"draw",value:function(t,e,i,n,o,r){var s=this;return this.resize(t,n,o),this.options.icon.size=this.options.icon.size||50,this.left=e-this.width/2,this.top=i-this.height/2,this._icon(t,e,i,n,o,r),{drawExternalLabel:function(){if(void 0!==s.options.label){s.labelModule.draw(t,s.left+s.iconSize.width/2+s.margin.left,i+s.height/2+5,n)}s.updateBoundingBox(e,i)}}}},{key:"updateBoundingBox",value:function(t,e){if(this.boundingBox.top=e-.5*this.options.icon.size,this.boundingBox.left=t-.5*this.options.icon.size,this.boundingBox.right=t+.5*this.options.icon.size,this.boundingBox.bottom=e+.5*this.options.icon.size,void 0!==this.options.label&&this.labelModule.size.width>0){this.boundingBox.left=Math.min(this.boundingBox.left,this.labelModule.size.left),this.boundingBox.right=Math.max(this.boundingBox.right,this.labelModule.size.left+this.labelModule.size.width),this.boundingBox.bottom=Math.max(this.boundingBox.bottom,this.boundingBox.bottom+this.labelModule.size.height+5)}}},{key:"_icon",value:function(t,e,i,n,o,r){var s=Number(this.options.icon.size);void 0!==this.options.icon.code?(t.font=[null!=this.options.icon.weight?this.options.icon.weight:n?"bold":"",(null!=this.options.icon.weight&&n?5:0)+s+"px",this.options.icon.face].join(" "),t.fillStyle=this.options.icon.color||"black",t.textAlign="center",t.textBaseline="middle",this.enableShadow(t,r),t.fillText(this.options.icon.code,e,i),this.disableShadow(t,r)):console.error("When using the icon shape, you need to define the code in the icon options object. This can be done per node or globally.")}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(CE);function ZE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var QE=function(t){zk(i,t);var e=ZE(i);function i(t,n,o,r,s){var a;return Yd(this,i),(a=e.call(this,t,n,o)).setImages(r,s),a}return Kd(i,[{key:"resize",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.selected,i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.hover,n=void 0===this.imageObj.src||void 0===this.imageObj.width||void 0===this.imageObj.height;if(n){var o=2*this.options.size;return this.width=o,void(this.height=o)}this.needsRefresh(e,i)&&this._resizeImage()}},{key:"draw",value:function(t,e,i,n,o,r){t.save(),this.switchImages(n),this.resize();var s=e,a=i;if("top-left"===this.options.shapeProperties.coordinateOrigin?(this.left=e,this.top=i,s+=this.width/2,a+=this.height/2):(this.left=e-this.width/2,this.top=i-this.height/2),!0===this.options.shapeProperties.useBorderWithImage){var h=this.options.borderWidth,l=this.options.borderWidthSelected||2*this.options.borderWidth,d=(n?l:h)/this.body.view.scale;t.lineWidth=Math.min(this.width,d),t.beginPath();var c=n?this.options.color.highlight.border:o?this.options.color.hover.border:this.options.color.border,u=n?this.options.color.highlight.background:o?this.options.color.hover.background:this.options.color.background;void 0!==r.opacity&&(c=pm(c,r.opacity),u=pm(u,r.opacity)),t.strokeStyle=c,t.fillStyle=u,t.rect(this.left-.5*t.lineWidth,this.top-.5*t.lineWidth,this.width+t.lineWidth,this.height+t.lineWidth),jv(t).call(t),this.performStroke(t,r),t.closePath()}this._drawImageAtPosition(t,r),this._drawImageLabel(t,s,a,n,o),this.updateBoundingBox(e,i),t.restore()}},{key:"updateBoundingBox",value:function(t,e){this.resize(),"top-left"===this.options.shapeProperties.coordinateOrigin?(this.left=t,this.top=e):(this.left=t-this.width/2,this.top=e-this.height/2),this.boundingBox.left=this.left,this.boundingBox.top=this.top,this.boundingBox.bottom=this.top+this.height,this.boundingBox.right=this.left+this.width,void 0!==this.options.label&&this.labelModule.size.width>0&&(this.boundingBox.left=Math.min(this.boundingBox.left,this.labelModule.size.left),this.boundingBox.right=Math.max(this.boundingBox.right,this.labelModule.size.left+this.labelModule.size.width),this.boundingBox.bottom=Math.max(this.boundingBox.bottom,this.boundingBox.bottom+this.labelOffset))}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(PE);function JE(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var tO=function(t){zk(i,t);var e=JE(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"square",2,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function eO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var iO=function(t){zk(i,t);var e=eO(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"hexagon",4,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function nO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var oO=function(t){zk(i,t);var e=nO(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"star",4,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function rO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var sO=function(t){zk(i,t);var e=rO(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o))._setMargins(o),r}return Kd(i,[{key:"resize",value:function(t,e,i){this.needsRefresh(e,i)&&(this.textSize=this.labelModule.getTextSize(t,e,i),this.width=this.textSize.width+this.margin.right+this.margin.left,this.height=this.textSize.height+this.margin.top+this.margin.bottom,this.radius=.5*this.width)}},{key:"draw",value:function(t,e,i,n,o,r){this.resize(t,n,o),this.left=e-this.width/2,this.top=i-this.height/2,this.enableShadow(t,r),this.labelModule.draw(t,this.left+this.textSize.width/2+this.margin.left,this.top+this.textSize.height/2+this.margin.top,n,o),this.disableShadow(t,r),this.updateBoundingBox(e,i,t,n,o)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(CE);function aO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var hO=function(t){zk(i,t);var e=aO(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"triangle",3,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function lO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var dO=function(t){zk(i,t);var e=lO(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"draw",value:function(t,e,i,n,o,r){return this._drawShape(t,"triangleDown",3,e,i,n,o,r)}},{key:"distanceToBorder",value:function(t,e){return this._distanceToBorder(t,e)}}]),i}(FE);function cO(t,e){var i=bu(t);if(hd){var n=hd(t);e&&(n=Xf(n).call(n,(function(e){return bd(t,e).enumerable}))),i.push.apply(i,n)}return i}function uO(t){for(var e=1;e<arguments.length;e++){var i,n,o=null!=arguments[e]?arguments[e]:{};e%2?Fu(i=cO(Object(o),!0)).call(i,(function(e){$d(t,e,o[e])})):Pd?Ad(t,Pd(o)):Fu(n=cO(Object(o))).call(n,(function(e){Ud(t,e,bd(o,e))}))}return t}var fO=function(){function t(e,i,n,o,r,s){Yd(this,t),this.options=Cm(r),this.globalOptions=r,this.defaultOptions=s,this.body=i,this.edges=[],this.id=void 0,this.imagelist=n,this.grouplist=o,this.x=void 0,this.y=void 0,this.baseSize=this.options.size,this.baseFontSize=this.options.font.size,this.predefinedPosition=!1,this.selected=!1,this.hover=!1,this.labelModule=new OE(this.body,this.options,!1),this.setOptions(e)}return Kd(t,[{key:"attachEdge",value:function(t){var e;-1===Fp(e=this.edges).call(e,t)&&this.edges.push(t)}},{key:"detachEdge",value:function(t){var e,i,n=Fp(e=this.edges).call(e,t);-1!=n&&ff(i=this.edges).call(i,n,1)}},{key:"setOptions",value:function(e){var i=this.options.shape;if(e){if(void 0!==e.color&&(this._localColor=e.color),void 0!==e.id&&(this.id=e.id),void 0===this.id)throw new Error("Node must have an id");t.checkMass(e,this.id),void 0!==e.x&&(null===e.x?(this.x=void 0,this.predefinedPosition=!1):(this.x=Ep(e.x),this.predefinedPosition=!0)),void 0!==e.y&&(null===e.y?(this.y=void 0,this.predefinedPosition=!1):(this.y=Ep(e.y),this.predefinedPosition=!0)),void 0!==e.size&&(this.baseSize=e.size),void 0!==e.value&&(e.value=lE(e.value)),t.parseOptions(this.options,e,!0,this.globalOptions,this.grouplist);var n=[e,this.options,this.defaultOptions];return this.chooser=gE("node",n),this._load_images(),this.updateLabelModule(e),void 0!==e.opacity&&t.checkOpacity(e.opacity)&&(this.options.opacity=e.opacity),this.updateShape(i),void 0!==e.hidden||void 0!==e.physics}}},{key:"_load_images",value:function(){if(("circularImage"===this.options.shape||"image"===this.options.shape)&&void 0===this.options.image)throw new Error("Option image must be defined for node type '"+this.options.shape+"'");if(void 0!==this.options.image){if(void 0===this.imagelist)throw new Error("Internal Error: No images provided");if("string"==typeof this.options.image)this.imageObj=this.imagelist.load(this.options.image,this.options.brokenImage,this.id);else{if(void 0===this.options.image.unselected)throw new Error("No unselected image provided");this.imageObj=this.imagelist.load(this.options.image.unselected,this.options.brokenImage,this.id),void 0!==this.options.image.selected?this.imageObjAlt=this.imagelist.load(this.options.image.selected,this.options.brokenImage,this.id):this.imageObjAlt=void 0}}}},{key:"getFormattingValues",value:function(){var t={color:this.options.color.background,opacity:this.options.opacity,borderWidth:this.options.borderWidth,borderColor:this.options.color.border,size:this.options.size,borderDashes:this.options.shapeProperties.borderDashes,borderRadius:this.options.shapeProperties.borderRadius,shadow:this.options.shadow.enabled,shadowColor:this.options.shadow.color,shadowSize:this.options.shadow.size,shadowX:this.options.shadow.x,shadowY:this.options.shadow.y};if(this.selected||this.hover?!0===this.chooser?this.selected?(null!=this.options.borderWidthSelected?t.borderWidth=this.options.borderWidthSelected:t.borderWidth*=2,t.color=this.options.color.highlight.background,t.borderColor=this.options.color.highlight.border,t.shadow=this.options.shadow.enabled):this.hover&&(t.color=this.options.color.hover.background,t.borderColor=this.options.color.hover.border,t.shadow=this.options.shadow.enabled):"function"==typeof this.chooser&&(this.chooser(t,this.options.id,this.selected,this.hover),!1===t.shadow&&(t.shadowColor===this.options.shadow.color&&t.shadowSize===this.options.shadow.size&&t.shadowX===this.options.shadow.x&&t.shadowY===this.options.shadow.y||(t.shadow=!0))):t.shadow=this.options.shadow.enabled,void 0!==this.options.opacity){var e=this.options.opacity;t.borderColor=pm(t.borderColor,e),t.color=pm(t.color,e),t.shadowColor=pm(t.shadowColor,e)}return t}},{key:"updateLabelModule",value:function(e){void 0!==this.options.label&&null!==this.options.label||(this.options.label=""),t.updateGroupOptions(this.options,uO(uO({},e),{},{color:e&&e.color||this._localColor||void 0}),this.grouplist);var i=this.grouplist.get(this.options.group,!1),n=[e,this.options,i,this.globalOptions,this.defaultOptions];this.labelModule.update(this.options,n),void 0!==this.labelModule.baseSize&&(this.baseFontSize=this.labelModule.baseSize)}},{key:"updateShape",value:function(t){if(t===this.options.shape&&this.shape)this.shape.setOptions(this.options,this.imageObj,this.imageObjAlt);else switch(this.options.shape){case"box":this.shape=new TE(this.options,this.body,this.labelModule);break;case"circle":this.shape=new IE(this.options,this.body,this.labelModule);break;case"circularImage":this.shape=new zE(this.options,this.body,this.labelModule,this.imageObj,this.imageObjAlt);break;case"custom":this.shape=new LE(this.options,this.body,this.labelModule,this.options.ctxRenderer);break;case"database":this.shape=new WE(this.options,this.body,this.labelModule);break;case"diamond":this.shape=new VE(this.options,this.body,this.labelModule);break;case"dot":this.shape=new YE(this.options,this.body,this.labelModule);break;case"ellipse":default:this.shape=new GE(this.options,this.body,this.labelModule);break;case"icon":this.shape=new $E(this.options,this.body,this.labelModule);break;case"image":this.shape=new QE(this.options,this.body,this.labelModule,this.imageObj,this.imageObjAlt);break;case"square":this.shape=new tO(this.options,this.body,this.labelModule);break;case"hexagon":this.shape=new iO(this.options,this.body,this.labelModule);break;case"star":this.shape=new oO(this.options,this.body,this.labelModule);break;case"text":this.shape=new sO(this.options,this.body,this.labelModule);break;case"triangle":this.shape=new hO(this.options,this.body,this.labelModule);break;case"triangleDown":this.shape=new dO(this.options,this.body,this.labelModule)}this.needsRefresh()}},{key:"select",value:function(){this.selected=!0,this.needsRefresh()}},{key:"unselect",value:function(){this.selected=!1,this.needsRefresh()}},{key:"needsRefresh",value:function(){this.shape.refreshNeeded=!0}},{key:"getTitle",value:function(){return this.options.title}},{key:"distanceToBorder",value:function(t,e){return this.shape.distanceToBorder(t,e)}},{key:"isFixed",value:function(){return this.options.fixed.x&&this.options.fixed.y}},{key:"isSelected",value:function(){return this.selected}},{key:"getValue",value:function(){return this.options.value}},{key:"getLabelSize",value:function(){return this.labelModule.size()}},{key:"setValueRange",value:function(t,e,i){if(void 0!==this.options.value){var n=this.options.scaling.customScalingFunction(t,e,i,this.options.value),o=this.options.scaling.max-this.options.scaling.min;if(!0===this.options.scaling.label.enabled){var r=this.options.scaling.label.max-this.options.scaling.label.min;this.options.font.size=this.options.scaling.label.min+n*r}this.options.size=this.options.scaling.min+n*o}else this.options.size=this.baseSize,this.options.font.size=this.baseFontSize;this.updateLabelModule()}},{key:"draw",value:function(t){var e=this.getFormattingValues();return this.shape.draw(t,this.x,this.y,this.selected,this.hover,e)||{}}},{key:"updateBoundingBox",value:function(t){this.shape.updateBoundingBox(this.x,this.y,t)}},{key:"resize",value:function(t){var e=this.getFormattingValues();this.shape.resize(t,this.selected,this.hover,e)}},{key:"getItemsOnPoint",value:function(t){var e=[];return this.labelModule.visible()&&yE(this.labelModule.getSize(),t)&&e.push({nodeId:this.id,labelId:0}),yE(this.shape.boundingBox,t)&&e.push({nodeId:this.id}),e}},{key:"isOverlappingWith",value:function(t){return this.shape.left<t.right&&this.shape.left+this.shape.width>t.left&&this.shape.top<t.bottom&&this.shape.top+this.shape.height>t.top}},{key:"isBoundingBoxOverlappingWith",value:function(t){return this.shape.boundingBox.left<t.right&&this.shape.boundingBox.right>t.left&&this.shape.boundingBox.top<t.bottom&&this.shape.boundingBox.bottom>t.top}}],[{key:"checkOpacity",value:function(t){return 0<=t&&t<=1}},{key:"checkCoordinateOrigin",value:function(t){return void 0===t||"center"===t||"top-left"===t}},{key:"updateGroupOptions",value:function(e,i,n){var o;if(void 0!==n){var r=e.group;if(void 0!==i&&void 0!==i.group&&r!==i.group)throw new Error("updateGroupOptions: group values in options don't match.");if("number"==typeof r||"string"==typeof r&&""!=r){var s=n.get(r);void 0!==s.opacity&&void 0===i.opacity&&(t.checkOpacity(s.opacity)||(console.error("Invalid option for node opacity. Value must be between 0 and 1, found: "+s.opacity),s.opacity=void 0));var a=Xf(o=vE(i)).call(o,(function(t){return null!=i[t]}));a.push("font"),im(a,e,s),e.color=gm(e.color)}}}},{key:"parseOptions",value:function(e,i){var n=arguments.length>2&&void 0!==arguments[2]&&arguments[2],o=arguments.length>3&&void 0!==arguments[3]?arguments[3]:{},r=arguments.length>4?arguments[4]:void 0,s=["color","fixed","shadow"];if(im(s,e,i,n),t.checkMass(i),void 0!==e.opacity&&(t.checkOpacity(e.opacity)||(console.error("Invalid option for node opacity. Value must be between 0 and 1, found: "+e.opacity),e.opacity=void 0)),void 0!==i.opacity&&(t.checkOpacity(i.opacity)||(console.error("Invalid option for node opacity. Value must be between 0 and 1, found: "+i.opacity),i.opacity=void 0)),i.shapeProperties&&!t.checkCoordinateOrigin(i.shapeProperties.coordinateOrigin)&&console.error("Invalid option for node coordinateOrigin, found: "+i.shapeProperties.coordinateOrigin),Sm(e,i,"shadow",o),void 0!==i.color&&null!==i.color){var a=gm(i.color);Jy(e.color,a)}else!0===n&&null===i.color&&(e.color=Cm(o.color));void 0!==i.fixed&&null!==i.fixed&&("boolean"==typeof i.fixed?(e.fixed.x=i.fixed,e.fixed.y=i.fixed):(void 0!==i.fixed.x&&"boolean"==typeof i.fixed.x&&(e.fixed.x=i.fixed.x),void 0!==i.fixed.y&&"boolean"==typeof i.fixed.y&&(e.fixed.y=i.fixed.y))),!0===n&&null===i.font&&(e.font=Cm(o.font)),t.updateGroupOptions(e,i,r),void 0!==i.scaling&&Sm(e.scaling,i.scaling,"label",o.scaling)}},{key:"checkMass",value:function(t,e){if(void 0!==t.mass&&t.mass<=0){var i="";void 0!==e&&(i=" in node id: "+e),console.error("%cNegative or zero mass disallowed"+i+", setting mass to 1.",Vm),t.mass=1}}}]),t}();function pO(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return vO(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return vO(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function vO(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}var gO=function(){function t(e,i,n,o){var r,s=this;if(Yd(this,t),this.body=e,this.images=i,this.groups=n,this.layoutEngine=o,this.body.functions.createNode=zn(r=this.create).call(r,this),this.nodesListeners={add:function(t,e){s.add(e.items)},update:function(t,e){s.update(e.items,e.data,e.oldData)},remove:function(t,e){s.remove(e.items)}},this.defaultOptions={borderWidth:1,borderWidthSelected:void 0,brokenImage:void 0,color:{border:"#2B7CE9",background:"#97C2FC",highlight:{border:"#2B7CE9",background:"#D2E5FF"},hover:{border:"#2B7CE9",background:"#D2E5FF"}},opacity:void 0,fixed:{x:!1,y:!1},font:{color:"#343434",size:14,face:"arial",background:"none",strokeWidth:0,strokeColor:"#ffffff",align:"center",vadjust:0,multi:!1,bold:{mod:"bold"},boldital:{mod:"bold italic"},ital:{mod:"italic"},mono:{mod:"",size:15,face:"monospace",vadjust:2}},group:void 0,hidden:!1,icon:{face:"FontAwesome",code:void 0,size:50,color:"#2B7CE9"},image:void 0,imagePadding:{top:0,right:0,bottom:0,left:0},label:void 0,labelHighlightBold:!0,level:void 0,margin:{top:5,right:5,bottom:5,left:5},mass:1,physics:!0,scaling:{min:10,max:30,label:{enabled:!1,min:14,max:30,maxVisible:30,drawThreshold:5},customScalingFunction:function(t,e,i,n){if(e===t)return.5;var o=1/(e-t);return Math.max(0,(n-t)*o)}},shadow:{enabled:!1,color:"rgba(0,0,0,0.5)",size:10,x:5,y:5},shape:"ellipse",shapeProperties:{borderDashes:!1,borderRadius:6,interpolation:!0,useImageSize:!1,useBorderWithImage:!1,coordinateOrigin:"center"},size:25,title:void 0,value:void 0,x:void 0,y:void 0},this.defaultOptions.mass<=0)throw"Internal error: mass in defaultOptions of NodesHandler may not be zero or negative";this.options=Cm(this.defaultOptions),this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t,e,i=this;this.body.emitter.on("refreshNodes",zn(t=this.refresh).call(t,this)),this.body.emitter.on("refresh",zn(e=this.refresh).call(e,this)),this.body.emitter.on("destroy",(function(){hm(i.nodesListeners,(function(t,e){i.body.data.nodes&&i.body.data.nodes.off(e,t)})),delete i.body.functions.createNode,delete i.nodesListeners.add,delete i.nodesListeners.update,delete i.nodesListeners.remove,delete i.nodesListeners}))}},{key:"setOptions",value:function(t){if(void 0!==t){if(fO.parseOptions(this.options,t),void 0!==t.opacity&&(ek(t.opacity)||!ok(t.opacity)||t.opacity<0||t.opacity>1?console.error("Invalid option for node opacity. Value must be between 0 and 1, found: "+t.opacity):this.options.opacity=t.opacity),void 0!==t.shape)for(var e in this.body.nodes)Object.prototype.hasOwnProperty.call(this.body.nodes,e)&&this.body.nodes[e].updateShape();if(void 0!==t.font||void 0!==t.widthConstraint||void 0!==t.heightConstraint)for(var i=0,n=bu(this.body.nodes);i<n.length;i++){var o=n[i];this.body.nodes[o].updateLabelModule(),this.body.nodes[o].needsRefresh()}if(void 0!==t.size)for(var r in this.body.nodes)Object.prototype.hasOwnProperty.call(this.body.nodes,r)&&this.body.nodes[r].needsRefresh();void 0===t.hidden&&void 0===t.physics||this.body.emitter.emit("_dataChanged")}}},{key:"setData",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]&&arguments[1],i=this.body.data.nodes;if(Qx("id",t))this.body.data.nodes=t;else if(lu(t))this.body.data.nodes=new Kx,this.body.data.nodes.add(t);else{if(t)throw new TypeError("Array or DataSet expected");this.body.data.nodes=new Kx}if(i&&hm(this.nodesListeners,(function(t,e){i.off(e,t)})),this.body.nodes={},this.body.data.nodes){var n=this;hm(this.nodesListeners,(function(t,e){n.body.data.nodes.on(e,t)}));var o=this.body.data.nodes.getIds();this.add(o,!0)}!1===e&&this.body.emitter.emit("_dataChanged")}},{key:"add",value:function(t){for(var e,i=arguments.length>1&&void 0!==arguments[1]&&arguments[1],n=[],o=0;o<t.length;o++){e=t[o];var r=this.body.data.nodes.get(e),s=this.create(r);n.push(s),this.body.nodes[e]=s}this.layoutEngine.positionInitially(n),!1===i&&this.body.emitter.emit("_dataChanged")}},{key:"update",value:function(t,e,i){for(var n=this.body.nodes,o=!1,r=0;r<t.length;r++){var s=t[r],a=n[s],h=e[r];void 0!==a?a.setOptions(h)&&(o=!0):(o=!0,a=this.create(h),n[s]=a)}o||void 0===i||(o=ck(e).call(e,(function(t,e){var n=i[e];return n&&n.level!==t.level}))),!0===o?this.body.emitter.emit("_dataChanged"):this.body.emitter.emit("_dataUpdated")}},{key:"remove",value:function(t){for(var e=this.body.nodes,i=0;i<t.length;i++){delete e[t[i]]}this.body.emitter.emit("_dataChanged")}},{key:"create",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:fO;return new e(t,this.body,this.images,this.groups,this.options,this.defaultOptions)}},{key:"refresh",value:function(){var t=this,e=arguments.length>0&&void 0!==arguments[0]&&arguments[0];hm(this.body.nodes,(function(i,n){var o=t.body.data.nodes.get(n);void 0!==o&&(!0===e&&i.setOptions({x:null,y:null}),i.setOptions({fixed:!1}),i.setOptions(o))}))}},{key:"getPositions",value:function(t){var e={};if(void 0!==t){if(!0===lu(t)){for(var i=0;i<t.length;i++)if(void 0!==this.body.nodes[t[i]]){var n=this.body.nodes[t[i]];e[t[i]]={x:Math.round(n.x),y:Math.round(n.y)}}}else if(void 0!==this.body.nodes[t]){var o=this.body.nodes[t];e[t]={x:Math.round(o.x),y:Math.round(o.y)}}}else for(var r=0;r<this.body.nodeIndices.length;r++){var s=this.body.nodes[this.body.nodeIndices[r]];e[this.body.nodeIndices[r]]={x:Math.round(s.x),y:Math.round(s.y)}}return e}},{key:"getPosition",value:function(t){if(null==t)throw new TypeError("No id was specified for getPosition method.");if(null==this.body.nodes[t])throw new ReferenceError("NodeId provided for getPosition does not exist. Provided: ".concat(t));return{x:Math.round(this.body.nodes[t].x),y:Math.round(this.body.nodes[t].y)}}},{key:"storePositions",value:function(){var t,e=[],i=this.body.data.nodes.getDataSet(),n=pO(i.get());try{for(n.s();!(t=n.n()).done;){var o=t.value,r=o.id,s=this.body.nodes[r],a=Math.round(s.x),h=Math.round(s.y);o.x===a&&o.y===h||e.push({id:r,x:a,y:h})}}catch(t){n.e(t)}finally{n.f()}i.update(e)}},{key:"getBoundingBox",value:function(t){if(void 0!==this.body.nodes[t])return this.body.nodes[t].shape.boundingBox}},{key:"getConnectedNodes",value:function(t,e){var i=[];if(void 0!==this.body.nodes[t])for(var n=this.body.nodes[t],o={},r=0;r<n.edges.length;r++){var s=n.edges[r];"to"!==e&&s.toId==n.id?void 0===o[s.fromId]&&(i.push(s.fromId),o[s.fromId]=!0):"from"!==e&&s.fromId==n.id&&void 0===o[s.toId]&&(i.push(s.toId),o[s.toId]=!0)}return i}},{key:"getConnectedEdges",value:function(t){var e=[];if(void 0!==this.body.nodes[t])for(var i=this.body.nodes[t],n=0;n<i.edges.length;n++)e.push(i.edges[n].id);else console.error("NodeId provided for getConnectedEdges does not exist. Provided: ",t);return e}},{key:"moveNode",value:function(t,e,i){var n=this;void 0!==this.body.nodes[t]?(this.body.nodes[t].x=Number(e),this.body.nodes[t].y=Number(i),Sv((function(){n.body.emitter.emit("startSimulation")}),0)):console.error("Node id supplied to moveNode does not exist. Provided: ",t)}}]),t}(),yO=Wt,mO=_,bO=Y,wO=$e,kO=function(t){return void 0!==t&&(yO(t,"value")||yO(t,"writable"))},_O=m,xO=Pr;_i({target:"Reflect",stat:!0},{get:function t(e,i){var n,o,r=arguments.length<3?e:arguments[2];return wO(e)===r?e[i]:(n=_O.f(e,i))?kO(n)?n.value:void 0===n.get?void 0:mO(n.get,r):bO(o=xO(e))?t(o,i,r):void 0}});var EO=X.Reflect.get,OO=md;function CO(t,e){for(;!Object.prototype.hasOwnProperty.call(t,e)&&null!==(t=Ak(t)););return t}function SO(){return SO="undefined"!=typeof Reflect&&EO?EO:function(t,e,i){var n=CO(t,e);if(n){var o=OO(n,e);return o.get?o.get.call(arguments.length<3?t:i):o.value}},SO.apply(this,arguments)}var TO=_i,MO=Math.hypot,PO=Math.abs,DO=Math.sqrt;TO({target:"Math",stat:!0,forced:!!MO&&MO(1/0,NaN)!==1/0},{hypot:function(t,e){for(var i,n,o=0,r=0,s=arguments.length,a=0;r<s;)a<(i=PO(arguments[r++]))?(o=o*(n=a/i)*n+1,a=i):o+=i>0?(n=i/a)*n:i;return a===1/0?1/0:a*DO(o)}});var IO=X.Math.hypot;function BO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var zO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"transform",value:function(t,e){lu(t)||(t=[t]);for(var i=e.point.x,n=e.point.y,o=e.angle,r=e.length,s=0;s<t.length;++s){var a=t[s],h=a.x*Math.cos(o)-a.y*Math.sin(o),l=a.x*Math.sin(o)+a.y*Math.cos(o);a.x=i+r*h,a.y=n+r*l}}},{key:"drawPath",value:function(t,e){t.beginPath(),t.moveTo(e[0].x,e[0].y);for(var i=1;i<e.length;++i)t.lineTo(e[i].x,e[i].y);t.closePath()}}]),t}(),NO=function(t){zk(i,t);var e=BO(i);function i(){return Yd(this,i),e.apply(this,arguments)}return Kd(i,null,[{key:"draw",value:function(t,e){if(e.image){t.save(),t.translate(e.point.x,e.point.y),t.rotate(Math.PI/2+e.angle);var i=null!=e.imageWidth?e.imageWidth:e.image.width,n=null!=e.imageHeight?e.imageHeight:e.image.height;e.image.drawImageAtPosition(t,1,-i/2,0,i,n),t.restore()}return!1}}]),i}(zO),FO=function(t){zk(i,t);var e=BO(i);function i(){return Yd(this,i),e.apply(this,arguments)}return Kd(i,null,[{key:"draw",value:function(t,e){var i=[{x:0,y:0},{x:-1,y:.3},{x:-.9,y:0},{x:-1,y:-.3}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),i}(zO),AO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:-1,y:0},{x:0,y:.3},{x:-.4,y:0},{x:0,y:-.3}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),jO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i={x:-.4,y:0};zO.transform(i,e),t.strokeStyle=t.fillStyle,t.fillStyle="rgba(0, 0, 0, 0)";var n=Math.PI,o=e.angle-n/2,r=e.angle+n/2;return t.beginPath(),t.arc(i.x,i.y,.4*e.length,o,r,!1),t.stroke(),!0}}]),t}(),RO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i={x:-.3,y:0};zO.transform(i,e),t.strokeStyle=t.fillStyle,t.fillStyle="rgba(0, 0, 0, 0)";var n=Math.PI,o=e.angle+n/2,r=e.angle+3*n/2;return t.beginPath(),t.arc(i.x,i.y,.4*e.length,o,r,!1),t.stroke(),!0}}]),t}(),LO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:.02,y:0},{x:-1,y:.3},{x:-1,y:-.3}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),HO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:0,y:.3},{x:0,y:-.3},{x:-1,y:0}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),WO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i={x:-.4,y:0};return zO.transform(i,e),Nn(t,i.x,i.y,.4*e.length),!0}}]),t}(),qO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:0,y:.5},{x:0,y:-.5},{x:-.15,y:-.5},{x:-.15,y:.5}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),VO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:0,y:.3},{x:0,y:-.3},{x:-.6,y:-.3},{x:-.6,y:.3}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),UO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:0,y:0},{x:-.5,y:-.3},{x:-1,y:0},{x:-.5,y:.3}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),YO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i=[{x:-1,y:.3},{x:-.5,y:0},{x:-1,y:-.3},{x:0,y:0}];return zO.transform(i,e),zO.drawPath(t,i),!0}}]),t}(),XO=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"draw",value:function(t,e){var i;switch(e.type&&(i=e.type.toLowerCase()),i){case"image":return NO.draw(t,e);case"circle":return WO.draw(t,e);case"box":return VO.draw(t,e);case"crow":return AO.draw(t,e);case"curve":return jO.draw(t,e);case"diamond":return UO.draw(t,e);case"inv_curve":return RO.draw(t,e);case"triangle":return LO.draw(t,e);case"inv_triangle":return HO.draw(t,e);case"bar":return qO.draw(t,e);case"vee":return YO.draw(t,e);default:return FO.draw(t,e)}}}]),t}();function GO(t,e){var i=bu(t);if(hd){var n=hd(t);e&&(n=Xf(n).call(n,(function(e){return bd(t,e).enumerable}))),i.push.apply(i,n)}return i}function KO(t){for(var e=1;e<arguments.length;e++){var i,n,o=null!=arguments[e]?arguments[e]:{};e%2?Fu(i=GO(Object(o),!0)).call(i,(function(e){$d(t,e,o[e])})):Pd?Ad(t,Pd(o)):Fu(n=GO(Object(o))).call(n,(function(e){Ud(t,e,bd(o,e))}))}return t}var $O=function(){function t(e,i,n){Yd(this,t),this._body=i,this._labelModule=n,this.color={},this.colorDirty=!0,this.hoverWidth=1.5,this.selectionWidth=2,this.setOptions(e),this.fromPoint=this.from,this.toPoint=this.to}return Kd(t,[{key:"connect",value:function(){this.from=this._body.nodes[this.options.from],this.to=this._body.nodes[this.options.to]}},{key:"cleanup",value:function(){return!1}},{key:"setOptions",value:function(t){this.options=t,this.from=this._body.nodes[this.options.from],this.to=this._body.nodes[this.options.to],this.id=this.options.id}},{key:"drawLine",value:function(t,e,i,n){var o=arguments.length>4&&void 0!==arguments[4]?arguments[4]:this.getViaNode();t.strokeStyle=this.getColor(t,e),t.lineWidth=e.width,!1!==e.dashes?this._drawDashedLine(t,e,o):this._drawLine(t,e,o)}},{key:"_drawLine",value:function(t,e,i,n,o){if(this.from!=this.to)this._line(t,e,i,n,o);else{var r=Kc(this._getCircleData(t),3),s=r[0],a=r[1],h=r[2];this._circle(t,e,s,a,h)}}},{key:"_drawDashedLine",value:function(t,e,i,n,o){t.lineCap="round";var r=lu(e.dashes)?e.dashes:[5,5];if(void 0!==t.setLineDash){if(t.save(),t.setLineDash(r),t.lineDashOffset=0,this.from!=this.to)this._line(t,e,i);else{var s=Kc(this._getCircleData(t),3),a=s[0],h=s[1],l=s[2];this._circle(t,e,a,h,l)}t.setLineDash([0]),t.lineDashOffset=0,t.restore()}else{if(this.from!=this.to)Rn(t,this.from.x,this.from.y,this.to.x,this.to.y,r);else{var d=Kc(this._getCircleData(t),3),c=d[0],u=d[1],f=d[2];this._circle(t,e,c,u,f)}this.enableShadow(t,e),t.stroke(),this.disableShadow(t,e)}}},{key:"findBorderPosition",value:function(t,e,i){return this.from!=this.to?this._findBorderPosition(t,e,i):this._findBorderPositionCircle(t,e,i)}},{key:"findBorderPositions",value:function(t){if(this.from!=this.to)return{from:this._findBorderPosition(this.from,t),to:this._findBorderPosition(this.to,t)};var e,i=Kc(au(e=this._getCircleData(t)).call(e,0,2),2),n=i[0],o=i[1];return{from:this._findBorderPositionCircle(this.from,t,{x:n,y:o,low:.25,high:.6,direction:-1}),to:this._findBorderPositionCircle(this.from,t,{x:n,y:o,low:.6,high:.8,direction:1})}}},{key:"_getCircleData",value:function(t){var e=this.options.selfReference.size;void 0!==t&&void 0===this.from.shape.width&&this.from.shape.resize(t);var i=bE(t,this.options.selfReference.angle,e,this.from);return[i.x,i.y,e]}},{key:"_pointOnCircle",value:function(t,e,i,n){var o=2*n*Math.PI;return{x:t+i*Math.cos(o),y:e-i*Math.sin(o)}}},{key:"_findBorderPositionCircle",value:function(t,e,i){var n,o=i.x,r=i.y,s=i.low,a=i.high,h=i.direction,l=this.options.selfReference.size,d=.5*(s+a),c=0;!0===this.options.arrowStrikethrough&&(-1===h?c=this.options.endPointOffset.from:1===h&&(c=this.options.endPointOffset.to));var u=0;do{d=.5*(s+a),n=this._pointOnCircle(o,r,l,d);var f=Math.atan2(t.y-n.y,t.x-n.x),p=t.distanceToBorder(e,f)+c-Math.sqrt(Math.pow(n.x-t.x,2)+Math.pow(n.y-t.y,2));if(Math.abs(p)<.05)break;p>0?h>0?s=d:a=d:h>0?a=d:s=d,++u}while(s<=a&&u<10);return KO(KO({},n),{},{t:d})}},{key:"getLineWidth",value:function(t,e){return!0===t?Math.max(this.selectionWidth,.3/this._body.view.scale):!0===e?Math.max(this.hoverWidth,.3/this._body.view.scale):Math.max(this.options.width,.3/this._body.view.scale)}},{key:"getColor",value:function(t,e){if(!1!==e.inheritsColor){if("both"===e.inheritsColor&&this.from.id!==this.to.id){var i=t.createLinearGradient(this.from.x,this.from.y,this.to.x,this.to.y),n=this.from.options.color.highlight.border,o=this.to.options.color.highlight.border;return!1===this.from.selected&&!1===this.to.selected?(n=pm(this.from.options.color.border,e.opacity),o=pm(this.to.options.color.border,e.opacity)):!0===this.from.selected&&!1===this.to.selected?o=this.to.options.color.border:!1===this.from.selected&&!0===this.to.selected&&(n=this.from.options.color.border),i.addColorStop(0,n),i.addColorStop(1,o),i}return"to"===e.inheritsColor?pm(this.to.options.color.border,e.opacity):pm(this.from.options.color.border,e.opacity)}return pm(e.color,e.opacity)}},{key:"_circle",value:function(t,e,i,n,o){this.enableShadow(t,e);var r=0,s=2*Math.PI;if(!this.options.selfReference.renderBehindTheNode){var a=this.options.selfReference.angle,h=this.options.selfReference.angle+Math.PI,l=this._findBorderPositionCircle(this.from,t,{x:i,y:n,low:a,high:h,direction:-1}),d=this._findBorderPositionCircle(this.from,t,{x:i,y:n,low:a,high:h,direction:1});r=Math.atan2(l.y-n,l.x-i),s=Math.atan2(d.y-n,d.x-i)}t.beginPath(),t.arc(i,n,o,r,s,!1),t.stroke(),this.disableShadow(t,e)}},{key:"getDistanceToEdge",value:function(t,e,i,n,o,r){if(this.from!=this.to)return this._getDistanceToEdge(t,e,i,n,o,r);var s=Kc(this._getCircleData(void 0),3),a=s[0],h=s[1],l=s[2],d=a-o,c=h-r;return Math.abs(Math.sqrt(d*d+c*c)-l)}},{key:"_getDistanceToLine",value:function(t,e,i,n,o,r){var s=i-t,a=n-e,h=((o-t)*s+(r-e)*a)/(s*s+a*a);h>1?h=1:h<0&&(h=0);var l=t+h*s-o,d=e+h*a-r;return Math.sqrt(l*l+d*d)}},{key:"getArrowData",value:function(t,e,i,n,o,r){var s,a,h,l,d,c,u,f=r.width;"from"===e?(h=this.from,l=this.to,d=r.fromArrowScale<0,c=Math.abs(r.fromArrowScale),u=r.fromArrowType):"to"===e?(h=this.to,l=this.from,d=r.toArrowScale<0,c=Math.abs(r.toArrowScale),u=r.toArrowType):(h=this.to,l=this.from,d=r.middleArrowScale<0,c=Math.abs(r.middleArrowScale),u=r.middleArrowType);var p=15*c+3*f;if(h!=l){var v=p/IO(h.x-l.x,h.y-l.y);if("middle"!==e)if(!0===this.options.smooth.enabled){var g=this._findBorderPosition(h,t,{via:i}),y=this.getPoint(g.t+v*("from"===e?1:-1),i);s=Math.atan2(g.y-y.y,g.x-y.x),a=g}else s=Math.atan2(h.y-l.y,h.x-l.x),a=this._findBorderPosition(h,t);else{var m=(d?-v:v)/2,b=this.getPoint(.5+m,i),w=this.getPoint(.5-m,i);s=Math.atan2(b.y-w.y,b.x-w.x),a=this.getPoint(.5,i)}}else{var k=Kc(this._getCircleData(t),3),_=k[0],x=k[1],E=k[2];if("from"===e){var O=this.options.selfReference.angle,C=this.options.selfReference.angle+Math.PI,S=this._findBorderPositionCircle(this.from,t,{x:_,y:x,low:O,high:C,direction:-1});s=-2*S.t*Math.PI+1.5*Math.PI+.1*Math.PI,a=S}else if("to"===e){var T=this.options.selfReference.angle,M=this.options.selfReference.angle+Math.PI,P=this._findBorderPositionCircle(this.from,t,{x:_,y:x,low:T,high:M,direction:1});s=-2*P.t*Math.PI+1.5*Math.PI-1.1*Math.PI,a=P}else{var D=this.options.selfReference.angle/(2*Math.PI);a=this._pointOnCircle(_,x,E,D),s=-2*D*Math.PI+1.5*Math.PI+.1*Math.PI}}return{point:a,core:{x:a.x-.9*p*Math.cos(s),y:a.y-.9*p*Math.sin(s)},angle:s,length:p,type:u}}},{key:"drawArrowHead",value:function(t,e,i,n,o){t.strokeStyle=this.getColor(t,e),t.fillStyle=t.strokeStyle,t.lineWidth=e.width,XO.draw(t,o)&&(this.enableShadow(t,e),jv(t).call(t),this.disableShadow(t,e))}},{key:"enableShadow",value:function(t,e){!0===e.shadow&&(t.shadowColor=e.shadowColor,t.shadowBlur=e.shadowSize,t.shadowOffsetX=e.shadowX,t.shadowOffsetY=e.shadowY)}},{key:"disableShadow",value:function(t,e){!0===e.shadow&&(t.shadowColor="rgba(0,0,0,0)",t.shadowBlur=0,t.shadowOffsetX=0,t.shadowOffsetY=0)}},{key:"drawBackground",value:function(t,e){if(!1!==e.background){var i={strokeStyle:t.strokeStyle,lineWidth:t.lineWidth,dashes:t.dashes};t.strokeStyle=e.backgroundColor,t.lineWidth=e.backgroundSize,this.setStrokeDashed(t,e.backgroundDashes),t.stroke(),t.strokeStyle=i.strokeStyle,t.lineWidth=i.lineWidth,t.dashes=i.dashes,this.setStrokeDashed(t,e.dashes)}}},{key:"setStrokeDashed",value:function(t,e){if(!1!==e)if(void 0!==t.setLineDash){var i=lu(e)?e:[5,5];t.setLineDash(i)}else console.warn("setLineDash is not supported in this browser. The dashed stroke cannot be used.");else void 0!==t.setLineDash?t.setLineDash([]):console.warn("setLineDash is not supported in this browser. The dashed stroke cannot be used.")}}]),t}();function ZO(t,e){var i=bu(t);if(hd){var n=hd(t);e&&(n=Xf(n).call(n,(function(e){return bd(t,e).enumerable}))),i.push.apply(i,n)}return i}function QO(t){for(var e=1;e<arguments.length;e++){var i,n,o=null!=arguments[e]?arguments[e]:{};e%2?Fu(i=ZO(Object(o),!0)).call(i,(function(e){$d(t,e,o[e])})):Pd?Ad(t,Pd(o)):Fu(n=ZO(Object(o))).call(n,(function(e){Ud(t,e,bd(o,e))}))}return t}function JO(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var tC=function(t){zk(i,t);var e=JO(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"_findBorderPositionBezier",value:function(t,e){var i,n,o=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this._getViaCoordinates(),r=10,s=.2,a=!1,h=1,l=0,d=this.to,c=this.options.endPointOffset?this.options.endPointOffset.to:0;t.id===this.from.id&&(d=this.from,a=!0,c=this.options.endPointOffset?this.options.endPointOffset.from:0),!1===this.options.arrowStrikethrough&&(c=0);var u=0;do{n=.5*(l+h),i=this.getPoint(n,o);var f=Math.atan2(d.y-i.y,d.x-i.x),p=d.distanceToBorder(e,f)+c,v=Math.sqrt(Math.pow(i.x-d.x,2)+Math.pow(i.y-d.y,2)),g=p-v;if(Math.abs(g)<s)break;g<0?!1===a?l=n:h=n:!1===a?h=n:l=n,++u}while(l<=h&&u<r);return QO(QO({},i),{},{t:n})}},{key:"_getDistanceToBezierEdge",value:function(t,e,i,n,o,r,s){var a,h,l,d,c,u=1e9,f=t,p=e;for(h=1;h<10;h++)l=.1*h,d=Math.pow(1-l,2)*t+2*l*(1-l)*s.x+Math.pow(l,2)*i,c=Math.pow(1-l,2)*e+2*l*(1-l)*s.y+Math.pow(l,2)*n,h>0&&(u=(a=this._getDistanceToLine(f,p,d,c,o,r))<u?a:u),f=d,p=c;return u}},{key:"_bezierCurve",value:function(t,e,i,n){t.beginPath(),t.moveTo(this.fromPoint.x,this.fromPoint.y),null!=i&&null!=i.x?null!=n&&null!=n.x?t.bezierCurveTo(i.x,i.y,n.x,n.y,this.toPoint.x,this.toPoint.y):t.quadraticCurveTo(i.x,i.y,this.toPoint.x,this.toPoint.y):t.lineTo(this.toPoint.x,this.toPoint.y),this.drawBackground(t,e),this.enableShadow(t,e),t.stroke(),this.disableShadow(t,e)}},{key:"getViaNode",value:function(){return this._getViaCoordinates()}}]),i}($O);function eC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var iC=function(t){zk(i,t);var e=eC(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o)).via=r.via,r._boundFunction=function(){r.positionBezierNode()},r._body.emitter.on("_repositionBezierNodes",r._boundFunction),r}return Kd(i,[{key:"setOptions",value:function(t){SO(Ak(i.prototype),"setOptions",this).call(this,t);var e=!1;this.options.physics!==t.physics&&(e=!0),this.options=t,this.id=this.options.id,this.from=this._body.nodes[this.options.from],this.to=this._body.nodes[this.options.to],this.setupSupportNode(),this.connect(),!0===e&&(this.via.setOptions({physics:this.options.physics}),this.positionBezierNode())}},{key:"connect",value:function(){this.from=this._body.nodes[this.options.from],this.to=this._body.nodes[this.options.to],void 0===this.from||void 0===this.to||!1===this.options.physics||this.from.id===this.to.id?this.via.setOptions({physics:!1}):this.via.setOptions({physics:!0})}},{key:"cleanup",value:function(){return this._body.emitter.off("_repositionBezierNodes",this._boundFunction),void 0!==this.via&&(delete this._body.nodes[this.via.id],this.via=void 0,!0)}},{key:"setupSupportNode",value:function(){if(void 0===this.via){var t="edgeId:"+this.id,e=this._body.functions.createNode({id:t,shape:"circle",physics:!0,hidden:!0});this._body.nodes[t]=e,this.via=e,this.via.parentEdgeId=this.id,this.positionBezierNode()}}},{key:"positionBezierNode",value:function(){void 0!==this.via&&void 0!==this.from&&void 0!==this.to?(this.via.x=.5*(this.from.x+this.to.x),this.via.y=.5*(this.from.y+this.to.y)):void 0!==this.via&&(this.via.x=0,this.via.y=0)}},{key:"_line",value:function(t,e,i){this._bezierCurve(t,e,i)}},{key:"_getViaCoordinates",value:function(){return this.via}},{key:"getViaNode",value:function(){return this.via}},{key:"getPoint",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.via;if(this.from===this.to){var i=this._getCircleData(),n=Kc(i,3),o=n[0],r=n[1],s=n[2],a=2*Math.PI*(1-t);return{x:o+s*Math.sin(a),y:r+s-s*(1-Math.cos(a))}}return{x:Math.pow(1-t,2)*this.fromPoint.x+2*t*(1-t)*e.x+Math.pow(t,2)*this.toPoint.x,y:Math.pow(1-t,2)*this.fromPoint.y+2*t*(1-t)*e.y+Math.pow(t,2)*this.toPoint.y}}},{key:"_findBorderPosition",value:function(t,e){return this._findBorderPositionBezier(t,e,this.via)}},{key:"_getDistanceToEdge",value:function(t,e,i,n,o,r){return this._getDistanceToBezierEdge(t,e,i,n,o,r,this.via)}}]),i}(tC);function nC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var oC=function(t){zk(i,t);var e=nC(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"_line",value:function(t,e,i){this._bezierCurve(t,e,i)}},{key:"getViaNode",value:function(){return this._getViaCoordinates()}},{key:"_getViaCoordinates",value:function(){var t,e,i=this.options.smooth.roundness,n=this.options.smooth.type,o=Math.abs(this.from.x-this.to.x),r=Math.abs(this.from.y-this.to.y);if("discrete"===n||"diagonalCross"===n){var s,a;s=a=o<=r?i*r:i*o,this.from.x>this.to.x&&(s=-s),this.from.y>=this.to.y&&(a=-a);var h=this.from.x+s,l=this.from.y+a;return"discrete"===n&&(o<=r?h=o<i*r?this.from.x:h:l=r<i*o?this.from.y:l),{x:h,y:l}}if("straightCross"===n){var d=(1-i)*o,c=(1-i)*r;return o<=r?(d=0,this.from.y<this.to.y&&(c=-c)):(this.from.x<this.to.x&&(d=-d),c=0),{x:this.to.x+d,y:this.to.y+c}}if("horizontal"===n){var u=(1-i)*o;return this.from.x<this.to.x&&(u=-u),{x:this.to.x+u,y:this.from.y}}if("vertical"===n){var f=(1-i)*r;return this.from.y<this.to.y&&(f=-f),{x:this.from.x,y:this.to.y+f}}if("curvedCW"===n){o=this.to.x-this.from.x,r=this.from.y-this.to.y;var p=Math.sqrt(o*o+r*r),v=Math.PI,g=(Math.atan2(r,o)+(.5*i+.5)*v)%(2*v);return{x:this.from.x+(.5*i+.5)*p*Math.sin(g),y:this.from.y+(.5*i+.5)*p*Math.cos(g)}}if("curvedCCW"===n){o=this.to.x-this.from.x,r=this.from.y-this.to.y;var y=Math.sqrt(o*o+r*r),m=Math.PI,b=(Math.atan2(r,o)+(.5*-i+.5)*m)%(2*m);return{x:this.from.x+(.5*i+.5)*y*Math.sin(b),y:this.from.y+(.5*i+.5)*y*Math.cos(b)}}t=e=o<=r?i*r:i*o,this.from.x>this.to.x&&(t=-t),this.from.y>=this.to.y&&(e=-e);var w=this.from.x+t,k=this.from.y+e;return o<=r?w=this.from.x<=this.to.x?this.to.x<w?this.to.x:w:this.to.x>w?this.to.x:w:k=this.from.y>=this.to.y?this.to.y>k?this.to.y:k:this.to.y<k?this.to.y:k,{x:w,y:k}}},{key:"_findBorderPosition",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};return this._findBorderPositionBezier(t,e,i.via)}},{key:"_getDistanceToEdge",value:function(t,e,i,n,o,r){var s=arguments.length>6&&void 0!==arguments[6]?arguments[6]:this._getViaCoordinates();return this._getDistanceToBezierEdge(t,e,i,n,o,r,s)}},{key:"getPoint",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this._getViaCoordinates(),i=t,n=Math.pow(1-i,2)*this.fromPoint.x+2*i*(1-i)*e.x+Math.pow(i,2)*this.toPoint.x,o=Math.pow(1-i,2)*this.fromPoint.y+2*i*(1-i)*e.y+Math.pow(i,2)*this.toPoint.y;return{x:n,y:o}}}]),i}(tC);function rC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var sC=function(t){zk(i,t);var e=rC(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"_getDistanceToBezierEdge2",value:function(t,e,i,n,o,r,s,a){for(var h=1e9,l=t,d=e,c=[0,0,0,0],u=1;u<10;u++){var f=.1*u;c[0]=Math.pow(1-f,3),c[1]=3*f*Math.pow(1-f,2),c[2]=3*Math.pow(f,2)*(1-f),c[3]=Math.pow(f,3);var p=c[0]*t+c[1]*s.x+c[2]*a.x+c[3]*i,v=c[0]*e+c[1]*s.y+c[2]*a.y+c[3]*n;if(u>0){var g=this._getDistanceToLine(l,d,p,v,o,r);h=g<h?g:h}l=p,d=v}return h}}]),i}(tC);function aC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var hC=function(t){zk(i,t);var e=aC(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"_line",value:function(t,e,i){var n=i[0],o=i[1];this._bezierCurve(t,e,n,o)}},{key:"_getViaCoordinates",value:function(){var t,e,i,n,o=this.from.x-this.to.x,r=this.from.y-this.to.y,s=this.options.smooth.roundness;return(Math.abs(o)>Math.abs(r)||!0===this.options.smooth.forceDirection||"horizontal"===this.options.smooth.forceDirection)&&"vertical"!==this.options.smooth.forceDirection?(e=this.from.y,n=this.to.y,t=this.from.x-s*o,i=this.to.x+s*o):(e=this.from.y-s*r,n=this.to.y+s*r,t=this.from.x,i=this.to.x),[{x:t,y:e},{x:i,y:n}]}},{key:"getViaNode",value:function(){return this._getViaCoordinates()}},{key:"_findBorderPosition",value:function(t,e){return this._findBorderPositionBezier(t,e)}},{key:"_getDistanceToEdge",value:function(t,e,i,n,o,r){var s=arguments.length>6&&void 0!==arguments[6]?arguments[6]:this._getViaCoordinates(),a=Kc(s,2),h=a[0],l=a[1];return this._getDistanceToBezierEdge2(t,e,i,n,o,r,h,l)}},{key:"getPoint",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this._getViaCoordinates(),i=Kc(e,2),n=i[0],o=i[1],r=t,s=[Math.pow(1-r,3),3*r*Math.pow(1-r,2),3*Math.pow(r,2)*(1-r),Math.pow(r,3)],a=s[0]*this.fromPoint.x+s[1]*n.x+s[2]*o.x+s[3]*this.toPoint.x,h=s[0]*this.fromPoint.y+s[1]*n.y+s[2]*o.y+s[3]*this.toPoint.y;return{x:a,y:h}}}]),i}(sC);function lC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var dC=function(t){zk(i,t);var e=lC(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"_line",value:function(t,e){t.beginPath(),t.moveTo(this.fromPoint.x,this.fromPoint.y),t.lineTo(this.toPoint.x,this.toPoint.y),this.enableShadow(t,e),t.stroke(),this.disableShadow(t,e)}},{key:"getViaNode",value:function(){}},{key:"getPoint",value:function(t){return{x:(1-t)*this.fromPoint.x+t*this.toPoint.x,y:(1-t)*this.fromPoint.y+t*this.toPoint.y}}},{key:"_findBorderPosition",value:function(t,e){var i=this.to,n=this.from;t.id===this.from.id&&(i=this.from,n=this.to);var o=Math.atan2(i.y-n.y,i.x-n.x),r=i.x-n.x,s=i.y-n.y,a=Math.sqrt(r*r+s*s),h=(a-t.distanceToBorder(e,o))/a;return{x:(1-h)*n.x+h*i.x,y:(1-h)*n.y+h*i.y,t:0}}},{key:"_getDistanceToEdge",value:function(t,e,i,n,o,r){return this._getDistanceToLine(t,e,i,n,o,r)}}]),i}($O),cC=function(){function t(e,i,n,o,r){if(Yd(this,t),void 0===i)throw new Error("No body provided");this.options=Cm(o),this.globalOptions=o,this.defaultOptions=r,this.body=i,this.imagelist=n,this.id=void 0,this.fromId=void 0,this.toId=void 0,this.selected=!1,this.hover=!1,this.labelDirty=!0,this.baseWidth=this.options.width,this.baseFontSize=this.options.font.size,this.from=void 0,this.to=void 0,this.edgeType=void 0,this.connected=!1,this.labelModule=new OE(this.body,this.options,!0),this.setOptions(e)}return Kd(t,[{key:"setOptions",value:function(e){if(e){var i=void 0!==e.physics&&this.options.physics!==e.physics||void 0!==e.hidden&&(this.options.hidden||!1)!==(e.hidden||!1)||void 0!==e.from&&this.options.from!==e.from||void 0!==e.to&&this.options.to!==e.to;t.parseOptions(this.options,e,!0,this.globalOptions),void 0!==e.id&&(this.id=e.id),void 0!==e.from&&(this.fromId=e.from),void 0!==e.to&&(this.toId=e.to),void 0!==e.title&&(this.title=e.title),void 0!==e.value&&(e.value=lE(e.value));var n=[e,this.options,this.defaultOptions];return this.chooser=gE("edge",n),this.updateLabelModule(e),i=this.updateEdgeType()||i,this._setInteractionWidths(),this.connect(),i}}},{key:"getFormattingValues",value:function(){var t=!0===this.options.arrows.to||!0===this.options.arrows.to.enabled,e=!0===this.options.arrows.from||!0===this.options.arrows.from.enabled,i=!0===this.options.arrows.middle||!0===this.options.arrows.middle.enabled,n=this.options.color.inherit,o={toArrow:t,toArrowScale:this.options.arrows.to.scaleFactor,toArrowType:this.options.arrows.to.type,toArrowSrc:this.options.arrows.to.src,toArrowImageWidth:this.options.arrows.to.imageWidth,toArrowImageHeight:this.options.arrows.to.imageHeight,middleArrow:i,middleArrowScale:this.options.arrows.middle.scaleFactor,middleArrowType:this.options.arrows.middle.type,middleArrowSrc:this.options.arrows.middle.src,middleArrowImageWidth:this.options.arrows.middle.imageWidth,middleArrowImageHeight:this.options.arrows.middle.imageHeight,fromArrow:e,fromArrowScale:this.options.arrows.from.scaleFactor,fromArrowType:this.options.arrows.from.type,fromArrowSrc:this.options.arrows.from.src,fromArrowImageWidth:this.options.arrows.from.imageWidth,fromArrowImageHeight:this.options.arrows.from.imageHeight,arrowStrikethrough:this.options.arrowStrikethrough,color:n?void 0:this.options.color.color,inheritsColor:n,opacity:this.options.color.opacity,hidden:this.options.hidden,length:this.options.length,shadow:this.options.shadow.enabled,shadowColor:this.options.shadow.color,shadowSize:this.options.shadow.size,shadowX:this.options.shadow.x,shadowY:this.options.shadow.y,dashes:this.options.dashes,width:this.options.width,background:this.options.background.enabled,backgroundColor:this.options.background.color,backgroundSize:this.options.background.size,backgroundDashes:this.options.background.dashes};if(this.selected||this.hover)if(!0===this.chooser){if(this.selected){var r=this.options.selectionWidth;"function"==typeof r?o.width=r(o.width):"number"==typeof r&&(o.width+=r),o.width=Math.max(o.width,.3/this.body.view.scale),o.color=this.options.color.highlight,o.shadow=this.options.shadow.enabled}else if(this.hover){var s=this.options.hoverWidth;"function"==typeof s?o.width=s(o.width):"number"==typeof s&&(o.width+=s),o.width=Math.max(o.width,.3/this.body.view.scale),o.color=this.options.color.hover,o.shadow=this.options.shadow.enabled}}else"function"==typeof this.chooser&&(this.chooser(o,this.options.id,this.selected,this.hover),void 0!==o.color&&(o.inheritsColor=!1),!1===o.shadow&&(o.shadowColor===this.options.shadow.color&&o.shadowSize===this.options.shadow.size&&o.shadowX===this.options.shadow.x&&o.shadowY===this.options.shadow.y||(o.shadow=!0)));else o.shadow=this.options.shadow.enabled,o.width=Math.max(o.width,.3/this.body.view.scale);return o}},{key:"updateLabelModule",value:function(t){var e=[t,this.options,this.globalOptions,this.defaultOptions];this.labelModule.update(this.options,e),void 0!==this.labelModule.baseSize&&(this.baseFontSize=this.labelModule.baseSize)}},{key:"updateEdgeType",value:function(){var t=this.options.smooth,e=!1,i=!0;return void 0!==this.edgeType&&((this.edgeType instanceof iC&&!0===t.enabled&&"dynamic"===t.type||this.edgeType instanceof hC&&!0===t.enabled&&"cubicBezier"===t.type||this.edgeType instanceof oC&&!0===t.enabled&&"dynamic"!==t.type&&"cubicBezier"!==t.type||this.edgeType instanceof dC&&!1===t.type.enabled)&&(i=!1),!0===i&&(e=this.cleanup())),!0===i?!0===t.enabled?"dynamic"===t.type?(e=!0,this.edgeType=new iC(this.options,this.body,this.labelModule)):"cubicBezier"===t.type?this.edgeType=new hC(this.options,this.body,this.labelModule):this.edgeType=new oC(this.options,this.body,this.labelModule):this.edgeType=new dC(this.options,this.body,this.labelModule):this.edgeType.setOptions(this.options),e}},{key:"connect",value:function(){this.disconnect(),this.from=this.body.nodes[this.fromId]||void 0,this.to=this.body.nodes[this.toId]||void 0,this.connected=void 0!==this.from&&void 0!==this.to,!0===this.connected?(this.from.attachEdge(this),this.to.attachEdge(this)):(this.from&&this.from.detachEdge(this),this.to&&this.to.detachEdge(this)),this.edgeType.connect()}},{key:"disconnect",value:function(){this.from&&(this.from.detachEdge(this),this.from=void 0),this.to&&(this.to.detachEdge(this),this.to=void 0),this.connected=!1}},{key:"getTitle",value:function(){return this.title}},{key:"isSelected",value:function(){return this.selected}},{key:"getValue",value:function(){return this.options.value}},{key:"setValueRange",value:function(t,e,i){if(void 0!==this.options.value){var n=this.options.scaling.customScalingFunction(t,e,i,this.options.value),o=this.options.scaling.max-this.options.scaling.min;if(!0===this.options.scaling.label.enabled){var r=this.options.scaling.label.max-this.options.scaling.label.min;this.options.font.size=this.options.scaling.label.min+n*r}this.options.width=this.options.scaling.min+n*o}else this.options.width=this.baseWidth,this.options.font.size=this.baseFontSize;this._setInteractionWidths(),this.updateLabelModule()}},{key:"_setInteractionWidths",value:function(){"function"==typeof this.options.hoverWidth?this.edgeType.hoverWidth=this.options.hoverWidth(this.options.width):this.edgeType.hoverWidth=this.options.hoverWidth+this.options.width,"function"==typeof this.options.selectionWidth?this.edgeType.selectionWidth=this.options.selectionWidth(this.options.width):this.edgeType.selectionWidth=this.options.selectionWidth+this.options.width}},{key:"draw",value:function(t){var e=this.getFormattingValues();if(!e.hidden){var i=this.edgeType.getViaNode();this.edgeType.drawLine(t,e,this.selected,this.hover,i),this.drawLabel(t,i)}}},{key:"drawArrows",value:function(t){var e=this.getFormattingValues();if(!e.hidden){var i=this.edgeType.getViaNode(),n={};this.edgeType.fromPoint=this.edgeType.from,this.edgeType.toPoint=this.edgeType.to,e.fromArrow&&(n.from=this.edgeType.getArrowData(t,"from",i,this.selected,this.hover,e),!1===e.arrowStrikethrough&&(this.edgeType.fromPoint=n.from.core),e.fromArrowSrc&&(n.from.image=this.imagelist.load(e.fromArrowSrc)),e.fromArrowImageWidth&&(n.from.imageWidth=e.fromArrowImageWidth),e.fromArrowImageHeight&&(n.from.imageHeight=e.fromArrowImageHeight)),e.toArrow&&(n.to=this.edgeType.getArrowData(t,"to",i,this.selected,this.hover,e),!1===e.arrowStrikethrough&&(this.edgeType.toPoint=n.to.core),e.toArrowSrc&&(n.to.image=this.imagelist.load(e.toArrowSrc)),e.toArrowImageWidth&&(n.to.imageWidth=e.toArrowImageWidth),e.toArrowImageHeight&&(n.to.imageHeight=e.toArrowImageHeight)),e.middleArrow&&(n.middle=this.edgeType.getArrowData(t,"middle",i,this.selected,this.hover,e),e.middleArrowSrc&&(n.middle.image=this.imagelist.load(e.middleArrowSrc)),e.middleArrowImageWidth&&(n.middle.imageWidth=e.middleArrowImageWidth),e.middleArrowImageHeight&&(n.middle.imageHeight=e.middleArrowImageHeight)),e.fromArrow&&this.edgeType.drawArrowHead(t,e,this.selected,this.hover,n.from),e.middleArrow&&this.edgeType.drawArrowHead(t,e,this.selected,this.hover,n.middle),e.toArrow&&this.edgeType.drawArrowHead(t,e,this.selected,this.hover,n.to)}}},{key:"drawLabel",value:function(t,e){if(void 0!==this.options.label){var i,n=this.from,o=this.to;if(this.labelModule.differentState(this.selected,this.hover)&&this.labelModule.getTextSize(t,this.selected,this.hover),n.id!=o.id){this.labelModule.pointToSelf=!1,i=this.edgeType.getPoint(.5,e),t.save();var r=this._getRotation(t);0!=r.angle&&(t.translate(r.x,r.y),t.rotate(r.angle)),this.labelModule.draw(t,i.x,i.y,this.selected,this.hover),t.restore()}else{this.labelModule.pointToSelf=!0;var s=bE(t,this.options.selfReference.angle,this.options.selfReference.size,n);i=this._pointOnCircle(s.x,s.y,this.options.selfReference.size,this.options.selfReference.angle),this.labelModule.draw(t,i.x,i.y,this.selected,this.hover)}}}},{key:"getItemsOnPoint",value:function(t){var e=[];if(this.labelModule.visible()){var i=this._getRotation();yE(this.labelModule.getSize(),t,i)&&e.push({edgeId:this.id,labelId:0})}var n={left:t.x,top:t.y};return this.isOverlappingWith(n)&&e.push({edgeId:this.id}),e}},{key:"isOverlappingWith",value:function(t){if(this.connected){var e=this.from.x,i=this.from.y,n=this.to.x,o=this.to.y,r=t.left,s=t.top;return this.edgeType.getDistanceToEdge(e,i,n,o,r,s)<10}return!1}},{key:"_getRotation",value:function(t){var e=this.edgeType.getViaNode(),i=this.edgeType.getPoint(.5,e);void 0!==t&&this.labelModule.calculateLabelSize(t,this.selected,this.hover,i.x,i.y);var n={x:i.x,y:this.labelModule.size.yLine,angle:0};if(!this.labelModule.visible())return n;if("horizontal"===this.options.font.align)return n;var o=this.from.y-this.to.y,r=this.from.x-this.to.x,s=Math.atan2(o,r);return(s<-1&&r<0||s>0&&r<0)&&(s+=Math.PI),n.angle=s,n}},{key:"_pointOnCircle",value:function(t,e,i,n){return{x:t+i*Math.cos(n),y:e-i*Math.sin(n)}}},{key:"select",value:function(){this.selected=!0}},{key:"unselect",value:function(){this.selected=!1}},{key:"cleanup",value:function(){return this.edgeType.cleanup()}},{key:"remove",value:function(){this.cleanup(),this.disconnect(),delete this.body.edges[this.id]}},{key:"endPointsValid",value:function(){return void 0!==this.body.nodes[this.fromId]&&void 0!==this.body.nodes[this.toId]}}],[{key:"parseOptions",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:{},o=arguments.length>4&&void 0!==arguments[4]&&arguments[4],r=["endPointOffset","arrowStrikethrough","id","from","hidden","hoverWidth","labelHighlightBold","length","line","opacity","physics","scaling","selectionWidth","selfReferenceSize","selfReference","to","title","value","width","font","chosen","widthConstraint"];if(em(r,t,e,i),void 0!==e.endPointOffset&&void 0!==e.endPointOffset.from&&(ok(e.endPointOffset.from)?t.endPointOffset.from=e.endPointOffset.from:(t.endPointOffset.from=void 0!==n.endPointOffset.from?n.endPointOffset.from:0,console.error("endPointOffset.from is not a valid number"))),void 0!==e.endPointOffset&&void 0!==e.endPointOffset.to&&(ok(e.endPointOffset.to)?t.endPointOffset.to=e.endPointOffset.to:(t.endPointOffset.to=void 0!==n.endPointOffset.to?n.endPointOffset.to:0,console.error("endPointOffset.to is not a valid number"))),mE(e.label)?t.label=e.label:mE(t.label)||(t.label=void 0),Sm(t,e,"smooth",n),Sm(t,e,"shadow",n),Sm(t,e,"background",n),void 0!==e.dashes&&null!==e.dashes?t.dashes=e.dashes:!0===i&&null===e.dashes&&(t.dashes=Kp(n.dashes)),void 0!==e.scaling&&null!==e.scaling?(void 0!==e.scaling.min&&(t.scaling.min=e.scaling.min),void 0!==e.scaling.max&&(t.scaling.max=e.scaling.max),Sm(t.scaling,e.scaling,"label",n.scaling)):!0===i&&null===e.scaling&&(t.scaling=Kp(n.scaling)),void 0!==e.arrows&&null!==e.arrows)if("string"==typeof e.arrows){var s=e.arrows.toLowerCase();t.arrows.to.enabled=-1!=Fp(s).call(s,"to"),t.arrows.middle.enabled=-1!=Fp(s).call(s,"middle"),t.arrows.from.enabled=-1!=Fp(s).call(s,"from")}else{if("object"!==Qc(e.arrows))throw new Error("The arrow newOptions can only be an object or a string. Refer to the documentation. You used:"+gv(e.arrows));Sm(t.arrows,e.arrows,"to",n.arrows),Sm(t.arrows,e.arrows,"middle",n.arrows),Sm(t.arrows,e.arrows,"from",n.arrows)}else!0===i&&null===e.arrows&&(t.arrows=Kp(n.arrows));if(void 0!==e.color&&null!==e.color){var a=$y(e.color)?{color:e.color,highlight:e.color,hover:e.color,inherit:!1,opacity:1}:e.color,h=t.color;if(o)nm(h,n.color,!1,i);else for(var l in h)Object.prototype.hasOwnProperty.call(h,l)&&delete h[l];if($y(h))h.color=h,h.highlight=h,h.hover=h,h.inherit=!1,void 0===a.opacity&&(h.opacity=1);else{var d=!1;void 0!==a.color&&(h.color=a.color,d=!0),void 0!==a.highlight&&(h.highlight=a.highlight,d=!0),void 0!==a.hover&&(h.hover=a.hover,d=!0),void 0!==a.inherit&&(h.inherit=a.inherit),void 0!==a.opacity&&(h.opacity=Math.min(1,Math.max(0,a.opacity))),!0===d?h.inherit=!1:void 0===h.inherit&&(h.inherit="from")}}else!0===i&&null===e.color&&(t.color=Cm(n.color));!0===i&&null===e.font&&(t.font=Cm(n.font)),Object.prototype.hasOwnProperty.call(e,"selfReferenceSize")&&(console.warn("The selfReferenceSize property has been deprecated. Please use selfReference property instead. The selfReference can be set like thise selfReference:{size:30, angle:Math.PI / 4}"),t.selfReference.size=e.selfReferenceSize)}}]),t}(),uC=function(){function t(e,i,n){var o,r=this;Yd(this,t),this.body=e,this.images=i,this.groups=n,this.body.functions.createEdge=zn(o=this.create).call(o,this),this.edgesListeners={add:function(t,e){r.add(e.items)},update:function(t,e){r.update(e.items)},remove:function(t,e){r.remove(e.items)}},this.options={},this.defaultOptions={arrows:{to:{enabled:!1,scaleFactor:1,type:"arrow"},middle:{enabled:!1,scaleFactor:1,type:"arrow"},from:{enabled:!1,scaleFactor:1,type:"arrow"}},endPointOffset:{from:0,to:0},arrowStrikethrough:!0,color:{color:"#848484",highlight:"#848484",hover:"#848484",inherit:"from",opacity:1},dashes:!1,font:{color:"#343434",size:14,face:"arial",background:"none",strokeWidth:2,strokeColor:"#ffffff",align:"horizontal",multi:!1,vadjust:0,bold:{mod:"bold"},boldital:{mod:"bold italic"},ital:{mod:"italic"},mono:{mod:"",size:15,face:"courier new",vadjust:2}},hidden:!1,hoverWidth:1.5,label:void 0,labelHighlightBold:!0,length:void 0,physics:!0,scaling:{min:1,max:15,label:{enabled:!0,min:14,max:30,maxVisible:30,drawThreshold:5},customScalingFunction:function(t,e,i,n){if(e===t)return.5;var o=1/(e-t);return Math.max(0,(n-t)*o)}},selectionWidth:1.5,selfReference:{size:20,angle:Math.PI/4,renderBehindTheNode:!0},shadow:{enabled:!1,color:"rgba(0,0,0,0.5)",size:10,x:5,y:5},background:{enabled:!1,color:"rgba(111,111,111,1)",size:10,dashes:!1},smooth:{enabled:!0,type:"dynamic",forceDirection:"none",roundness:.5},title:void 0,width:1,value:void 0},nm(this.options,this.defaultOptions),this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t,e,i=this;this.body.emitter.on("_forceDisableDynamicCurves",(function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];"dynamic"===t&&(t="continuous");var n=!1;for(var o in i.body.edges)if(Object.prototype.hasOwnProperty.call(i.body.edges,o)){var r=i.body.edges[o],s=i.body.data.edges.get(o);if(null!=s){var a=s.smooth;void 0!==a&&!0===a.enabled&&"dynamic"===a.type&&(void 0===t?r.setOptions({smooth:!1}):r.setOptions({smooth:{type:t}}),n=!0)}}!0===e&&!0===n&&i.body.emitter.emit("_dataChanged")})),this.body.emitter.on("_dataUpdated",(function(){i.reconnectEdges()})),this.body.emitter.on("refreshEdges",zn(t=this.refresh).call(t,this)),this.body.emitter.on("refresh",zn(e=this.refresh).call(e,this)),this.body.emitter.on("destroy",(function(){hm(i.edgesListeners,(function(t,e){i.body.data.edges&&i.body.data.edges.off(e,t)})),delete i.body.functions.createEdge,delete i.edgesListeners.add,delete i.edgesListeners.update,delete i.edgesListeners.remove,delete i.edgesListeners}))}},{key:"setOptions",value:function(t){if(void 0!==t){cC.parseOptions(this.options,t,!0,this.defaultOptions,!0);var e=!1;if(void 0!==t.smooth)for(var i in this.body.edges)Object.prototype.hasOwnProperty.call(this.body.edges,i)&&(e=this.body.edges[i].updateEdgeType()||e);if(void 0!==t.font)for(var n in this.body.edges)Object.prototype.hasOwnProperty.call(this.body.edges,n)&&this.body.edges[n].updateLabelModule();void 0===t.hidden&&void 0===t.physics&&!0!==e||this.body.emitter.emit("_dataChanged")}}},{key:"setData",value:function(t){var e=this,i=arguments.length>1&&void 0!==arguments[1]&&arguments[1],n=this.body.data.edges;if(Qx("id",t))this.body.data.edges=t;else if(lu(t))this.body.data.edges=new Kx,this.body.data.edges.add(t);else{if(t)throw new TypeError("Array or DataSet expected");this.body.data.edges=new Kx}if(n&&hm(this.edgesListeners,(function(t,e){n.off(e,t)})),this.body.edges={},this.body.data.edges){hm(this.edgesListeners,(function(t,i){e.body.data.edges.on(i,t)}));var o=this.body.data.edges.getIds();this.add(o,!0)}this.body.emitter.emit("_adjustEdgesForHierarchicalLayout"),!1===i&&this.body.emitter.emit("_dataChanged")}},{key:"add",value:function(t){for(var e=arguments.length>1&&void 0!==arguments[1]&&arguments[1],i=this.body.edges,n=this.body.data.edges,o=0;o<t.length;o++){var r=t[o],s=i[r];s&&s.disconnect();var a=n.get(r,{showInternalIds:!0});i[r]=this.create(a)}this.body.emitter.emit("_adjustEdgesForHierarchicalLayout"),!1===e&&this.body.emitter.emit("_dataChanged")}},{key:"update",value:function(t){for(var e=this.body.edges,i=this.body.data.edges,n=!1,o=0;o<t.length;o++){var r=t[o],s=i.get(r),a=e[r];void 0!==a?(a.disconnect(),n=a.setOptions(s)||n,a.connect()):(this.body.edges[r]=this.create(s),n=!0)}!0===n?(this.body.emitter.emit("_adjustEdgesForHierarchicalLayout"),this.body.emitter.emit("_dataChanged")):this.body.emitter.emit("_dataUpdated")}},{key:"remove",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];if(0!==t.length){var i=this.body.edges;hm(t,(function(t){var e=i[t];void 0!==e&&e.remove()})),e&&this.body.emitter.emit("_dataChanged")}}},{key:"refresh",value:function(){var t=this;hm(this.body.edges,(function(e,i){var n=t.body.data.edges.get(i);void 0!==n&&e.setOptions(n)}))}},{key:"create",value:function(t){return new cC(t,this.body,this.images,this.options,this.defaultOptions)}},{key:"reconnectEdges",value:function(){var t,e=this.body.nodes,i=this.body.edges;for(t in e)Object.prototype.hasOwnProperty.call(e,t)&&(e[t].edges=[]);for(t in i)if(Object.prototype.hasOwnProperty.call(i,t)){var n=i[t];n.from=null,n.to=null,n.connect()}}},{key:"getConnectedNodes",value:function(t){var e=[];if(void 0!==this.body.edges[t]){var i=this.body.edges[t];void 0!==i.fromId&&e.push(i.fromId),void 0!==i.toId&&e.push(i.toId)}return e}},{key:"_updateState",value:function(){this._addMissingEdges(),this._removeInvalidEdges()}},{key:"_removeInvalidEdges",value:function(){var t=this,e=[];hm(this.body.edges,(function(i,n){var o=t.body.nodes[i.toId],r=t.body.nodes[i.fromId];void 0!==o&&!0===o.isCluster||void 0!==r&&!0===r.isCluster||void 0!==o&&void 0!==r||e.push(n)})),this.remove(e,!1)}},{key:"_addMissingEdges",value:function(){var t=this.body.data.edges;if(null!=t){var e=this.body.edges,i=[];Fu(t).call(t,(function(t,n){void 0===e[n]&&i.push(n)})),this.add(i,!0)}}}]),t}(),fC=function(){function t(e,i,n){Yd(this,t),this.body=e,this.physicsBody=i,this.barnesHutTree,this.setOptions(n),this._rng=jy("BARNES HUT SOLVER")}return Kd(t,[{key:"setOptions",value:function(t){this.options=t,this.thetaInversed=1/this.options.theta,this.overlapAvoidanceFactor=1-Math.max(0,Math.min(1,this.options.avoidOverlap))}},{key:"solve",value:function(){if(0!==this.options.gravitationalConstant&&this.physicsBody.physicsNodeIndices.length>0){var t,e=this.body.nodes,i=this.physicsBody.physicsNodeIndices,n=i.length,o=this._formBarnesHutTree(e,i);this.barnesHutTree=o;for(var r=0;r<n;r++)(t=e[i[r]]).options.mass>0&&this._getForceContributions(o.root,t)}}},{key:"_getForceContributions",value:function(t,e){this._getForceContribution(t.children.NW,e),this._getForceContribution(t.children.NE,e),this._getForceContribution(t.children.SW,e),this._getForceContribution(t.children.SE,e)}},{key:"_getForceContribution",value:function(t,e){if(t.childrenCount>0){var i=t.centerOfMass.x-e.x,n=t.centerOfMass.y-e.y,o=Math.sqrt(i*i+n*n);o*t.calcSize>this.thetaInversed?this._calculateForces(o,i,n,e,t):4===t.childrenCount?this._getForceContributions(t,e):t.children.data.id!=e.id&&this._calculateForces(o,i,n,e,t)}}},{key:"_calculateForces",value:function(t,e,i,n,o){0===t&&(e=t=.1),this.overlapAvoidanceFactor<1&&n.shape.radius&&(t=Math.max(.1+this.overlapAvoidanceFactor*n.shape.radius,t-n.shape.radius));var r=this.options.gravitationalConstant*o.mass*n.options.mass/Math.pow(t,3),s=e*r,a=i*r;this.physicsBody.forces[n.id].x+=s,this.physicsBody.forces[n.id].y+=a}},{key:"_formBarnesHutTree",value:function(t,e){for(var i,n=e.length,o=t[e[0]].x,r=t[e[0]].y,s=t[e[0]].x,a=t[e[0]].y,h=1;h<n;h++){var l=t[e[h]],d=l.x,c=l.y;l.options.mass>0&&(d<o&&(o=d),d>s&&(s=d),c<r&&(r=c),c>a&&(a=c))}var u=Math.abs(s-o)-Math.abs(a-r);u>0?(r-=.5*u,a+=.5*u):(o+=.5*u,s-=.5*u);var f=Math.max(1e-5,Math.abs(s-o)),p=.5*f,v=.5*(o+s),g=.5*(r+a),y={root:{centerOfMass:{x:0,y:0},mass:0,range:{minX:v-p,maxX:v+p,minY:g-p,maxY:g+p},size:f,calcSize:1/f,children:{data:null},maxWidth:0,level:0,childrenCount:4}};this._splitBranch(y.root);for(var m=0;m<n;m++)(i=t[e[m]]).options.mass>0&&this._placeInTree(y.root,i);return y}},{key:"_updateBranchMass",value:function(t,e){var i=t.centerOfMass,n=t.mass+e.options.mass,o=1/n;i.x=i.x*t.mass+e.x*e.options.mass,i.x*=o,i.y=i.y*t.mass+e.y*e.options.mass,i.y*=o,t.mass=n;var r=Math.max(Math.max(e.height,e.radius),e.width);t.maxWidth=t.maxWidth<r?r:t.maxWidth}},{key:"_placeInTree",value:function(t,e,i){1==i&&void 0!==i||this._updateBranchMass(t,e);var n,o=t.children.NW.range;n=o.maxX>e.x?o.maxY>e.y?"NW":"SW":o.maxY>e.y?"NE":"SE",this._placeInRegion(t,e,n)}},{key:"_placeInRegion",value:function(t,e,i){var n=t.children[i];switch(n.childrenCount){case 0:n.children.data=e,n.childrenCount=1,this._updateBranchMass(n,e);break;case 1:n.children.data.x===e.x&&n.children.data.y===e.y?(e.x+=this._rng(),e.y+=this._rng()):(this._splitBranch(n),this._placeInTree(n,e));break;case 4:this._placeInTree(n,e)}}},{key:"_splitBranch",value:function(t){var e=null;1===t.childrenCount&&(e=t.children.data,t.mass=0,t.centerOfMass.x=0,t.centerOfMass.y=0),t.childrenCount=4,t.children.data=null,this._insertRegion(t,"NW"),this._insertRegion(t,"NE"),this._insertRegion(t,"SW"),this._insertRegion(t,"SE"),null!=e&&this._placeInTree(t,e)}},{key:"_insertRegion",value:function(t,e){var i,n,o,r,s=.5*t.size;switch(e){case"NW":i=t.range.minX,n=t.range.minX+s,o=t.range.minY,r=t.range.minY+s;break;case"NE":i=t.range.minX+s,n=t.range.maxX,o=t.range.minY,r=t.range.minY+s;break;case"SW":i=t.range.minX,n=t.range.minX+s,o=t.range.minY+s,r=t.range.maxY;break;case"SE":i=t.range.minX+s,n=t.range.maxX,o=t.range.minY+s,r=t.range.maxY}t.children[e]={centerOfMass:{x:0,y:0},mass:0,range:{minX:i,maxX:n,minY:o,maxY:r},size:.5*t.size,calcSize:2*t.calcSize,children:{data:null},maxWidth:0,level:t.level+1,childrenCount:0}}},{key:"_debug",value:function(t,e){void 0!==this.barnesHutTree&&(t.lineWidth=1,this._drawBranch(this.barnesHutTree.root,t,e))}},{key:"_drawBranch",value:function(t,e,i){void 0===i&&(i="#FF0000"),4===t.childrenCount&&(this._drawBranch(t.children.NW,e),this._drawBranch(t.children.NE,e),this._drawBranch(t.children.SE,e),this._drawBranch(t.children.SW,e)),e.strokeStyle=i,e.beginPath(),e.moveTo(t.range.minX,t.range.minY),e.lineTo(t.range.maxX,t.range.minY),e.stroke(),e.beginPath(),e.moveTo(t.range.maxX,t.range.minY),e.lineTo(t.range.maxX,t.range.maxY),e.stroke(),e.beginPath(),e.moveTo(t.range.maxX,t.range.maxY),e.lineTo(t.range.minX,t.range.maxY),e.stroke(),e.beginPath(),e.moveTo(t.range.minX,t.range.maxY),e.lineTo(t.range.minX,t.range.minY),e.stroke()}}]),t}(),pC=function(){function t(e,i,n){Yd(this,t),this._rng=jy("REPULSION SOLVER"),this.body=e,this.physicsBody=i,this.setOptions(n)}return Kd(t,[{key:"setOptions",value:function(t){this.options=t}},{key:"solve",value:function(){for(var t,e,i,n,o,r,s,a,h=this.body.nodes,l=this.physicsBody.physicsNodeIndices,d=this.physicsBody.forces,c=this.options.nodeDistance,u=-2/3/c,f=0;f<l.length-1;f++){s=h[l[f]];for(var p=f+1;p<l.length;p++)t=(a=h[l[p]]).x-s.x,e=a.y-s.y,0===(i=Math.sqrt(t*t+e*e))&&(t=i=.1*this._rng()),i<2*c&&(r=i<.5*c?1:u*i+1.3333333333333333,n=t*(r/=i),o=e*r,d[s.id].x-=n,d[s.id].y-=o,d[a.id].x+=n,d[a.id].y+=o)}}}]),t}(),vC=function(){function t(e,i,n){Yd(this,t),this.body=e,this.physicsBody=i,this.setOptions(n)}return Kd(t,[{key:"setOptions",value:function(t){this.options=t,this.overlapAvoidanceFactor=Math.max(0,Math.min(1,this.options.avoidOverlap||0))}},{key:"solve",value:function(){for(var t=this.body.nodes,e=this.physicsBody.physicsNodeIndices,i=this.physicsBody.forces,n=this.options.nodeDistance,o=0;o<e.length-1;o++)for(var r=t[e[o]],s=o+1;s<e.length;s++){var a=t[e[s]];if(r.level===a.level){var h=n+this.overlapAvoidanceFactor*((r.shape.radius||0)/2+(a.shape.radius||0)/2),l=a.x-r.x,d=a.y-r.y,c=Math.sqrt(l*l+d*d),u=void 0;u=c<h?-Math.pow(.05*c,2)+Math.pow(.05*h,2):0,0!==c&&(u/=c);var f=l*u,p=d*u;i[r.id].x-=f,i[r.id].y-=p,i[a.id].x+=f,i[a.id].y+=p}}}}]),t}(),gC=function(){function t(e,i,n){Yd(this,t),this.body=e,this.physicsBody=i,this.setOptions(n)}return Kd(t,[{key:"setOptions",value:function(t){this.options=t}},{key:"solve",value:function(){for(var t,e,i,n,o,r=this.physicsBody.physicsEdgeIndices,s=this.body.edges,a=0;a<r.length;a++)!0===(e=s[r[a]]).connected&&e.toId!==e.fromId&&void 0!==this.body.nodes[e.toId]&&void 0!==this.body.nodes[e.fromId]&&(void 0!==e.edgeType.via?(t=void 0===e.options.length?this.options.springLength:e.options.length,i=e.to,n=e.edgeType.via,o=e.from,this._calculateSpringForce(i,n,.5*t),this._calculateSpringForce(n,o,.5*t)):(t=void 0===e.options.length?1.5*this.options.springLength:e.options.length,this._calculateSpringForce(e.from,e.to,t)))}},{key:"_calculateSpringForce",value:function(t,e,i){var n=t.x-e.x,o=t.y-e.y,r=Math.max(Math.sqrt(n*n+o*o),.01),s=this.options.springConstant*(i-r)/r,a=n*s,h=o*s;void 0!==this.physicsBody.forces[t.id]&&(this.physicsBody.forces[t.id].x+=a,this.physicsBody.forces[t.id].y+=h),void 0!==this.physicsBody.forces[e.id]&&(this.physicsBody.forces[e.id].x-=a,this.physicsBody.forces[e.id].y-=h)}}]),t}(),yC=function(){function t(e,i,n){Yd(this,t),this.body=e,this.physicsBody=i,this.setOptions(n)}return Kd(t,[{key:"setOptions",value:function(t){this.options=t}},{key:"solve",value:function(){for(var t,e,i,n,o,r,s,a,h,l,d=this.body.edges,c=.5,u=this.physicsBody.physicsEdgeIndices,f=this.physicsBody.physicsNodeIndices,p=this.physicsBody.forces,v=0;v<f.length;v++){var g=f[v];p[g].springFx=0,p[g].springFy=0}for(var y=0;y<u.length;y++)!0===(e=d[u[y]]).connected&&(t=void 0===e.options.length?this.options.springLength:e.options.length,i=e.from.x-e.to.x,n=e.from.y-e.to.y,a=0===(a=Math.sqrt(i*i+n*n))?.01:a,o=i*(s=this.options.springConstant*(t-a)/a),r=n*s,e.to.level!=e.from.level?(void 0!==p[e.toId]&&(p[e.toId].springFx-=o,p[e.toId].springFy-=r),void 0!==p[e.fromId]&&(p[e.fromId].springFx+=o,p[e.fromId].springFy+=r)):(void 0!==p[e.toId]&&(p[e.toId].x-=c*o,p[e.toId].y-=c*r),void 0!==p[e.fromId]&&(p[e.fromId].x+=c*o,p[e.fromId].y+=c*r)));s=1;for(var m=0;m<f.length;m++){var b=f[m];h=Math.min(s,Math.max(-s,p[b].springFx)),l=Math.min(s,Math.max(-s,p[b].springFy)),p[b].x+=h,p[b].y+=l}for(var w=0,k=0,_=0;_<f.length;_++){var x=f[_];w+=p[x].x,k+=p[x].y}for(var E=w/f.length,O=k/f.length,C=0;C<f.length;C++){var S=f[C];p[S].x-=E,p[S].y-=O}}}]),t}(),mC=function(){function t(e,i,n){Yd(this,t),this.body=e,this.physicsBody=i,this.setOptions(n)}return Kd(t,[{key:"setOptions",value:function(t){this.options=t}},{key:"solve",value:function(){for(var t,e,i,n,o=this.body.nodes,r=this.physicsBody.physicsNodeIndices,s=this.physicsBody.forces,a=0;a<r.length;a++){t=-(n=o[r[a]]).x,e=-n.y,i=Math.sqrt(t*t+e*e),this._calculateForces(i,t,e,s,n)}}},{key:"_calculateForces",value:function(t,e,i,n,o){var r=0===t?0:this.options.centralGravity/t;n[o.id].x=e*r,n[o.id].y=i*r}}]),t}();function bC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var wC=function(t){zk(i,t);var e=bC(i);function i(t,n,o){var r;return Yd(this,i),(r=e.call(this,t,n,o))._rng=jy("FORCE ATLAS 2 BASED REPULSION SOLVER"),r}return Kd(i,[{key:"_calculateForces",value:function(t,e,i,n,o){0===t&&(e=t=.1*this._rng()),this.overlapAvoidanceFactor<1&&n.shape.radius&&(t=Math.max(.1+this.overlapAvoidanceFactor*n.shape.radius,t-n.shape.radius));var r=n.edges.length+1,s=this.options.gravitationalConstant*o.mass*n.options.mass*r/Math.pow(t,2),a=e*s,h=i*s;this.physicsBody.forces[n.id].x+=a,this.physicsBody.forces[n.id].y+=h}}]),i}(fC);function kC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var _C=function(t){zk(i,t);var e=kC(i);function i(t,n,o){return Yd(this,i),e.call(this,t,n,o)}return Kd(i,[{key:"_calculateForces",value:function(t,e,i,n,o){if(t>0){var r=o.edges.length+1,s=this.options.centralGravity*r*o.options.mass;n[o.id].x=e*s,n[o.id].y=i*s}}}]),i}(mC),xC=function(){function t(e){Yd(this,t),this.body=e,this.physicsBody={physicsNodeIndices:[],physicsEdgeIndices:[],forces:{},velocities:{}},this.physicsEnabled=!0,this.simulationInterval=1e3/60,this.requiresTimeout=!0,this.previousStates={},this.referenceState={},this.freezeCache={},this.renderTimer=void 0,this.adaptiveTimestep=!1,this.adaptiveTimestepEnabled=!1,this.adaptiveCounter=0,this.adaptiveInterval=3,this.stabilized=!1,this.startedStabilization=!1,this.stabilizationIterations=0,this.ready=!1,this.options={},this.defaultOptions={enabled:!0,barnesHut:{theta:.5,gravitationalConstant:-2e3,centralGravity:.3,springLength:95,springConstant:.04,damping:.09,avoidOverlap:0},forceAtlas2Based:{theta:.5,gravitationalConstant:-50,centralGravity:.01,springConstant:.08,springLength:100,damping:.4,avoidOverlap:0},repulsion:{centralGravity:.2,springLength:200,springConstant:.05,nodeDistance:100,damping:.09,avoidOverlap:0},hierarchicalRepulsion:{centralGravity:0,springLength:100,springConstant:.01,nodeDistance:120,damping:.09},maxVelocity:50,minVelocity:.75,solver:"barnesHut",stabilization:{enabled:!0,iterations:1e3,updateInterval:50,onlyDynamicEdges:!1,fit:!0},timestep:.5,adaptiveTimestep:!0,wind:{x:0,y:0}},un(this.options,this.defaultOptions),this.timestep=.5,this.layoutFailed=!1,this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t=this;this.body.emitter.on("initPhysics",(function(){t.initPhysics()})),this.body.emitter.on("_layoutFailed",(function(){t.layoutFailed=!0})),this.body.emitter.on("resetPhysics",(function(){t.stopSimulation(),t.ready=!1})),this.body.emitter.on("disablePhysics",(function(){t.physicsEnabled=!1,t.stopSimulation()})),this.body.emitter.on("restorePhysics",(function(){t.setOptions(t.options),!0===t.ready&&t.startSimulation()})),this.body.emitter.on("startSimulation",(function(){!0===t.ready&&t.startSimulation()})),this.body.emitter.on("stopSimulation",(function(){t.stopSimulation()})),this.body.emitter.on("destroy",(function(){t.stopSimulation(!1),t.body.emitter.off()})),this.body.emitter.on("_dataChanged",(function(){t.updatePhysicsData()}))}},{key:"setOptions",value:function(t){if(void 0!==t)if(!1===t)this.options.enabled=!1,this.physicsEnabled=!1,this.stopSimulation();else if(!0===t)this.options.enabled=!0,this.physicsEnabled=!0,this.startSimulation();else{this.physicsEnabled=!0,im(["stabilization"],this.options,t),Sm(this.options,t,"stabilization"),void 0===t.enabled&&(this.options.enabled=!0),!1===this.options.enabled&&(this.physicsEnabled=!1,this.stopSimulation());var e=this.options.wind;e&&(("number"!=typeof e.x||ek(e.x))&&(e.x=0),("number"!=typeof e.y||ek(e.y))&&(e.y=0)),this.timestep=this.options.timestep}this.init()}},{key:"init",value:function(){var t;"forceAtlas2Based"===this.options.solver?(t=this.options.forceAtlas2Based,this.nodesSolver=new wC(this.body,this.physicsBody,t),this.edgesSolver=new gC(this.body,this.physicsBody,t),this.gravitySolver=new _C(this.body,this.physicsBody,t)):"repulsion"===this.options.solver?(t=this.options.repulsion,this.nodesSolver=new pC(this.body,this.physicsBody,t),this.edgesSolver=new gC(this.body,this.physicsBody,t),this.gravitySolver=new mC(this.body,this.physicsBody,t)):"hierarchicalRepulsion"===this.options.solver?(t=this.options.hierarchicalRepulsion,this.nodesSolver=new vC(this.body,this.physicsBody,t),this.edgesSolver=new yC(this.body,this.physicsBody,t),this.gravitySolver=new mC(this.body,this.physicsBody,t)):(t=this.options.barnesHut,this.nodesSolver=new fC(this.body,this.physicsBody,t),this.edgesSolver=new gC(this.body,this.physicsBody,t),this.gravitySolver=new mC(this.body,this.physicsBody,t)),this.modelOptions=t}},{key:"initPhysics",value:function(){!0===this.physicsEnabled&&!0===this.options.enabled?!0===this.options.stabilization.enabled?this.stabilize():(this.stabilized=!1,this.ready=!0,this.body.emitter.emit("fit",{},this.layoutFailed),this.startSimulation()):(this.ready=!0,this.body.emitter.emit("fit"))}},{key:"startSimulation",value:function(){var t;!0===this.physicsEnabled&&!0===this.options.enabled?(this.stabilized=!1,this.adaptiveTimestep=!1,this.body.emitter.emit("_resizeNodes"),void 0===this.viewFunction&&(this.viewFunction=zn(t=this.simulationStep).call(t,this),this.body.emitter.on("initRedraw",this.viewFunction),this.body.emitter.emit("_startRendering"))):this.body.emitter.emit("_redraw")}},{key:"stopSimulation",value:function(){var t=!(arguments.length>0&&void 0!==arguments[0])||arguments[0];this.stabilized=!0,!0===t&&this._emitStabilized(),void 0!==this.viewFunction&&(this.body.emitter.off("initRedraw",this.viewFunction),this.viewFunction=void 0,!0===t&&this.body.emitter.emit("_stopRendering"))}},{key:"simulationStep",value:function(){var t=Eu();this.physicsTick(),(Eu()-t<.4*this.simulationInterval||!0===this.runDoubleSpeed)&&!1===this.stabilized&&(this.physicsTick(),this.runDoubleSpeed=!0),!0===this.stabilized&&this.stopSimulation()}},{key:"_emitStabilized",value:function(){var t=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.stabilizationIterations;(this.stabilizationIterations>1||!0===this.startedStabilization)&&Sv((function(){t.body.emitter.emit("stabilized",{iterations:e}),t.startedStabilization=!1,t.stabilizationIterations=0}),0)}},{key:"physicsStep",value:function(){this.gravitySolver.solve(),this.nodesSolver.solve(),this.edgesSolver.solve(),this.moveNodes()}},{key:"adjustTimeStep",value:function(){!0===this._evaluateStepQuality()?this.timestep=1.2*this.timestep:this.timestep/1.2<this.options.timestep?this.timestep=this.options.timestep:(this.adaptiveCounter=-1,this.timestep=Math.max(this.options.timestep,this.timestep/1.2))}},{key:"physicsTick",value:function(){if(this._startStabilizing(),!0!==this.stabilized){if(!0===this.adaptiveTimestep&&!0===this.adaptiveTimestepEnabled)this.adaptiveCounter%this.adaptiveInterval==0?(this.timestep=2*this.timestep,this.physicsStep(),this.revert(),this.timestep=.5*this.timestep,this.physicsStep(),this.physicsStep(),this.adjustTimeStep()):this.physicsStep(),this.adaptiveCounter+=1;else this.timestep=this.options.timestep,this.physicsStep();!0===this.stabilized&&this.revert(),this.stabilizationIterations++}}},{key:"updatePhysicsData",value:function(){this.physicsBody.forces={},this.physicsBody.physicsNodeIndices=[],this.physicsBody.physicsEdgeIndices=[];var t=this.body.nodes,e=this.body.edges;for(var i in t)Object.prototype.hasOwnProperty.call(t,i)&&!0===t[i].options.physics&&this.physicsBody.physicsNodeIndices.push(t[i].id);for(var n in e)Object.prototype.hasOwnProperty.call(e,n)&&!0===e[n].options.physics&&this.physicsBody.physicsEdgeIndices.push(e[n].id);for(var o=0;o<this.physicsBody.physicsNodeIndices.length;o++){var r=this.physicsBody.physicsNodeIndices[o];this.physicsBody.forces[r]={x:0,y:0},void 0===this.physicsBody.velocities[r]&&(this.physicsBody.velocities[r]={x:0,y:0})}for(var s in this.physicsBody.velocities)void 0===t[s]&&delete this.physicsBody.velocities[s]}},{key:"revert",value:function(){var t=bu(this.previousStates),e=this.body.nodes,i=this.physicsBody.velocities;this.referenceState={};for(var n=0;n<t.length;n++){var o=t[n];void 0!==e[o]?!0===e[o].options.physics&&(this.referenceState[o]={positions:{x:e[o].x,y:e[o].y}},i[o].x=this.previousStates[o].vx,i[o].y=this.previousStates[o].vy,e[o].x=this.previousStates[o].x,e[o].y=this.previousStates[o].y):delete this.previousStates[o]}}},{key:"_evaluateStepQuality",value:function(){var t,e,i=this.body.nodes,n=this.referenceState;for(var o in this.referenceState)if(Object.prototype.hasOwnProperty.call(this.referenceState,o)&&void 0!==i[o]&&(t=i[o].x-n[o].positions.x,e=i[o].y-n[o].positions.y,Math.sqrt(Math.pow(t,2)+Math.pow(e,2))>.3))return!1;return!0}},{key:"moveNodes",value:function(){for(var t=this.physicsBody.physicsNodeIndices,e=0,i=0,n=0;n<t.length;n++){var o=t[n],r=this._performStep(o);e=Math.max(e,r),i+=r}this.adaptiveTimestepEnabled=i/t.length<5,this.stabilized=e<this.options.minVelocity}},{key:"calculateComponentVelocity",value:function(t,e,i){t+=(e-this.modelOptions.damping*t)/i*this.timestep;var n=this.options.maxVelocity||1e9;return Math.abs(t)>n&&(t=t>0?n:-n),t}},{key:"_performStep",value:function(t){var e=this.body.nodes[t],i=this.physicsBody.forces[t];this.options.wind&&(i.x+=this.options.wind.x,i.y+=this.options.wind.y);var n=this.physicsBody.velocities[t];return this.previousStates[t]={x:e.x,y:e.y,vx:n.x,vy:n.y},!1===e.options.fixed.x?(n.x=this.calculateComponentVelocity(n.x,i.x,e.options.mass),e.x+=n.x*this.timestep):(i.x=0,n.x=0),!1===e.options.fixed.y?(n.y=this.calculateComponentVelocity(n.y,i.y,e.options.mass),e.y+=n.y*this.timestep):(i.y=0,n.y=0),Math.sqrt(Math.pow(n.x,2)+Math.pow(n.y,2))}},{key:"_freezeNodes",value:function(){var t=this.body.nodes;for(var e in t)if(Object.prototype.hasOwnProperty.call(t,e)&&t[e].x&&t[e].y){var i=t[e].options.fixed;this.freezeCache[e]={x:i.x,y:i.y},i.x=!0,i.y=!0}}},{key:"_restoreFrozenNodes",value:function(){var t=this.body.nodes;for(var e in t)Object.prototype.hasOwnProperty.call(t,e)&&void 0!==this.freezeCache[e]&&(t[e].options.fixed.x=this.freezeCache[e].x,t[e].options.fixed.y=this.freezeCache[e].y);this.freezeCache={}}},{key:"stabilize",value:function(){var t=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.stabilization.iterations;"number"!=typeof e&&(e=this.options.stabilization.iterations,console.error("The stabilize method needs a numeric amount of iterations. Switching to default: ",e)),0!==this.physicsBody.physicsNodeIndices.length?(this.adaptiveTimestep=this.options.adaptiveTimestep,this.body.emitter.emit("_resizeNodes"),this.stopSimulation(),this.stabilized=!1,this.body.emitter.emit("_blockRedraw"),this.targetIterations=e,!0===this.options.stabilization.onlyDynamicEdges&&this._freezeNodes(),this.stabilizationIterations=0,Sv((function(){return t._stabilizationBatch()}),0)):this.ready=!0}},{key:"_startStabilizing",value:function(){return!0!==this.startedStabilization&&(this.body.emitter.emit("startStabilizing"),this.startedStabilization=!0,!0)}},{key:"_stabilizationBatch",value:function(){var t=this,e=function(){return!1===t.stabilized&&t.stabilizationIterations<t.targetIterations},i=function(){t.body.emitter.emit("stabilizationProgress",{iterations:t.stabilizationIterations,total:t.targetIterations})};this._startStabilizing()&&i();for(var n,o=0;e()&&o<this.options.stabilization.updateInterval;)this.physicsTick(),o++;(i(),e())?Sv(zn(n=this._stabilizationBatch).call(n,this),0):this._finalizeStabilization()}},{key:"_finalizeStabilization",value:function(){this.body.emitter.emit("_allowRedraw"),!0===this.options.stabilization.fit&&this.body.emitter.emit("fit"),!0===this.options.stabilization.onlyDynamicEdges&&this._restoreFrozenNodes(),this.body.emitter.emit("stabilizationIterationsDone"),this.body.emitter.emit("_requestRedraw"),!0===this.stabilized?this._emitStabilized():this.startSimulation(),this.ready=!0}},{key:"_drawForces",value:function(t){for(var e=0;e<this.physicsBody.physicsNodeIndices.length;e++){var i=this.physicsBody.physicsNodeIndices[e],n=this.body.nodes[i],o=this.physicsBody.forces[i],r=Math.sqrt(Math.pow(o.x,2)+Math.pow(o.x,2)),s=Math.min(Math.max(5,r),15),a=3*s,h=km((180-180*Math.min(1,Math.max(0,.03*r)))/360,1,1),l={x:n.x+20*o.x,y:n.y+20*o.y};t.lineWidth=s,t.strokeStyle=h,t.beginPath(),t.moveTo(n.x,n.y),t.lineTo(l.x,l.y),t.stroke();var d=Math.atan2(o.y,o.x);t.fillStyle=h,XO.draw(t,{type:"arrow",point:l,angle:d,length:a}),jv(t).call(t)}}}]),t}(),EC=function(){function t(){Yd(this,t)}return Kd(t,null,[{key:"getRange",value:function(t){var e,i=arguments.length>1&&void 0!==arguments[1]?arguments[1]:[],n=1e9,o=-1e9,r=1e9,s=-1e9;if(i.length>0)for(var a=0;a<i.length;a++)r>(e=t[i[a]]).shape.boundingBox.left&&(r=e.shape.boundingBox.left),s<e.shape.boundingBox.right&&(s=e.shape.boundingBox.right),n>e.shape.boundingBox.top&&(n=e.shape.boundingBox.top),o<e.shape.boundingBox.bottom&&(o=e.shape.boundingBox.bottom);return 1e9===r&&-1e9===s&&1e9===n&&-1e9===o&&(n=0,o=0,r=0,s=0),{minX:r,maxX:s,minY:n,maxY:o}}},{key:"getRangeCore",value:function(t){var e,i=arguments.length>1&&void 0!==arguments[1]?arguments[1]:[],n=1e9,o=-1e9,r=1e9,s=-1e9;if(i.length>0)for(var a=0;a<i.length;a++)r>(e=t[i[a]]).x&&(r=e.x),s<e.x&&(s=e.x),n>e.y&&(n=e.y),o<e.y&&(o=e.y);return 1e9===r&&-1e9===s&&1e9===n&&-1e9===o&&(n=0,o=0,r=0,s=0),{minX:r,maxX:s,minY:n,maxY:o}}},{key:"findCenter",value:function(t){return{x:.5*(t.maxX+t.minX),y:.5*(t.maxY+t.minY)}}},{key:"cloneOptions",value:function(t,e){var i={};return void 0===e||"node"===e?(nm(i,t.options,!0),i.x=t.x,i.y=t.y,i.amountOfConnections=t.edges.length):nm(i,t.options,!0),i}}]),t}();function OC(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var CC=function(t){zk(i,t);var e=OC(i);function i(t,n,o,r,s,a){var h;return Yd(this,i),(h=e.call(this,t,n,o,r,s,a)).isCluster=!0,h.containedNodes={},h.containedEdges={},h}return Kd(i,[{key:"_openChildCluster",value:function(t){var e=this,i=this.body.nodes[t];if(void 0===this.containedNodes[t])throw new Error("node with id: "+t+" not in current cluster");if(!i.isCluster)throw new Error("node with id: "+t+" is not a cluster");delete this.containedNodes[t],hm(i.edges,(function(t){delete e.containedEdges[t.id]})),hm(i.containedNodes,(function(t,i){e.containedNodes[i]=t})),i.containedNodes={},hm(i.containedEdges,(function(t,i){e.containedEdges[i]=t})),i.containedEdges={},hm(i.edges,(function(t){hm(e.edges,(function(i){var n,o,r=Fp(n=i.clusteringEdgeReplacingIds).call(n,t.id);-1!==r&&(hm(t.clusteringEdgeReplacingIds,(function(t){i.clusteringEdgeReplacingIds.push(t),e.body.edges[t].edgeReplacedById=i.id})),ff(o=i.clusteringEdgeReplacingIds).call(o,r,1))}))})),i.edges=[]}}]),i}(fO),SC=function(){function t(e){var i=this;Yd(this,t),this.body=e,this.clusteredNodes={},this.clusteredEdges={},this.options={},this.defaultOptions={},un(this.options,this.defaultOptions),this.body.emitter.on("_resetData",(function(){i.clusteredNodes={},i.clusteredEdges={}}))}return Kd(t,[{key:"clusterByHubsize",value:function(t,e){void 0===t?t=this._getHubSize():"object"===Qc(t)&&(e=this._checkOptions(t),t=this._getHubSize());for(var i=[],n=0;n<this.body.nodeIndices.length;n++){var o=this.body.nodes[this.body.nodeIndices[n]];o.edges.length>=t&&i.push(o.id)}for(var r=0;r<i.length;r++)this.clusterByConnection(i[r],e,!0);this.body.emitter.emit("_dataChanged")}},{key:"cluster",value:function(){var t=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},i=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];if(void 0===e.joinCondition)throw new Error("Cannot call clusterByNodeData without a joinCondition function in the options.");e=this._checkOptions(e);var n={},o={};hm(this.body.nodes,(function(i,r){i.options&&!0===e.joinCondition(i.options)&&(n[r]=i,hm(i.edges,(function(e){void 0===t.clusteredEdges[e.id]&&(o[e.id]=e)})))})),this._cluster(n,o,e,i)}},{key:"clusterByEdgeCount",value:function(t,e){var i=this,n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];e=this._checkOptions(e);for(var o,r,s,a=[],h={},l=function(n){var l={},d={},c=i.body.nodeIndices[n],u=i.body.nodes[c];if(void 0===h[c]){s=0,r=[];for(var f=0;f<u.edges.length;f++)o=u.edges[f],void 0===i.clusteredEdges[o.id]&&(o.toId!==o.fromId&&s++,r.push(o));if(s===t){for(var p=function(t){if(void 0===e.joinCondition||null===e.joinCondition)return!0;var i=EC.cloneOptions(t);return e.joinCondition(i)},v=!0,g=0;g<r.length;g++){o=r[g];var y=i._getConnectedId(o,c);if(!p(u)){v=!1;break}d[o.id]=o,l[c]=u,l[y]=i.body.nodes[y],h[c]=!0}if(bu(l).length>0&&bu(d).length>0&&!0===v){var m=function(){for(var t=0;t<a.length;++t)for(var e in l)if(void 0!==a[t].nodes[e])return a[t]}();if(void 0!==m){for(var b in l)void 0===m.nodes[b]&&(m.nodes[b]=l[b]);for(var w in d)void 0===m.edges[w]&&(m.edges[w]=d[w])}else a.push({nodes:l,edges:d})}}}},d=0;d<this.body.nodeIndices.length;d++)l(d);for(var c=0;c<a.length;c++)this._cluster(a[c].nodes,a[c].edges,e,!1);!0===n&&this.body.emitter.emit("_dataChanged")}},{key:"clusterOutliers",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];this.clusterByEdgeCount(1,t,e)}},{key:"clusterBridges",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];this.clusterByEdgeCount(2,t,e)}},{key:"clusterByConnection",value:function(t,e){var i,n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];if(void 0===t)throw new Error("No nodeId supplied to clusterByConnection!");if(void 0===this.body.nodes[t])throw new Error("The nodeId given to clusterByConnection does not exist!");var o=this.body.nodes[t];void 0===(e=this._checkOptions(e,o)).clusterNodeProperties.x&&(e.clusterNodeProperties.x=o.x),void 0===e.clusterNodeProperties.y&&(e.clusterNodeProperties.y=o.y),void 0===e.clusterNodeProperties.fixed&&(e.clusterNodeProperties.fixed={},e.clusterNodeProperties.fixed.x=o.options.fixed.x,e.clusterNodeProperties.fixed.y=o.options.fixed.y);var r={},s={},a=o.id,h=EC.cloneOptions(o);r[a]=o;for(var l=0;l<o.edges.length;l++){var d=o.edges[l];if(void 0===this.clusteredEdges[d.id]){var c=this._getConnectedId(d,a);if(void 0===this.clusteredNodes[c])if(c!==a)if(void 0===e.joinCondition)s[d.id]=d,r[c]=this.body.nodes[c];else{var u=EC.cloneOptions(this.body.nodes[c]);!0===e.joinCondition(h,u)&&(s[d.id]=d,r[c]=this.body.nodes[c])}else s[d.id]=d}}var f=gu(i=bu(r)).call(i,(function(t){return r[t].id}));for(var p in r)if(Object.prototype.hasOwnProperty.call(r,p))for(var v=r[p],g=0;g<v.edges.length;g++){var y=v.edges[g];Fp(f).call(f,this._getConnectedId(y,v.id))>-1&&(s[y.id]=y)}this._cluster(r,s,e,n)}},{key:"_createClusterEdges",value:function(t,e,i,n){for(var o,r,s,a,h,l,d=bu(t),c=[],u=0;u<d.length;u++){s=t[r=d[u]];for(var f=0;f<s.edges.length;f++)o=s.edges[f],void 0===this.clusteredEdges[o.id]&&(o.toId==o.fromId?e[o.id]=o:o.toId==r?(a=i.id,l=h=o.fromId):(a=o.toId,h=i.id,l=a),void 0===t[l]&&c.push({edge:o,fromId:h,toId:a}))}for(var p=[],v=function(t){for(var e=0;e<p.length;e++){var i=p[e],n=t.fromId===i.fromId&&t.toId===i.toId,o=t.fromId===i.toId&&t.toId===i.fromId;if(n||o)return i}return null},g=0;g<c.length;g++){var y=c[g],m=y.edge,b=v(y);null===b?(b=this._createClusteredEdge(y.fromId,y.toId,m,n),p.push(b)):b.clusteringEdgeReplacingIds.push(m.id),this.body.edges[m.id].edgeReplacedById=b.id,this._backupEdgeOptions(m),m.setOptions({physics:!1})}}},{key:"_checkOptions",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};return void 0===t.clusterEdgeProperties&&(t.clusterEdgeProperties={}),void 0===t.clusterNodeProperties&&(t.clusterNodeProperties={}),t}},{key:"_cluster",value:function(t,e,i){var n=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],o=[];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&void 0!==this.clusteredNodes[r]&&o.push(r);for(var s=0;s<o.length;++s)delete t[o[s]];if(0!=bu(t).length&&(1!=bu(t).length||1==i.clusterNodeProperties.allowSingleNodeCluster)){var a=nm({},i.clusterNodeProperties);if(void 0!==i.processProperties){var h=[];for(var l in t)if(Object.prototype.hasOwnProperty.call(t,l)){var d=EC.cloneOptions(t[l]);h.push(d)}var c=[];for(var u in e)if(Object.prototype.hasOwnProperty.call(e,u)&&"clusterEdge:"!==u.substr(0,12)){var f=EC.cloneOptions(e[u],"edge");c.push(f)}if(!(a=i.processProperties(a,h,c)))throw new Error("The processProperties function does not return properties!")}void 0===a.id&&(a.id="cluster:"+Ax());var p=a.id;void 0===a.label&&(a.label="cluster");var v=void 0;void 0===a.x&&(v=this._getClusterPosition(t),a.x=v.x),void 0===a.y&&(void 0===v&&(v=this._getClusterPosition(t)),a.y=v.y),a.id=p;var g=this.body.functions.createNode(a,CC);g.containedNodes=t,g.containedEdges=e,g.clusterEdgeProperties=i.clusterEdgeProperties,this.body.nodes[a.id]=g,this._clusterEdges(t,e,a,i.clusterEdgeProperties),a.id=void 0,!0===n&&this.body.emitter.emit("_dataChanged")}}},{key:"_backupEdgeOptions",value:function(t){void 0===this.clusteredEdges[t.id]&&(this.clusteredEdges[t.id]={physics:t.options.physics})}},{key:"_restoreEdge",value:function(t){var e=this.clusteredEdges[t.id];void 0!==e&&(t.setOptions({physics:e.physics}),delete this.clusteredEdges[t.id])}},{key:"isCluster",value:function(t){return void 0!==this.body.nodes[t]?!0===this.body.nodes[t].isCluster:(console.error("Node does not exist."),!1)}},{key:"_getClusterPosition",value:function(t){for(var e,i=bu(t),n=t[i[0]].x,o=t[i[0]].x,r=t[i[0]].y,s=t[i[0]].y,a=1;a<i.length;a++)n=(e=t[i[a]]).x<n?e.x:n,o=e.x>o?e.x:o,r=e.y<r?e.y:r,s=e.y>s?e.y:s;return{x:.5*(n+o),y:.5*(r+s)}}},{key:"openCluster",value:function(t,e){var i=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];if(void 0===t)throw new Error("No clusterNodeId supplied to openCluster.");var n=this.body.nodes[t];if(void 0===n)throw new Error("The clusterNodeId supplied to openCluster does not exist.");if(!0!==n.isCluster||void 0===n.containedNodes||void 0===n.containedEdges)throw new Error("The node:"+t+" is not a valid cluster.");var o=this.findNode(t),r=Fp(o).call(o,t)-1;if(r>=0){var s=o[r],a=this.body.nodes[s];return a._openChildCluster(t),delete this.body.nodes[t],void(!0===i&&this.body.emitter.emit("_dataChanged"))}var h=n.containedNodes,l=n.containedEdges;if(void 0!==e&&void 0!==e.releaseFunction&&"function"==typeof e.releaseFunction){var d={},c={x:n.x,y:n.y};for(var u in h)if(Object.prototype.hasOwnProperty.call(h,u)){var f=this.body.nodes[u];d[u]={x:f.x,y:f.y}}var p=e.releaseFunction(c,d);for(var v in h)if(Object.prototype.hasOwnProperty.call(h,v)){var g=this.body.nodes[v];void 0!==p[v]&&(g.x=void 0===p[v].x?n.x:p[v].x,g.y=void 0===p[v].y?n.y:p[v].y)}}else hm(h,(function(t){!1===t.options.fixed.x&&(t.x=n.x),!1===t.options.fixed.y&&(t.y=n.y)}));for(var y in h)if(Object.prototype.hasOwnProperty.call(h,y)){var m=this.body.nodes[y];m.vx=n.vx,m.vy=n.vy,m.setOptions({physics:!0}),delete this.clusteredNodes[y]}for(var b=[],w=0;w<n.edges.length;w++)b.push(n.edges[w]);for(var k=0;k<b.length;k++){for(var _=b[k],x=this._getConnectedId(_,t),E=this.clusteredNodes[x],O=0;O<_.clusteringEdgeReplacingIds.length;O++){var C=_.clusteringEdgeReplacingIds[O],S=this.body.edges[C];if(void 0!==S)if(void 0!==E){var T=this.body.nodes[E.clusterId];T.containedEdges[S.id]=S,delete l[S.id];var M=S.fromId,P=S.toId;S.toId==x?P=E.clusterId:M=E.clusterId,this._createClusteredEdge(M,P,S,T.clusterEdgeProperties,{hidden:!1,physics:!0})}else this._restoreEdge(S)}_.remove()}for(var D in l)Object.prototype.hasOwnProperty.call(l,D)&&this._restoreEdge(l[D]);delete this.body.nodes[t],!0===i&&this.body.emitter.emit("_dataChanged")}},{key:"getNodesInCluster",value:function(t){var e=[];if(!0===this.isCluster(t)){var i=this.body.nodes[t].containedNodes;for(var n in i)Object.prototype.hasOwnProperty.call(i,n)&&e.push(this.body.nodes[n].id)}return e}},{key:"findNode",value:function(t){for(var e,i=[],n=0;void 0!==this.clusteredNodes[t]&&n<100;){if(void 0===(e=this.body.nodes[t]))return[];i.push(e.id),t=this.clusteredNodes[t].clusterId,n++}return void 0===(e=this.body.nodes[t])?[]:(i.push(e.id),Yu(i).call(i),i)}},{key:"updateClusteredNode",value:function(t,e){if(void 0===t)throw new Error("No clusteredNodeId supplied to updateClusteredNode.");if(void 0===e)throw new Error("No newOptions supplied to updateClusteredNode.");if(void 0===this.body.nodes[t])throw new Error("The clusteredNodeId supplied to updateClusteredNode does not exist.");this.body.nodes[t].setOptions(e),this.body.emitter.emit("_dataChanged")}},{key:"updateEdge",value:function(t,e){if(void 0===t)throw new Error("No startEdgeId supplied to updateEdge.");if(void 0===e)throw new Error("No newOptions supplied to updateEdge.");if(void 0===this.body.edges[t])throw new Error("The startEdgeId supplied to updateEdge does not exist.");for(var i=this.getClusteredEdges(t),n=0;n<i.length;n++){this.body.edges[i[n]].setOptions(e)}this.body.emitter.emit("_dataChanged")}},{key:"getClusteredEdges",value:function(t){for(var e=[],i=0;void 0!==t&&void 0!==this.body.edges[t]&&i<100;)e.push(this.body.edges[t].id),t=this.body.edges[t].edgeReplacedById,i++;return Yu(e).call(e),e}},{key:"getBaseEdge",value:function(t){return this.getBaseEdges(t)[0]}},{key:"getBaseEdges",value:function(t){for(var e=[t],i=[],n=[],o=0;e.length>0&&o<100;){var r=e.pop();if(void 0!==r){var s=this.body.edges[r];if(void 0!==s){o++;var a=s.clusteringEdgeReplacingIds;if(void 0===a)n.push(r);else for(var h=0;h<a.length;++h){var l=a[h];-1===Fp(e).call(e,a)&&-1===Fp(i).call(i,a)&&e.push(l)}i.push(r)}}}return n}},{key:"_getConnectedId",value:function(t,e){return t.toId!=e?t.toId:(t.fromId,t.fromId)}},{key:"_getHubSize",value:function(){for(var t=0,e=0,i=0,n=0,o=0;o<this.body.nodeIndices.length;o++){var r=this.body.nodes[this.body.nodeIndices[o]];r.edges.length>n&&(n=r.edges.length),t+=r.edges.length,e+=Math.pow(r.edges.length,2),i+=1}t/=i;var s=(e/=i)-Math.pow(t,2),a=Math.sqrt(s),h=Math.floor(t+2*a);return h>n&&(h=n),h}},{key:"_createClusteredEdge",value:function(t,e,i,n,o){var r=EC.cloneOptions(i,"edge");nm(r,n),r.from=t,r.to=e,r.id="clusterEdge:"+Ax(),void 0!==o&&nm(r,o);var s=this.body.functions.createEdge(r);return s.clusteringEdgeReplacingIds=[i.id],s.connect(),this.body.edges[s.id]=s,s}},{key:"_clusterEdges",value:function(t,e,i,n){if(e instanceof cC){var o=e,r={};r[o.id]=o,e=r}if(t instanceof fO){var s=t,a={};a[s.id]=s,t=a}if(null==i)throw new Error("_clusterEdges: parameter clusterNode required");for(var h in void 0===n&&(n=i.clusterEdgeProperties),this._createClusterEdges(t,e,i,n),e)if(Object.prototype.hasOwnProperty.call(e,h)&&void 0!==this.body.edges[h]){var l=this.body.edges[h];this._backupEdgeOptions(l),l.setOptions({physics:!1})}for(var d in t)Object.prototype.hasOwnProperty.call(t,d)&&(this.clusteredNodes[d]={clusterId:i.id,node:this.body.nodes[d]},this.body.nodes[d].setOptions({physics:!1}))}},{key:"_getClusterNodeForNode",value:function(t){if(void 0!==t){var e=this.clusteredNodes[t];if(void 0!==e){var i=e.clusterId;if(void 0!==i)return this.body.nodes[i]}}}},{key:"_filter",value:function(t,e){var i=[];return hm(t,(function(t){e(t)&&i.push(t)})),i}},{key:"_updateState",value:function(){var t,e=this,i=[],n={},o=function(t){hm(e.body.nodes,(function(e){!0===e.isCluster&&t(e)}))};for(t in this.clusteredNodes){if(Object.prototype.hasOwnProperty.call(this.clusteredNodes,t))void 0===this.body.nodes[t]&&i.push(t)}o((function(t){for(var e=0;e<i.length;e++)delete t.containedNodes[i[e]]}));for(var r=0;r<i.length;r++)delete this.clusteredNodes[i[r]];hm(this.clusteredEdges,(function(t){var i=e.body.edges[t];void 0!==i&&i.endPointsValid()||(n[t]=t)})),o((function(t){hm(t.containedEdges,(function(t,e){t.endPointsValid()||n[e]||(n[e]=e)}))})),hm(this.body.edges,(function(t,i){var o=!0,r=t.clusteringEdgeReplacingIds;if(void 0!==r){var s=0;hm(r,(function(t){var i=e.body.edges[t];void 0!==i&&i.endPointsValid()&&(s+=1)})),o=s>0}t.endPointsValid()&&o||(n[i]=i)})),o((function(t){hm(n,(function(i){delete t.containedEdges[i],hm(t.edges,(function(o,r){o.id!==i?o.clusteringEdgeReplacingIds=e._filter(o.clusteringEdgeReplacingIds,(function(t){return!n[t]})):t.edges[r]=null})),t.edges=e._filter(t.edges,(function(t){return null!==t}))}))})),hm(n,(function(t){delete e.clusteredEdges[t]})),hm(n,(function(t){delete e.body.edges[t]})),hm(bu(this.body.edges),(function(t){var i=e.body.edges[t],n=e._isClusteredNode(i.fromId)||e._isClusteredNode(i.toId);if(n!==e._isClusteredEdge(i.id))if(n){var o=e._getClusterNodeForNode(i.fromId);void 0!==o&&e._clusterEdges(e.body.nodes[i.fromId],i,o);var r=e._getClusterNodeForNode(i.toId);void 0!==r&&e._clusterEdges(e.body.nodes[i.toId],i,r)}else delete e._clusterEdges[t],e._restoreEdge(i)}));for(var s=!1,a=!0,h=function(){var t=[];o((function(e){var i=bu(e.containedNodes).length,n=!0===e.options.allowSingleNodeCluster;(n&&i<1||!n&&i<2)&&t.push(e.id)}));for(var i=0;i<t.length;++i)e.openCluster(t[i],{},!1);a=t.length>0,s=s||a};a;)h();s&&this._updateState()}},{key:"_isClusteredNode",value:function(t){return void 0!==this.clusteredNodes[t]}},{key:"_isClusteredEdge",value:function(t){return void 0!==this.clusteredEdges[t]}}]),t}();function TC(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return MC(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return MC(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function MC(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}var PC=function(){function t(e,i){var n;Yd(this,t),void 0!==window&&(n=window.requestAnimationFrame||window.mozRequestAnimationFrame||window.webkitRequestAnimationFrame||window.msRequestAnimationFrame),window.requestAnimationFrame=void 0===n?function(t){t()}:n,this.body=e,this.canvas=i,this.redrawRequested=!1,this.renderTimer=void 0,this.requiresTimeout=!0,this.renderingActive=!1,this.renderRequests=0,this.allowRedraw=!0,this.dragging=!1,this.zooming=!1,this.options={},this.defaultOptions={hideEdgesOnDrag:!1,hideEdgesOnZoom:!1,hideNodesOnDrag:!1},un(this.options,this.defaultOptions),this._determineBrowserMethod(),this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t,e=this;this.body.emitter.on("dragStart",(function(){e.dragging=!0})),this.body.emitter.on("dragEnd",(function(){e.dragging=!1})),this.body.emitter.on("zoom",(function(){e.zooming=!0,window.clearTimeout(e.zoomTimeoutId),e.zoomTimeoutId=Sv((function(){var t;e.zooming=!1,zn(t=e._requestRedraw).call(t,e)()}),250)})),this.body.emitter.on("_resizeNodes",(function(){e._resizeNodes()})),this.body.emitter.on("_redraw",(function(){!1===e.renderingActive&&e._redraw()})),this.body.emitter.on("_blockRedraw",(function(){e.allowRedraw=!1})),this.body.emitter.on("_allowRedraw",(function(){e.allowRedraw=!0,e.redrawRequested=!1})),this.body.emitter.on("_requestRedraw",zn(t=this._requestRedraw).call(t,this)),this.body.emitter.on("_startRendering",(function(){e.renderRequests+=1,e.renderingActive=!0,e._startRendering()})),this.body.emitter.on("_stopRendering",(function(){e.renderRequests-=1,e.renderingActive=e.renderRequests>0,e.renderTimer=void 0})),this.body.emitter.on("destroy",(function(){e.renderRequests=0,e.allowRedraw=!1,e.renderingActive=!1,!0===e.requiresTimeout?clearTimeout(e.renderTimer):window.cancelAnimationFrame(e.renderTimer),e.body.emitter.off()}))}},{key:"setOptions",value:function(t){if(void 0!==t){em(["hideEdgesOnDrag","hideEdgesOnZoom","hideNodesOnDrag"],this.options,t)}}},{key:"_requestNextFrame",value:function(t,e){if("undefined"!=typeof window){var i,n=window;return!0===this.requiresTimeout?i=Sv(t,e):n.requestAnimationFrame&&(i=n.requestAnimationFrame(t)),i}}},{key:"_startRendering",value:function(){var t;!0===this.renderingActive&&(void 0===this.renderTimer&&(this.renderTimer=this._requestNextFrame(zn(t=this._renderStep).call(t,this),this.simulationInterval)))}},{key:"_renderStep",value:function(){!0===this.renderingActive&&(this.renderTimer=void 0,!0===this.requiresTimeout&&this._startRendering(),this._redraw(),!1===this.requiresTimeout&&this._startRendering())}},{key:"redraw",value:function(){this.body.emitter.emit("setSize"),this._redraw()}},{key:"_requestRedraw",value:function(){var t=this;!0!==this.redrawRequested&&!1===this.renderingActive&&!0===this.allowRedraw&&(this.redrawRequested=!0,this._requestNextFrame((function(){t._redraw(!1)}),0))}},{key:"_redraw",value:function(){var t=arguments.length>0&&void 0!==arguments[0]&&arguments[0];if(!0===this.allowRedraw){this.body.emitter.emit("initRedraw"),this.redrawRequested=!1;var e={drawExternalLabels:null};0!==this.canvas.frame.canvas.width&&0!==this.canvas.frame.canvas.height||this.canvas.setSize(),this.canvas.setTransform();var i=this.canvas.getContext(),n=this.canvas.frame.canvas.clientWidth,o=this.canvas.frame.canvas.clientHeight;if(i.clearRect(0,0,n,o),0===this.canvas.frame.clientWidth)return;if(i.save(),i.translate(this.body.view.translation.x,this.body.view.translation.y),i.scale(this.body.view.scale,this.body.view.scale),i.beginPath(),this.body.emitter.emit("beforeDrawing",i),i.closePath(),!1===t&&(!1===this.dragging||!0===this.dragging&&!1===this.options.hideEdgesOnDrag)&&(!1===this.zooming||!0===this.zooming&&!1===this.options.hideEdgesOnZoom)&&this._drawEdges(i),!1===this.dragging||!0===this.dragging&&!1===this.options.hideNodesOnDrag){var r=this._drawNodes(i,t),s=r.drawExternalLabels;e.drawExternalLabels=s}!1===t&&(!1===this.dragging||!0===this.dragging&&!1===this.options.hideEdgesOnDrag)&&(!1===this.zooming||!0===this.zooming&&!1===this.options.hideEdgesOnZoom)&&this._drawArrows(i),null!=e.drawExternalLabels&&e.drawExternalLabels(),!1===t&&this._drawSelectionBox(i),i.beginPath(),this.body.emitter.emit("afterDrawing",i),i.closePath(),i.restore(),!0===t&&i.clearRect(0,0,n,o)}}},{key:"_resizeNodes",value:function(){this.canvas.setTransform();var t=this.canvas.getContext();t.save(),t.translate(this.body.view.translation.x,this.body.view.translation.y),t.scale(this.body.view.scale,this.body.view.scale);var e,i=this.body.nodes;for(var n in i)Object.prototype.hasOwnProperty.call(i,n)&&((e=i[n]).resize(t),e.updateBoundingBox(t,e.selected));t.restore()}},{key:"_drawNodes",value:function(t){for(var e,i,n=arguments.length>1&&void 0!==arguments[1]&&arguments[1],o=this.body.nodes,r=this.body.nodeIndices,s=[],a=[],h=20,l=this.canvas.DOMtoCanvas({x:-h,y:-h}),d=this.canvas.DOMtoCanvas({x:this.canvas.frame.canvas.clientWidth+h,y:this.canvas.frame.canvas.clientHeight+h}),c={top:l.y,left:l.x,bottom:d.y,right:d.x},u=[],f=0;f<r.length;f++)if((e=o[r[f]]).hover)a.push(r[f]);else if(e.isSelected())s.push(r[f]);else if(!0===n){var p=e.draw(t);null!=p.drawExternalLabel&&u.push(p.drawExternalLabel)}else if(!0===e.isBoundingBoxOverlappingWith(c)){var v=e.draw(t);null!=v.drawExternalLabel&&u.push(v.drawExternalLabel)}else e.updateBoundingBox(t,e.selected);var g=s.length,y=a.length;for(i=0;i<g;i++){var m=(e=o[s[i]]).draw(t);null!=m.drawExternalLabel&&u.push(m.drawExternalLabel)}for(i=0;i<y;i++){var b=(e=o[a[i]]).draw(t);null!=b.drawExternalLabel&&u.push(b.drawExternalLabel)}return{drawExternalLabels:function(){var t,e=TC(u);try{for(e.s();!(t=e.n()).done;){(0,t.value)()}}catch(t){e.e(t)}finally{e.f()}}}}},{key:"_drawEdges",value:function(t){for(var e=this.body.edges,i=this.body.edgeIndices,n=0;n<i.length;n++){var o=e[i[n]];!0===o.connected&&o.draw(t)}}},{key:"_drawArrows",value:function(t){for(var e=this.body.edges,i=this.body.edgeIndices,n=0;n<i.length;n++){var o=e[i[n]];!0===o.connected&&o.drawArrows(t)}}},{key:"_determineBrowserMethod",value:function(){if("undefined"!=typeof window){var t=navigator.userAgent.toLowerCase();this.requiresTimeout=!1,(-1!=Fp(t).call(t,"msie 9.0")||-1!=Fp(t).call(t,"safari")&&Fp(t).call(t,"chrome")<=-1)&&(this.requiresTimeout=!0)}else this.requiresTimeout=!0}},{key:"_drawSelectionBox",value:function(t){if(this.body.selectionBox.show){t.beginPath();var e=this.body.selectionBox.position.end.x-this.body.selectionBox.position.start.x,i=this.body.selectionBox.position.end.y-this.body.selectionBox.position.start.y;t.rect(this.body.selectionBox.position.start.x,this.body.selectionBox.position.start.y,e,i),t.fillStyle="rgba(151, 194, 252, 0.2)",t.fillRect(this.body.selectionBox.position.start.x,this.body.selectionBox.position.start.y,e,i),t.strokeStyle="rgba(151, 194, 252, 1)",t.stroke()}else t.closePath()}}]),t}(),DC=X.setInterval;function IC(t,e){e.inputHandler=function(t){t.isFirst&&e(t)},t.on("hammer.input",e.inputHandler)}function BC(t,e){return e.inputHandler=function(t){t.isFinal&&e(t)},t.on("hammer.input",e.inputHandler)}var zC=function(){function t(e){Yd(this,t),this.body=e,this.pixelRatio=1,this.cameraState={},this.initialized=!1,this.canvasViewCenter={},this._cleanupCallbacks=[],this.options={},this.defaultOptions={autoResize:!0,height:"100%",width:"100%"},un(this.options,this.defaultOptions),this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t,e=this;this.body.emitter.once("resize",(function(t){0!==t.width&&(e.body.view.translation.x=.5*t.width),0!==t.height&&(e.body.view.translation.y=.5*t.height)})),this.body.emitter.on("setSize",zn(t=this.setSize).call(t,this)),this.body.emitter.on("destroy",(function(){e.hammerFrame.destroy(),e.hammer.destroy(),e._cleanUp()}))}},{key:"setOptions",value:function(t){var e=this;if(void 0!==t){em(["width","height","autoResize"],this.options,t)}if(this._cleanUp(),!0===this.options.autoResize){var i;if(window.ResizeObserver){var n=new ResizeObserver((function(){!0===e.setSize()&&e.body.emitter.emit("_requestRedraw")})),o=this.frame;n.observe(o),this._cleanupCallbacks.push((function(){n.unobserve(o)}))}else{var r=DC((function(){!0===e.setSize()&&e.body.emitter.emit("_requestRedraw")}),1e3);this._cleanupCallbacks.push((function(){clearInterval(r)}))}var s=zn(i=this._onResize).call(i,this);dm(window,"resize",s),this._cleanupCallbacks.push((function(){cm(window,"resize",s)}))}}},{key:"_cleanUp",value:function(){var t,e,i;Fu(t=Yu(e=ff(i=this._cleanupCallbacks).call(i,0)).call(e)).call(t,(function(t){try{t()}catch(t){console.error(t)}}))}},{key:"_onResize",value:function(){this.setSize(),this.body.emitter.emit("_redraw")}},{key:"_getCameraState",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.pixelRatio;!0===this.initialized&&(this.cameraState.previousWidth=this.frame.canvas.width/t,this.cameraState.previousHeight=this.frame.canvas.height/t,this.cameraState.scale=this.body.view.scale,this.cameraState.position=this.DOMtoCanvas({x:.5*this.frame.canvas.width/t,y:.5*this.frame.canvas.height/t}))}},{key:"_setCameraState",value:function(){if(void 0!==this.cameraState.scale&&0!==this.frame.canvas.clientWidth&&0!==this.frame.canvas.clientHeight&&0!==this.pixelRatio&&this.cameraState.previousWidth>0&&this.cameraState.previousHeight>0){var t=this.frame.canvas.width/this.pixelRatio/this.cameraState.previousWidth,e=this.frame.canvas.height/this.pixelRatio/this.cameraState.previousHeight,i=this.cameraState.scale;1!=t&&1!=e?i=.5*this.cameraState.scale*(t+e):1!=t?i=this.cameraState.scale*t:1!=e&&(i=this.cameraState.scale*e),this.body.view.scale=i;var n=this.DOMtoCanvas({x:.5*this.frame.canvas.clientWidth,y:.5*this.frame.canvas.clientHeight}),o={x:n.x-this.cameraState.position.x,y:n.y-this.cameraState.position.y};this.body.view.translation.x+=o.x*this.body.view.scale,this.body.view.translation.y+=o.y*this.body.view.scale}}},{key:"_prepareValue",value:function(t){if("number"==typeof t)return t+"px";if("string"==typeof t){if(-1!==Fp(t).call(t,"%")||-1!==Fp(t).call(t,"px"))return t;if(-1===Fp(t).call(t,"%"))return t+"px"}throw new Error("Could not use the value supplied for width or height:"+t)}},{key:"_create",value:function(){for(;this.body.container.hasChildNodes();)this.body.container.removeChild(this.body.container.firstChild);if(this.frame=document.createElement("div"),this.frame.className="vis-network",this.frame.style.position="relative",this.frame.style.overflow="hidden",this.frame.tabIndex=0,this.frame.canvas=document.createElement("canvas"),this.frame.canvas.style.position="relative",this.frame.appendChild(this.frame.canvas),this.frame.canvas.getContext)this._setPixelRatio(),this.setTransform();else{var t=document.createElement("DIV");t.style.color="red",t.style.fontWeight="bold",t.style.padding="10px",t.innerText="Error: your browser does not support HTML canvas",this.frame.canvas.appendChild(t)}this.body.container.appendChild(this.frame),this.body.view.scale=1,this.body.view.translation={x:.5*this.frame.canvas.clientWidth,y:.5*this.frame.canvas.clientHeight},this._bindHammer()}},{key:"_bindHammer",value:function(){var t=this;void 0!==this.hammer&&this.hammer.destroy(),this.drag={},this.pinch={},this.hammer=new Wm(this.frame.canvas),this.hammer.get("pinch").set({enable:!0}),this.hammer.get("pan").set({threshold:5,direction:Wm.DIRECTION_ALL}),IC(this.hammer,(function(e){t.body.eventListeners.onTouch(e)})),this.hammer.on("tap",(function(e){t.body.eventListeners.onTap(e)})),this.hammer.on("doubletap",(function(e){t.body.eventListeners.onDoubleTap(e)})),this.hammer.on("press",(function(e){t.body.eventListeners.onHold(e)})),this.hammer.on("panstart",(function(e){t.body.eventListeners.onDragStart(e)})),this.hammer.on("panmove",(function(e){t.body.eventListeners.onDrag(e)})),this.hammer.on("panend",(function(e){t.body.eventListeners.onDragEnd(e)})),this.hammer.on("pinch",(function(e){t.body.eventListeners.onPinch(e)})),this.frame.canvas.addEventListener("wheel",(function(e){t.body.eventListeners.onMouseWheel(e)})),this.frame.canvas.addEventListener("mousemove",(function(e){t.body.eventListeners.onMouseMove(e)})),this.frame.canvas.addEventListener("contextmenu",(function(e){t.body.eventListeners.onContext(e)})),this.hammerFrame=new Wm(this.frame),BC(this.hammerFrame,(function(e){t.body.eventListeners.onRelease(e)}))}},{key:"setSize",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.width,e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.height;t=this._prepareValue(t),e=this._prepareValue(e);var i=!1,n=this.frame.canvas.width,o=this.frame.canvas.height,r=this.pixelRatio;if(this._setPixelRatio(),t!=this.options.width||e!=this.options.height||this.frame.style.width!=t||this.frame.style.height!=e)this._getCameraState(r),this.frame.style.width=t,this.frame.style.height=e,this.frame.canvas.style.width="100%",this.frame.canvas.style.height="100%",this.frame.canvas.width=Math.round(this.frame.canvas.clientWidth*this.pixelRatio),this.frame.canvas.height=Math.round(this.frame.canvas.clientHeight*this.pixelRatio),this.options.width=t,this.options.height=e,this.canvasViewCenter={x:.5*this.frame.clientWidth,y:.5*this.frame.clientHeight},i=!0;else{var s=Math.round(this.frame.canvas.clientWidth*this.pixelRatio),a=Math.round(this.frame.canvas.clientHeight*this.pixelRatio);this.frame.canvas.width===s&&this.frame.canvas.height===a||this._getCameraState(r),this.frame.canvas.width!==s&&(this.frame.canvas.width=s,i=!0),this.frame.canvas.height!==a&&(this.frame.canvas.height=a,i=!0)}return!0===i&&(this.body.emitter.emit("resize",{width:Math.round(this.frame.canvas.width/this.pixelRatio),height:Math.round(this.frame.canvas.height/this.pixelRatio),oldWidth:Math.round(n/this.pixelRatio),oldHeight:Math.round(o/this.pixelRatio)}),this._setCameraState()),this.initialized=!0,i}},{key:"getContext",value:function(){return this.frame.canvas.getContext("2d")}},{key:"_determinePixelRatio",value:function(){var t=this.getContext();if(void 0===t)throw new Error("Could not get canvax context");var e=1;return"undefined"!=typeof window&&(e=window.devicePixelRatio||1),e/(t.webkitBackingStorePixelRatio||t.mozBackingStorePixelRatio||t.msBackingStorePixelRatio||t.oBackingStorePixelRatio||t.backingStorePixelRatio||1)}},{key:"_setPixelRatio",value:function(){this.pixelRatio=this._determinePixelRatio()}},{key:"setTransform",value:function(){var t=this.getContext();if(void 0===t)throw new Error("Could not get canvax context");t.setTransform(this.pixelRatio,0,0,this.pixelRatio,0,0)}},{key:"_XconvertDOMtoCanvas",value:function(t){return(t-this.body.view.translation.x)/this.body.view.scale}},{key:"_XconvertCanvasToDOM",value:function(t){return t*this.body.view.scale+this.body.view.translation.x}},{key:"_YconvertDOMtoCanvas",value:function(t){return(t-this.body.view.translation.y)/this.body.view.scale}},{key:"_YconvertCanvasToDOM",value:function(t){return t*this.body.view.scale+this.body.view.translation.y}},{key:"canvasToDOM",value:function(t){return{x:this._XconvertCanvasToDOM(t.x),y:this._YconvertCanvasToDOM(t.y)}}},{key:"DOMtoCanvas",value:function(t){return{x:this._XconvertDOMtoCanvas(t.x),y:this._YconvertDOMtoCanvas(t.y)}}}]),t}();function NC(t,e){var i=un({nodes:e,minZoomLevel:Number.MIN_VALUE,maxZoomLevel:1},null!=t?t:{});if(!lu(i.nodes))throw new TypeError("Nodes has to be an array of ids.");if(0===i.nodes.length&&(i.nodes=e),!("number"==typeof i.minZoomLevel&&i.minZoomLevel>0))throw new TypeError("Min zoom level has to be a number higher than zero.");if(!("number"==typeof i.maxZoomLevel&&i.minZoomLevel<=i.maxZoomLevel))throw new TypeError("Max zoom level has to be a number higher than min zoom level.");return i}var FC=function(){function t(e,i){var n,o,r=this;Yd(this,t),this.body=e,this.canvas=i,this.animationSpeed=1/this.renderRefreshRate,this.animationEasingFunction="easeInOutQuint",this.easingTime=0,this.sourceScale=0,this.targetScale=0,this.sourceTranslation=0,this.targetTranslation=0,this.lockedOnNodeId=void 0,this.lockedOnNodeOffset=void 0,this.touchTime=0,this.viewFunction=void 0,this.body.emitter.on("fit",zn(n=this.fit).call(n,this)),this.body.emitter.on("animationFinished",(function(){r.body.emitter.emit("_stopRendering")})),this.body.emitter.on("unlockNode",zn(o=this.releaseNode).call(o,this))}return Kd(t,[{key:"setOptions",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};this.options=t}},{key:"fit",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]&&arguments[1];t=NC(t,this.body.nodeIndices);var i,n,o=this.canvas.frame.canvas.clientWidth,r=this.canvas.frame.canvas.clientHeight;if(0===o||0===r)n=1,i=EC.getRange(this.body.nodes,t.nodes);else if(!0===e){var s=0;for(var a in this.body.nodes)if(Object.prototype.hasOwnProperty.call(this.body.nodes,a)){var h=this.body.nodes[a];!0===h.predefinedPosition&&(s+=1)}if(s>.5*this.body.nodeIndices.length)return void this.fit(t,!1);i=EC.getRange(this.body.nodes,t.nodes);var l=this.body.nodeIndices.length;n=12.662/(l+7.4147)+.0964822;var d=Math.min(o/600,r/600);n*=d}else{this.body.emitter.emit("_resizeNodes"),i=EC.getRange(this.body.nodes,t.nodes);var c=1.1*Math.abs(i.maxX-i.minX),u=1.1*Math.abs(i.maxY-i.minY),f=o/c,p=r/u;n=f<=p?f:p}n>t.maxZoomLevel?n=t.maxZoomLevel:n<t.minZoomLevel&&(n=t.minZoomLevel);var v=EC.findCenter(i),g={position:v,scale:n,animation:t.animation};this.moveTo(g)}},{key:"focus",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if(void 0!==this.body.nodes[t]){var i={x:this.body.nodes[t].x,y:this.body.nodes[t].y};e.position=i,e.lockedOnNode=t,this.moveTo(e)}else console.error("Node: "+t+" cannot be found.")}},{key:"moveTo",value:function(t){if(void 0!==t){if(null!=t.offset){if(null!=t.offset.x){if(t.offset.x=+t.offset.x,!ok(t.offset.x))throw new TypeError('The option "offset.x" has to be a finite number.')}else t.offset.x=0;if(null!=t.offset.y){if(t.offset.y=+t.offset.y,!ok(t.offset.y))throw new TypeError('The option "offset.y" has to be a finite number.')}else t.offset.x=0}else t.offset={x:0,y:0};if(null!=t.position){if(null!=t.position.x){if(t.position.x=+t.position.x,!ok(t.position.x))throw new TypeError('The option "position.x" has to be a finite number.')}else t.position.x=0;if(null!=t.position.y){if(t.position.y=+t.position.y,!ok(t.position.y))throw new TypeError('The option "position.y" has to be a finite number.')}else t.position.x=0}else t.position=this.getViewPosition();if(null!=t.scale){if(t.scale=+t.scale,!(t.scale>0))throw new TypeError('The option "scale" has to be a number greater than zero.')}else t.scale=this.body.view.scale;void 0===t.animation&&(t.animation={duration:0}),!1===t.animation&&(t.animation={duration:0}),!0===t.animation&&(t.animation={}),void 0===t.animation.duration&&(t.animation.duration=1e3),void 0===t.animation.easingFunction&&(t.animation.easingFunction="easeInOutQuad"),this.animateView(t)}else t={}}},{key:"animateView",value:function(t){if(void 0!==t){this.animationEasingFunction=t.animation.easingFunction,this.releaseNode(),!0===t.locked&&(this.lockedOnNodeId=t.lockedOnNode,this.lockedOnNodeOffset=t.offset),0!=this.easingTime&&this._transitionRedraw(!0),this.sourceScale=this.body.view.scale,this.sourceTranslation=this.body.view.translation,this.targetScale=t.scale,this.body.view.scale=this.targetScale;var e,i,n=this.canvas.DOMtoCanvas({x:.5*this.canvas.frame.canvas.clientWidth,y:.5*this.canvas.frame.canvas.clientHeight}),o=n.x-t.position.x,r=n.y-t.position.y;if(this.targetTranslation={x:this.sourceTranslation.x+o*this.targetScale+t.offset.x,y:this.sourceTranslation.y+r*this.targetScale+t.offset.y},0===t.animation.duration)if(null!=this.lockedOnNodeId)this.viewFunction=zn(e=this._lockedRedraw).call(e,this),this.body.emitter.on("initRedraw",this.viewFunction);else this.body.view.scale=this.targetScale,this.body.view.translation=this.targetTranslation,this.body.emitter.emit("_requestRedraw");else this.animationSpeed=1/(60*t.animation.duration*.001)||1/60,this.animationEasingFunction=t.animation.easingFunction,this.viewFunction=zn(i=this._transitionRedraw).call(i,this),this.body.emitter.on("initRedraw",this.viewFunction),this.body.emitter.emit("_startRendering")}}},{key:"_lockedRedraw",value:function(){var t=this.body.nodes[this.lockedOnNodeId].x,e=this.body.nodes[this.lockedOnNodeId].y,i=this.canvas.DOMtoCanvas({x:.5*this.canvas.frame.canvas.clientWidth,y:.5*this.canvas.frame.canvas.clientHeight}),n=i.x-t,o=i.y-e,r=this.body.view.translation,s={x:r.x+n*this.body.view.scale+this.lockedOnNodeOffset.x,y:r.y+o*this.body.view.scale+this.lockedOnNodeOffset.y};this.body.view.translation=s}},{key:"releaseNode",value:function(){void 0!==this.lockedOnNodeId&&void 0!==this.viewFunction&&(this.body.emitter.off("initRedraw",this.viewFunction),this.lockedOnNodeId=void 0,this.lockedOnNodeOffset=void 0)}},{key:"_transitionRedraw",value:function(){var t=arguments.length>0&&void 0!==arguments[0]&&arguments[0];this.easingTime+=this.animationSpeed,this.easingTime=!0===t?1:this.easingTime;var e=Tm[this.animationEasingFunction](this.easingTime);if(this.body.view.scale=this.sourceScale+(this.targetScale-this.sourceScale)*e,this.body.view.translation={x:this.sourceTranslation.x+(this.targetTranslation.x-this.sourceTranslation.x)*e,y:this.sourceTranslation.y+(this.targetTranslation.y-this.sourceTranslation.y)*e},this.easingTime>=1){var i;if(this.body.emitter.off("initRedraw",this.viewFunction),this.easingTime=0,null!=this.lockedOnNodeId)this.viewFunction=zn(i=this._lockedRedraw).call(i,this),this.body.emitter.on("initRedraw",this.viewFunction);this.body.emitter.emit("animationFinished")}}},{key:"getScale",value:function(){return this.body.view.scale}},{key:"getViewPosition",value:function(){return this.canvas.DOMtoCanvas({x:.5*this.canvas.frame.canvas.clientWidth,y:.5*this.canvas.frame.canvas.clientHeight})}}]),t}();function AC(t){var e,i=t&&t.preventDefault||!1,n=t&&t.container||window,o={},r={keydown:{},keyup:{}},s={};for(e=97;e<=122;e++)s[String.fromCharCode(e)]={code:e-97+65,shift:!1};for(e=65;e<=90;e++)s[String.fromCharCode(e)]={code:e,shift:!0};for(e=0;e<=9;e++)s[""+e]={code:48+e,shift:!1};for(e=1;e<=12;e++)s["F"+e]={code:111+e,shift:!1};for(e=0;e<=9;e++)s["num"+e]={code:96+e,shift:!1};s["num*"]={code:106,shift:!1},s["num+"]={code:107,shift:!1},s["num-"]={code:109,shift:!1},s["num/"]={code:111,shift:!1},s["num."]={code:110,shift:!1},s.left={code:37,shift:!1},s.up={code:38,shift:!1},s.right={code:39,shift:!1},s.down={code:40,shift:!1},s.space={code:32,shift:!1},s.enter={code:13,shift:!1},s.shift={code:16,shift:void 0},s.esc={code:27,shift:!1},s.backspace={code:8,shift:!1},s.tab={code:9,shift:!1},s.ctrl={code:17,shift:!1},s.alt={code:18,shift:!1},s.delete={code:46,shift:!1},s.pageup={code:33,shift:!1},s.pagedown={code:34,shift:!1},s["="]={code:187,shift:!1},s["-"]={code:189,shift:!1},s["]"]={code:221,shift:!1},s["["]={code:219,shift:!1};var a=function(t){l(t,"keydown")},h=function(t){l(t,"keyup")},l=function(t,e){if(void 0!==r[e][t.keyCode]){for(var n=r[e][t.keyCode],o=0;o<n.length;o++)(void 0===n[o].shift||1==n[o].shift&&1==t.shiftKey||0==n[o].shift&&0==t.shiftKey)&&n[o].fn(t);1==i&&t.preventDefault()}};return o.bind=function(t,e,i){if(void 0===i&&(i="keydown"),void 0===s[t])throw new Error("unsupported key: "+t);void 0===r[i][s[t].code]&&(r[i][s[t].code]=[]),r[i][s[t].code].push({fn:e,shift:s[t].shift})},o.bindAll=function(t,e){for(var i in void 0===e&&(e="keydown"),s)s.hasOwnProperty(i)&&o.bind(i,t,e)},o.getKey=function(t){for(var e in s)if(s.hasOwnProperty(e)){if(1==t.shiftKey&&1==s[e].shift&&t.keyCode==s[e].code)return e;if(0==t.shiftKey&&0==s[e].shift&&t.keyCode==s[e].code)return e;if(t.keyCode==s[e].code&&"shift"==e)return e}return"unknown key, currently not supported"},o.unbind=function(t,e,i){if(void 0===i&&(i="keydown"),void 0===s[t])throw new Error("unsupported key: "+t);if(void 0!==e){var n=[],o=r[i][s[t].code];if(void 0!==o)for(var a=0;a<o.length;a++)o[a].fn==e&&o[a].shift==s[t].shift||n.push(r[i][s[t].code][a]);r[i][s[t].code]=n}else r[i][s[t].code]=[]},o.reset=function(){r={keydown:{},keyup:{}}},o.destroy=function(){r={keydown:{},keyup:{}},n.removeEventListener("keydown",a,!0),n.removeEventListener("keyup",h,!0)},n.addEventListener("keydown",a,!0),n.addEventListener("keyup",h,!0),o}var jC=Object.freeze({__proto__:null,default:AC}),RC=function(){function t(e,i){var n=this;Yd(this,t),this.body=e,this.canvas=i,this.iconsCreated=!1,this.navigationHammers=[],this.boundFunctions={},this.touchTime=0,this.activated=!1,this.body.emitter.on("activate",(function(){n.activated=!0,n.configureKeyboardBindings()})),this.body.emitter.on("deactivate",(function(){n.activated=!1,n.configureKeyboardBindings()})),this.body.emitter.on("destroy",(function(){void 0!==n.keycharm&&n.keycharm.destroy()})),this.options={}}return Kd(t,[{key:"setOptions",value:function(t){void 0!==t&&(this.options=t,this.create())}},{key:"create",value:function(){!0===this.options.navigationButtons?!1===this.iconsCreated&&this.loadNavigationElements():!0===this.iconsCreated&&this.cleanNavigation(),this.configureKeyboardBindings()}},{key:"cleanNavigation",value:function(){if(0!=this.navigationHammers.length){for(var t=0;t<this.navigationHammers.length;t++)this.navigationHammers[t].destroy();this.navigationHammers=[]}this.navigationDOM&&this.navigationDOM.wrapper&&this.navigationDOM.wrapper.parentNode&&this.navigationDOM.wrapper.parentNode.removeChild(this.navigationDOM.wrapper),this.iconsCreated=!1}},{key:"loadNavigationElements",value:function(){var t=this;this.cleanNavigation(),this.navigationDOM={};var e=["up","down","left","right","zoomIn","zoomOut","zoomExtends"],i=["_moveUp","_moveDown","_moveLeft","_moveRight","_zoomIn","_zoomOut","_fit"];this.navigationDOM.wrapper=document.createElement("div"),this.navigationDOM.wrapper.className="vis-navigation",this.canvas.frame.appendChild(this.navigationDOM.wrapper);for(var n=0;n<e.length;n++){this.navigationDOM[e[n]]=document.createElement("div"),this.navigationDOM[e[n]].className="vis-button vis-"+e[n],this.navigationDOM.wrapper.appendChild(this.navigationDOM[e[n]]);var o,r,s=new Wm(this.navigationDOM[e[n]]);if("_fit"===i[n])IC(s,zn(o=this._fit).call(o,this));else IC(s,zn(r=this.bindToRedraw).call(r,this,i[n]));this.navigationHammers.push(s)}var a=new Wm(this.canvas.frame);BC(a,(function(){t._stopMovement()})),this.navigationHammers.push(a),this.iconsCreated=!0}},{key:"bindToRedraw",value:function(t){var e;void 0===this.boundFunctions[t]&&(this.boundFunctions[t]=zn(e=this[t]).call(e,this),this.body.emitter.on("initRedraw",this.boundFunctions[t]),this.body.emitter.emit("_startRendering"))}},{key:"unbindFromRedraw",value:function(t){void 0!==this.boundFunctions[t]&&(this.body.emitter.off("initRedraw",this.boundFunctions[t]),this.body.emitter.emit("_stopRendering"),delete this.boundFunctions[t])}},{key:"_fit",value:function(){(new Date).valueOf()-this.touchTime>700&&(this.body.emitter.emit("fit",{duration:700}),this.touchTime=(new Date).valueOf())}},{key:"_stopMovement",value:function(){for(var t in this.boundFunctions)Object.prototype.hasOwnProperty.call(this.boundFunctions,t)&&(this.body.emitter.off("initRedraw",this.boundFunctions[t]),this.body.emitter.emit("_stopRendering"));this.boundFunctions={}}},{key:"_moveUp",value:function(){this.body.view.translation.y+=this.options.keyboard.speed.y}},{key:"_moveDown",value:function(){this.body.view.translation.y-=this.options.keyboard.speed.y}},{key:"_moveLeft",value:function(){this.body.view.translation.x+=this.options.keyboard.speed.x}},{key:"_moveRight",value:function(){this.body.view.translation.x-=this.options.keyboard.speed.x}},{key:"_zoomIn",value:function(){var t=this.body.view.scale,e=this.body.view.scale*(1+this.options.keyboard.speed.zoom),i=this.body.view.translation,n=e/t,o=(1-n)*this.canvas.canvasViewCenter.x+i.x*n,r=(1-n)*this.canvas.canvasViewCenter.y+i.y*n;this.body.view.scale=e,this.body.view.translation={x:o,y:r},this.body.emitter.emit("zoom",{direction:"+",scale:this.body.view.scale,pointer:null})}},{key:"_zoomOut",value:function(){var t=this.body.view.scale,e=this.body.view.scale/(1+this.options.keyboard.speed.zoom),i=this.body.view.translation,n=e/t,o=(1-n)*this.canvas.canvasViewCenter.x+i.x*n,r=(1-n)*this.canvas.canvasViewCenter.y+i.y*n;this.body.view.scale=e,this.body.view.translation={x:o,y:r},this.body.emitter.emit("zoom",{direction:"-",scale:this.body.view.scale,pointer:null})}},{key:"configureKeyboardBindings",value:function(){var t,e,i,n,o,r,s,a,h,l,d,c,u,f,p,v,g,y,m,b,w,k,_,x,E=this;(void 0!==this.keycharm&&this.keycharm.destroy(),!0===this.options.keyboard.enabled)&&(!0===this.options.keyboard.bindToWindow?this.keycharm=AC({container:window,preventDefault:!0}):this.keycharm=AC({container:this.canvas.frame,preventDefault:!0}),this.keycharm.reset(),!0===this.activated&&(zn(t=this.keycharm).call(t,"up",(function(){E.bindToRedraw("_moveUp")}),"keydown"),zn(e=this.keycharm).call(e,"down",(function(){E.bindToRedraw("_moveDown")}),"keydown"),zn(i=this.keycharm).call(i,"left",(function(){E.bindToRedraw("_moveLeft")}),"keydown"),zn(n=this.keycharm).call(n,"right",(function(){E.bindToRedraw("_moveRight")}),"keydown"),zn(o=this.keycharm).call(o,"=",(function(){E.bindToRedraw("_zoomIn")}),"keydown"),zn(r=this.keycharm).call(r,"num+",(function(){E.bindToRedraw("_zoomIn")}),"keydown"),zn(s=this.keycharm).call(s,"num-",(function(){E.bindToRedraw("_zoomOut")}),"keydown"),zn(a=this.keycharm).call(a,"-",(function(){E.bindToRedraw("_zoomOut")}),"keydown"),zn(h=this.keycharm).call(h,"[",(function(){E.bindToRedraw("_zoomOut")}),"keydown"),zn(l=this.keycharm).call(l,"]",(function(){E.bindToRedraw("_zoomIn")}),"keydown"),zn(d=this.keycharm).call(d,"pageup",(function(){E.bindToRedraw("_zoomIn")}),"keydown"),zn(c=this.keycharm).call(c,"pagedown",(function(){E.bindToRedraw("_zoomOut")}),"keydown"),zn(u=this.keycharm).call(u,"up",(function(){E.unbindFromRedraw("_moveUp")}),"keyup"),zn(f=this.keycharm).call(f,"down",(function(){E.unbindFromRedraw("_moveDown")}),"keyup"),zn(p=this.keycharm).call(p,"left",(function(){E.unbindFromRedraw("_moveLeft")}),"keyup"),zn(v=this.keycharm).call(v,"right",(function(){E.unbindFromRedraw("_moveRight")}),"keyup"),zn(g=this.keycharm).call(g,"=",(function(){E.unbindFromRedraw("_zoomIn")}),"keyup"),zn(y=this.keycharm).call(y,"num+",(function(){E.unbindFromRedraw("_zoomIn")}),"keyup"),zn(m=this.keycharm).call(m,"num-",(function(){E.unbindFromRedraw("_zoomOut")}),"keyup"),zn(b=this.keycharm).call(b,"-",(function(){E.unbindFromRedraw("_zoomOut")}),"keyup"),zn(w=this.keycharm).call(w,"[",(function(){E.unbindFromRedraw("_zoomOut")}),"keyup"),zn(k=this.keycharm).call(k,"]",(function(){E.unbindFromRedraw("_zoomIn")}),"keyup"),zn(_=this.keycharm).call(_,"pageup",(function(){E.unbindFromRedraw("_zoomIn")}),"keyup"),zn(x=this.keycharm).call(x,"pagedown",(function(){E.unbindFromRedraw("_zoomOut")}),"keyup")))}}]),t}();function LC(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return HC(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return HC(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function HC(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}var WC=function(){function t(e,i,n){var o,r,s,a,h,l,d,c,u,f,p,v,g;Yd(this,t),this.body=e,this.canvas=i,this.selectionHandler=n,this.navigationHandler=new RC(e,i),this.body.eventListeners.onTap=zn(o=this.onTap).call(o,this),this.body.eventListeners.onTouch=zn(r=this.onTouch).call(r,this),this.body.eventListeners.onDoubleTap=zn(s=this.onDoubleTap).call(s,this),this.body.eventListeners.onHold=zn(a=this.onHold).call(a,this),this.body.eventListeners.onDragStart=zn(h=this.onDragStart).call(h,this),this.body.eventListeners.onDrag=zn(l=this.onDrag).call(l,this),this.body.eventListeners.onDragEnd=zn(d=this.onDragEnd).call(d,this),this.body.eventListeners.onMouseWheel=zn(c=this.onMouseWheel).call(c,this),this.body.eventListeners.onPinch=zn(u=this.onPinch).call(u,this),this.body.eventListeners.onMouseMove=zn(f=this.onMouseMove).call(f,this),this.body.eventListeners.onRelease=zn(p=this.onRelease).call(p,this),this.body.eventListeners.onContext=zn(v=this.onContext).call(v,this),this.touchTime=0,this.drag={},this.pinch={},this.popup=void 0,this.popupObj=void 0,this.popupTimer=void 0,this.body.functions.getPointer=zn(g=this.getPointer).call(g,this),this.options={},this.defaultOptions={dragNodes:!0,dragView:!0,hover:!1,keyboard:{enabled:!1,speed:{x:10,y:10,zoom:.02},bindToWindow:!0,autoFocus:!0},navigationButtons:!1,tooltipDelay:300,zoomView:!0,zoomSpeed:1},un(this.options,this.defaultOptions),this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t=this;this.body.emitter.on("destroy",(function(){clearTimeout(t.popupTimer),delete t.body.functions.getPointer}))}},{key:"setOptions",value:function(t){if(void 0!==t){im(["hideEdgesOnDrag","hideEdgesOnZoom","hideNodesOnDrag","keyboard","multiselect","selectable","selectConnectedEdges"],this.options,t),Sm(this.options,t,"keyboard"),t.tooltip&&(un(this.options.tooltip,t.tooltip),t.tooltip.color&&(this.options.tooltip.color=gm(t.tooltip.color)))}this.navigationHandler.setOptions(this.options)}},{key:"getPointer",value:function(t){return{x:t.x-sm(this.canvas.frame.canvas),y:t.y-am(this.canvas.frame.canvas)}}},{key:"onTouch",value:function(t){(new Date).valueOf()-this.touchTime>50&&(this.drag.pointer=this.getPointer(t.center),this.drag.pinched=!1,this.pinch.scale=this.body.view.scale,this.touchTime=(new Date).valueOf())}},{key:"onTap",value:function(t){var e=this.getPointer(t.center),i=this.selectionHandler.options.multiselect&&(t.changedPointers[0].ctrlKey||t.changedPointers[0].metaKey);this.checkSelectionChanges(e,i),this.selectionHandler.commitAndEmit(e,t),this.selectionHandler.generateClickEvent("click",t,e)}},{key:"onDoubleTap",value:function(t){var e=this.getPointer(t.center);this.selectionHandler.generateClickEvent("doubleClick",t,e)}},{key:"onHold",value:function(t){var e=this.getPointer(t.center),i=this.selectionHandler.options.multiselect;this.checkSelectionChanges(e,i),this.selectionHandler.commitAndEmit(e,t),this.selectionHandler.generateClickEvent("click",t,e),this.selectionHandler.generateClickEvent("hold",t,e)}},{key:"onRelease",value:function(t){if((new Date).valueOf()-this.touchTime>10){var e=this.getPointer(t.center);this.selectionHandler.generateClickEvent("release",t,e),this.touchTime=(new Date).valueOf()}}},{key:"onContext",value:function(t){var e=this.getPointer({x:t.clientX,y:t.clientY});this.selectionHandler.generateClickEvent("oncontext",t,e)}},{key:"checkSelectionChanges",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]&&arguments[1];!0===e?this.selectionHandler.selectAdditionalOnPoint(t):this.selectionHandler.selectOnPoint(t)}},{key:"_determineDifference",value:function(t,e){var i=function(t,e){for(var i=[],n=0;n<t.length;n++){var o=t[n];-1===Fp(e).call(e,o)&&i.push(o)}return i};return{nodes:i(t.nodes,e.nodes),edges:i(t.edges,e.edges)}}},{key:"onDragStart",value:function(t){if(!this.drag.dragging){void 0===this.drag.pointer&&this.onTouch(t);var e=this.selectionHandler.getNodeAt(this.drag.pointer);if(this.drag.dragging=!0,this.drag.selection=[],this.drag.translation=un({},this.body.view.translation),this.drag.nodeId=void 0,t.srcEvent.shiftKey){this.body.selectionBox.show=!0;var i=this.getPointer(t.center);this.body.selectionBox.position.start={x:this.canvas._XconvertDOMtoCanvas(i.x),y:this.canvas._YconvertDOMtoCanvas(i.y)},this.body.selectionBox.position.end={x:this.canvas._XconvertDOMtoCanvas(i.x),y:this.canvas._YconvertDOMtoCanvas(i.y)}}if(void 0!==e&&!0===this.options.dragNodes){this.drag.nodeId=e.id,!1===e.isSelected()&&this.selectionHandler.setSelection({nodes:[e.id]}),this.selectionHandler.generateClickEvent("dragStart",t,this.drag.pointer);var n,o=LC(this.selectionHandler.getSelectedNodes());try{for(o.s();!(n=o.n()).done;){var r=n.value,s={id:r.id,node:r,x:r.x,y:r.y,xFixed:r.options.fixed.x,yFixed:r.options.fixed.y};r.options.fixed.x=!0,r.options.fixed.y=!0,this.drag.selection.push(s)}}catch(t){o.e(t)}finally{o.f()}}else this.selectionHandler.generateClickEvent("dragStart",t,this.drag.pointer,void 0,!0)}}},{key:"onDrag",value:function(t){var e=this;if(!0!==this.drag.pinched){this.body.emitter.emit("unlockNode");var i=this.getPointer(t.center),n=this.drag.selection;if(n&&n.length&&!0===this.options.dragNodes){this.selectionHandler.generateClickEvent("dragging",t,i);var o=i.x-this.drag.pointer.x,r=i.y-this.drag.pointer.y;Fu(n).call(n,(function(t){var i=t.node;!1===t.xFixed&&(i.x=e.canvas._XconvertDOMtoCanvas(e.canvas._XconvertCanvasToDOM(t.x)+o)),!1===t.yFixed&&(i.y=e.canvas._YconvertDOMtoCanvas(e.canvas._YconvertCanvasToDOM(t.y)+r))})),this.body.emitter.emit("startSimulation")}else{if(t.srcEvent.shiftKey){if(this.selectionHandler.generateClickEvent("dragging",t,i,void 0,!0),void 0===this.drag.pointer)return void this.onDragStart(t);this.body.selectionBox.position.end={x:this.canvas._XconvertDOMtoCanvas(i.x),y:this.canvas._YconvertDOMtoCanvas(i.y)},this.body.emitter.emit("_requestRedraw")}if(!0===this.options.dragView&&!t.srcEvent.shiftKey){if(this.selectionHandler.generateClickEvent("dragging",t,i,void 0,!0),void 0===this.drag.pointer)return void this.onDragStart(t);var s=i.x-this.drag.pointer.x,a=i.y-this.drag.pointer.y;this.body.view.translation={x:this.drag.translation.x+s,y:this.drag.translation.y+a},this.body.emitter.emit("_requestRedraw")}}}}},{key:"onDragEnd",value:function(t){var e=this;if(this.drag.dragging=!1,this.body.selectionBox.show){var i;this.body.selectionBox.show=!1;var n=this.body.selectionBox.position,o={minX:Math.min(n.start.x,n.end.x),minY:Math.min(n.start.y,n.end.y),maxX:Math.max(n.start.x,n.end.x),maxY:Math.max(n.start.y,n.end.y)},r=Xf(i=this.body.nodeIndices).call(i,(function(t){var i=e.body.nodes[t];return i.x>=o.minX&&i.x<=o.maxX&&i.y>=o.minY&&i.y<=o.maxY}));Fu(r).call(r,(function(t){return e.selectionHandler.selectObject(e.body.nodes[t])}));var s=this.getPointer(t.center);this.selectionHandler.commitAndEmit(s,t),this.selectionHandler.generateClickEvent("dragEnd",t,this.getPointer(t.center),void 0,!0),this.body.emitter.emit("_requestRedraw")}else{var a=this.drag.selection;a&&a.length?(Fu(a).call(a,(function(t){t.node.options.fixed.x=t.xFixed,t.node.options.fixed.y=t.yFixed})),this.selectionHandler.generateClickEvent("dragEnd",t,this.getPointer(t.center)),this.body.emitter.emit("startSimulation")):(this.selectionHandler.generateClickEvent("dragEnd",t,this.getPointer(t.center),void 0,!0),this.body.emitter.emit("_requestRedraw"))}}},{key:"onPinch",value:function(t){var e=this.getPointer(t.center);this.drag.pinched=!0,void 0===this.pinch.scale&&(this.pinch.scale=1);var i=this.pinch.scale*t.scale;this.zoom(i,e)}},{key:"zoom",value:function(t,e){if(!0===this.options.zoomView){var i=this.body.view.scale;t<1e-5&&(t=1e-5),t>10&&(t=10);var n=void 0;void 0!==this.drag&&!0===this.drag.dragging&&(n=this.canvas.DOMtoCanvas(this.drag.pointer));var o=this.body.view.translation,r=t/i,s=(1-r)*e.x+o.x*r,a=(1-r)*e.y+o.y*r;if(this.body.view.scale=t,this.body.view.translation={x:s,y:a},null!=n){var h=this.canvas.canvasToDOM(n);this.drag.pointer.x=h.x,this.drag.pointer.y=h.y}this.body.emitter.emit("_requestRedraw"),i<t?this.body.emitter.emit("zoom",{direction:"+",scale:this.body.view.scale,pointer:e}):this.body.emitter.emit("zoom",{direction:"-",scale:this.body.view.scale,pointer:e})}}},{key:"onMouseWheel",value:function(t){if(!0===this.options.zoomView){if(0!==t.deltaY){var e=this.body.view.scale;e*=1+(t.deltaY<0?1:-1)*(.1*this.options.zoomSpeed);var i=this.getPointer({x:t.clientX,y:t.clientY});this.zoom(e,i)}t.preventDefault()}}},{key:"onMouseMove",value:function(t){var e=this,i=this.getPointer({x:t.clientX,y:t.clientY}),n=!1;void 0!==this.popup&&(!1===this.popup.hidden&&this._checkHidePopup(i),!1===this.popup.hidden&&(n=!0,this.popup.setPosition(i.x+3,i.y-5),this.popup.show())),this.options.keyboard.autoFocus&&!1===this.options.keyboard.bindToWindow&&!0===this.options.keyboard.enabled&&this.canvas.frame.focus(),!1===n&&(void 0!==this.popupTimer&&(clearInterval(this.popupTimer),this.popupTimer=void 0),this.drag.dragging||(this.popupTimer=Sv((function(){return e._checkShowPopup(i)}),this.options.tooltipDelay))),!0===this.options.hover&&this.selectionHandler.hoverObject(t,i)}},{key:"_checkShowPopup",value:function(t){var e=this.canvas._XconvertDOMtoCanvas(t.x),i=this.canvas._YconvertDOMtoCanvas(t.y),n={left:e,top:i,right:e,bottom:i},o=void 0===this.popupObj?void 0:this.popupObj.id,r=!1,s="node";if(void 0===this.popupObj){for(var a,h=this.body.nodeIndices,l=this.body.nodes,d=[],c=0;c<h.length;c++)!0===(a=l[h[c]]).isOverlappingWith(n)&&(r=!0,void 0!==a.getTitle()&&d.push(h[c]));d.length>0&&(this.popupObj=l[d[d.length-1]],r=!0)}if(void 0===this.popupObj&&!1===r){for(var u,f=this.body.edgeIndices,p=this.body.edges,v=[],g=0;g<f.length;g++)!0===(u=p[f[g]]).isOverlappingWith(n)&&!0===u.connected&&void 0!==u.getTitle()&&v.push(f[g]);v.length>0&&(this.popupObj=p[v[v.length-1]],s="edge")}void 0!==this.popupObj?this.popupObj.id!==o&&(void 0===this.popup&&(this.popup=new qm(this.canvas.frame)),this.popup.popupTargetType=s,this.popup.popupTargetId=this.popupObj.id,this.popup.setPosition(t.x+3,t.y-5),this.popup.setText(this.popupObj.getTitle()),this.popup.show(),this.body.emitter.emit("showPopup",this.popupObj.id)):void 0!==this.popup&&(this.popup.hide(),this.body.emitter.emit("hidePopup"))}},{key:"_checkHidePopup",value:function(t){var e=this.selectionHandler._pointerToPositionObject(t),i=!1;if("node"===this.popup.popupTargetType){if(void 0!==this.body.nodes[this.popup.popupTargetId]&&!0===(i=this.body.nodes[this.popup.popupTargetId].isOverlappingWith(e))){var n=this.selectionHandler.getNodeAt(t);i=void 0!==n&&n.id===this.popup.popupTargetId}}else void 0===this.selectionHandler.getNodeAt(t)&&void 0!==this.body.edges[this.popup.popupTargetId]&&(i=this.body.edges[this.popup.popupTargetId].isOverlappingWith(e));!1===i&&(this.popupObj=void 0,this.popup.hide(),this.body.emitter.emit("hidePopup"))}}]),t}(),qC=g,VC=Nw,UC=Db.exports.getWeakData,YC=$e,XC=Y,GC=yw,KC=pw,$C=Wt,ZC=Vo.set,QC=Vo.getterFor,JC=Wh.find,tS=Wh.findIndex,eS=qC([].splice),iS=0,nS=function(t){return t.frozen||(t.frozen=new oS)},oS=function(){this.entries=[]},rS=function(t,e){return JC(t.entries,(function(t){return t[0]===e}))};oS.prototype={get:function(t){var e=rS(this,t);if(e)return e[1]},has:function(t){return!!rS(this,t)},set:function(t,e){var i=rS(this,t);i?i[1]=e:this.entries.push([t,e])},delete:function(t){var e=tS(this.entries,(function(e){return e[0]===t}));return~e&&eS(this.entries,e,1),!!~e}};var sS,aS={getConstructor:function(t,e,i,n){var o=t((function(t,o){GC(t,r),ZC(t,{type:e,id:iS++,frozen:void 0}),null!=o&&KC(o,t[n],{that:t,AS_ENTRIES:i})})),r=o.prototype,s=QC(e),a=function(t,e,i){var n=s(t),o=UC(YC(e),!0);return!0===o?nS(n).set(e,i):o[n.id]=i,t};return VC(r,{delete:function(t){var e=s(this);if(!XC(t))return!1;var i=UC(t);return!0===i?nS(e).delete(t):i&&$C(i,e.id)&&delete i[e.id]},has:function(t){var e=s(this);if(!XC(t))return!1;var i=UC(t);return!0===i?nS(e).has(t):i&&$C(i,e.id)}}),VC(r,i?{get:function(t){var e=s(this);if(XC(t)){var i=UC(t);return!0===i?nS(e).get(t):i?i[e.id]:void 0}},set:function(t,e){return a(this,t,e)}}:{add:function(t){return a(this,t,!0)}}),o}},hS=n,lS=g,dS=Nw,cS=Db.exports,uS=Bw,fS=aS,pS=Y,vS=jb,gS=Vo.enforce,yS=_o,mS=!hS.ActiveXObject&&"ActiveXObject"in hS,bS=function(t){return function(){return t(this,arguments.length?arguments[0]:void 0)}},wS=uS("WeakMap",bS,fS);if(yS&&mS){sS=fS.getConstructor(bS,"WeakMap",!0),cS.enable();var kS=wS.prototype,_S=lS(kS.delete),xS=lS(kS.has),ES=lS(kS.get),OS=lS(kS.set);dS(kS,{delete:function(t){if(pS(t)&&!vS(t)){var e=gS(this);return e.frozen||(e.frozen=new sS),_S(this,t)||e.frozen.delete(t)}return _S(this,t)},has:function(t){if(pS(t)&&!vS(t)){var e=gS(this);return e.frozen||(e.frozen=new sS),xS(this,t)||e.frozen.has(t)}return xS(this,t)},get:function(t){if(pS(t)&&!vS(t)){var e=gS(this);return e.frozen||(e.frozen=new sS),xS(this,t)?ES(this,t):e.frozen.get(t)}return ES(this,t)},set:function(t,e){if(pS(t)&&!vS(t)){var i=gS(this);i.frozen||(i.frozen=new sS),xS(this,t)?OS(this,t,e):i.frozen.set(t,e)}else OS(this,t,e);return this}})}var CS,SS,TS,MS,PS,DS=X.WeakMap;function IS(t,e,i,n){if("a"===i&&!n)throw new TypeError("Private accessor was defined without a getter");if("function"==typeof e?t!==e||!n:!e.has(t))throw new TypeError("Cannot read private member from an object whose class did not declare it");return"m"===i?n:"a"===i?n.call(t):n?n.value:e.get(t)}function BS(t,e,i,n,o){if("m"===n)throw new TypeError("Private method is not writable");if("a"===n&&!o)throw new TypeError("Private accessor was defined without a setter");if("function"==typeof e?t!==e||!o:!e.has(t))throw new TypeError("Cannot write private member to an object whose class did not declare it");return"a"===n?o.call(t,i):o?o.value=i:e.set(t,i),i}function zS(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return NS(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return NS(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function NS(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}function FS(t,e){var i,n=new b_,o=zS(e);try{for(o.s();!(i=o.n()).done;){var r=i.value;t.has(r)||n.add(r)}}catch(t){o.e(t)}finally{o.f()}return n}var AS=function(){function t(){Yd(this,t),CS.set(this,new b_),SS.set(this,new b_)}return Kd(t,[{key:"size",get:function(){return IS(this,SS,"f").size}},{key:"add",value:function(){for(var t=arguments.length,e=new Array(t),i=0;i<t;i++)e[i]=arguments[i];for(var n=0,o=e;n<o.length;n++){var r=o[n];IS(this,SS,"f").add(r)}}},{key:"delete",value:function(){for(var t=arguments.length,e=new Array(t),i=0;i<t;i++)e[i]=arguments[i];for(var n=0,o=e;n<o.length;n++){var r=o[n];IS(this,SS,"f").delete(r)}}},{key:"clear",value:function(){IS(this,SS,"f").clear()}},{key:"getSelection",value:function(){return Jc(IS(this,SS,"f"))}},{key:"getChanges",value:function(){return{added:Jc(FS(IS(this,CS,"f"),IS(this,SS,"f"))),deleted:Jc(FS(IS(this,SS,"f"),IS(this,CS,"f"))),previous:Jc(new b_(IS(this,CS,"f"))),current:Jc(new b_(IS(this,SS,"f")))}}},{key:"commit",value:function(){var t=this.getChanges();BS(this,CS,IS(this,SS,"f"),"f"),BS(this,SS,new b_(IS(this,CS,"f")),"f");var e,i=zS(t.added);try{for(i.s();!(e=i.n()).done;){e.value.select()}}catch(t){i.e(t)}finally{i.f()}var n,o=zS(t.deleted);try{for(o.s();!(n=o.n()).done;){n.value.unselect()}}catch(t){o.e(t)}finally{o.f()}return t}}]),t}();CS=new DS,SS=new DS;var jS=function(){function t(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:function(){};Yd(this,t),TS.set(this,new AS),MS.set(this,new AS),PS.set(this,void 0),BS(this,PS,e,"f")}return Kd(t,[{key:"sizeNodes",get:function(){return IS(this,TS,"f").size}},{key:"sizeEdges",get:function(){return IS(this,MS,"f").size}},{key:"getNodes",value:function(){return IS(this,TS,"f").getSelection()}},{key:"getEdges",value:function(){return IS(this,MS,"f").getSelection()}},{key:"addNodes",value:function(){var t;(t=IS(this,TS,"f")).add.apply(t,arguments)}},{key:"addEdges",value:function(){var t;(t=IS(this,MS,"f")).add.apply(t,arguments)}},{key:"deleteNodes",value:function(t){IS(this,TS,"f").delete(t)}},{key:"deleteEdges",value:function(t){IS(this,MS,"f").delete(t)}},{key:"clear",value:function(){IS(this,TS,"f").clear(),IS(this,MS,"f").clear()}},{key:"commit",value:function(){for(var t,e,i={nodes:IS(this,TS,"f").commit(),edges:IS(this,MS,"f").commit()},n=arguments.length,o=new Array(n),r=0;r<n;r++)o[r]=arguments[r];return(t=IS(this,PS,"f")).call.apply(t,su(e=[this,i]).call(e,o)),i}}]),t}();function RS(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return LS(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return LS(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function LS(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}TS=new DS,MS=new DS,PS=new DS;var HS=function(){function t(e,i){var n=this;Yd(this,t),this.body=e,this.canvas=i,this._selectionAccumulator=new jS,this.hoverObj={nodes:{},edges:{}},this.options={},this.defaultOptions={multiselect:!1,selectable:!0,selectConnectedEdges:!0,hoverConnectedEdges:!0},un(this.options,this.defaultOptions),this.body.emitter.on("_dataChanged",(function(){n.updateSelection()}))}return Kd(t,[{key:"setOptions",value:function(t){if(void 0!==t){em(["multiselect","hoverConnectedEdges","selectable","selectConnectedEdges"],this.options,t)}}},{key:"selectOnPoint",value:function(t){var e=!1;if(!0===this.options.selectable){var i=this.getNodeAt(t)||this.getEdgeAt(t);this.unselectAll(),void 0!==i&&(e=this.selectObject(i)),this.body.emitter.emit("_requestRedraw")}return e}},{key:"selectAdditionalOnPoint",value:function(t){var e=!1;if(!0===this.options.selectable){var i=this.getNodeAt(t)||this.getEdgeAt(t);void 0!==i&&(e=!0,!0===i.isSelected()?this.deselectObject(i):this.selectObject(i),this.body.emitter.emit("_requestRedraw"))}return e}},{key:"_initBaseEvent",value:function(t,e){var i={};return i.pointer={DOM:{x:e.x,y:e.y},canvas:this.canvas.DOMtoCanvas(e)},i.event=t,i}},{key:"generateClickEvent",value:function(t,e,i,n){var o=arguments.length>4&&void 0!==arguments[4]&&arguments[4],r=this._initBaseEvent(e,i);if(!0===o)r.nodes=[],r.edges=[];else{var s=this.getSelection();r.nodes=s.nodes,r.edges=s.edges}void 0!==n&&(r.previousSelection=n),"click"==t&&(r.items=this.getClickedItems(i)),void 0!==e.controlEdge&&(r.controlEdge=e.controlEdge),this.body.emitter.emit(t,r)}},{key:"selectObject",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.selectConnectedEdges;if(void 0!==t){if(t instanceof fO){var i;if(!0===e)(i=this._selectionAccumulator).addEdges.apply(i,Jc(t.edges));this._selectionAccumulator.addNodes(t)}else this._selectionAccumulator.addEdges(t);return!0}return!1}},{key:"deselectObject",value:function(t){!0===t.isSelected()&&(t.selected=!1,this._removeFromSelection(t))}},{key:"_getAllNodesOverlappingWith",value:function(t){for(var e=[],i=this.body.nodes,n=0;n<this.body.nodeIndices.length;n++){var o=this.body.nodeIndices[n];i[o].isOverlappingWith(t)&&e.push(o)}return e}},{key:"_pointerToPositionObject",value:function(t){var e=this.canvas.DOMtoCanvas(t);return{left:e.x-1,top:e.y+1,right:e.x+1,bottom:e.y-1}}},{key:"getNodeAt",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1],i=this._pointerToPositionObject(t),n=this._getAllNodesOverlappingWith(i);return n.length>0?!0===e?this.body.nodes[n[n.length-1]]:n[n.length-1]:void 0}},{key:"_getEdgesOverlappingWith",value:function(t,e){for(var i=this.body.edges,n=0;n<this.body.edgeIndices.length;n++){var o=this.body.edgeIndices[n];i[o].isOverlappingWith(t)&&e.push(o)}}},{key:"_getAllEdgesOverlappingWith",value:function(t){var e=[];return this._getEdgesOverlappingWith(t,e),e}},{key:"getEdgeAt",value:function(t){for(var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1],i=this.canvas.DOMtoCanvas(t),n=10,o=null,r=this.body.edges,s=0;s<this.body.edgeIndices.length;s++){var a=this.body.edgeIndices[s],h=r[a];if(h.connected){var l=h.from.x,d=h.from.y,c=h.to.x,u=h.to.y,f=h.edgeType.getDistanceToEdge(l,d,c,u,i.x,i.y);f<n&&(o=a,n=f)}}return null!==o?!0===e?this.body.edges[o]:o:void 0}},{key:"_addToHover",value:function(t){t instanceof fO?this.hoverObj.nodes[t.id]=t:this.hoverObj.edges[t.id]=t}},{key:"_removeFromSelection",value:function(t){var e;t instanceof fO?(this._selectionAccumulator.deleteNodes(t),(e=this._selectionAccumulator).deleteEdges.apply(e,Jc(t.edges))):this._selectionAccumulator.deleteEdges(t)}},{key:"unselectAll",value:function(){this._selectionAccumulator.clear()}},{key:"getSelectedNodeCount",value:function(){return this._selectionAccumulator.sizeNodes}},{key:"getSelectedEdgeCount",value:function(){return this._selectionAccumulator.sizeEdges}},{key:"_hoverConnectedEdges",value:function(t){for(var e=0;e<t.edges.length;e++){var i=t.edges[e];i.hover=!0,this._addToHover(i)}}},{key:"emitBlurEvent",value:function(t,e,i){var n=this._initBaseEvent(t,e);!0===i.hover&&(i.hover=!1,i instanceof fO?(n.node=i.id,this.body.emitter.emit("blurNode",n)):(n.edge=i.id,this.body.emitter.emit("blurEdge",n)))}},{key:"emitHoverEvent",value:function(t,e,i){var n=this._initBaseEvent(t,e),o=!1;return!1===i.hover&&(i.hover=!0,this._addToHover(i),o=!0,i instanceof fO?(n.node=i.id,this.body.emitter.emit("hoverNode",n)):(n.edge=i.id,this.body.emitter.emit("hoverEdge",n))),o}},{key:"hoverObject",value:function(t,e){var i=this.getNodeAt(e);void 0===i&&(i=this.getEdgeAt(e));var n=!1;for(var o in this.hoverObj.nodes)Object.prototype.hasOwnProperty.call(this.hoverObj.nodes,o)&&(void 0===i||i instanceof fO&&i.id!=o||i instanceof cC)&&(this.emitBlurEvent(t,e,this.hoverObj.nodes[o]),delete this.hoverObj.nodes[o],n=!0);for(var r in this.hoverObj.edges)Object.prototype.hasOwnProperty.call(this.hoverObj.edges,r)&&(!0===n?(this.hoverObj.edges[r].hover=!1,delete this.hoverObj.edges[r]):(void 0===i||i instanceof cC&&i.id!=r||i instanceof fO&&!i.hover)&&(this.emitBlurEvent(t,e,this.hoverObj.edges[r]),delete this.hoverObj.edges[r],n=!0));if(void 0!==i){var s=bu(this.hoverObj.edges).length,a=bu(this.hoverObj.nodes).length;(n||i instanceof cC&&0===s&&0===a||i instanceof fO&&0===s&&0===a)&&(n=this.emitHoverEvent(t,e,i)),i instanceof fO&&!0===this.options.hoverConnectedEdges&&this._hoverConnectedEdges(i)}!0===n&&this.body.emitter.emit("_requestRedraw")}},{key:"commitWithoutEmitting",value:function(){this._selectionAccumulator.commit()}},{key:"commitAndEmit",value:function(t,e){var i=!1,n=this._selectionAccumulator.commit(),o={nodes:n.nodes.previous,edges:n.edges.previous};n.edges.deleted.length>0&&(this.generateClickEvent("deselectEdge",e,t,o),i=!0),n.nodes.deleted.length>0&&(this.generateClickEvent("deselectNode",e,t,o),i=!0),n.nodes.added.length>0&&(this.generateClickEvent("selectNode",e,t),i=!0),n.edges.added.length>0&&(this.generateClickEvent("selectEdge",e,t),i=!0),!0===i&&this.generateClickEvent("select",e,t)}},{key:"getSelection",value:function(){return{nodes:this.getSelectedNodeIds(),edges:this.getSelectedEdgeIds()}}},{key:"getSelectedNodes",value:function(){return this._selectionAccumulator.getNodes()}},{key:"getSelectedEdges",value:function(){return this._selectionAccumulator.getEdges()}},{key:"getSelectedNodeIds",value:function(){var t;return gu(t=this._selectionAccumulator.getNodes()).call(t,(function(t){return t.id}))}},{key:"getSelectedEdgeIds",value:function(){var t;return gu(t=this._selectionAccumulator.getEdges()).call(t,(function(t){return t.id}))}},{key:"setSelection",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if(!t||!t.nodes&&!t.edges)throw new TypeError("Selection must be an object with nodes and/or edges properties");if((e.unselectAll||void 0===e.unselectAll)&&this.unselectAll(),t.nodes){var i,n=RS(t.nodes);try{for(n.s();!(i=n.n()).done;){var o=i.value,r=this.body.nodes[o];if(!r)throw new RangeError('Node with id "'+o+'" not found');this.selectObject(r,e.highlightEdges)}}catch(t){n.e(t)}finally{n.f()}}if(t.edges){var s,a=RS(t.edges);try{for(a.s();!(s=a.n()).done;){var h=s.value,l=this.body.edges[h];if(!l)throw new RangeError('Edge with id "'+h+'" not found');this.selectObject(l)}}catch(t){a.e(t)}finally{a.f()}}this.body.emitter.emit("_requestRedraw"),this._selectionAccumulator.commit()}},{key:"selectNodes",value:function(t){var e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];if(!t||void 0===t.length)throw"Selection must be an array with ids";this.setSelection({nodes:t},{highlightEdges:e})}},{key:"selectEdges",value:function(t){if(!t||void 0===t.length)throw"Selection must be an array with ids";this.setSelection({edges:t})}},{key:"updateSelection",value:function(){for(var t in this._selectionAccumulator.getNodes())Object.prototype.hasOwnProperty.call(this.body.nodes,t.id)||this._selectionAccumulator.deleteNodes(t);for(var e in this._selectionAccumulator.getEdges())Object.prototype.hasOwnProperty.call(this.body.edges,e.id)||this._selectionAccumulator.deleteEdges(e)}},{key:"getClickedItems",value:function(t){for(var e=this.canvas.DOMtoCanvas(t),i=[],n=this.body.nodeIndices,o=this.body.nodes,r=n.length-1;r>=0;r--){var s=o[n[r]].getItemsOnPoint(e);i.push.apply(i,s)}for(var a=this.body.edgeIndices,h=this.body.edges,l=a.length-1;l>=0;l--){var d=h[a[l]].getItemsOnPoint(e);i.push.apply(i,d)}return i}}]),t}(),WS={};!function(t){!function(t){function e(t,e){if(!(t instanceof e))throw new TypeError("Cannot call a class as a function")}t.__esModule=!0,t.sort=v;var i=32,n=7,o=256,r=[1,10,100,1e3,1e4,1e5,1e6,1e7,1e8,1e9];function s(t){return t<1e5?t<100?t<10?0:1:t<1e4?t<1e3?2:3:4:t<1e7?t<1e6?5:6:t<1e9?t<1e8?7:8:9}function a(t,e){if(t===e)return 0;if(~~t===t&&~~e===e){if(0===t||0===e)return t<e?-1:1;if(t<0||e<0){if(e>=0)return-1;if(t>=0)return 1;t=-t,e=-e}var i=s(t),n=s(e),o=0;return i<n?(t*=r[n-i-1],e/=10,o=-1):i>n&&(e*=r[i-n-1],t/=10,o=1),t===e?o:t<e?-1:1}var a=String(t),h=String(e);return a===h?0:a<h?-1:1}function h(t){for(var e=0;t>=i;)e|=1&t,t>>=1;return t+e}function l(t,e,i,n){var o=e+1;if(o===i)return 1;if(n(t[o++],t[e])<0){for(;o<i&&n(t[o],t[o-1])<0;)o++;d(t,e,o)}else for(;o<i&&n(t[o],t[o-1])>=0;)o++;return o-e}function d(t,e,i){for(i--;e<i;){var n=t[e];t[e++]=t[i],t[i--]=n}}function c(t,e,i,n,o){for(n===e&&n++;n<i;n++){for(var r=t[n],s=e,a=n;s<a;){var h=s+a>>>1;o(r,t[h])<0?a=h:s=h+1}var l=n-s;switch(l){case 3:t[s+3]=t[s+2];case 2:t[s+2]=t[s+1];case 1:t[s+1]=t[s];break;default:for(;l>0;)t[s+l]=t[s+l-1],l--}t[s]=r}}function u(t,e,i,n,o,r){var s=0,a=0,h=1;if(r(t,e[i+o])>0){for(a=n-o;h<a&&r(t,e[i+o+h])>0;)s=h,(h=1+(h<<1))<=0&&(h=a);h>a&&(h=a),s+=o,h+=o}else{for(a=o+1;h<a&&r(t,e[i+o-h])<=0;)s=h,(h=1+(h<<1))<=0&&(h=a);h>a&&(h=a);var l=s;s=o-h,h=o-l}for(s++;s<h;){var d=s+(h-s>>>1);r(t,e[i+d])>0?s=d+1:h=d}return h}function f(t,e,i,n,o,r){var s=0,a=0,h=1;if(r(t,e[i+o])<0){for(a=o+1;h<a&&r(t,e[i+o-h])<0;)s=h,(h=1+(h<<1))<=0&&(h=a);h>a&&(h=a);var l=s;s=o-h,h=o-l}else{for(a=n-o;h<a&&r(t,e[i+o+h])>=0;)s=h,(h=1+(h<<1))<=0&&(h=a);h>a&&(h=a),s+=o,h+=o}for(s++;s<h;){var d=s+(h-s>>>1);r(t,e[i+d])<0?h=d:s=d+1}return h}var p=function(){function t(i,r){e(this,t),this.array=null,this.compare=null,this.minGallop=n,this.length=0,this.tmpStorageLength=o,this.stackLength=0,this.runStart=null,this.runLength=null,this.stackSize=0,this.array=i,this.compare=r,this.length=i.length,this.length<2*o&&(this.tmpStorageLength=this.length>>>1),this.tmp=new Array(this.tmpStorageLength),this.stackLength=this.length<120?5:this.length<1542?10:this.length<119151?19:40,this.runStart=new Array(this.stackLength),this.runLength=new Array(this.stackLength)}return t.prototype.pushRun=function(t,e){this.runStart[this.stackSize]=t,this.runLength[this.stackSize]=e,this.stackSize+=1},t.prototype.mergeRuns=function(){for(;this.stackSize>1;){var t=this.stackSize-2;if(t>=1&&this.runLength[t-1]<=this.runLength[t]+this.runLength[t+1]||t>=2&&this.runLength[t-2]<=this.runLength[t]+this.runLength[t-1])this.runLength[t-1]<this.runLength[t+1]&&t--;else if(this.runLength[t]>this.runLength[t+1])break;this.mergeAt(t)}},t.prototype.forceMergeRuns=function(){for(;this.stackSize>1;){var t=this.stackSize-2;t>0&&this.runLength[t-1]<this.runLength[t+1]&&t--,this.mergeAt(t)}},t.prototype.mergeAt=function(t){var e=this.compare,i=this.array,n=this.runStart[t],o=this.runLength[t],r=this.runStart[t+1],s=this.runLength[t+1];this.runLength[t]=o+s,t===this.stackSize-3&&(this.runStart[t+1]=this.runStart[t+2],this.runLength[t+1]=this.runLength[t+2]),this.stackSize--;var a=f(i[r],i,n,o,0,e);n+=a,0!=(o-=a)&&0!==(s=u(i[n+o-1],i,r,s,s-1,e))&&(o<=s?this.mergeLow(n,o,r,s):this.mergeHigh(n,o,r,s))},t.prototype.mergeLow=function(t,e,i,o){var r=this.compare,s=this.array,a=this.tmp,h=0;for(h=0;h<e;h++)a[h]=s[t+h];var l=0,d=i,c=t;if(s[c++]=s[d++],0!=--o)if(1!==e){for(var p=this.minGallop;;){var v=0,g=0,y=!1;do{if(r(s[d],a[l])<0){if(s[c++]=s[d++],g++,v=0,0==--o){y=!0;break}}else if(s[c++]=a[l++],v++,g=0,1==--e){y=!0;break}}while((v|g)<p);if(y)break;do{if(0!==(v=f(s[d],a,l,e,0,r))){for(h=0;h<v;h++)s[c+h]=a[l+h];if(c+=v,l+=v,(e-=v)<=1){y=!0;break}}if(s[c++]=s[d++],0==--o){y=!0;break}if(0!==(g=u(a[l],s,d,o,0,r))){for(h=0;h<g;h++)s[c+h]=s[d+h];if(c+=g,d+=g,0==(o-=g)){y=!0;break}}if(s[c++]=a[l++],1==--e){y=!0;break}p--}while(v>=n||g>=n);if(y)break;p<0&&(p=0),p+=2}if(this.minGallop=p,p<1&&(this.minGallop=1),1===e){for(h=0;h<o;h++)s[c+h]=s[d+h];s[c+o]=a[l]}else{if(0===e)throw new Error("mergeLow preconditions were not respected");for(h=0;h<e;h++)s[c+h]=a[l+h]}}else{for(h=0;h<o;h++)s[c+h]=s[d+h];s[c+o]=a[l]}else for(h=0;h<e;h++)s[c+h]=a[l+h]},t.prototype.mergeHigh=function(t,e,i,o){var r=this.compare,s=this.array,a=this.tmp,h=0;for(h=0;h<o;h++)a[h]=s[i+h];var l=t+e-1,d=o-1,c=i+o-1,p=0,v=0;if(s[c--]=s[l--],0!=--e)if(1!==o){for(var g=this.minGallop;;){var y=0,m=0,b=!1;do{if(r(a[d],s[l])<0){if(s[c--]=s[l--],y++,m=0,0==--e){b=!0;break}}else if(s[c--]=a[d--],m++,y=0,1==--o){b=!0;break}}while((y|m)<g);if(b)break;do{if(0!=(y=e-f(a[d],s,t,e,e-1,r))){for(e-=y,v=1+(c-=y),p=1+(l-=y),h=y-1;h>=0;h--)s[v+h]=s[p+h];if(0===e){b=!0;break}}if(s[c--]=a[d--],1==--o){b=!0;break}if(0!=(m=o-u(s[l],a,0,o,o-1,r))){for(o-=m,v=1+(c-=m),p=1+(d-=m),h=0;h<m;h++)s[v+h]=a[p+h];if(o<=1){b=!0;break}}if(s[c--]=s[l--],0==--e){b=!0;break}g--}while(y>=n||m>=n);if(b)break;g<0&&(g=0),g+=2}if(this.minGallop=g,g<1&&(this.minGallop=1),1===o){for(v=1+(c-=e),p=1+(l-=e),h=e-1;h>=0;h--)s[v+h]=s[p+h];s[c]=a[d]}else{if(0===o)throw new Error("mergeHigh preconditions were not respected");for(p=c-(o-1),h=0;h<o;h++)s[p+h]=a[h]}}else{for(v=1+(c-=e),p=1+(l-=e),h=e-1;h>=0;h--)s[v+h]=s[p+h];s[c]=a[d]}else for(p=c-(o-1),h=0;h<o;h++)s[p+h]=a[h]},t}();function v(t,e,n,o){if(!Array.isArray(t))throw new TypeError("Can only sort arrays");e?"function"!=typeof e&&(o=n,n=e,e=a):e=a,n||(n=0),o||(o=t.length);var r=o-n;if(!(r<2)){var s=0;if(r<i)c(t,n,o,n+(s=l(t,n,o,e)),e);else{var d=new p(t,e),u=h(r);do{if((s=l(t,n,o,e))<u){var f=r;f>u&&(f=u),c(t,n,n+f,n+s,e),s=f}d.pushRun(n,s),d.mergeRuns(),r-=s,n+=s}while(0!==r);d.forceMergeRuns()}}}}(t)}(WS);var qS=WS;function VS(t){var e=function(){if("undefined"==typeof Reflect||!Mk)return!1;if(Mk.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Mk(Boolean,[],(function(){}))),!0}catch(t){return!1}}();return function(){var i,n=Ak(t);if(e){var o=Ak(this).constructor;i=Mk(n,arguments,o)}else i=n.apply(this,arguments);return Nk(this,i)}}var US=function(){function t(){Yd(this,t)}return Kd(t,[{key:"abstract",value:function(){throw new Error("Can't instantiate abstract class!")}},{key:"fake_use",value:function(){}},{key:"curveType",value:function(){return this.abstract()}},{key:"getPosition",value:function(t){return this.fake_use(t),this.abstract()}},{key:"setPosition",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:void 0;this.fake_use(t,e,i),this.abstract()}},{key:"getTreeSize",value:function(t){return this.fake_use(t),this.abstract()}},{key:"sort",value:function(t){this.fake_use(t),this.abstract()}},{key:"fix",value:function(t,e){this.fake_use(t,e),this.abstract()}},{key:"shift",value:function(t,e){this.fake_use(t,e),this.abstract()}}]),t}(),YS=function(t){zk(i,t);var e=VS(i);function i(t){var n;return Yd(this,i),(n=e.call(this)).layout=t,n}return Kd(i,[{key:"curveType",value:function(){return"horizontal"}},{key:"getPosition",value:function(t){return t.x}},{key:"setPosition",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:void 0;void 0!==i&&this.layout.hierarchical.addToOrdering(t,i),t.x=e}},{key:"getTreeSize",value:function(t){var e=this.layout.hierarchical.getTreeSize(this.layout.body.nodes,t);return{min:e.min_x,max:e.max_x}}},{key:"sort",value:function(t){qS.sort(t,(function(t,e){return t.x-e.x}))}},{key:"fix",value:function(t,e){t.y=this.layout.options.hierarchical.levelSeparation*e,t.options.fixed.y=!0}},{key:"shift",value:function(t,e){this.layout.body.nodes[t].x+=e}}]),i}(US),XS=function(t){zk(i,t);var e=VS(i);function i(t){var n;return Yd(this,i),(n=e.call(this)).layout=t,n}return Kd(i,[{key:"curveType",value:function(){return"vertical"}},{key:"getPosition",value:function(t){return t.y}},{key:"setPosition",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:void 0;void 0!==i&&this.layout.hierarchical.addToOrdering(t,i),t.y=e}},{key:"getTreeSize",value:function(t){var e=this.layout.hierarchical.getTreeSize(this.layout.body.nodes,t);return{min:e.min_y,max:e.max_y}}},{key:"sort",value:function(t){qS.sort(t,(function(t,e){return t.y-e.y}))}},{key:"fix",value:function(t,e){t.x=this.layout.options.hierarchical.levelSeparation*e,t.options.fixed.x=!0}},{key:"shift",value:function(t,e){this.layout.body.nodes[t].y+=e}}]),i}(US),GS=Wh.every;_i({target:"Array",proto:!0,forced:!Cu("every")},{every:function(t){return GS(this,t,arguments.length>1?arguments[1]:void 0)}});var KS=Tn("Array").every,$S=J,ZS=KS,QS=Array.prototype,JS=function(t){var e=t.every;return t===QS||$S(QS,t)&&e===QS.every?ZS:e},tT=JS;function eT(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return iT(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return iT(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function iT(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}function nT(t,e){var i=new b_;return Fu(t).call(t,(function(t){var e;Fu(e=t.edges).call(e,(function(t){t.connected&&i.add(t)}))})),Fu(i).call(i,(function(t){var i=t.from.id,n=t.to.id;null==e[i]&&(e[i]=0),(null==e[n]||e[i]>=e[n])&&(e[n]=e[i]+1)})),e}function oT(t,e,i,n){var o,r,s=Kp(null),a=i_(o=Jc(kx(n).call(n))).call(o,(function(t,e){return t+1+e.edges.length}),0),h=i+"Id",l="to"===i?1:-1,d=eT(n);try{var c=function(){var o=Kc(r.value,2),d=o[0],c=o[1];if(!n.has(d)||!t(c))return"continue";s[d]=0;for(var u=[c],f=0,p=void 0,v=function(){var t,o;if(!n.has(d))return"continue";var r=s[p.id]+l;if(Fu(t=Xf(o=p.edges).call(o,(function(t){return t.connected&&t.to!==t.from&&t[i]!==p&&n.has(t.toId)&&n.has(t.fromId)}))).call(t,(function(t){var n=t[h],o=s[n];(null==o||e(r,o))&&(s[n]=r,u.push(t[i]))})),f>a)return{v:{v:nT(n,s)}};++f};p=u.pop();){var g=v();if("continue"!==g&&"object"===Qc(g))return g.v}};for(d.s();!(r=d.n()).done;){var u=c();if("continue"!==u&&"object"===Qc(u))return u.v}}catch(t){d.e(t)}finally{d.f()}return s}var rT=function(){function t(){Yd(this,t),this.childrenReference={},this.parentReference={},this.trees={},this.distributionOrdering={},this.levels={},this.distributionIndex={},this.isTree=!1,this.treeIndex=-1}return Kd(t,[{key:"addRelation",value:function(t,e){void 0===this.childrenReference[t]&&(this.childrenReference[t]=[]),this.childrenReference[t].push(e),void 0===this.parentReference[e]&&(this.parentReference[e]=[]),this.parentReference[e].push(t)}},{key:"checkIfTree",value:function(){for(var t in this.parentReference)if(this.parentReference[t].length>1)return void(this.isTree=!1);this.isTree=!0}},{key:"numTrees",value:function(){return this.treeIndex+1}},{key:"setTreeIndex",value:function(t,e){void 0!==e&&void 0===this.trees[t.id]&&(this.trees[t.id]=e,this.treeIndex=Math.max(e,this.treeIndex))}},{key:"ensureLevel",value:function(t){void 0===this.levels[t]&&(this.levels[t]=0)}},{key:"getMaxLevel",value:function(t){var e=this,i={};return function t(n){if(void 0!==i[n])return i[n];var o=e.levels[n];if(e.childrenReference[n]){var r=e.childrenReference[n];if(r.length>0)for(var s=0;s<r.length;s++)o=Math.max(o,t(r[s]))}return i[n]=o,o}(t)}},{key:"levelDownstream",value:function(t,e){void 0===this.levels[e.id]&&(void 0===this.levels[t.id]&&(this.levels[t.id]=0),this.levels[e.id]=this.levels[t.id]+1)}},{key:"setMinLevelToZero",value:function(t){var e=1e9;for(var i in t)Object.prototype.hasOwnProperty.call(t,i)&&void 0!==this.levels[i]&&(e=Math.min(this.levels[i],e));for(var n in t)Object.prototype.hasOwnProperty.call(t,n)&&void 0!==this.levels[n]&&(this.levels[n]-=e)}},{key:"getTreeSize",value:function(t,e){var i=1e9,n=-1e9,o=1e9,r=-1e9;for(var s in this.trees)if(Object.prototype.hasOwnProperty.call(this.trees,s)&&this.trees[s]===e){var a=t[s];i=Math.min(a.x,i),n=Math.max(a.x,n),o=Math.min(a.y,o),r=Math.max(a.y,r)}return{min_x:i,max_x:n,min_y:o,max_y:r}}},{key:"hasSameParent",value:function(t,e){var i=this.parentReference[t.id],n=this.parentReference[e.id];if(void 0===i||void 0===n)return!1;for(var o=0;o<i.length;o++)for(var r=0;r<n.length;r++)if(i[o]==n[r])return!0;return!1}},{key:"inSameSubNetwork",value:function(t,e){return this.trees[t.id]===this.trees[e.id]}},{key:"getLevels",value:function(){return bu(this.distributionOrdering)}},{key:"addToOrdering",value:function(t,e){void 0===this.distributionOrdering[e]&&(this.distributionOrdering[e]=[]);var i=!1,n=this.distributionOrdering[e];for(var o in n)if(n[o]===t){i=!0;break}i||(this.distributionOrdering[e].push(t),this.distributionIndex[t.id]=this.distributionOrdering[e].length-1)}}]),t}(),sT=function(){function t(e){Yd(this,t),this.body=e,this._resetRNG(Math.random()+":"+Eu()),this.setPhysics=!1,this.options={},this.optionsBackup={physics:{}},this.defaultOptions={randomSeed:void 0,improvedLayout:!0,clusterThreshold:150,hierarchical:{enabled:!1,levelSeparation:150,nodeSpacing:100,treeSpacing:200,blockShifting:!0,edgeMinimization:!0,parentCentralization:!0,direction:"UD",sortMethod:"hubsize"}},un(this.options,this.defaultOptions),this.bindEventListeners()}return Kd(t,[{key:"bindEventListeners",value:function(){var t=this;this.body.emitter.on("_dataChanged",(function(){t.setupHierarchicalLayout()})),this.body.emitter.on("_dataLoaded",(function(){t.layoutNetwork()})),this.body.emitter.on("_resetHierarchicalLayout",(function(){t.setupHierarchicalLayout()})),this.body.emitter.on("_adjustEdgesForHierarchicalLayout",(function(){if(!0===t.options.hierarchical.enabled){var e=t.direction.curveType();t.body.emitter.emit("_forceDisableDynamicCurves",e,!1)}}))}},{key:"setOptions",value:function(t,e){if(void 0!==t){var i=this.options.hierarchical,n=i.enabled;if(em(["randomSeed","improvedLayout","clusterThreshold"],this.options,t),Sm(this.options,t,"hierarchical"),void 0!==t.randomSeed&&this._resetRNG(t.randomSeed),!0===i.enabled)return!0===n&&this.body.emitter.emit("refresh",!0),"RL"===i.direction||"DU"===i.direction?i.levelSeparation>0&&(i.levelSeparation*=-1):i.levelSeparation<0&&(i.levelSeparation*=-1),this.setDirectionStrategy(),this.body.emitter.emit("_resetHierarchicalLayout"),this.adaptAllOptionsForHierarchicalLayout(e);if(!0===n)return this.body.emitter.emit("refresh"),nm(e,this.optionsBackup)}return e}},{key:"_resetRNG",value:function(t){this.initialRandomSeed=t,this._rng=jy(this.initialRandomSeed)}},{key:"adaptAllOptionsForHierarchicalLayout",value:function(t){if(!0===this.options.hierarchical.enabled){var e=this.optionsBackup.physics;void 0===t.physics||!0===t.physics?(t.physics={enabled:void 0===e.enabled||e.enabled,solver:"hierarchicalRepulsion"},e.enabled=void 0===e.enabled||e.enabled,e.solver=e.solver||"barnesHut"):"object"===Qc(t.physics)?(e.enabled=void 0===t.physics.enabled||t.physics.enabled,e.solver=t.physics.solver||"barnesHut",t.physics.solver="hierarchicalRepulsion"):!1!==t.physics&&(e.solver="barnesHut",t.physics={solver:"hierarchicalRepulsion"});var i=this.direction.curveType();if(void 0===t.edges)this.optionsBackup.edges={smooth:{enabled:!0,type:"dynamic"}},t.edges={smooth:!1};else if(void 0===t.edges.smooth)this.optionsBackup.edges={smooth:{enabled:!0,type:"dynamic"}},t.edges.smooth=!1;else if("boolean"==typeof t.edges.smooth)this.optionsBackup.edges={smooth:t.edges.smooth},t.edges.smooth={enabled:t.edges.smooth,type:i};else{var n=t.edges.smooth;void 0!==n.type&&"dynamic"!==n.type&&(i=n.type),this.optionsBackup.edges={smooth:{enabled:void 0===n.enabled||n.enabled,type:void 0===n.type?"dynamic":n.type,roundness:void 0===n.roundness?.5:n.roundness,forceDirection:void 0!==n.forceDirection&&n.forceDirection}},t.edges.smooth={enabled:void 0===n.enabled||n.enabled,type:i,roundness:void 0===n.roundness?.5:n.roundness,forceDirection:void 0!==n.forceDirection&&n.forceDirection}}this.body.emitter.emit("_forceDisableDynamicCurves",i)}return t}},{key:"positionInitially",value:function(t){if(!0!==this.options.hierarchical.enabled){this._resetRNG(this.initialRandomSeed);for(var e=t.length+50,i=0;i<t.length;i++){var n=t[i],o=2*Math.PI*this._rng();void 0===n.x&&(n.x=e*Math.cos(o)),void 0===n.y&&(n.y=e*Math.sin(o))}}}},{key:"layoutNetwork",value:function(){if(!0!==this.options.hierarchical.enabled&&!0===this.options.improvedLayout){for(var t=this.body.nodeIndices,e=0,i=0;i<t.length;i++){!0===this.body.nodes[t[i]].predefinedPosition&&(e+=1)}if(e<.5*t.length){var n=0,o=this.options.clusterThreshold,r={clusterNodeProperties:{shape:"ellipse",label:"",group:"",font:{multi:!1}},clusterEdgeProperties:{label:"",font:{multi:!1},smooth:{enabled:!1}}};if(t.length>o){for(var s=t.length;t.length>o&&n<=10;){n+=1;var a=t.length;if(n%3==0?this.body.modules.clustering.clusterBridges(r):this.body.modules.clustering.clusterOutliers(r),a==t.length&&n%3!=0)return this._declusterAll(),this.body.emitter.emit("_layoutFailed"),void console.info("This network could not be positioned by this version of the improved layout algorithm. Please disable improvedLayout for better performance.")}this.body.modules.kamadaKawai.setOptions({springLength:Math.max(150,2*s)})}n>10&&console.info("The clustering didn't succeed within the amount of interations allowed, progressing with partial result."),this.body.modules.kamadaKawai.solve(t,this.body.edgeIndices,!0),this._shiftToCenter();for(var h=0;h<t.length;h++){var l=this.body.nodes[t[h]];!1===l.predefinedPosition&&(l.x+=70*(.5-this._rng()),l.y+=70*(.5-this._rng()))}this._declusterAll(),this.body.emitter.emit("_repositionBezierNodes")}}}},{key:"_shiftToCenter",value:function(){for(var t=EC.getRangeCore(this.body.nodes,this.body.nodeIndices),e=EC.findCenter(t),i=0;i<this.body.nodeIndices.length;i++){var n=this.body.nodes[this.body.nodeIndices[i]];n.x-=e.x,n.y-=e.y}}},{key:"_declusterAll",value:function(){for(var t=!0;!0===t;){t=!1;for(var e=0;e<this.body.nodeIndices.length;e++)!0===this.body.nodes[this.body.nodeIndices[e]].isCluster&&(t=!0,this.body.modules.clustering.openCluster(this.body.nodeIndices[e],{},!1));!0===t&&this.body.emitter.emit("_dataChanged")}}},{key:"getSeed",value:function(){return this.initialRandomSeed}},{key:"setupHierarchicalLayout",value:function(){if(!0===this.options.hierarchical.enabled&&this.body.nodeIndices.length>0){var t,e,i=!1,n=!1;for(e in this.lastNodeOnLevel={},this.hierarchical=new rT,this.body.nodes)Object.prototype.hasOwnProperty.call(this.body.nodes,e)&&(void 0!==(t=this.body.nodes[e]).options.level?(i=!0,this.hierarchical.levels[e]=t.options.level):n=!0);if(!0===n&&!0===i)throw new Error("To use the hierarchical layout, nodes require either no predefined levels or levels have to be defined for all nodes.");if(!0===n){var o=this.options.hierarchical.sortMethod;"hubsize"===o?this._determineLevelsByHubsize():"directed"===o?this._determineLevelsDirected():"custom"===o&&this._determineLevelsCustomCallback()}for(var r in this.body.nodes)Object.prototype.hasOwnProperty.call(this.body.nodes,r)&&this.hierarchical.ensureLevel(r);var s=this._getDistribution();this._generateMap(),this._placeNodesByHierarchy(s),this._condenseHierarchy(),this._shiftToCenter()}}},{key:"_condenseHierarchy",value:function(){var t=this,e=!1,i={},n=function(e,i){var n=t.hierarchical.trees;for(var o in n)Object.prototype.hasOwnProperty.call(n,o)&&n[o]===e&&t.direction.shift(o,i)},o=function(){for(var e=[],i=0;i<t.hierarchical.numTrees();i++)e.push(t.direction.getTreeSize(i));return e},r=function e(i,n){if(!n[i.id]&&(n[i.id]=!0,t.hierarchical.childrenReference[i.id])){var o=t.hierarchical.childrenReference[i.id];if(o.length>0)for(var r=0;r<o.length;r++)e(t.body.nodes[o[r]],n)}},s=function(e){var i=arguments.length>1&&void 0!==arguments[1]?arguments[1]:1e9,n=1e9,o=1e9,r=1e9,s=-1e9;for(var a in e)if(Object.prototype.hasOwnProperty.call(e,a)){var h=t.body.nodes[a],l=t.hierarchical.levels[h.id],d=t.direction.getPosition(h),c=t._getSpaceAroundNode(h,e),u=Kc(c,2),f=u[0],p=u[1];n=Math.min(f,n),o=Math.min(p,o),l<=i&&(r=Math.min(d,r),s=Math.max(d,s))}return[r,s,n,o]},a=function(e,i){var n=t.hierarchical.getMaxLevel(e.id),o=t.hierarchical.getMaxLevel(i.id);return Math.min(n,o)},h=function(e,i,n){for(var o=t.hierarchical,r=0;r<i.length;r++){var s=i[r],a=o.distributionOrdering[s];if(a.length>1)for(var h=0;h<a.length-1;h++){var l=a[h],d=a[h+1];o.hasSameParent(l,d)&&o.inSameSubNetwork(l,d)&&e(l,d,n)}}},l=function(i,n){var o=arguments.length>2&&void 0!==arguments[2]&&arguments[2],h=t.direction.getPosition(i),l=t.direction.getPosition(n),d=Math.abs(l-h),c=t.options.hierarchical.nodeSpacing;if(d>c){var u={},f={};r(i,u),r(n,f);var p=a(i,n),v=s(u,p),g=s(f,p),y=v[1],m=g[0],b=g[2],w=Math.abs(y-m);if(w>c){var k=y-m+c;k<-b+c&&(k=-b+c),k<0&&(t._shiftBlock(n.id,k),e=!0,!0===o&&t._centerParent(n))}}},d=function(n,o){for(var a=o.id,h=o.edges,l=t.hierarchical.levels[o.id],d=t.options.hierarchical.levelSeparation*t.options.hierarchical.levelSeparation,c={},u=[],f=0;f<h.length;f++){var p=h[f];if(p.toId!=p.fromId){var v=p.toId==a?p.from:p.to;c[h[f].id]=v,t.hierarchical.levels[v.id]<l&&u.push(p)}}var g=function(e,i){for(var n=0,o=0;o<i.length;o++)if(void 0!==c[i[o].id]){var r=t.direction.getPosition(c[i[o].id])-e;n+=r/Math.sqrt(r*r+d)}return n},y=function(e,i){for(var n=0,o=0;o<i.length;o++)if(void 0!==c[i[o].id]){var r=t.direction.getPosition(c[i[o].id])-e;n-=d*Math.pow(r*r+d,-1.5)}return n},m=function(e,i){for(var n=t.direction.getPosition(o),r={},s=0;s<e;s++){var a=g(n,i),h=y(n,i);if(void 0!==r[n-=Math.max(-40,Math.min(40,Math.round(a/h)))])break;r[n]=s}return n},b=m(n,u);!function(n){var a=t.direction.getPosition(o);if(void 0===i[o.id]){var h={};r(o,h),i[o.id]=h}var l=s(i[o.id]),d=l[2],c=l[3],u=n-a,f=0;u>0?f=Math.min(u,c-t.options.hierarchical.nodeSpacing):u<0&&(f=-Math.min(-u,d-t.options.hierarchical.nodeSpacing)),0!=f&&(t._shiftBlock(o.id,f),e=!0)}(b),function(i){var n=t.direction.getPosition(o),r=Kc(t._getSpaceAroundNode(o),2),s=r[0],a=r[1],h=i-n,l=n;h>0?l=Math.min(n+(a-t.options.hierarchical.nodeSpacing),i):h<0&&(l=Math.max(n-(s-t.options.hierarchical.nodeSpacing),i)),l!==n&&(t.direction.setPosition(o,l),e=!0)}(b=m(n,h))};!0===this.options.hierarchical.blockShifting&&(function(i){var n=t.hierarchical.getLevels();n=Yu(n).call(n);for(var o=0;o<i&&(e=!1,h(l,n,!0),!0===e);o++);}(5),function(){for(var e in t.body.nodes)Object.prototype.hasOwnProperty.call(t.body.nodes,e)&&t._centerParent(t.body.nodes[e])}()),!0===this.options.hierarchical.edgeMinimization&&function(i){var n=t.hierarchical.getLevels();n=Yu(n).call(n);for(var o=0;o<i;o++){e=!1;for(var r=0;r<n.length;r++)for(var s=n[r],a=t.hierarchical.distributionOrdering[s],h=0;h<a.length;h++)d(1e3,a[h]);if(!0!==e)break}}(20),!0===this.options.hierarchical.parentCentralization&&function(){var e=t.hierarchical.getLevels();e=Yu(e).call(e);for(var i=0;i<e.length;i++)for(var n=e[i],o=t.hierarchical.distributionOrdering[n],r=0;r<o.length;r++)t._centerParent(o[r])}(),function(){for(var e=o(),i=0,r=0;r<e.length-1;r++){i+=e[r].max-e[r+1].min+t.options.hierarchical.treeSpacing,n(r+1,i)}}()}},{key:"_getSpaceAroundNode",value:function(t,e){var i=!0;void 0===e&&(i=!1);var n=this.hierarchical.levels[t.id];if(void 0!==n){var o=this.hierarchical.distributionIndex[t.id],r=this.direction.getPosition(t),s=this.hierarchical.distributionOrdering[n],a=1e9,h=1e9;if(0!==o){var l=s[o-1];if(!0===i&&void 0===e[l.id]||!1===i)a=r-this.direction.getPosition(l)}if(o!=s.length-1){var d=s[o+1];if(!0===i&&void 0===e[d.id]||!1===i){var c=this.direction.getPosition(d);h=Math.min(h,c-r)}}return[a,h]}return[0,0]}},{key:"_centerParent",value:function(t){if(this.hierarchical.parentReference[t.id])for(var e=this.hierarchical.parentReference[t.id],i=0;i<e.length;i++){var n=e[i],o=this.body.nodes[n],r=this.hierarchical.childrenReference[n];if(void 0!==r){var s=this._getCenterPosition(r),a=this.direction.getPosition(o),h=Kc(this._getSpaceAroundNode(o),2),l=h[0],d=h[1],c=a-s;(c<0&&Math.abs(c)<d-this.options.hierarchical.nodeSpacing||c>0&&Math.abs(c)<l-this.options.hierarchical.nodeSpacing)&&this.direction.setPosition(o,s)}}}},{key:"_placeNodesByHierarchy",value:function(t){for(var e in this.positionedNodes={},t)if(Object.prototype.hasOwnProperty.call(t,e)){var i,n=bu(t[e]);n=this._indexArrayToNodes(n),rx(i=this.direction).call(i,n);for(var o=0,r=0;r<n.length;r++){var s=n[r];if(void 0===this.positionedNodes[s.id]){var a=this.options.hierarchical.nodeSpacing,h=a*o;o>0&&(h=this.direction.getPosition(n[r-1])+a),this.direction.setPosition(s,h,e),this._validatePositionAndContinue(s,e,h),o++}}}}},{key:"_placeBranchNodes",value:function(t,e){var i,n=this.hierarchical.childrenReference[t];if(void 0!==n){for(var o=[],r=0;r<n.length;r++)o.push(this.body.nodes[n[r]]);rx(i=this.direction).call(i,o);for(var s=0;s<o.length;s++){var a=o[s],h=this.hierarchical.levels[a.id];if(!(h>e&&void 0===this.positionedNodes[a.id]))return;var l=this.options.hierarchical.nodeSpacing,d=void 0;d=0===s?this.direction.getPosition(this.body.nodes[t]):this.direction.getPosition(o[s-1])+l,this.direction.setPosition(a,d,h),this._validatePositionAndContinue(a,h,d)}var c=this._getCenterPosition(o);this.direction.setPosition(this.body.nodes[t],c,e)}}},{key:"_validatePositionAndContinue",value:function(t,e,i){if(this.hierarchical.isTree){if(void 0!==this.lastNodeOnLevel[e]){var n=this.direction.getPosition(this.body.nodes[this.lastNodeOnLevel[e]]);if(i-n<this.options.hierarchical.nodeSpacing){var o=n+this.options.hierarchical.nodeSpacing-i,r=this._findCommonParent(this.lastNodeOnLevel[e],t.id);this._shiftBlock(r.withChild,o)}}this.lastNodeOnLevel[e]=t.id,this.positionedNodes[t.id]=!0,this._placeBranchNodes(t.id,e)}}},{key:"_indexArrayToNodes",value:function(t){for(var e=[],i=0;i<t.length;i++)e.push(this.body.nodes[t[i]]);return e}},{key:"_getDistribution",value:function(){var t,e,i={};for(t in this.body.nodes)if(Object.prototype.hasOwnProperty.call(this.body.nodes,t)){e=this.body.nodes[t];var n=void 0===this.hierarchical.levels[t]?0:this.hierarchical.levels[t];this.direction.fix(e,n),void 0===i[n]&&(i[n]={}),i[n][t]=e}return i}},{key:"_getActiveEdges",value:function(t){var e=this,i=[];return hm(t.edges,(function(t){var n;-1!==Fp(n=e.body.edgeIndices).call(n,t.id)&&i.push(t)})),i}},{key:"_getHubSizes",value:function(){var t=this,e={};hm(this.body.nodeIndices,(function(i){var n=t.body.nodes[i],o=t._getActiveEdges(n).length;e[o]=!0}));var i=[];return hm(e,(function(t){i.push(Number(t))})),rx(qS).call(qS,i,(function(t,e){return e-t})),i}},{key:"_determineLevelsByHubsize",value:function(){for(var t=this,e=function(e,i){t.hierarchical.levelDownstream(e,i)},i=this._getHubSizes(),n=function(n){var o=i[n];if(0===o)return"break";hm(t.body.nodeIndices,(function(i){var n=t.body.nodes[i];o===t._getActiveEdges(n).length&&t._crawlNetwork(e,i)}))},o=0;o<i.length;++o){if("break"===n(o))break}}},{key:"_determineLevelsCustomCallback",value:function(){var t=this;this._crawlNetwork((function(e,i,n){var o=t.hierarchical.levels[e.id];void 0===o&&(o=t.hierarchical.levels[e.id]=1e5);var r=(EC.cloneOptions(e,"node"),EC.cloneOptions(i,"node"),void EC.cloneOptions(n,"edge"));t.hierarchical.levels[i.id]=o+r})),this.hierarchical.setMinLevelToZero(this.body.nodes)}},{key:"_determineLevelsDirected",value:function(){var t,e=this,i=i_(t=this.body.nodeIndices).call(t,(function(t,i){return t.set(i,e.body.nodes[i]),t}),new Jw);"roots"===this.options.hierarchical.shakeTowards?this.hierarchical.levels=function(t){return oT((function(e){var i,n;return tT(i=Xf(n=e.edges).call(n,(function(e){return t.has(e.toId)}))).call(i,(function(t){return t.from===e}))}),(function(t,e){return e<t}),"to",t)}(i):this.hierarchical.levels=function(t){return oT((function(e){var i,n;return tT(i=Xf(n=e.edges).call(n,(function(e){return t.has(e.toId)}))).call(i,(function(t){return t.to===e}))}),(function(t,e){return e>t}),"from",t)}(i),this.hierarchical.setMinLevelToZero(this.body.nodes)}},{key:"_generateMap",value:function(){var t=this;this._crawlNetwork((function(e,i){t.hierarchical.levels[i.id]>t.hierarchical.levels[e.id]&&t.hierarchical.addRelation(e.id,i.id)})),this.hierarchical.checkIfTree()}},{key:"_crawlNetwork",value:function(){var t=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:function(){},i=arguments.length>1?arguments[1]:void 0,n={},o=function i(o,r){if(void 0===n[o.id]){var s;t.hierarchical.setTreeIndex(o,r),n[o.id]=!0;for(var a=t._getActiveEdges(o),h=0;h<a.length;h++){var l=a[h];!0===l.connected&&(s=l.toId==o.id?l.from:l.to,o.id!=s.id&&(e(o,s,l),i(s,r)))}}};if(void 0===i)for(var r=0,s=0;s<this.body.nodeIndices.length;s++){var a=this.body.nodeIndices[s];if(void 0===n[a]){var h=this.body.nodes[a];o(h,r),r+=1}}else{var l=this.body.nodes[i];if(void 0===l)return void console.error("Node not found:",i);o(l)}}},{key:"_shiftBlock",value:function(t,e){var i=this,n={};!function t(o){if(!n[o]){n[o]=!0,i.direction.shift(o,e);var r=i.hierarchical.childrenReference[o];if(void 0!==r)for(var s=0;s<r.length;s++)t(r[s])}}(t)}},{key:"_findCommonParent",value:function(t,e){var i=this,n={};return function t(e,n){var o=i.hierarchical.parentReference[n];if(void 0!==o)for(var r=0;r<o.length;r++){var s=o[r];e[s]=!0,t(e,s)}}(n,t),function t(e,n){var o=i.hierarchical.parentReference[n];if(void 0!==o)for(var r=0;r<o.length;r++){var s=o[r];if(void 0!==e[s])return{foundParent:s,withChild:n};var a=t(e,s);if(null!==a.foundParent)return a}return{foundParent:null,withChild:n}}(n,e)}},{key:"setDirectionStrategy",value:function(){var t="UD"===this.options.hierarchical.direction||"DU"===this.options.hierarchical.direction;this.direction=t?new YS(this):new XS(this)}},{key:"_getCenterPosition",value:function(t){for(var e=1e9,i=-1e9,n=0;n<t.length;n++){var o=void 0;if(void 0!==t[n].id)o=t[n];else{var r=t[n];o=this.body.nodes[r]}var s=this.direction.getPosition(o);e=Math.min(e,s),i=Math.max(i,s)}return.5*(e+i)}}]),t}();function aT(t,e){var i=void 0!==tu&&ih(t)||t["@@iterator"];if(!i){if(lu(t)||(i=function(t,e){var i;if(!t)return;if("string"==typeof t)return hT(t,e);var n=au(i=Object.prototype.toString.call(t)).call(i,8,-1);"Object"===n&&t.constructor&&(n=t.constructor.name);if("Map"===n||"Set"===n)return ja(t);if("Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return hT(t,e)}(t))||e&&t&&"number"==typeof t.length){i&&(t=i);var n=0,o=function(){};return{s:o,n:function(){return n>=t.length?{done:!0}:{done:!1,value:t[n++]}},e:function(t){throw t},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,s=!0,a=!1;return{s:function(){i=i.call(t)},n:function(){var t=i.next();return s=t.done,t},e:function(t){a=!0,r=t},f:function(){try{s||null==i.return||i.return()}finally{if(a)throw r}}}}function hT(t,e){(null==e||e>t.length)&&(e=t.length);for(var i=0,n=new Array(e);i<e;i++)n[i]=t[i];return n}var lT=function(){function t(e,i,n,o){var r,s,a=this;Yd(this,t),this.body=e,this.canvas=i,this.selectionHandler=n,this.interactionHandler=o,this.editMode=!1,this.manipulationDiv=void 0,this.editModeDiv=void 0,this.closeDiv=void 0,this._domEventListenerCleanupQueue=[],this.temporaryUIFunctions={},this.temporaryEventFunctions=[],this.touchTime=0,this.temporaryIds={nodes:[],edges:[]},this.guiEnabled=!1,this.inMode=!1,this.selectedControlNode=void 0,this.options={},this.defaultOptions={enabled:!1,initiallyActive:!1,addNode:!0,addEdge:!0,editNode:void 0,editEdge:!0,deleteNode:!0,deleteEdge:!0,controlNodeStyle:{shape:"dot",size:6,color:{background:"#ff0000",border:"#3c3c3c",highlight:{background:"#07f968",border:"#3c3c3c"}},borderWidth:2,borderWidthSelected:2}},un(this.options,this.defaultOptions),this.body.emitter.on("destroy",(function(){a._clean()})),this.body.emitter.on("_dataChanged",zn(r=this._restore).call(r,this)),this.body.emitter.on("_resetData",zn(s=this._restore).call(s,this))}return Kd(t,[{key:"_restore",value:function(){!1!==this.inMode&&(!0===this.options.initiallyActive?this.enableEditMode():this.disableEditMode())}},{key:"setOptions",value:function(t,e,i){void 0!==e&&(void 0!==e.locale?this.options.locale=e.locale:this.options.locale=i.locale,void 0!==e.locales?this.options.locales=e.locales:this.options.locales=i.locales),void 0!==t&&("boolean"==typeof t?this.options.enabled=t:(this.options.enabled=!0,nm(this.options,t)),!0===this.options.initiallyActive&&(this.editMode=!0),this._setup())}},{key:"toggleEditMode",value:function(){!0===this.editMode?this.disableEditMode():this.enableEditMode()}},{key:"enableEditMode",value:function(){this.editMode=!0,this._clean(),!0===this.guiEnabled&&(this.manipulationDiv.style.display="block",this.closeDiv.style.display="block",this.editModeDiv.style.display="none",this.showManipulatorToolbar())}},{key:"disableEditMode",value:function(){this.editMode=!1,this._clean(),!0===this.guiEnabled&&(this.manipulationDiv.style.display="none",this.closeDiv.style.display="none",this.editModeDiv.style.display="block",this._createEditButton())}},{key:"showManipulatorToolbar",value:function(){if(this._clean(),this.manipulationDOM={},!0===this.guiEnabled){var t,e;this.editMode=!0,this.manipulationDiv.style.display="block",this.closeDiv.style.display="block";var i=this.selectionHandler.getSelectedNodeCount(),n=this.selectionHandler.getSelectedEdgeCount(),o=i+n,r=this.options.locales[this.options.locale],s=!1;!1!==this.options.addNode&&(this._createAddNodeButton(r),s=!0),!1!==this.options.addEdge&&(!0===s?this._createSeperator(1):s=!0,this._createAddEdgeButton(r)),1===i&&"function"==typeof this.options.editNode?(!0===s?this._createSeperator(2):s=!0,this._createEditNodeButton(r)):1===n&&0===i&&!1!==this.options.editEdge&&(!0===s?this._createSeperator(3):s=!0,this._createEditEdgeButton(r)),0!==o&&(i>0&&!1!==this.options.deleteNode||0===i&&!1!==this.options.deleteEdge)&&(!0===s&&this._createSeperator(4),this._createDeleteButton(r)),this._bindElementEvents(this.closeDiv,zn(t=this.toggleEditMode).call(t,this)),this._temporaryBindEvent("select",zn(e=this.showManipulatorToolbar).call(e,this))}this.body.emitter.emit("_redraw")}},{key:"addNodeMode",value:function(){var t;if(!0!==this.editMode&&this.enableEditMode(),this._clean(),this.inMode="addNode",!0===this.guiEnabled){var e,i=this.options.locales[this.options.locale];this.manipulationDOM={},this._createBackButton(i),this._createSeperator(),this._createDescription(i.addDescription||this.options.locales.en.addDescription),this._bindElementEvents(this.closeDiv,zn(e=this.toggleEditMode).call(e,this))}this._temporaryBindEvent("click",zn(t=this._performAddNode).call(t,this))}},{key:"editNode",value:function(){var t=this;!0!==this.editMode&&this.enableEditMode(),this._clean();var e=this.selectionHandler.getSelectedNodes()[0];if(void 0!==e){if(this.inMode="editNode","function"!=typeof this.options.editNode)throw new Error("No function has been configured to handle the editing of nodes.");if(!0!==e.isCluster){var i=nm({},e.options,!1);if(i.x=e.x,i.y=e.y,2!==this.options.editNode.length)throw new Error("The function for edit does not support two arguments (data, callback)");this.options.editNode(i,(function(e){null!=e&&"editNode"===t.inMode&&t.body.data.nodes.getDataSet().update(e),t.showManipulatorToolbar()}))}else alert(this.options.locales[this.options.locale].editClusterError||this.options.locales.en.editClusterError)}else this.showManipulatorToolbar()}},{key:"addEdgeMode",value:function(){var t,e,i,n,o;if(!0!==this.editMode&&this.enableEditMode(),this._clean(),this.inMode="addEdge",!0===this.guiEnabled){var r,s=this.options.locales[this.options.locale];this.manipulationDOM={},this._createBackButton(s),this._createSeperator(),this._createDescription(s.edgeDescription||this.options.locales.en.edgeDescription),this._bindElementEvents(this.closeDiv,zn(r=this.toggleEditMode).call(r,this))}this._temporaryBindUI("onTouch",zn(t=this._handleConnect).call(t,this)),this._temporaryBindUI("onDragEnd",zn(e=this._finishConnect).call(e,this)),this._temporaryBindUI("onDrag",zn(i=this._dragControlNode).call(i,this)),this._temporaryBindUI("onRelease",zn(n=this._finishConnect).call(n,this)),this._temporaryBindUI("onDragStart",zn(o=this._dragStartEdge).call(o,this)),this._temporaryBindUI("onHold",(function(){}))}},{key:"editEdgeMode",value:function(){if(!0!==this.editMode&&this.enableEditMode(),this._clean(),this.inMode="editEdge","object"!==Qc(this.options.editEdge)||"function"!=typeof this.options.editEdge.editWithoutDrag||(this.edgeBeingEditedId=this.selectionHandler.getSelectedEdgeIds()[0],void 0===this.edgeBeingEditedId)){if(!0===this.guiEnabled){var t,e=this.options.locales[this.options.locale];this.manipulationDOM={},this._createBackButton(e),this._createSeperator(),this._createDescription(e.editEdgeDescription||this.options.locales.en.editEdgeDescription),this._bindElementEvents(this.closeDiv,zn(t=this.toggleEditMode).call(t,this))}if(this.edgeBeingEditedId=this.selectionHandler.getSelectedEdgeIds()[0],void 0!==this.edgeBeingEditedId){var i,n,o,r,s=this.body.edges[this.edgeBeingEditedId],a=this._getNewTargetNode(s.from.x,s.from.y),h=this._getNewTargetNode(s.to.x,s.to.y);this.temporaryIds.nodes.push(a.id),this.temporaryIds.nodes.push(h.id),this.body.nodes[a.id]=a,this.body.nodeIndices.push(a.id),this.body.nodes[h.id]=h,this.body.nodeIndices.push(h.id),this._temporaryBindUI("onTouch",zn(i=this._controlNodeTouch).call(i,this)),this._temporaryBindUI("onTap",(function(){})),this._temporaryBindUI("onHold",(function(){})),this._temporaryBindUI("onDragStart",zn(n=this._controlNodeDragStart).call(n,this)),this._temporaryBindUI("onDrag",zn(o=this._controlNodeDrag).call(o,this)),this._temporaryBindUI("onDragEnd",zn(r=this._controlNodeDragEnd).call(r,this)),this._temporaryBindUI("onMouseMove",(function(){})),this._temporaryBindEvent("beforeDrawing",(function(t){var e=s.edgeType.findBorderPositions(t);!1===a.selected&&(a.x=e.from.x,a.y=e.from.y),!1===h.selected&&(h.x=e.to.x,h.y=e.to.y)})),this.body.emitter.emit("_redraw")}else this.showManipulatorToolbar()}else{var l=this.body.edges[this.edgeBeingEditedId];this._performEditEdge(l.from.id,l.to.id)}}},{key:"deleteSelected",value:function(){var t=this;!0!==this.editMode&&this.enableEditMode(),this._clean(),this.inMode="delete";var e=this.selectionHandler.getSelectedNodeIds(),i=this.selectionHandler.getSelectedEdgeIds(),n=void 0;if(e.length>0){for(var o=0;o<e.length;o++)if(!0===this.body.nodes[e[o]].isCluster)return void alert(this.options.locales[this.options.locale].deleteClusterError||this.options.locales.en.deleteClusterError);"function"==typeof this.options.deleteNode&&(n=this.options.deleteNode)}else i.length>0&&"function"==typeof this.options.deleteEdge&&(n=this.options.deleteEdge);if("function"==typeof n){var r={nodes:e,edges:i};if(2!==n.length)throw new Error("The function for delete does not support two arguments (data, callback)");n(r,(function(e){null!=e&&"delete"===t.inMode?(t.body.data.edges.getDataSet().remove(e.edges),t.body.data.nodes.getDataSet().remove(e.nodes),t.body.emitter.emit("startSimulation"),t.showManipulatorToolbar()):(t.body.emitter.emit("startSimulation"),t.showManipulatorToolbar())}))}else this.body.data.edges.getDataSet().remove(i),this.body.data.nodes.getDataSet().remove(e),this.body.emitter.emit("startSimulation"),this.showManipulatorToolbar()}},{key:"_setup",value:function(){!0===this.options.enabled?(this.guiEnabled=!0,this._createWrappers(),!1===this.editMode?this._createEditButton():this.showManipulatorToolbar()):(this._removeManipulationDOM(),this.guiEnabled=!1)}},{key:"_createWrappers",value:function(){var t,e;(void 0===this.manipulationDiv&&(this.manipulationDiv=document.createElement("div"),this.manipulationDiv.className="vis-manipulation",!0===this.editMode?this.manipulationDiv.style.display="block":this.manipulationDiv.style.display="none",this.canvas.frame.appendChild(this.manipulationDiv)),void 0===this.editModeDiv&&(this.editModeDiv=document.createElement("div"),this.editModeDiv.className="vis-edit-mode",!0===this.editMode?this.editModeDiv.style.display="none":this.editModeDiv.style.display="block",this.canvas.frame.appendChild(this.editModeDiv)),void 0===this.closeDiv)&&(this.closeDiv=document.createElement("button"),this.closeDiv.className="vis-close",this.closeDiv.setAttribute("aria-label",null!==(t=null===(e=this.options.locales[this.options.locale])||void 0===e?void 0:e.close)&&void 0!==t?t:this.options.locales.en.close),this.closeDiv.style.display=this.manipulationDiv.style.display,this.canvas.frame.appendChild(this.closeDiv))}},{key:"_getNewTargetNode",value:function(t,e){var i=nm({},this.options.controlNodeStyle);i.id="targetNode"+Ax(),i.hidden=!1,i.physics=!1,i.x=t,i.y=e;var n=this.body.functions.createNode(i);return n.shape.boundingBox={left:t,right:t,top:e,bottom:e},n}},{key:"_createEditButton",value:function(){var t;this._clean(),this.manipulationDOM={},Ky(this.editModeDiv);var e=this.options.locales[this.options.locale],i=this._createButton("editMode","vis-edit vis-edit-mode",e.edit||this.options.locales.en.edit);this.editModeDiv.appendChild(i),this._bindElementEvents(i,zn(t=this.toggleEditMode).call(t,this))}},{key:"_clean",value:function(){this.inMode=!1,!0===this.guiEnabled&&(Ky(this.editModeDiv),Ky(this.manipulationDiv),this._cleanupDOMEventListeners()),this._cleanupTemporaryNodesAndEdges(),this._unbindTemporaryUIs(),this._unbindTemporaryEvents(),this.body.emitter.emit("restorePhysics")}},{key:"_cleanupDOMEventListeners",value:function(){var t,e,i=aT(ff(t=this._domEventListenerCleanupQueue).call(t,0));try{for(i.s();!(e=i.n()).done;){(0,e.value)()}}catch(t){i.e(t)}finally{i.f()}}},{key:"_removeManipulationDOM",value:function(){this._clean(),Ky(this.manipulationDiv),Ky(this.editModeDiv),Ky(this.closeDiv),this.manipulationDiv&&this.canvas.frame.removeChild(this.manipulationDiv),this.editModeDiv&&this.canvas.frame.removeChild(this.editModeDiv),this.closeDiv&&this.canvas.frame.removeChild(this.closeDiv),this.manipulationDiv=void 0,this.editModeDiv=void 0,this.closeDiv=void 0}},{key:"_createSeperator",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:1;this.manipulationDOM["seperatorLineDiv"+t]=document.createElement("div"),this.manipulationDOM["seperatorLineDiv"+t].className="vis-separator-line",this.manipulationDiv.appendChild(this.manipulationDOM["seperatorLineDiv"+t])}},{key:"_createAddNodeButton",value:function(t){var e,i=this._createButton("addNode","vis-add",t.addNode||this.options.locales.en.addNode);this.manipulationDiv.appendChild(i),this._bindElementEvents(i,zn(e=this.addNodeMode).call(e,this))}},{key:"_createAddEdgeButton",value:function(t){var e,i=this._createButton("addEdge","vis-connect",t.addEdge||this.options.locales.en.addEdge);this.manipulationDiv.appendChild(i),this._bindElementEvents(i,zn(e=this.addEdgeMode).call(e,this))}},{key:"_createEditNodeButton",value:function(t){var e,i=this._createButton("editNode","vis-edit",t.editNode||this.options.locales.en.editNode);this.manipulationDiv.appendChild(i),this._bindElementEvents(i,zn(e=this.editNode).call(e,this))}},{key:"_createEditEdgeButton",value:function(t){var e,i=this._createButton("editEdge","vis-edit",t.editEdge||this.options.locales.en.editEdge);this.manipulationDiv.appendChild(i),this._bindElementEvents(i,zn(e=this.editEdgeMode).call(e,this))}},{key:"_createDeleteButton",value:function(t){var e,i;i=this.options.rtl?"vis-delete-rtl":"vis-delete";var n=this._createButton("delete",i,t.del||this.options.locales.en.del);this.manipulationDiv.appendChild(n),this._bindElementEvents(n,zn(e=this.deleteSelected).call(e,this))}},{key:"_createBackButton",value:function(t){var e,i=this._createButton("back","vis-back",t.back||this.options.locales.en.back);this.manipulationDiv.appendChild(i),this._bindElementEvents(i,zn(e=this.showManipulatorToolbar).call(e,this))}},{key:"_createButton",value:function(t,e,i){var n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:"vis-label";return this.manipulationDOM[t+"Div"]=document.createElement("button"),this.manipulationDOM[t+"Div"].className="vis-button "+e,this.manipulationDOM[t+"Label"]=document.createElement("div"),this.manipulationDOM[t+"Label"].className=n,this.manipulationDOM[t+"Label"].innerText=i,this.manipulationDOM[t+"Div"].appendChild(this.manipulationDOM[t+"Label"]),this.manipulationDOM[t+"Div"]}},{key:"_createDescription",value:function(t){this.manipulationDOM.descriptionLabel=document.createElement("div"),this.manipulationDOM.descriptionLabel.className="vis-none",this.manipulationDOM.descriptionLabel.innerText=t,this.manipulationDiv.appendChild(this.manipulationDOM.descriptionLabel)}},{key:"_temporaryBindEvent",value:function(t,e){this.temporaryEventFunctions.push({event:t,boundFunction:e}),this.body.emitter.on(t,e)}},{key:"_temporaryBindUI",value:function(t,e){if(void 0===this.body.eventListeners[t])throw new Error("This UI function does not exist. Typo? You tried: "+t+" possible are: "+gv(bu(this.body.eventListeners)));this.temporaryUIFunctions[t]=this.body.eventListeners[t],this.body.eventListeners[t]=e}},{key:"_unbindTemporaryUIs",value:function(){for(var t in this.temporaryUIFunctions)Object.prototype.hasOwnProperty.call(this.temporaryUIFunctions,t)&&(this.body.eventListeners[t]=this.temporaryUIFunctions[t],delete this.temporaryUIFunctions[t]);this.temporaryUIFunctions={}}},{key:"_unbindTemporaryEvents",value:function(){for(var t=0;t<this.temporaryEventFunctions.length;t++){var e=this.temporaryEventFunctions[t].event,i=this.temporaryEventFunctions[t].boundFunction;this.body.emitter.off(e,i)}this.temporaryEventFunctions=[]}},{key:"_bindElementEvents",value:function(t,e){var i=new Wm(t,{});IC(i,e),this._domEventListenerCleanupQueue.push((function(){i.destroy()}));var n=function(t){var i=t.keyCode,n=t.key;"Enter"!==n&&" "!==n&&13!==i&&32!==i||e()};t.addEventListener("keyup",n,!1),this._domEventListenerCleanupQueue.push((function(){t.removeEventListener("keyup",n,!1)}))}},{key:"_cleanupTemporaryNodesAndEdges",value:function(){for(var t=0;t<this.temporaryIds.edges.length;t++){var e;this.body.edges[this.temporaryIds.edges[t]].disconnect(),delete this.body.edges[this.temporaryIds.edges[t]];var i,n=Fp(e=this.body.edgeIndices).call(e,this.temporaryIds.edges[t]);if(-1!==n)ff(i=this.body.edgeIndices).call(i,n,1)}for(var o=0;o<this.temporaryIds.nodes.length;o++){var r;delete this.body.nodes[this.temporaryIds.nodes[o]];var s,a=Fp(r=this.body.nodeIndices).call(r,this.temporaryIds.nodes[o]);if(-1!==a)ff(s=this.body.nodeIndices).call(s,a,1)}this.temporaryIds={nodes:[],edges:[]}}},{key:"_controlNodeTouch",value:function(t){this.selectionHandler.unselectAll(),this.lastTouch=this.body.functions.getPointer(t.center),this.lastTouch.translation=un({},this.body.view.translation)}},{key:"_controlNodeDragStart",value:function(){var t=this.lastTouch,e=this.selectionHandler._pointerToPositionObject(t),i=this.body.nodes[this.temporaryIds.nodes[0]],n=this.body.nodes[this.temporaryIds.nodes[1]],o=this.body.edges[this.edgeBeingEditedId];this.selectedControlNode=void 0;var r=i.isOverlappingWith(e),s=n.isOverlappingWith(e);!0===r?(this.selectedControlNode=i,o.edgeType.from=i):!0===s&&(this.selectedControlNode=n,o.edgeType.to=n),void 0!==this.selectedControlNode&&this.selectionHandler.selectObject(this.selectedControlNode),this.body.emitter.emit("_redraw")}},{key:"_controlNodeDrag",value:function(t){this.body.emitter.emit("disablePhysics");var e=this.body.functions.getPointer(t.center),i=this.canvas.DOMtoCanvas(e);void 0!==this.selectedControlNode?(this.selectedControlNode.x=i.x,this.selectedControlNode.y=i.y):this.interactionHandler.onDrag(t),this.body.emitter.emit("_redraw")}},{key:"_controlNodeDragEnd",value:function(t){var e=this.body.functions.getPointer(t.center),i=this.selectionHandler._pointerToPositionObject(e),n=this.body.edges[this.edgeBeingEditedId];if(void 0!==this.selectedControlNode){this.selectionHandler.unselectAll();for(var o=this.selectionHandler._getAllNodesOverlappingWith(i),r=void 0,s=o.length-1;s>=0;s--)if(o[s]!==this.selectedControlNode.id){r=this.body.nodes[o[s]];break}if(void 0!==r&&void 0!==this.selectedControlNode)if(!0===r.isCluster)alert(this.options.locales[this.options.locale].createEdgeError||this.options.locales.en.createEdgeError);else{var a=this.body.nodes[this.temporaryIds.nodes[0]];this.selectedControlNode.id===a.id?this._performEditEdge(r.id,n.to.id):this._performEditEdge(n.from.id,r.id)}else n.updateEdgeType(),this.body.emitter.emit("restorePhysics");this.body.emitter.emit("_redraw")}}},{key:"_handleConnect",value:function(t){if((new Date).valueOf()-this.touchTime>100){this.lastTouch=this.body.functions.getPointer(t.center),this.lastTouch.translation=un({},this.body.view.translation),this.interactionHandler.drag.pointer=this.lastTouch,this.interactionHandler.drag.translation=this.lastTouch.translation;var e=this.lastTouch,i=this.selectionHandler.getNodeAt(e);if(void 0!==i)if(!0===i.isCluster)alert(this.options.locales[this.options.locale].createEdgeError||this.options.locales.en.createEdgeError);else{var n=this._getNewTargetNode(i.x,i.y);this.body.nodes[n.id]=n,this.body.nodeIndices.push(n.id);var o=this.body.functions.createEdge({id:"connectionEdge"+Ax(),from:i.id,to:n.id,physics:!1,smooth:{enabled:!0,type:"continuous",roundness:.5}});this.body.edges[o.id]=o,this.body.edgeIndices.push(o.id),this.temporaryIds.nodes.push(n.id),this.temporaryIds.edges.push(o.id)}this.touchTime=(new Date).valueOf()}}},{key:"_dragControlNode",value:function(t){var e=this.body.functions.getPointer(t.center),i=this.selectionHandler._pointerToPositionObject(e),n=void 0;void 0!==this.temporaryIds.edges[0]&&(n=this.body.edges[this.temporaryIds.edges[0]].fromId);for(var o=this.selectionHandler._getAllNodesOverlappingWith(i),r=void 0,s=o.length-1;s>=0;s--){var a;if(-1===Fp(a=this.temporaryIds.nodes).call(a,o[s])){r=this.body.nodes[o[s]];break}}if(t.controlEdge={from:n,to:r?r.id:void 0},this.selectionHandler.generateClickEvent("controlNodeDragging",t,e),void 0!==this.temporaryIds.nodes[0]){var h=this.body.nodes[this.temporaryIds.nodes[0]];h.x=this.canvas._XconvertDOMtoCanvas(e.x),h.y=this.canvas._YconvertDOMtoCanvas(e.y),this.body.emitter.emit("_redraw")}else this.interactionHandler.onDrag(t)}},{key:"_finishConnect",value:function(t){var e=this.body.functions.getPointer(t.center),i=this.selectionHandler._pointerToPositionObject(e),n=void 0;void 0!==this.temporaryIds.edges[0]&&(n=this.body.edges[this.temporaryIds.edges[0]].fromId);for(var o=this.selectionHandler._getAllNodesOverlappingWith(i),r=void 0,s=o.length-1;s>=0;s--){var a;if(-1===Fp(a=this.temporaryIds.nodes).call(a,o[s])){r=this.body.nodes[o[s]];break}}this._cleanupTemporaryNodesAndEdges(),void 0!==r&&(!0===r.isCluster?alert(this.options.locales[this.options.locale].createEdgeError||this.options.locales.en.createEdgeError):void 0!==this.body.nodes[n]&&void 0!==this.body.nodes[r.id]&&this._performAddEdge(n,r.id)),t.controlEdge={from:n,to:r?r.id:void 0},this.selectionHandler.generateClickEvent("controlNodeDragEnd",t,e),this.body.emitter.emit("_redraw")}},{key:"_dragStartEdge",value:function(t){var e=this.lastTouch;this.selectionHandler.generateClickEvent("dragStart",t,e,void 0,!0)}},{key:"_performAddNode",value:function(t){var e=this,i={id:Ax(),x:t.pointer.canvas.x,y:t.pointer.canvas.y,label:"new"};if("function"==typeof this.options.addNode){if(2!==this.options.addNode.length)throw this.showManipulatorToolbar(),new Error("The function for add does not support two arguments (data,callback)");this.options.addNode(i,(function(t){null!=t&&"addNode"===e.inMode&&e.body.data.nodes.getDataSet().add(t),e.showManipulatorToolbar()}))}else this.body.data.nodes.getDataSet().add(i),this.showManipulatorToolbar()}},{key:"_performAddEdge",value:function(t,e){var i=this,n={from:t,to:e};if("function"==typeof this.options.addEdge){if(2!==this.options.addEdge.length)throw new Error("The function for connect does not support two arguments (data,callback)");this.options.addEdge(n,(function(t){null!=t&&"addEdge"===i.inMode&&(i.body.data.edges.getDataSet().add(t),i.selectionHandler.unselectAll(),i.showManipulatorToolbar())}))}else this.body.data.edges.getDataSet().add(n),this.selectionHandler.unselectAll(),this.showManipulatorToolbar()}},{key:"_performEditEdge",value:function(t,e){var i=this,n={id:this.edgeBeingEditedId,from:t,to:e,label:this.body.data.edges.get(this.edgeBeingEditedId).label},o=this.options.editEdge;if("object"===Qc(o)&&(o=o.editWithoutDrag),"function"==typeof o){if(2!==o.length)throw new Error("The function for edit does not support two arguments (data, callback)");o(n,(function(t){null==t||"editEdge"!==i.inMode?(i.body.edges[n.id].updateEdgeType(),i.body.emitter.emit("_redraw"),i.showManipulatorToolbar()):(i.body.data.edges.getDataSet().update(t),i.selectionHandler.unselectAll(),i.showManipulatorToolbar())}))}else this.body.data.edges.getDataSet().update(n),this.selectionHandler.unselectAll(),this.showManipulatorToolbar()}}]),t}(),dT="string",cT="boolean",uT="number",fT="array",pT="object",vT=["arrow","bar","box","circle","crow","curve","diamond","image","inv_curve","inv_triangle","triangle","vee"],gT={borderWidth:{number:uT},borderWidthSelected:{number:uT,undefined:"undefined"},brokenImage:{string:dT,undefined:"undefined"},chosen:{label:{boolean:cT,function:"function"},node:{boolean:cT,function:"function"},__type__:{object:pT,boolean:cT}},color:{border:{string:dT},background:{string:dT},highlight:{border:{string:dT},background:{string:dT},__type__:{object:pT,string:dT}},hover:{border:{string:dT},background:{string:dT},__type__:{object:pT,string:dT}},__type__:{object:pT,string:dT}},opacity:{number:uT,undefined:"undefined"},fixed:{x:{boolean:cT},y:{boolean:cT},__type__:{object:pT,boolean:cT}},font:{align:{string:dT},color:{string:dT},size:{number:uT},face:{string:dT},background:{string:dT},strokeWidth:{number:uT},strokeColor:{string:dT},vadjust:{number:uT},multi:{boolean:cT,string:dT},bold:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},boldital:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},ital:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},mono:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},__type__:{object:pT,string:dT}},group:{string:dT,number:uT,undefined:"undefined"},heightConstraint:{minimum:{number:uT},valign:{string:dT},__type__:{object:pT,boolean:cT,number:uT}},hidden:{boolean:cT},icon:{face:{string:dT},code:{string:dT},size:{number:uT},color:{string:dT},weight:{string:dT,number:uT},__type__:{object:pT}},id:{string:dT,number:uT},image:{selected:{string:dT,undefined:"undefined"},unselected:{string:dT,undefined:"undefined"},__type__:{object:pT,string:dT}},imagePadding:{top:{number:uT},right:{number:uT},bottom:{number:uT},left:{number:uT},__type__:{object:pT,number:uT}},label:{string:dT,undefined:"undefined"},labelHighlightBold:{boolean:cT},level:{number:uT,undefined:"undefined"},margin:{top:{number:uT},right:{number:uT},bottom:{number:uT},left:{number:uT},__type__:{object:pT,number:uT}},mass:{number:uT},physics:{boolean:cT},scaling:{min:{number:uT},max:{number:uT},label:{enabled:{boolean:cT},min:{number:uT},max:{number:uT},maxVisible:{number:uT},drawThreshold:{number:uT},__type__:{object:pT,boolean:cT}},customScalingFunction:{function:"function"},__type__:{object:pT}},shadow:{enabled:{boolean:cT},color:{string:dT},size:{number:uT},x:{number:uT},y:{number:uT},__type__:{object:pT,boolean:cT}},shape:{string:["custom","ellipse","circle","database","box","text","image","circularImage","diamond","dot","star","triangle","triangleDown","square","icon","hexagon"]},ctxRenderer:{function:"function"},shapeProperties:{borderDashes:{boolean:cT,array:fT},borderRadius:{number:uT},interpolation:{boolean:cT},useImageSize:{boolean:cT},useBorderWithImage:{boolean:cT},coordinateOrigin:{string:["center","top-left"]},__type__:{object:pT}},size:{number:uT},title:{string:dT,dom:"dom",undefined:"undefined"},value:{number:uT,undefined:"undefined"},widthConstraint:{minimum:{number:uT},maximum:{number:uT},__type__:{object:pT,boolean:cT,number:uT}},x:{number:uT},y:{number:uT},__type__:{object:pT}},yT={configure:{enabled:{boolean:cT},filter:{boolean:cT,string:dT,array:fT,function:"function"},container:{dom:"dom"},showButton:{boolean:cT},__type__:{object:pT,boolean:cT,string:dT,array:fT,function:"function"}},edges:{arrows:{to:{enabled:{boolean:cT},scaleFactor:{number:uT},type:{string:vT},imageHeight:{number:uT},imageWidth:{number:uT},src:{string:dT},__type__:{object:pT,boolean:cT}},middle:{enabled:{boolean:cT},scaleFactor:{number:uT},type:{string:vT},imageWidth:{number:uT},imageHeight:{number:uT},src:{string:dT},__type__:{object:pT,boolean:cT}},from:{enabled:{boolean:cT},scaleFactor:{number:uT},type:{string:vT},imageWidth:{number:uT},imageHeight:{number:uT},src:{string:dT},__type__:{object:pT,boolean:cT}},__type__:{string:["from","to","middle"],object:pT}},endPointOffset:{from:{number:uT},to:{number:uT},__type__:{object:pT,number:uT}},arrowStrikethrough:{boolean:cT},background:{enabled:{boolean:cT},color:{string:dT},size:{number:uT},dashes:{boolean:cT,array:fT},__type__:{object:pT,boolean:cT}},chosen:{label:{boolean:cT,function:"function"},edge:{boolean:cT,function:"function"},__type__:{object:pT,boolean:cT}},color:{color:{string:dT},highlight:{string:dT},hover:{string:dT},inherit:{string:["from","to","both"],boolean:cT},opacity:{number:uT},__type__:{object:pT,string:dT}},dashes:{boolean:cT,array:fT},font:{color:{string:dT},size:{number:uT},face:{string:dT},background:{string:dT},strokeWidth:{number:uT},strokeColor:{string:dT},align:{string:["horizontal","top","middle","bottom"]},vadjust:{number:uT},multi:{boolean:cT,string:dT},bold:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},boldital:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},ital:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},mono:{color:{string:dT},size:{number:uT},face:{string:dT},mod:{string:dT},vadjust:{number:uT},__type__:{object:pT,string:dT}},__type__:{object:pT,string:dT}},hidden:{boolean:cT},hoverWidth:{function:"function",number:uT},label:{string:dT,undefined:"undefined"},labelHighlightBold:{boolean:cT},length:{number:uT,undefined:"undefined"},physics:{boolean:cT},scaling:{min:{number:uT},max:{number:uT},label:{enabled:{boolean:cT},min:{number:uT},max:{number:uT},maxVisible:{number:uT},drawThreshold:{number:uT},__type__:{object:pT,boolean:cT}},customScalingFunction:{function:"function"},__type__:{object:pT}},selectionWidth:{function:"function",number:uT},selfReferenceSize:{number:uT},selfReference:{size:{number:uT},angle:{number:uT},renderBehindTheNode:{boolean:cT},__type__:{object:pT}},shadow:{enabled:{boolean:cT},color:{string:dT},size:{number:uT},x:{number:uT},y:{number:uT},__type__:{object:pT,boolean:cT}},smooth:{enabled:{boolean:cT},type:{string:["dynamic","continuous","discrete","diagonalCross","straightCross","horizontal","vertical","curvedCW","curvedCCW","cubicBezier"]},roundness:{number:uT},forceDirection:{string:["horizontal","vertical","none"],boolean:cT},__type__:{object:pT,boolean:cT}},title:{string:dT,undefined:"undefined"},width:{number:uT},widthConstraint:{maximum:{number:uT},__type__:{object:pT,boolean:cT,number:uT}},value:{number:uT,undefined:"undefined"},__type__:{object:pT}},groups:{useDefaultGroups:{boolean:cT},__any__:gT,__type__:{object:pT}},interaction:{dragNodes:{boolean:cT},dragView:{boolean:cT},hideEdgesOnDrag:{boolean:cT},hideEdgesOnZoom:{boolean:cT},hideNodesOnDrag:{boolean:cT},hover:{boolean:cT},keyboard:{enabled:{boolean:cT},speed:{x:{number:uT},y:{number:uT},zoom:{number:uT},__type__:{object:pT}},bindToWindow:{boolean:cT},autoFocus:{boolean:cT},__type__:{object:pT,boolean:cT}},multiselect:{boolean:cT},navigationButtons:{boolean:cT},selectable:{boolean:cT},selectConnectedEdges:{boolean:cT},hoverConnectedEdges:{boolean:cT},tooltipDelay:{number:uT},zoomView:{boolean:cT},zoomSpeed:{number:uT},__type__:{object:pT}},layout:{randomSeed:{undefined:"undefined",number:uT,string:dT},improvedLayout:{boolean:cT},clusterThreshold:{number:uT},hierarchical:{enabled:{boolean:cT},levelSeparation:{number:uT},nodeSpacing:{number:uT},treeSpacing:{number:uT},blockShifting:{boolean:cT},edgeMinimization:{boolean:cT},parentCentralization:{boolean:cT},direction:{string:["UD","DU","LR","RL"]},sortMethod:{string:["hubsize","directed"]},shakeTowards:{string:["leaves","roots"]},__type__:{object:pT,boolean:cT}},__type__:{object:pT}},manipulation:{enabled:{boolean:cT},initiallyActive:{boolean:cT},addNode:{boolean:cT,function:"function"},addEdge:{boolean:cT,function:"function"},editNode:{function:"function"},editEdge:{editWithoutDrag:{function:"function"},__type__:{object:pT,boolean:cT,function:"function"}},deleteNode:{boolean:cT,function:"function"},deleteEdge:{boolean:cT,function:"function"},controlNodeStyle:gT,__type__:{object:pT,boolean:cT}},nodes:gT,physics:{enabled:{boolean:cT},barnesHut:{theta:{number:uT},gravitationalConstant:{number:uT},centralGravity:{number:uT},springLength:{number:uT},springConstant:{number:uT},damping:{number:uT},avoidOverlap:{number:uT},__type__:{object:pT}},forceAtlas2Based:{theta:{number:uT},gravitationalConstant:{number:uT},centralGravity:{number:uT},springLength:{number:uT},springConstant:{number:uT},damping:{number:uT},avoidOverlap:{number:uT},__type__:{object:pT}},repulsion:{centralGravity:{number:uT},springLength:{number:uT},springConstant:{number:uT},nodeDistance:{number:uT},damping:{number:uT},__type__:{object:pT}},hierarchicalRepulsion:{centralGravity:{number:uT},springLength:{number:uT},springConstant:{number:uT},nodeDistance:{number:uT},damping:{number:uT},avoidOverlap:{number:uT},__type__:{object:pT}},maxVelocity:{number:uT},minVelocity:{number:uT},solver:{string:["barnesHut","repulsion","hierarchicalRepulsion","forceAtlas2Based"]},stabilization:{enabled:{boolean:cT},iterations:{number:uT},updateInterval:{number:uT},onlyDynamicEdges:{boolean:cT},fit:{boolean:cT},__type__:{object:pT,boolean:cT}},timestep:{number:uT},adaptiveTimestep:{boolean:cT},wind:{x:{number:uT},y:{number:uT},__type__:{object:pT}},__type__:{object:pT,boolean:cT}},autoResize:{boolean:cT},clickToUse:{boolean:cT},locale:{string:dT},locales:{__any__:{any:"any"},__type__:{object:pT}},height:{string:dT},width:{string:dT},__type__:{object:pT}},mT={nodes:{borderWidth:[1,0,10,1],borderWidthSelected:[2,0,10,1],color:{border:["color","#2B7CE9"],background:["color","#97C2FC"],highlight:{border:["color","#2B7CE9"],background:["color","#D2E5FF"]},hover:{border:["color","#2B7CE9"],background:["color","#D2E5FF"]}},opacity:[0,0,1,.1],fixed:{x:!1,y:!1},font:{color:["color","#343434"],size:[14,0,100,1],face:["arial","verdana","tahoma"],background:["color","none"],strokeWidth:[0,0,50,1],strokeColor:["color","#ffffff"]},hidden:!1,labelHighlightBold:!0,physics:!0,scaling:{min:[10,0,200,1],max:[30,0,200,1],label:{enabled:!1,min:[14,0,200,1],max:[30,0,200,1],maxVisible:[30,0,200,1],drawThreshold:[5,0,20,1]}},shadow:{enabled:!1,color:"rgba(0,0,0,0.5)",size:[10,0,20,1],x:[5,-30,30,1],y:[5,-30,30,1]},shape:["ellipse","box","circle","database","diamond","dot","square","star","text","triangle","triangleDown","hexagon"],shapeProperties:{borderDashes:!1,borderRadius:[6,0,20,1],interpolation:!0,useImageSize:!1},size:[25,0,200,1]},edges:{arrows:{to:{enabled:!1,scaleFactor:[1,0,3,.05],type:"arrow"},middle:{enabled:!1,scaleFactor:[1,0,3,.05],type:"arrow"},from:{enabled:!1,scaleFactor:[1,0,3,.05],type:"arrow"}},endPointOffset:{from:[0,-10,10,1],to:[0,-10,10,1]},arrowStrikethrough:!0,color:{color:["color","#848484"],highlight:["color","#848484"],hover:["color","#848484"],inherit:["from","to","both",!0,!1],opacity:[1,0,1,.05]},dashes:!1,font:{color:["color","#343434"],size:[14,0,100,1],face:["arial","verdana","tahoma"],background:["color","none"],strokeWidth:[2,0,50,1],strokeColor:["color","#ffffff"],align:["horizontal","top","middle","bottom"]},hidden:!1,hoverWidth:[1.5,0,5,.1],labelHighlightBold:!0,physics:!0,scaling:{min:[1,0,100,1],max:[15,0,100,1],label:{enabled:!0,min:[14,0,200,1],max:[30,0,200,1],maxVisible:[30,0,200,1],drawThreshold:[5,0,20,1]}},selectionWidth:[1.5,0,5,.1],selfReferenceSize:[20,0,200,1],selfReference:{size:[20,0,200,1],angle:[Math.PI/2,-6*Math.PI,6*Math.PI,Math.PI/8],renderBehindTheNode:!0},shadow:{enabled:!1,color:"rgba(0,0,0,0.5)",size:[10,0,20,1],x:[5,-30,30,1],y:[5,-30,30,1]},smooth:{enabled:!0,type:["dynamic","continuous","discrete","diagonalCross","straightCross","horizontal","vertical","curvedCW","curvedCCW","cubicBezier"],forceDirection:["horizontal","vertical","none"],roundness:[.5,0,1,.05]},width:[1,0,30,1]},layout:{hierarchical:{enabled:!1,levelSeparation:[150,20,500,5],nodeSpacing:[100,20,500,5],treeSpacing:[200,20,500,5],blockShifting:!0,edgeMinimization:!0,parentCentralization:!0,direction:["UD","DU","LR","RL"],sortMethod:["hubsize","directed"],shakeTowards:["leaves","roots"]}},interaction:{dragNodes:!0,dragView:!0,hideEdgesOnDrag:!1,hideEdgesOnZoom:!1,hideNodesOnDrag:!1,hover:!1,keyboard:{enabled:!1,speed:{x:[10,0,40,1],y:[10,0,40,1],zoom:[.02,0,.1,.005]},bindToWindow:!0,autoFocus:!0},multiselect:!1,navigationButtons:!1,selectable:!0,selectConnectedEdges:!0,hoverConnectedEdges:!0,tooltipDelay:[300,0,1e3,25],zoomView:!0,zoomSpeed:[1,.1,2,.1]},manipulation:{enabled:!1,initiallyActive:!1},physics:{enabled:!0,barnesHut:{theta:[.5,.1,1,.05],gravitationalConstant:[-2e3,-3e4,0,50],centralGravity:[.3,0,10,.05],springLength:[95,0,500,5],springConstant:[.04,0,1.2,.005],damping:[.09,0,1,.01],avoidOverlap:[0,0,1,.01]},forceAtlas2Based:{theta:[.5,.1,1,.05],gravitationalConstant:[-50,-500,0,1],centralGravity:[.01,0,1,.005],springLength:[95,0,500,5],springConstant:[.08,0,1.2,.005],damping:[.4,0,1,.01],avoidOverlap:[0,0,1,.01]},repulsion:{centralGravity:[.2,0,10,.05],springLength:[200,0,500,5],springConstant:[.05,0,1.2,.005],nodeDistance:[100,0,500,5],damping:[.09,0,1,.01]},hierarchicalRepulsion:{centralGravity:[.2,0,10,.05],springLength:[100,0,500,5],springConstant:[.01,0,1.2,.005],nodeDistance:[120,0,500,5],damping:[.09,0,1,.01],avoidOverlap:[0,0,1,.01]},maxVelocity:[50,0,150,1],minVelocity:[.1,.01,.5,.01],solver:["barnesHut","forceAtlas2Based","repulsion","hierarchicalRepulsion"],timestep:[.5,.01,1,.01],wind:{x:[0,-10,10,.1],y:[0,-10,10,.1]}}},bT=function(t,e,i){var n;return!(!Nf(t).call(t,"physics")||!Nf(n=mT.physics.solver).call(n,e)||i.physics.solver===e||"wind"===e)},wT=Object.freeze({__proto__:null,configuratorHideOption:bT,allOptions:yT,configureOptions:mT}),kT=function(){function t(){Yd(this,t)}return Kd(t,[{key:"getDistances",value:function(t,e,i){for(var n={},o=t.edges,r=0;r<e.length;r++){var s={};n[e[r]]=s;for(var a=0;a<e.length;a++)s[e[a]]=r==a?0:1e9}for(var h=0;h<i.length;h++){var l=o[i[h]];!0===l.connected&&void 0!==n[l.fromId]&&void 0!==n[l.toId]&&(n[l.fromId][l.toId]=1,n[l.toId][l.fromId]=1)}for(var d=e.length,c=0;c<d;c++)for(var u=e[c],f=n[u],p=0;p<d-1;p++)for(var v=e[p],g=n[v],y=p+1;y<d;y++){var m=e[y],b=n[m],w=Math.min(g[m],g[u]+f[m]);g[m]=w,b[v]=w}return n}}]),t}(),_T=function(){function t(e,i,n){Yd(this,t),this.body=e,this.springLength=i,this.springConstant=n,this.distanceSolver=new kT}return Kd(t,[{key:"setOptions",value:function(t){t&&(t.springLength&&(this.springLength=t.springLength),t.springConstant&&(this.springConstant=t.springConstant))}},{key:"solve",value:function(t,e){var i=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=this.distanceSolver.getDistances(this.body,t,e);this._createL_matrix(n),this._createK_matrix(n),this._createE_matrix();for(var o=.01,r=1,s=0,a=Math.max(1e3,Math.min(10*this.body.nodeIndices.length,6e3)),h=5,l=1e9,d=0,c=0,u=0,f=0,p=0;l>o&&s<a;){s+=1;var v=this._getHighestEnergyNode(i),g=Kc(v,4);for(d=g[0],l=g[1],c=g[2],u=g[3],f=l,p=0;f>r&&p<h;){p+=1,this._moveNode(d,c,u);var y=this._getEnergy(d),m=Kc(y,3);f=m[0],c=m[1],u=m[2]}}}},{key:"_getHighestEnergyNode",value:function(t){for(var e=this.body.nodeIndices,i=this.body.nodes,n=0,o=e[0],r=0,s=0,a=0;a<e.length;a++){var h=e[a];if(!0!==i[h].predefinedPosition||!0===i[h].isCluster&&!0===t||!0!==i[h].options.fixed.x||!0!==i[h].options.fixed.y){var l=Kc(this._getEnergy(h),3),d=l[0],c=l[1],u=l[2];n<d&&(n=d,o=h,r=c,s=u)}}return[o,n,r,s]}},{key:"_getEnergy",value:function(t){var e=Kc(this.E_sums[t],2),i=e[0],n=e[1];return[Math.sqrt(Math.pow(i,2)+Math.pow(n,2)),i,n]}},{key:"_moveNode",value:function(t,e,i){for(var n=this.body.nodeIndices,o=this.body.nodes,r=0,s=0,a=0,h=o[t].x,l=o[t].y,d=this.K_matrix[t],c=this.L_matrix[t],u=0;u<n.length;u++){var f=n[u];if(f!==t){var p=o[f].x,v=o[f].y,g=d[f],y=c[f],m=1/Math.pow(Math.pow(h-p,2)+Math.pow(l-v,2),1.5);r+=g*(1-y*Math.pow(l-v,2)*m),s+=g*(y*(h-p)*(l-v)*m),a+=g*(1-y*Math.pow(h-p,2)*m)}}var b=(e/r+i/s)/(s/r-a/s),w=-(s*b+e)/r;o[t].x+=w,o[t].y+=b,this._updateE_matrix(t)}},{key:"_createL_matrix",value:function(t){var e=this.body.nodeIndices,i=this.springLength;this.L_matrix=[];for(var n=0;n<e.length;n++){this.L_matrix[e[n]]={};for(var o=0;o<e.length;o++)this.L_matrix[e[n]][e[o]]=i*t[e[n]][e[o]]}}},{key:"_createK_matrix",value:function(t){var e=this.body.nodeIndices,i=this.springConstant;this.K_matrix=[];for(var n=0;n<e.length;n++){this.K_matrix[e[n]]={};for(var o=0;o<e.length;o++)this.K_matrix[e[n]][e[o]]=i*Math.pow(t[e[n]][e[o]],-2)}}},{key:"_createE_matrix",value:function(){var t=this.body.nodeIndices,e=this.body.nodes;this.E_matrix={},this.E_sums={};for(var i=0;i<t.length;i++)this.E_matrix[t[i]]=[];for(var n=0;n<t.length;n++){for(var o=t[n],r=e[o].x,s=e[o].y,a=0,h=0,l=n;l<t.length;l++){var d=t[l];if(d!==o){var c=e[d].x,u=e[d].y,f=1/Math.sqrt(Math.pow(r-c,2)+Math.pow(s-u,2));this.E_matrix[o][l]=[this.K_matrix[o][d]*(r-c-this.L_matrix[o][d]*(r-c)*f),this.K_matrix[o][d]*(s-u-this.L_matrix[o][d]*(s-u)*f)],this.E_matrix[d][n]=this.E_matrix[o][l],a+=this.E_matrix[o][l][0],h+=this.E_matrix[o][l][1]}}this.E_sums[o]=[a,h]}}},{key:"_updateE_matrix",value:function(t){for(var e=this.body.nodeIndices,i=this.body.nodes,n=this.E_matrix[t],o=this.K_matrix[t],r=this.L_matrix[t],s=i[t].x,a=i[t].y,h=0,l=0,d=0;d<e.length;d++){var c=e[d];if(c!==t){var u=n[d],f=u[0],p=u[1],v=i[c].x,g=i[c].y,y=1/Math.sqrt(Math.pow(s-v,2)+Math.pow(a-g,2)),m=o[c]*(s-v-r[c]*(s-v)*y),b=o[c]*(a-g-r[c]*(a-g)*y);n[d]=[m,b],h+=m,l+=b;var w=this.E_sums[c];w[0]+=m-f,w[1]+=b-p}}this.E_sums[t]=[h,l]}}]),t}();function xT(t,e,i){var n,o,r,s,a=this;if(!(this instanceof xT))throw new SyntaxError("Constructor must be called with the new operator");this.options={},this.defaultOptions={locale:"en",locales:Tb,clickToUse:!1},un(this.options,this.defaultOptions),this.body={container:t,nodes:{},nodeIndices:[],edges:{},edgeIndices:[],emitter:{on:zn(n=this.on).call(n,this),off:zn(o=this.off).call(o,this),emit:zn(r=this.emit).call(r,this),once:zn(s=this.once).call(s,this)},eventListeners:{onTap:function(){},onTouch:function(){},onDoubleTap:function(){},onHold:function(){},onDragStart:function(){},onDrag:function(){},onDragEnd:function(){},onMouseWheel:function(){},onPinch:function(){},onMouseMove:function(){},onRelease:function(){},onContext:function(){}},data:{nodes:null,edges:null},functions:{createNode:function(){},createEdge:function(){},getPointer:function(){}},modules:{},view:{scale:1,translation:{x:0,y:0}},selectionBox:{show:!1,position:{start:{x:0,y:0},end:{x:0,y:0}}}},this.bindEventListeners(),this.images=new Pb((function(){return a.body.emitter.emit("_requestRedraw")})),this.groups=new tk,this.canvas=new zC(this.body),this.selectionHandler=new HS(this.body,this.canvas),this.interactionHandler=new WC(this.body,this.canvas,this.selectionHandler),this.view=new FC(this.body,this.canvas),this.renderer=new PC(this.body,this.canvas),this.physics=new xC(this.body),this.layoutEngine=new sT(this.body),this.clustering=new SC(this.body),this.manipulation=new lT(this.body,this.canvas,this.selectionHandler,this.interactionHandler),this.nodesHandler=new gO(this.body,this.images,this.groups,this.layoutEngine),this.edgesHandler=new uC(this.body,this.images,this.groups),this.body.modules.kamadaKawai=new _T(this.body,150,.05),this.body.modules.clustering=this.clustering,this.canvas._create(),this.setOptions(i),this.setData(e)}function ET(t){for(var e in t)Object.prototype.hasOwnProperty.call(t,e)&&(t[e].redundant=t[e].used,t[e].used=[])}function OT(t){for(var e in t)if(Object.prototype.hasOwnProperty.call(t,e)&&t[e].redundant){for(var i=0;i<t[e].redundant.length;i++)t[e].redundant[i].parentNode.removeChild(t[e].redundant[i]);t[e].redundant=[]}}function CT(t,e,i){var n;return Object.prototype.hasOwnProperty.call(e,t)?e[t].redundant.length>0?(n=e[t].redundant[0],e[t].redundant.shift()):(n=document.createElementNS("http://www.w3.org/2000/svg",t),i.appendChild(n)):(n=document.createElementNS("http://www.w3.org/2000/svg",t),e[t]={used:[],redundant:[]},i.appendChild(n)),e[t].used.push(n),n}Wn(xT.prototype),xT.prototype.setOptions=function(t){var e=this;if(null===t&&(t=void 0),void 0!==t){!0===Um.validate(t,yT)&&console.error("%cErrors have been found in the supplied options object.",Vm);if(em(["locale","locales","clickToUse"],this.options,t),void 0!==t.locale&&(t.locale=function(t,e){try{var i=Kc(e.split(/[-_ /]/,2),2),n=i[0],o=i[1],r=null!=n?n.toLowerCase():null,s=null!=o?o.toUpperCase():null;if(r&&s){var a,h=r+"-"+s;if(Object.prototype.hasOwnProperty.call(t,h))return h;console.warn(su(a="Unknown variant ".concat(s," of language ")).call(a,r,"."))}if(r){var l=r;if(Object.prototype.hasOwnProperty.call(t,l))return l;console.warn("Unknown language ".concat(r))}return console.warn("Unknown locale ".concat(e,", falling back to English.")),"en"}catch(t){return console.error(t),console.warn("Unexpected error while normalizing locale ".concat(e,", falling back to English.")),"en"}}(t.locales||this.options.locales,t.locale)),t=this.layoutEngine.setOptions(t.layout,t),this.canvas.setOptions(t),this.groups.setOptions(t.groups),this.nodesHandler.setOptions(t.nodes),this.edgesHandler.setOptions(t.edges),this.physics.setOptions(t.physics),this.manipulation.setOptions(t.manipulation,t,this.options),this.interactionHandler.setOptions(t.interaction),this.renderer.setOptions(t.interaction),this.selectionHandler.setOptions(t.interaction),void 0!==t.groups&&this.body.emitter.emit("refreshNodes"),"configure"in t&&(this.configurator||(this.configurator=new Hm(this,this.body.container,mT,this.canvas.pixelRatio,bT)),this.configurator.setOptions(t.configure)),this.configurator&&!0===this.configurator.options.enabled){var i={nodes:{},edges:{},layout:{},interaction:{},manipulation:{},physics:{},global:{}};nm(i.nodes,this.nodesHandler.options),nm(i.edges,this.edgesHandler.options),nm(i.layout,this.layoutEngine.options),nm(i.interaction,this.selectionHandler.options),nm(i.interaction,this.renderer.options),nm(i.interaction,this.interactionHandler.options),nm(i.manipulation,this.manipulation.options),nm(i.physics,this.physics.options),nm(i.global,this.canvas.options),nm(i.global,this.options),this.configurator.setModuleOptions(i)}void 0!==t.clickToUse?!0===t.clickToUse?void 0===this.activator&&(this.activator=new Rm(this.canvas.frame),this.activator.on("change",(function(){e.body.emitter.emit("activate")}))):(void 0!==this.activator&&(this.activator.destroy(),delete this.activator),this.body.emitter.emit("activate")):this.body.emitter.emit("activate"),this.canvas.setSize(),this.body.emitter.emit("startSimulation")}},xT.prototype._updateVisibleIndices=function(){var t=this.body.nodes,e=this.body.edges;for(var i in this.body.nodeIndices=[],this.body.edgeIndices=[],t)Object.prototype.hasOwnProperty.call(t,i)&&(this.clustering._isClusteredNode(i)||!1!==t[i].options.hidden||this.body.nodeIndices.push(t[i].id));for(var n in e)if(Object.prototype.hasOwnProperty.call(e,n)){var o=e[n],r=t[o.fromId],s=t[o.toId],a=void 0!==r&&void 0!==s;!this.clustering._isClusteredEdge(n)&&!1===o.options.hidden&&a&&!1===r.options.hidden&&!1===s.options.hidden&&this.body.edgeIndices.push(o.id)}},xT.prototype.bindEventListeners=function(){var t=this;this.body.emitter.on("_dataChanged",(function(){t.edgesHandler._updateState(),t.body.emitter.emit("_dataUpdated")})),this.body.emitter.on("_dataUpdated",(function(){t.clustering._updateState(),t._updateVisibleIndices(),t._updateValueRange(t.body.nodes),t._updateValueRange(t.body.edges),t.body.emitter.emit("startSimulation"),t.body.emitter.emit("_requestRedraw")}))},xT.prototype.setData=function(t){if(this.body.emitter.emit("resetPhysics"),this.body.emitter.emit("_resetData"),this.selectionHandler.unselectAll(),t&&t.dot&&(t.nodes||t.edges))throw new SyntaxError('Data must contain either parameter "dot" or  parameter pair "nodes" and "edges", but not both.');if(this.setOptions(t&&t.options),t&&t.dot){console.warn("The dot property has been deprecated. Please use the static convertDot method to convert DOT into vis.network format and use the normal data format with nodes and edges. This converter is used like this: var data = vis.network.convertDot(dotString);");var e=Eb(t.dot);this.setData(e)}else if(t&&t.gephi){console.warn("The gephi property has been deprecated. Please use the static convertGephi method to convert gephi into vis.network format and use the normal data format with nodes and edges. This converter is used like this: var data = vis.network.convertGephi(gephiJson);");var i=Cb(t.gephi);this.setData(i)}else this.nodesHandler.setData(t&&t.nodes,!0),this.edgesHandler.setData(t&&t.edges,!0),this.body.emitter.emit("_dataChanged"),this.body.emitter.emit("_dataLoaded"),this.body.emitter.emit("initPhysics")},xT.prototype.destroy=function(){for(var t in this.body.emitter.emit("destroy"),this.body.emitter.off(),this.off(),delete this.groups,delete this.canvas,delete this.selectionHandler,delete this.interactionHandler,delete this.view,delete this.renderer,delete this.physics,delete this.layoutEngine,delete this.clustering,delete this.manipulation,delete this.nodesHandler,delete this.edgesHandler,delete this.configurator,delete this.images,this.body.nodes)Object.prototype.hasOwnProperty.call(this.body.nodes,t)&&delete this.body.nodes[t];for(var e in this.body.edges)Object.prototype.hasOwnProperty.call(this.body.edges,e)&&delete this.body.edges[e];Ky(this.body.container)},xT.prototype._updateValueRange=function(t){var e,i=void 0,n=void 0,o=0;for(e in t)if(Object.prototype.hasOwnProperty.call(t,e)){var r=t[e].getValue();void 0!==r&&(i=void 0===i?r:Math.min(r,i),n=void 0===n?r:Math.max(r,n),o+=r)}if(void 0!==i&&void 0!==n)for(e in t)Object.prototype.hasOwnProperty.call(t,e)&&t[e].setValueRange(i,n,o)},xT.prototype.isActive=function(){return!this.activator||this.activator.active},xT.prototype.setSize=function(){return this.canvas.setSize.apply(this.canvas,arguments)},xT.prototype.canvasToDOM=function(){return this.canvas.canvasToDOM.apply(this.canvas,arguments)},xT.prototype.DOMtoCanvas=function(){return this.canvas.DOMtoCanvas.apply(this.canvas,arguments)},xT.prototype.findNode=function(){return this.clustering.findNode.apply(this.clustering,arguments)},xT.prototype.isCluster=function(){return this.clustering.isCluster.apply(this.clustering,arguments)},xT.prototype.openCluster=function(){return this.clustering.openCluster.apply(this.clustering,arguments)},xT.prototype.cluster=function(){return this.clustering.cluster.apply(this.clustering,arguments)},xT.prototype.getNodesInCluster=function(){return this.clustering.getNodesInCluster.apply(this.clustering,arguments)},xT.prototype.clusterByConnection=function(){return this.clustering.clusterByConnection.apply(this.clustering,arguments)},xT.prototype.clusterByHubsize=function(){return this.clustering.clusterByHubsize.apply(this.clustering,arguments)},xT.prototype.updateClusteredNode=function(){return this.clustering.updateClusteredNode.apply(this.clustering,arguments)},xT.prototype.getClusteredEdges=function(){return this.clustering.getClusteredEdges.apply(this.clustering,arguments)},xT.prototype.getBaseEdge=function(){return this.clustering.getBaseEdge.apply(this.clustering,arguments)},xT.prototype.getBaseEdges=function(){return this.clustering.getBaseEdges.apply(this.clustering,arguments)},xT.prototype.updateEdge=function(){return this.clustering.updateEdge.apply(this.clustering,arguments)},xT.prototype.clusterOutliers=function(){return this.clustering.clusterOutliers.apply(this.clustering,arguments)},xT.prototype.getSeed=function(){return this.layoutEngine.getSeed.apply(this.layoutEngine,arguments)},xT.prototype.enableEditMode=function(){return this.manipulation.enableEditMode.apply(this.manipulation,arguments)},xT.prototype.disableEditMode=function(){return this.manipulation.disableEditMode.apply(this.manipulation,arguments)},xT.prototype.addNodeMode=function(){return this.manipulation.addNodeMode.apply(this.manipulation,arguments)},xT.prototype.editNode=function(){return this.manipulation.editNode.apply(this.manipulation,arguments)},xT.prototype.editNodeMode=function(){return console.warn("Deprecated: Please use editNode instead of editNodeMode."),this.manipulation.editNode.apply(this.manipulation,arguments)},xT.prototype.addEdgeMode=function(){return this.manipulation.addEdgeMode.apply(this.manipulation,arguments)},xT.prototype.editEdgeMode=function(){return this.manipulation.editEdgeMode.apply(this.manipulation,arguments)},xT.prototype.deleteSelected=function(){return this.manipulation.deleteSelected.apply(this.manipulation,arguments)},xT.prototype.getPositions=function(){return this.nodesHandler.getPositions.apply(this.nodesHandler,arguments)},xT.prototype.getPosition=function(){return this.nodesHandler.getPosition.apply(this.nodesHandler,arguments)},xT.prototype.storePositions=function(){return this.nodesHandler.storePositions.apply(this.nodesHandler,arguments)},xT.prototype.moveNode=function(){return this.nodesHandler.moveNode.apply(this.nodesHandler,arguments)},xT.prototype.getBoundingBox=function(){return this.nodesHandler.getBoundingBox.apply(this.nodesHandler,arguments)},xT.prototype.getConnectedNodes=function(t){return void 0!==this.body.nodes[t]?this.nodesHandler.getConnectedNodes.apply(this.nodesHandler,arguments):this.edgesHandler.getConnectedNodes.apply(this.edgesHandler,arguments)},xT.prototype.getConnectedEdges=function(){return this.nodesHandler.getConnectedEdges.apply(this.nodesHandler,arguments)},xT.prototype.startSimulation=function(){return this.physics.startSimulation.apply(this.physics,arguments)},xT.prototype.stopSimulation=function(){return this.physics.stopSimulation.apply(this.physics,arguments)},xT.prototype.stabilize=function(){return this.physics.stabilize.apply(this.physics,arguments)},xT.prototype.getSelection=function(){return this.selectionHandler.getSelection.apply(this.selectionHandler,arguments)},xT.prototype.setSelection=function(){return this.selectionHandler.setSelection.apply(this.selectionHandler,arguments)},xT.prototype.getSelectedNodes=function(){return this.selectionHandler.getSelectedNodeIds.apply(this.selectionHandler,arguments)},xT.prototype.getSelectedEdges=function(){return this.selectionHandler.getSelectedEdgeIds.apply(this.selectionHandler,arguments)},xT.prototype.getNodeAt=function(){var t=this.selectionHandler.getNodeAt.apply(this.selectionHandler,arguments);return void 0!==t&&void 0!==t.id?t.id:t},xT.prototype.getEdgeAt=function(){var t=this.selectionHandler.getEdgeAt.apply(this.selectionHandler,arguments);return void 0!==t&&void 0!==t.id?t.id:t},xT.prototype.selectNodes=function(){return this.selectionHandler.selectNodes.apply(this.selectionHandler,arguments)},xT.prototype.selectEdges=function(){return this.selectionHandler.selectEdges.apply(this.selectionHandler,arguments)},xT.prototype.unselectAll=function(){this.selectionHandler.unselectAll.apply(this.selectionHandler,arguments),this.selectionHandler.commitWithoutEmitting.apply(this.selectionHandler),this.redraw()},xT.prototype.redraw=function(){return this.renderer.redraw.apply(this.renderer,arguments)},xT.prototype.getScale=function(){return this.view.getScale.apply(this.view,arguments)},xT.prototype.getViewPosition=function(){return this.view.getViewPosition.apply(this.view,arguments)},xT.prototype.fit=function(){return this.view.fit.apply(this.view,arguments)},xT.prototype.moveTo=function(){return this.view.moveTo.apply(this.view,arguments)},xT.prototype.focus=function(){return this.view.focus.apply(this.view,arguments)},xT.prototype.releaseNode=function(){return this.view.releaseNode.apply(this.view,arguments)},xT.prototype.getOptionsFromConfigurator=function(){var t={};return this.configurator&&(t=this.configurator.getOptions.apply(this.configurator)),t};var ST=Object.freeze({__proto__:null,prepareElements:ET,cleanupElements:OT,resetElements:function(t){ET(t),OT(t),ET(t)},getSVGElement:CT,getDOMElement:function(t,e,i,n){var o;return Object.prototype.hasOwnProperty.call(e,t)?e[t].redundant.length>0?(o=e[t].redundant[0],e[t].redundant.shift()):(o=document.createElement(t),void 0!==n?i.insertBefore(o,n):i.appendChild(o)):(o=document.createElement(t),e[t]={used:[],redundant:[]},void 0!==n?i.insertBefore(o,n):i.appendChild(o)),e[t].used.push(o),o},drawPoint:function(t,e,i,n,o,r){var s;if("circle"==i.style?((s=CT("circle",n,o)).setAttributeNS(null,"cx",t),s.setAttributeNS(null,"cy",e),s.setAttributeNS(null,"r",.5*i.size)):((s=CT("rect",n,o)).setAttributeNS(null,"x",t-.5*i.size),s.setAttributeNS(null,"y",e-.5*i.size),s.setAttributeNS(null,"width",i.size),s.setAttributeNS(null,"height",i.size)),void 0!==i.styles&&s.setAttributeNS(null,"style",i.styles),s.setAttributeNS(null,"class",i.className+" vis-point"),r){var a=CT("text",n,o);r.xOffset&&(t+=r.xOffset),r.yOffset&&(e+=r.yOffset),r.content&&(a.textContent=r.content),r.className&&a.setAttributeNS(null,"class",r.className+" vis-label"),a.setAttributeNS(null,"x",t),a.setAttributeNS(null,"y",e)}return s},drawBar:function(t,e,i,n,o,r,s,a){if(0!=n){n<0&&(e-=n*=-1);var h=CT("rect",r,s);h.setAttributeNS(null,"x",t-.5*i),h.setAttributeNS(null,"y",e),h.setAttributeNS(null,"width",i),h.setAttributeNS(null,"height",n),h.setAttributeNS(null,"class",o),a&&h.setAttributeNS(null,"style",a)}}}),TT={Images:Pb,dotparser:Ob,gephiParser:Sb,allOptions:wT,convertDot:Eb,convertGephi:Cb},MT=Object.freeze({__proto__:null,network:TT,DOMutil:ST,util:Ym,data:Jx,Hammer:Wm,keycharm:jC,DataSet:Kx,DataView:$x,Queue:Yx,Network:xT});t.DOMutil=ST,t.DataSet=Kx,t.DataView=$x,t.Hammer=Wm,t.Network=xT,t.Queue=Yx,t.data=Jx,t.default=MT,t.keycharm=jC,t.network=TT,t.util=Ym,Object.defineProperty(t,"__esModule",{value:!0})}));
//# sourceMappingURL=vis-network.min.js.map</script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 600px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 600px;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"TIME SERIES\" can be generated as follows:\n\nThe entity \"TIME SERIES\" refers to a sequence of data points measured at regular time intervals, which is a concept used in various fields, including analysis, modeling, and forecasting. It is characterized by its sequential order and temporal dependencies, and is often used to analyze and forecast trends and patterns. Time series can be used in anomaly detection, and is a field of study that deals with the analysis and modeling of data that varies over time. It can be represented as a sequence of data points, and is handled by TimeGPT, a concept that refers to a sequence of data points measured at regular time intervals.\n\nThe descriptions provided are not contradictory, but rather complementary, and provide a comprehensive understanding of the entity \"TIME SERIES\". The use of phrases such as \"time series forecasting\", \"time-series windows\", and \"time-series\" further emphasize the concept of time series as a sequence of data points measured at regular time intervals.\n\nThe entity \"TIME SERIES\" is a fundamental concept in data analysis and modeling, and is used in various applications, including anomaly detection, trend analysis, and forecasting. Its sequential order and temporal dependencies make it a valuable tool for understanding and predicting patterns in data.\n\nRelevant information from the nearby text includes the use of technical terms, mathematical equations, and references to academic papers, which suggests that the text is from a research paper or a technical document. The presence of English-language abbreviations and citations further supports the conclusion that the primary language of the text is English.", "id": "TIME SERIES", "label": "TIME SERIES", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26,0e3c8905bd533021b4f9bf9875ea66e0,116332ac4538a1430c83a34fcbec22d1,15c3350fad556f666d93817c5036109c,171fb6df1bf9905b151dfb846d75d0f8,1f1221583d838c2407fe9864225e9eda,2bb4fc2b46b9c8bdd052b2755d986aa8,2cd8d6d8f61b953c1922dd3a39c0dec2,308a1b7226a66096a6e686d5e99848ad,3e0985c172a09bb6c99e305b9f40a513,3e937ba8de0e7eca993c50506ceb8f1f,41fe893a178ebc8790ef4da83da5ab6e,42c90ca40b234c098c55632ec70038a1,42e1bb44edbe787e104f589e74b95d6c,4742f536818b2fce762157bdb2cb1a3c,477c5b7f23f4e00030ab389788a4c88d,4ab34d32601d452f14b4a1c31415292d,518bfcd6711530089fe3914ca16459c2,51ee17c1f0212d4c94010d8e376f649b,53ae42e8cf874f9df3817b3e2589d60c,53dbeea6d05c3695460c71f89f468def,53f80422fbf85856ecf940d5c2450665,55099ca8e21d1db3bdaf7bd04e590b72,55eb54ef455d14cd1a11760924f99eb8,56613213ed14f292e8ff44f4f0a8bab1,56806630593fdf0c3d95ad7555080dcf,587ca8c6cc86a93299e9540153babbc6,59e8e02df5f942071e733def7884b457,5bb0db923f70e5f431183c6ab7dc25dc,6383536333b1a09cc9e6f8d4ea5e9bce,63e284ec7e76fa859cc46b72d3746654,6917a14af275e30abe2d30a8f8a9a4e6,69457f873272a693c1f813c75ecf030a,6a976b57f1171abc9ea2ba6bb5b638f8,6ae1ee8f0c4bcf7ec4d04b1048451e96,6c11bd339c9630f1d61f2024e90bce5e,6ea15432a841705c2e74cbc01f6004e9,6eb4c16edf2eedfd03721efb199478d8,6f6e27166a1506482bfcaf5585322595,71f873c12230e6ede47583d81eb85e23,736a43d5fef4417bac9757301b4721c3,79b26808691b6333aafce43c5de531f7,79c0a74b91761402b67c3574db02e8e7,7a36c78af074ba651b0404f11cdf481d,7dbc961fe1959f844ebac283498e7fb0,83b49bf68dab6c36bdc09f02a63803fe,83f62beddf5cf53ed2e2d4517569deb8,8f4724ff6541b8924f0cebe9872ed040,9125a7d3794226f109debe90e5db041c,99b3fe01a22282fe6d02bf29dacf8875,9c155897f3e35902f57696e0abaeb161,ac37bdb991d1513e44e4c5fc7a28b187,b13b2cc422483985c354844b166a0151,b8ce119147e4d1454453c514e68dc4dc,b9d22fe9f2eecb1665f4bea365c7d612,bb10b1a7f98a4f869b7abbc5f9632dd1,bb87457fce8d4214bfe1f398b7ea35f2,bc54a718d1886698232d578fd88c3ac7,bca10b04933dc3a7f98a3f1b610e419b,c4e47033b8ecc85beacde9d560e66062,c522ea3766ae5129c11b833252340695,c5c841baa0bc205103ac433d446da3b6,c84edbea28fbbed451e8d0b7df4ffb7c,c9efd571b05c136c0bf9d7e89194ec88,ca3dfe42c66d68dd6dbad037936b2360,cc1063ba5913ea38c84ac0250c01fe84,cf0d73cbe44e03ef7300f5c53b72090a,d16a81565fcd654ab21768510dcc5d7f,de8e097ce66fbc954bd529888cbc15ea,e5e4a8f03f502fada5b17cba5dc942ba,e8151dde9661b4cce6a6b8e5a8371c96,e900311a447985c0967f87fff49c58b8,e995e5477f470244a4a6afb9417f6d96,ec3fbfb800fd9bf1d913584fda4ae925,ee42bd06cc3770a3384df85cc16fef54,efb7975581b22620b8277417edeee3e9,f0808ad97bc4d26391284145427770e5,f0c52387a7b3a5c3850fe6991f0a7c83,f6fac8e5c6fd12724fe3aa84a2e1cfa6,f7266cfccedb1d9840d10afa689a05e9,f72ba51a17dd00cff43bcdda22c273e8,f7e3207706b170296cb036538a709689,f8f6fec6d1d7eda0349d3c52c4c96d97,f98d2bec31738be3f7be750b5fe6180e,fa01b75dccc556af8aca0d6c47e1970f,fb67fcff21ac521a5ed8b202412ec1fc,fc51ca9f56bcadd343d50d9cb5cf9adb,fd1092903d83bf6e90a6caa371d7c514,fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"RESTAD\" can be generated as follows:\n\nRESTAD is a model designed for time series anomaly detection, which integrates the Transformer architecture with a radial basis function (RBF) layer to improve anomaly detection capabilities. Specifically, RESTAD combines the vanilla Transformer with the RBF layer and a composite anomaly score to achieve its goals. This model utilizes a combination of reconstruction and similarity-based transformer for anomaly detection, leveraging the strengths of both approaches to identify anomalies in time series data. By incorporating the RBF layer, RESTAD enhances its ability to detect anomalies, making it a robust and effective model for unsupervised anomaly detection in time series data.\n\nThe key features of RESTAD include:\n\n1. Integration of the Transformer architecture with the RBF layer for improved anomaly detection.\n2. Combination of reconstruction and similarity-based transformer for anomaly detection.\n3. Use of a composite anomaly score to enhance detection capabilities.\n4. Application to time series data for anomaly detection.\n\nOverall, RESTAD is a sophisticated model that leverages the strengths of both the Transformer architecture and the RBF layer to provide accurate and effective anomaly detection in time series data.", "id": "RESTAD", "label": "RESTAD", "shape": "dot", "size": 10, "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2,308a1b7226a66096a6e686d5e99848ad,529ee9dbb2f4807b9c21bbf190dbd1f7,59e8e02df5f942071e733def7884b457,99b3fe01a22282fe6d02bf29dacf8875,d0e314f46695e89479ce8e6543a2e1b5", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"ANOMALY DETECTION\" can be generated as follows:\n\n\"Anomaly detection is a concept in machine learning that refers to the process of identifying unusual patterns or outliers in data, particularly in time series data. It involves identifying data points that are significantly different from the rest of the data, the majority of the data, or the expected pattern or behavior. This task is often used in time series analysis to identify unusual or unexpected patterns in a time series. Anomaly detection is a technique used to identify unusual patterns or data points in time series data, and it is a task in machine learning that involves identifying data points that are significantly different from the norm or expected behavior.\"\n\nThis summary is a compilation of the various descriptions provided, with any contradictions resolved to provide a single, coherent summary. The summary includes information from all the descriptions, and it is written in third person to provide a clear and concise description of the entity \"ANOMALY DETECTION\".", "id": "ANOMALY DETECTION", "label": "ANOMALY DETECTION", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3,1303ca4c43652bb8052df34d21c78eca,1902e651467179a9a1d4c4df0035e980,22df9b37bd353b7b484fb07edeeb66fd,2cd8d6d8f61b953c1922dd3a39c0dec2,308a1b7226a66096a6e686d5e99848ad,3e0985c172a09bb6c99e305b9f40a513,477c5b7f23f4e00030ab389788a4c88d,587ca8c6cc86a93299e9540153babbc6,59e8e02df5f942071e733def7884b457,6383536333b1a09cc9e6f8d4ea5e9bce,6eb4c16edf2eedfd03721efb199478d8,736a43d5fef4417bac9757301b4721c3,78ee4a3d7a2bffd4405a03d94a4f6cb1,83b49bf68dab6c36bdc09f02a63803fe,9125a7d3794226f109debe90e5db041c,99b3fe01a22282fe6d02bf29dacf8875,9c6e74299923071f9fc2b7cce49efc90,b4d5306b46bbfa4564727fe5ac6630e0,bd73ee0439609823e12a877840c6ebae,d0e314f46695e89479ce8e6543a2e1b5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"DEEP LEARNING\" can be generated as follows:\n\nDeep learning is a subfield of machine learning that involves the use of artificial neural networks to analyze data, particularly well-suited for time series analysis. It is a type of machine learning model used in various domains, including language, perception, and image recognition. Deep learning models are capable of analyzing complex data and have achieved widespread acclaim for their generative capabilities. They are often used for tasks such as image and speech recognition, and are a key component of the TranAD approach.\n\nThis summary incorporates information from all the provided descriptions, resolving any potential contradictions and providing a coherent overview of the entity \"DEEP LEARNING\". The use of artificial neural networks, time series analysis, and generative capabilities are all key aspects of deep learning, highlighting its versatility and effectiveness in various applications.", "id": "DEEP LEARNING", "label": "DEEP LEARNING", "shape": "dot", "size": 10, "source_id": "5bb0db923f70e5f431183c6ab7dc25dc,6771b6279846e780d6807b15184ae008,7a36c78af074ba651b0404f11cdf481d,83b49bf68dab6c36bdc09f02a63803fe,9c6e74299923071f9fc2b7cce49efc90,f8f6fec6d1d7eda0349d3c52c4c96d97", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"FOUNDATION MODEL\" can be generated as follows:\n\nA foundation model is a type of machine learning model that is designed to be a general-purpose model, capable of being fine-tuned for a variety of tasks and domains. It is a pre-trained model that has been trained on a large corpus of data, allowing it to capture wide-ranging and generic features. Foundation models are large-scale, general-purpose neural networks that have been pretrained in an unsupervised manner on large amounts of diverse data across various data distributions. They are trained on a vast amount of data to cultivate general-purpose representations, which can be fine-tuned or used for various tasks.\n\nFoundation models can be used as a starting point for other models or applications, and they have the capability for zero-shot and few-shot generalization. They have caused a paradigm shift in machine learning due to their unprecedented capabilities. The concept of foundation models is related to artificial intelligence or machine learning, and they are pre-trained models that can be fine-tuned for specific tasks.\n\nThe mention of Lag-Llama in the description suggests that foundation models can be tailored to time series data, indicating their potential applications in this area. Overall, foundation models are a type of machine learning model that has the potential to revolutionize various tasks and domains due to their general-purpose nature and ability to be fine-tuned for specific tasks.\n\nThe provided descriptions are consistent in their portrayal of foundation models as general-purpose machine learning models that can be fine-tuned for various tasks and domains. The information collected from all the descriptions has been used to generate a comprehensive summary that highlights the key characteristics and capabilities of foundation models.", "id": "FOUNDATION MODEL", "label": "FOUNDATION MODEL", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,3174231a67593609c727151c9df31d0a,42e1bb44edbe787e104f589e74b95d6c,4de223d7df157faf857c3e17d9e6e5b6,53f80422fbf85856ecf940d5c2450665,55eb54ef455d14cd1a11760924f99eb8,5bb0db923f70e5f431183c6ab7dc25dc,6f6e27166a1506482bfcaf5585322595,ac37bdb991d1513e44e4c5fc7a28b187,b8ce119147e4d1454453c514e68dc4dc,c4e47033b8ecc85beacde9d560e66062,c7167cf92abe7513ccb936ca79871932,ca3dfe42c66d68dd6dbad037936b2360,ec7705b83cf4fe3aa18662c917b18c1a,f8f6fec6d1d7eda0349d3c52c4c96d97", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"FOUNDATION MODELS\" can be generated as follows:\n\nFOUNDATION MODELS are a type of pre-trained model that can be used as a starting point for various tasks, including natural language processing and time series forecasting. They are large-scale models that have been pre-trained on a vast amount of data, capturing wide-ranging and generic features. These models can be fine-tuned for specific downstream tasks, including time series analysis, and have demonstrated efficacy across a diverse array of temporal datasets. Foundation models are related to time series analysis as they can be adapted to this task using few-shot learning or fine-tuning. They are also pre-trained models that can be fine-tuned for various tasks, including computer vision and natural language processing, and have driven rapid progress in these fields.\n\nThe primary characteristics of foundation models include:\n\n1. Pre-training on a large dataset\n2. Fine-tuning for specific tasks\n3. Ability to capture wide-ranging and generic features\n4. Adaptability to various tasks, including time series analysis\n5. Efficacy across diverse temporal datasets\n\nOverall, foundation models are a powerful tool in the field of machine learning, enabling rapid progress in various tasks and applications.\n\nNote: The contradictions in the descriptions have been resolved by focusing on the common characteristics and features of foundation models, while excluding any conflicting information.", "id": "FOUNDATION MODELS", "label": "FOUNDATION MODELS", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26,2e2e2fa4e717e09d31996f7f22bd50a0,479fc10bea12b01a37fbc5cef21eea76,55eb54ef455d14cd1a11760924f99eb8,5805d8a1afe3d6bc6300ac71f7163831,6ae1ee8f0c4bcf7ec4d04b1048451e96,736a43d5fef4417bac9757301b4721c3,79b26808691b6333aafce43c5de531f7,79c0a74b91761402b67c3574db02e8e7,7a36c78af074ba651b0404f11cdf481d,7d5d82d600620153153772a9bc498ac0,8f4724ff6541b8924f0cebe9872ed040,9125a7d3794226f109debe90e5db041c,ad8efcfda80253036294f2eeb48926e8,b67d18d306fde251ee94b0a831d1e075,bb10b1a7f98a4f869b7abbc5f9632dd1,c4e47033b8ecc85beacde9d560e66062,ca3dfe42c66d68dd6dbad037936b2360,de8e097ce66fbc954bd529888cbc15ea,f912df936d0b735fe654d1a9c53caa3b,f92205091f8d3b7e1e60b9bc80883a62,ff641d7e46d16e86c55d25c86b49bd52", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"TRAJECTORY\" can be generated as follows:\n\nThe entity \"TRAJECTORY\" refers to a sequence of timestamped locations that describe the movement of objects over time, representing a path or sequence of points in space or time. Specifically, it describes the movements of an object in geographical space, providing a detailed and temporal representation of its movement.\n\nThis summary is derived from the three descriptions provided, which are consistent in their definition of a trajectory as a sequence of timestamped locations. The descriptions also highlight the temporal and spatial aspects of a trajectory, emphasizing its ability to describe the movement of objects over time and in geographical space.\n\nThe use of the term \"timestamped locations\" in all three descriptions suggests a focus on the temporal dimension of a trajectory, while the reference to \"geographical space\" in the third description provides additional context on the spatial aspect of a trajectory.\n\nOverall, the summary provides a clear and concise definition of the entity \"TRAJECTORY\" and highlights its key characteristics, including its temporal and spatial dimensions.", "id": "TRAJECTORY", "label": "TRAJECTORY", "shape": "dot", "size": 10, "source_id": "53f80422fbf85856ecf940d5c2450665,736a43d5fef4417bac9757301b4721c3,79c0a74b91761402b67c3574db02e8e7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"SPATIAL TIME SERIES\" can be generated as follows:\n\nThe entity \"SPATIAL TIME SERIES\" refers to a sequence of data points that possess both temporal and spatial dimensions. This type of time series is utilized in various applications, including urban computing and environmental monitoring. It involves spatial data, which is measured at regular time intervals, and exhibits spatial correlations between different locations. Furthermore, spatial time series includes data points with spatial coordinates, allowing for the analysis of spatial patterns and relationships over time.\n\nThis summary is derived from the collective information provided in the description list, which highlights the key characteristics and applications of spatial time series. The contradictions present in the descriptions have been resolved to provide a coherent and concise summary of the entity.", "id": "SPATIAL TIME SERIES", "label": "SPATIAL TIME SERIES", "shape": "dot", "size": 10, "source_id": "736a43d5fef4417bac9757301b4721c3,79b26808691b6333aafce43c5de531f7,79c0a74b91761402b67c3574db02e8e7,9125a7d3794226f109debe90e5db041c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"EVENTS\" can be generated as follows:\n\nThe entity \"EVENTS\" refers to specific occurrences or happenings that can be analyzed using time series techniques. These events involve specific happenings that take place at a specific time or location, and they can be considered as a type of time series. Time series analysis can be applied to events to understand patterns, trends, and relationships between different occurrences. This analysis can be useful for long-term series forecasting, frequency analysis, and other applications.\n\nThe use of time series techniques for event analysis is supported by the presence of mathematical equations and formulas, which are commonly used in English-language academic papers. The entity \"EVENTS\" is also related to concepts such as multi-head cross-attention, which is a technique used in natural language processing and machine learning.\n\nOverall, the entity \"EVENTS\" is a type of time series that involves specific occurrences or happenings, and it can be analyzed using various time series techniques to gain insights and make predictions.\n\nRelevant information from the nearby text that was used to enrich the summary includes:\n\n* The use of English words and phrases such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention.\"\n* The presence of mathematical equations and formulas, which are written in a standard mathematical notation used in English-language academic papers.\n* The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals.\n* The citation of English-language academic papers and authors, which suggests that the text is written in English.\n\nNote that the summary is written in third person and includes the entity name \"EVENTS\" to provide full context.", "id": "EVENTS", "label": "EVENTS", "shape": "dot", "size": 10, "source_id": "736a43d5fef4417bac9757301b4721c3,79b26808691b6333aafce43c5de531f7,ef8fd6170b08af3bd99c9df44ffa8b57", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Diffusion-based models are a type of neural network architecture used for generative tasks", "id": "DIFFUSION-BASED MODELS", "label": "DIFFUSION-BASED MODELS", "shape": "dot", "size": 10, "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "type": "MODEL"}, {"color": "#97c2fc", "description": "Prediction refers to the process of forecasting future values or outcomes based on historical data", "id": "PREDICTION", "label": "PREDICTION", "shape": "dot", "size": 10, "source_id": "9125a7d3794226f109debe90e5db041c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"IMPUTATION\" refers to the process of filling in missing or incomplete data points in a time series. This process is often used in time series analysis to handle missing values, which are common in datasets. Imputation is a crucial step in ensuring the accuracy and reliability of time series forecasting models, as missing data can significantly impact the performance of these models.\n\nIn the context of time series analysis, imputation involves replacing missing values with estimated or predicted values, which can be obtained through various methods such as mean imputation, median imputation, or more advanced techniques like regression-based imputation. The choice of imputation method depends on the nature of the data, the frequency of missing values, and the goals of the analysis.\n\nOverall, imputation is a vital component of time series analysis, enabling researchers and practitioners to work with incomplete datasets and make informed decisions based on the available data.\n\nRelevant information from the nearby text:\n\n* The text mentions that imputation is often used in time series analysis, which suggests that it is a common practice in this field.\n* The text does not provide any information about the specific methods used for imputation, but it implies that there are various techniques available for handling missing values.\n* The text does not mention any specific applications or domains where imputation is used, but it is likely that imputation is used in a wide range of fields, including finance, economics, and engineering.", "id": "IMPUTATION", "label": "IMPUTATION", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8,9125a7d3794226f109debe90e5db041c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMASKED AUTOENCODING is a strategy used in pre-training time series foundation models. It is a type of self-supervised learning where the model is trained to recover the original input space. This approach involves masking certain parts of the input data and training the model to predict the missing values, which enables the model to learn the underlying patterns and relationships in the data.\n\nThe use of masked autoencoding in pre-training time series foundation models is a key aspect of this strategy, as it allows the model to learn from the data in a self-supervised manner. By training the model to recover the original input space, masked autoencoding enables the model to develop a deep understanding of the underlying patterns and relationships in the data, which can be leveraged for various time series forecasting tasks.\n\nOverall, MASKED AUTOENCODING is a powerful strategy for pre-training time series foundation models, and its use in self-supervised learning has the potential to revolutionize the field of time series forecasting.", "id": "MASKED AUTOENCODING", "label": "MASKED AUTOENCODING", "shape": "dot", "size": 10, "source_id": "9125a7d3794226f109debe90e5db041c,de8e097ce66fbc954bd529888cbc15ea", "type": "PHASE"}, {"color": "#97c2fc", "description": "Raster refers to a type of data representation used in geospatial applications", "id": "RASTER", "label": "RASTER", "shape": "dot", "size": 10, "source_id": "53f80422fbf85856ecf940d5c2450665", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"TEXT\" refers to written or spoken language. This description is a fundamental definition of text, encompassing both written and spoken forms of language. The primary language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers written in English.\n\nThe text is formal and academic in nature, suggesting that it is from a research paper or a technical document. The presence of technical terms, mathematical equations, and English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports this conclusion.\n\nOverall, the entity \"TEXT\" is a broad term that encompasses various forms of written and spoken language, with the specific text in question being written in English and exhibiting formal and academic characteristics.", "id": "TEXT", "label": "TEXT", "shape": "dot", "size": 10, "source_id": "53f80422fbf85856ecf940d5c2450665,deb67d5386710136cf24bcbf135a66c4", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Embeddings refer to the process of converting high-dimensional data into a lower-dimensional space, often used in natural language processing and computer vision", "id": "EMBEDDINGS", "label": "EMBEDDINGS", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Context refers to the input data that is used to make a prediction", "id": "CONTEXT", "label": "CONTEXT", "shape": "dot", "size": 10, "source_id": "6f6e27166a1506482bfcaf5585322595", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nPATCH BASED MODELING is a concept mentioned in the text, as indicated by the use of the phrase \"patch based modeling.\" It is a technique where a time series is broken down into smaller patches, allowing for more efficient and effective analysis and forecasting. This approach is likely used in time series forecasting and long-term series forecasting, where frequency analysis and multi-head cross-attention may be employed to improve model performance.\n\nThe use of patch based modeling is likely mentioned in academic papers and conferences, such as ICLR, AAAI, and PMLR, and may be cited by authors in their research. Overall, patch based modeling is a technique that has the potential to improve time series forecasting and analysis, and is likely to be of interest to researchers and practitioners in the field of machine learning and time series forecasting.", "id": "PATCH BASED MODELING", "label": "PATCH BASED MODELING", "shape": "dot", "size": 10, "source_id": "6f6e27166a1506482bfcaf5585322595,f98d2bec31738be3f7be750b5fe6180e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"TOKEN\" is a concept mentioned in language models, as indicated by the use of the phrase \"token in language models.\" It refers to a single unit of data, often used in language models and time series analysis. This suggests that the term \"TOKEN\" is closely related to the field of natural language processing and machine learning, where it is used to represent individual elements of text or data.\n\nIn the context of language models, tokens are typically used to represent words, characters, or subwords, which are then processed and analyzed to generate predictions or make decisions. The use of tokens in language models is a fundamental aspect of many machine learning algorithms, including those used for text classification, sentiment analysis, and language translation.\n\nIn time series analysis, tokens may refer to individual data points or observations, which are then used to analyze and forecast trends and patterns in data. This suggests that the term \"TOKEN\" has a broader application in data analysis and machine learning, extending beyond its use in language models.\n\nOverall, the entity \"TOKEN\" is a fundamental concept in machine learning and data analysis, representing a single unit of data that is used to process and analyze information in a variety of contexts.", "id": "TOKEN", "label": "TOKEN", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8,f98d2bec31738be3f7be750b5fe6180e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"PATCH\" can be generated as follows:\n\nThe entity \"PATCH\" refers to a concept mentioned in the text, specifically in the context of time-series analysis. A patch is defined as a contiguous non-overlapping segment of the input time-series, which is often used in data analysis and forecasting. In this context, a patch can be considered as a specific instance or example of a time series patch, representing a subset of data points in a time series. This concept is relevant in various applications, including data analysis, forecasting, and modeling.\n\nThe use of the term \"patch\" in this context is likely related to the idea of extracting meaningful segments or patterns from a larger time-series dataset, which can be used to improve forecasting accuracy or gain insights into the underlying dynamics of the system being modeled. The concept of patches is also related to the idea of patching or segmenting a time series into smaller, more manageable pieces, which can be analyzed and modeled separately.\n\nOverall, the entity \"PATCH\" is a key concept in time-series analysis and forecasting, representing a specific approach to segmenting and analyzing time-series data.\n\nRelevant information from the nearby text:\n\n* The text mentions the use of the phrase \"patch of a time-series\", which suggests that the concept of patches is closely related to time-series analysis.\n* The text also mentions the use of mathematical equations and formulas, which are likely used to model and analyze the patches extracted from the time-series data.\n* The text references various academic papers and authors, which suggests that the concept of patches is a topic of ongoing research and development in the field of time-series analysis and forecasting.\n\nNote: The contradictions in the description list have been resolved by considering the most relevant and accurate information provided. The summary is written in third person and includes the entity name \"PATCH\" for context.", "id": "PATCH", "label": "PATCH", "shape": "dot", "size": 10, "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8,3cf566c27c75f886658002bf9b3b0b15,3e937ba8de0e7eca993c50506ceb8f1f,4742f536818b2fce762157bdb2cb1a3c,4ab34d32601d452f14b4a1c31415292d,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a014d14e003d0998b877eacffa8ebfc4,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,cc1063ba5913ea38c84ac0250c01fe84,e8151dde9661b4cce6a6b8e5a8371c96,f98d2bec31738be3f7be750b5fe6180e,fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"HORIZON LENGTH\" refers to a concept mentioned in the text, which is a parameter used in models for time series forecasting. Specifically, it refers to the number of time steps ahead that a model is trying to predict. This concept is relevant in the context of long-term series forecasting, where the horizon length determines the extent of the prediction.\n\nIn the context of machine learning models, the horizon length is a critical parameter that affects the performance and accuracy of the predictions. It is often used in conjunction with other parameters, such as frequency analysis and multi-head cross-attention, to improve the forecasting capabilities of the models.\n\nThe use of horizon length in models is supported by various academic papers and research studies, which have demonstrated its effectiveness in improving the accuracy of time series forecasting. For example, studies published in conferences such as ICLR, AAAI, and PMLR have explored the use of horizon length in various machine learning models, including those that employ frequency analysis and multi-head cross-attention.\n\nOverall, the concept of horizon length is a crucial aspect of time series forecasting, and its use in machine learning models has been extensively studied and validated in the academic literature.", "id": "HORIZON LENGTH", "label": "HORIZON LENGTH", "shape": "dot", "size": 10, "source_id": "39365aee753fb73130c208fdf4046bb7,efb7975581b22620b8277417edeee3e9,f98d2bec31738be3f7be750b5fe6180e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"CONTEXT LENGTH\" can be generated as follows:\n\nThe entity \"CONTEXT LENGTH\" refers to a parameter used in various models, representing the length of the context window or input sequence. It is a critical parameter that controls the number of time steps, tokens, or words used to make a prediction or represent a piece of text during inference. Specifically, context length is the number of time steps to use as input, which can also be interpreted as the number of tokens or words in a model\u0027s input sequence. This parameter is essential in determining the scope of the context used by the model, allowing it to capture relevant information and make accurate predictions.\n\nIn the context of time series forecasting, context length may refer to the number of time steps used as input to the model, which can be used to forecast future values. The choice of context length can significantly impact the performance of the model, and it is often a hyperparameter that needs to be tuned during the training process.\n\nOverall, context length is a fundamental parameter in various machine learning models, including those used for time series forecasting, and its proper selection is crucial for achieving accurate results.", "id": "CONTEXT LENGTH", "label": "CONTEXT LENGTH", "shape": "dot", "size": 10, "source_id": "1ddbab2dca370c9ef7b5a724075518cc,39365aee753fb73130c208fdf4046bb7,56613213ed14f292e8ff44f4f0a8bab1,973ea1d37e8f6bb0ab004f360c04ddc6,cd4ff69415c7b37cec643f8289d1c98d,efb7975581b22620b8277417edeee3e9,fa01b75dccc556af8aca0d6c47e1970f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Date derived features refer to the extraction of meaningful information from date and time data, such as day of the week or month of the year", "id": "DATE DERIVED FEATURES", "label": "DATE DERIVED FEATURES", "shape": "dot", "size": 10, "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Vector refers to a mathematical representation of a set of values or features, often used in machine learning and data analysis", "id": "VECTOR", "label": "VECTOR", "shape": "dot", "size": 10, "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time-points refer to specific points in time, often used to represent data or events in a time series", "id": "TIME-POINTS", "label": "TIME-POINTS", "shape": "dot", "size": 10, "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Self-supervised transformer is a type of model that learns from unlabeled data", "id": "SELF-SUPERVISED TRANSFORMER", "label": "SELF-SUPERVISED TRANSFORMER", "shape": "dot", "size": 10, "source_id": "3e0985c172a09bb6c99e305b9f40a513", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Entity:** ANOMALYBERT\n\n**Summary:** ANOMALYBERT is a model that specializes in anomaly detection, particularly in the context of time series data. It has been demonstrated to outperform previous methods on five benchmark datasets, showcasing its effectiveness in identifying anomalies. As a self-supervised method, ANOMALYBERT is capable of detecting anomalies without the need for labeled data, making it a valuable tool for various applications. The model\u0027s name, \"AnomalyBERT,\" suggests its connection to the BERT (Bidirectional Encoder Representations from Transformers) architecture, which is a popular choice for natural language processing tasks. However, in this context, ANOMALYBERT is specifically designed for time series anomaly detection, leveraging its capabilities to identify patterns and anomalies in temporal data.\n\n**Key Features:**\n\n* Specializes in anomaly detection for time series data\n* Outperforms previous methods on five benchmark datasets\n* Self-supervised method, capable of detecting anomalies without labeled data\n* Likely based on the BERT architecture, adapted for time series anomaly detection\n\n**Context:** The summary is based on the provided descriptions, which collectively paint a picture of ANOMALYBERT as a powerful tool for anomaly detection in time series data. The model\u0027s performance on benchmark datasets and its self-supervised nature make it a valuable asset for various applications, including but not limited to, industrial monitoring, financial analysis, and healthcare.", "id": "ANOMALYBERT", "label": "ANOMALYBERT", "shape": "dot", "size": 10, "source_id": "19a1a12db412295d0ddd19ffffb78332,3e0985c172a09bb6c99e305b9f40a513,587ca8c6cc86a93299e9540153babbc6,71f873c12230e6ede47583d81eb85e23,83b49bf68dab6c36bdc09f02a63803fe,8ff636d53a50d7a3bad17443bf104ba2,9f30a997c7ea3ed8fe02f631a3bd9649,ee42bd06cc3770a3384df85cc16fef54,ee651b9bedb0cbcb6272a67deda44bb0", "type": ""}, {"color": "#97c2fc", "description": "Local outlier factor is a method for density-based outlier detection", "id": "LOCAL OUTLIER FACTOR", "label": "LOCAL OUTLIER FACTOR", "shape": "dot", "size": 10, "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "type": "METHOD"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the \"DATA DEGRADATION SCHEME\" can be generated as follows:\n\nThe \"DATA DEGRADATION SCHEME\" is a method used to degrade or modify input data to simulate real-world anomalies or to improve model performance. This process involves degrading the quality of data, which can be achieved through various techniques. The primary goal of the data degradation scheme is to enhance model performance by introducing realistic anomalies or imperfections into the data, thereby making the model more robust and accurate in its predictions.\n\nThe data degradation scheme can be used in various applications, including time series forecasting, where it can be employed to simulate real-world anomalies and improve the accuracy of long-term series forecasting models. The scheme can also be used in frequency analysis and multi-head cross-attention techniques to improve model performance.\n\nOverall, the data degradation scheme is a valuable tool in the field of machine learning and data analysis, allowing researchers and practitioners to improve model performance and accuracy by introducing realistic anomalies and imperfections into the data.\n\nRelevant information from the nearby text suggests that the data degradation scheme is a technical term used in academic papers and research documents, often cited in conferences and journals such as ICLR, AAAI, and PMLR. The scheme is often used in conjunction with mathematical equations and formulas to describe its implementation and effectiveness.\n\nNote: The contradictions in the descriptions have been resolved by focusing on the primary goal of the data degradation scheme, which is to improve model performance by degrading the quality of data. The summary has been written in third person and includes relevant information from the nearby text to provide a comprehensive understanding of the data degradation scheme.", "id": "DATA DEGRADATION SCHEME", "label": "DATA DEGRADATION SCHEME", "shape": "dot", "size": 10, "source_id": "3e0985c172a09bb6c99e305b9f40a513,587ca8c6cc86a93299e9540153babbc6,6917a14af275e30abe2d30a8f8a9a4e6,71f873c12230e6ede47583d81eb85e23", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The text is likely from a technical document, as indicated by the presence of technical terms and mathematical equations", "id": "TECHNICAL DOCUMENT", "label": "TECHNICAL DOCUMENT", "shape": "dot", "size": 10, "source_id": "4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nLONG-TERM SERIES FORECASTING is a concept mentioned in the text, referring to the technique used to predict future values in a time series. This technique is utilized for long-term forecasting, which involves analyzing and modeling time series data to make accurate predictions about future values. The use of phrases such as \"long-term series forecasting\" and the presence of mathematical equations and formulas in the text suggest that this concept is a key aspect of the text\u0027s content.\n\nThe text also indicates that LONG-TERM SERIES FORECASTING is related to time series analysis, which involves examining and modeling time series data to identify patterns and trends. This analysis is crucial for making accurate predictions and forecasts, particularly in the context of long-term series forecasting.\n\nFurthermore, the text mentions that LONG-TERM SERIES FORECASTING involves the use of techniques such as frequency analysis, which is a method used to analyze and understand the underlying patterns and structures of time series data. This analysis is essential for developing accurate models and making reliable predictions.\n\nOverall, LONG-TERM SERIES FORECASTING is a critical concept in the text, and its relationship to time series analysis and frequency analysis highlights its importance in making accurate predictions and forecasts.\n\nRelevant information from the nearby text:\n\n* The text is written in English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers.\n* The text is formal and academic, suggesting that it is from a research paper or a technical document.\n* The text mentions various technical terms and concepts, including time series, long-term series forecasting, frequency analysis, and multi-head cross-attention.\n\nNote: The provided descriptions are not contradictory, and the summary is based on the information collected from all the descriptions.", "id": "LONG-TERM SERIES FORECASTING", "label": "LONG-TERM SERIES FORECASTING", "shape": "dot", "size": 10, "source_id": "15c3350fad556f666d93817c5036109c,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"DATASET\" refers to a collection of data used for training or testing a model. This collection of data is a crucial component in the development and evaluation of machine learning models, serving as the foundation for model training and validation.\n\nIn the context of machine learning, a dataset is a critical resource that enables the development of accurate and reliable models. The dataset is used to train the model, allowing it to learn patterns and relationships within the data, and to make predictions or classify new, unseen data.\n\nThe dataset can be used for various purposes, including training, testing, and validation of machine learning models. It is essential to ensure that the dataset is representative of the problem or task at hand, and that it is properly preprocessed and formatted to facilitate model development and evaluation.\n\nOverall, the \"DATASET\" entity plays a vital role in the machine learning process, serving as the primary source of data for model development and evaluation.", "id": "DATASET", "label": "DATASET", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23,74527a4337ed6919731be520311ae774,ac37bdb991d1513e44e4c5fc7a28b187", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"ANOMALY\" refers to a data point or sequence that is significantly different from the rest of the data. This difference is substantial enough to distinguish it from the rest of the data, indicating a deviation from the expected pattern or behavior. The descriptions provided are consistent in their definition of an anomaly, emphasizing its distinctiveness from the normal data.\n\nGiven the formal and academic tone of the language used, it is likely that this definition is from a research paper or technical document in the field of data science or machine learning. The context in which this definition is used is likely related to data analysis, pattern recognition, or predictive modeling, where anomalies can be critical to identify and understand.\n\nIn the context of time series analysis, long-term series forecasting, frequency analysis, and multi-head cross-attention, anomalies can be particularly challenging to detect and model. However, understanding and addressing anomalies is crucial for improving the accuracy and reliability of predictive models and maintaining the integrity of data-driven insights.\n\nOverall, the entity \"ANOMALY\" is a critical concept in data science and machine learning, representing a data point or sequence that deviates significantly from the expected behavior, and its accurate identification and analysis are essential for informed decision-making and data-driven insights.", "id": "ANOMALY", "label": "ANOMALY", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23,f2e78b25a535b82e2743a0ea4052eb9a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"TRANSFORMER\" can be generated as follows:\n\nThe Transformer architecture is a type of neural network architecture that has been applied to various tasks, including language modeling, computer vision, anomaly detection, sequence-to-sequence tasks, natural language processing, and time series forecasting. It uses self-attention mechanisms to process sequential data and is mentioned in the text as a model used for evaluation. The Transformer architecture is also used in applications such as TimeGPT and has been utilized in various other tasks beyond anomaly detection.\n\nThis summary is based on the information collected from all the descriptions provided, and any contradictions have been resolved to provide a single, coherent description. The entity name \"TRANSFORMER\" is included to provide full context, and relevant information from the nearby text has been incorporated to enrich the summary.", "id": "TRANSFORMER", "label": "TRANSFORMER", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca,22df9b37bd353b7b484fb07edeeb66fd,308a1b7226a66096a6e686d5e99848ad,41b0bd14ce7ff7419b7ee1f78b4701ae,518bfcd6711530089fe3914ca16459c2,71f873c12230e6ede47583d81eb85e23,7dbc961fe1959f844ebac283498e7fb0,89e5f6a93205d5e87ad7e7641c0bbb91,99b3fe01a22282fe6d02bf29dacf8875,a5d60c1a355b77187003182f6df57646,f7266cfccedb1d9840d10afa689a05e9", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"CHRONOS\" can be generated as follows:\n\nCHRONOS is a language modeling framework specifically designed for time series analysis and forecasting. It is a pre-trained model that can be used for time series forecasting, adapting existing language model architectures and training procedures. CHRONOS is a model that significantly out-performs local statistical models on probabilistic forecasting and trains language models from scratch on a large collection of time series. This framework is developed for time series forecasting and is mentioned in the text as a model that can be used for this purpose.\n\nThe key points about CHRONOS can be summarized as follows:\n\n1. **Purpose**: CHRONOS is designed for time series analysis and forecasting.\n2. **Architecture**: It adapts existing language model architectures and training procedures.\n3. **Performance**: CHRONOS significantly out-performs local statistical models on probabilistic forecasting.\n4. **Training**: It trains language models from scratch on a large collection of time series.\n5. **Application**: CHRONOS is a pre-trained model used in time series analysis and can be used for time series forecasting.\n\nOverall, CHRONOS is a powerful language modeling framework for time series forecasting that has shown significant performance improvements over local statistical models.", "id": "CHRONOS", "label": "CHRONOS", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26,116332ac4538a1430c83a34fcbec22d1,42c90ca40b234c098c55632ec70038a1,532a6d434dab611a80aef1e94dd2fb45,6212b2146d9ad27124114c334a946854,6a976b57f1171abc9ea2ba6bb5b638f8,6c273e9f841addeed2a33240ddaa3d96,6eb4c16edf2eedfd03721efb199478d8,85e4fbea9a08a0a663f3c5dc3f3a95a7,af36d1634490149b96980fb1dff57cd1,b4d5306b46bbfa4564727fe5ac6630e0,baa887ae552c6b3dac10f74896d8c4a2,bb10b1a7f98a4f869b7abbc5f9632dd1,bc54a718d1886698232d578fd88c3ac7,bca10b04933dc3a7f98a3f1b610e419b,c5c841baa0bc205103ac433d446da3b6,f622f27b5c3f52c6b04ada48bd63b03d,f72ba51a17dd00cff43bcdda22c273e8,f7e3207706b170296cb036538a709689,fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "MODEL"}, {"color": "#97c2fc", "description": "Probabilistic time series models are models that capture the uncertainty in time series data", "id": "PROBABILISTIC TIME SERIES MODELS", "label": "PROBABILISTIC TIME SERIES MODELS", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Deep forecasters are a type of model that uses deep learning techniques to predict future values in a time series", "id": "DEEP FORECASTERS", "label": "DEEP FORECASTERS", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe \"LANGUAGE MODEL\" is a type of machine learning model that operates on tokens from a finite vocabulary, often used for natural language processing tasks. It is a model that has been trained on a large corpus of text data to predict future patterns. Specifically, language models are designed to operate over a fixed vocabulary, utilizing this training data to generate predictions and perform tasks such as natural language processing.\n\nThis summary incorporates information from all the provided descriptions, resolving any potential contradictions and providing a coherent overview of the \"LANGUAGE MODEL\" entity. The key points include:\n\n* The model operates on tokens from a finite vocabulary.\n* It is used for natural language processing tasks.\n* The model has been trained on a large corpus of text data.\n* It is designed to predict future patterns.\n* The model operates over a fixed vocabulary.\n\nThis summary provides a clear and concise description of the \"LANGUAGE MODEL\" entity, incorporating relevant information from the provided descriptions.", "id": "LANGUAGE MODEL", "label": "LANGUAGE MODEL", "shape": "dot", "size": 10, "source_id": "42c90ca40b234c098c55632ec70038a1,6eb4c16edf2eedfd03721efb199478d8,973ea1d37e8f6bb0ab004f360c04ddc6,a8638786e37b5d4f9d005cc1dbf2b8cb,f7e3207706b170296cb036538a709689", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, it is clear that the primary entity of interest is \"TOKENIZATION.\" After analyzing the descriptions, a comprehensive summary can be generated as follows:\n\nTokenization is a process that involves breaking down a time series or text into smaller units, such as tokens or individual words. This process is used in various applications, including language models, time series analysis, and Chronos, requiring minimal modifications to existing architectures. Tokenization can be achieved through different methods, such as simple scaling and quantization of real values, mapping observations into a finite set of tokens, or breaking down text into individual tokens or units. The primary goal of tokenization is to transform complex data into a more manageable and interpretable format, often used in machine learning and time series forecasting.\n\nRelevant information from the nearby text suggests that tokenization is a concept mentioned in the text, as indicated by the use of the phrase \"proposed tokenization approach.\" Additionally, the use of technical terms and mathematical equations in the text, such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention,\" further supports the idea that tokenization is a technical concept used in the context of machine learning and time series analysis.\n\nOverall, the summary provides a clear and concise description of tokenization, highlighting its importance in various applications and its role in transforming complex data into a more manageable format.", "id": "TOKENIZATION", "label": "TOKENIZATION", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26,3174231a67593609c727151c9df31d0a,3e937ba8de0e7eca993c50506ceb8f1f,55099ca8e21d1db3bdaf7bd04e590b72,6c273e9f841addeed2a33240ddaa3d96,6eb4c16edf2eedfd03721efb199478d8,ac37bdb991d1513e44e4c5fc7a28b187,bca10b04933dc3a7f98a3f1b610e419b,f622f27b5c3f52c6b04ada48bd63b03d,f7e3207706b170296cb036538a709689,fc51ca9f56bcadd343d50d9cb5cf9adb,fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Data augmentation is a technique used to increase the size of a training dataset by applying transformations to the existing data", "id": "DATA AUGMENTATION", "label": "DATA AUGMENTATION", "shape": "dot", "size": 10, "source_id": "42c90ca40b234c098c55632ec70038a1", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"TSMIXUP\" can be generated as follows:\n\nTSMIXUP is a data augmentation technique used in the Chronos framework, specifically designed for generating synthetic time series data. It generalizes the idea of Mixup to more than two datapoints by randomly sampling a set of base time series from different training datasets and generating new time series based on a convex combination of them. This approach allows for the creation of diverse and realistic synthetic time series data, which can be used to augment existing datasets and improve the robustness and generalizability of time series forecasting models.\n\nThe Chronos framework utilizes TSMIXUP as an algorithm, incorporating its training protocols to leverage the benefits of data augmentation. By employing TSMIXUP, researchers and practitioners can generate high-quality synthetic time series data, which can be used to enhance the performance of time series forecasting models, particularly in scenarios where real-world data is limited or noisy.\n\nOverall, TSMIXUP is a valuable tool for time series data augmentation, offering a flexible and effective approach to generating synthetic data that can be used to improve the accuracy and reliability of time series forecasting models.", "id": "TSMIXUP", "label": "TSMIXUP", "shape": "dot", "size": 10, "source_id": "0e3c8905bd533021b4f9bf9875ea66e0,42c90ca40b234c098c55632ec70038a1,6212b2146d9ad27124114c334a946854,c522ea3766ae5129c11b833252340695,f72ba51a17dd00cff43bcdda22c273e8,f7e3207706b170296cb036538a709689", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"KERNELSYNTH\" can be generated as follows:\n\nKERNELSYNTH is a data augmentation strategy and technique used to generate synthetic time series data. It utilizes Gaussian processes to randomly compose kernel functions, enabling the creation of synthetic time series. This method is employed to generate synthetic data for training time series forecasting models, thereby augmenting the existing dataset. KERNELSYNTH can be considered a model for time series data augmentation, which is essential for improving the performance and robustness of time series forecasting models.\n\nThe use of Gaussian processes and kernel functions in KERNELSYNTH allows for the generation of realistic and diverse synthetic time series, making it a valuable tool for researchers and practitioners working with time series data. By leveraging KERNELSYNTH, users can create large datasets for training and testing purposes, which can help to improve the accuracy and reliability of time series forecasting models.\n\nIn the context of time series forecasting, KERNELSYNTH can be used to augment existing datasets by generating synthetic data that mimics the patterns and characteristics of the original data. This can be particularly useful for improving the performance of models on out-of-sample data or for evaluating the robustness of models to different types of data. Overall, KERNELSYNTH is a powerful tool for time series data augmentation, and its use can lead to significant improvements in the accuracy and reliability of time series forecasting models.", "id": "KERNELSYNTH", "label": "KERNELSYNTH", "shape": "dot", "size": 10, "source_id": "0e3c8905bd533021b4f9bf9875ea66e0,42c90ca40b234c098c55632ec70038a1,66b7675d8c62321e1cc8916401159787,c522ea3766ae5129c11b833252340695,e37f4063d074e1697e1f539131b3d963,f7e3207706b170296cb036538a709689", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"PATTERN RECOGNITION\" can be described as follows:\n\nPATTERN RECOGNITION refers to the ability of a model to identify patterns in data, which involves recognizing regularities or consistencies within the data. This concept is crucial in various fields, including machine learning, where models are trained to learn from data and make predictions or decisions based on the identified patterns.\n\nThe descriptions provided are consistent and reinforce each other, highlighting the core idea of pattern recognition as the ability to identify patterns or regularities in data. The entity \"PATTERN RECOGNITION\" is a fundamental concept in machine learning and data analysis, enabling models to extract meaningful insights from complex data sets.\n\nIn the context of machine learning, pattern recognition is a key aspect of model development, where algorithms are designed to learn from data and make predictions or decisions based on the identified patterns. This concept is closely related to other entities, such as \"TIME SERIES ANALYSIS\" and \"LONG-TERM SERIES FORECASTING,\" which involve analyzing and predicting patterns in data over time.\n\nOverall, the entity \"PATTERN RECOGNITION\" is a critical concept in machine learning and data analysis, enabling models to identify patterns and make informed decisions based on the insights gained from the data.", "id": "PATTERN RECOGNITION", "label": "PATTERN RECOGNITION", "shape": "dot", "size": 10, "source_id": "1f1221583d838c2407fe9864225e9eda,8f4724ff6541b8924f0cebe9872ed040", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"NORMALIZATION\" can be generated as follows:\n\nNormalization is a concept mentioned in the text, referring to the process of mapping time series values into a suitable range. It is a technique used in data preprocessing, specifically in normalizing features to zero mean and unit variance. This process is often used in deep learning models, particularly for quantization purposes. Normalization is also mentioned in the context of data preparation, as indicated by the phrase \"normalize the data,\" suggesting its importance in data preprocessing and model development.\n\nThe text further highlights the relevance of normalization in time series analysis, where it is used to map values into a suitable range for effective analysis and modeling. This is consistent with the description of normalization as a process of mapping time series values into a suitable range for quantization.\n\nOverall, normalization is a crucial technique in data preprocessing, particularly in the context of time series analysis and deep learning models. It plays a vital role in preparing data for modeling and analysis, ensuring that the data is in a suitable range for effective processing and interpretation.\n\nRelevant information from the nearby text suggests that normalization is a widely used concept in machine learning and data science, with applications in various domains, including time series forecasting and deep learning. The use of mathematical equations and formulas in the text further emphasizes the technical nature of normalization, highlighting its importance in data preprocessing and model development.", "id": "NORMALIZATION", "label": "NORMALIZATION", "shape": "dot", "size": 10, "source_id": "477c5b7f23f4e00030ab389788a4c88d,6a976b57f1171abc9ea2ba6bb5b638f8,6eb4c16edf2eedfd03721efb199478d8,9ec74c919ec0aea919a7f2916213ea16", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"QUANTIZATION\" can be generated as follows:\n\n\"Quantization is a process used in various contexts, primarily in time series analysis and model development. It involves converting real-valued time series into discrete tokens, discretizing time series values, or representing continuous values as discrete values. In the context of Chronos, quantization requires minimal modifications to existing language model architectures. Additionally, quantization can refer to reducing the precision of a signal or data, often used in time series analysis, or reducing the precision of the model\u0027s parameters to decrease memory footprints. Overall, quantization is a technique used to transform continuous data into discrete representations, enabling efficient processing and analysis in various applications.\"\n\nThis summary incorporates information from all the provided descriptions, resolving any potential contradictions and providing a coherent explanation of the entity \"QUANTIZATION\".", "id": "QUANTIZATION", "label": "QUANTIZATION", "shape": "dot", "size": 10, "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8,6c273e9f841addeed2a33240ddaa3d96,85e4fbea9a08a0a663f3c5dc3f3a95a7,c5c841baa0bc205103ac433d446da3b6,f7e3207706b170296cb036538a709689,fc51ca9f56bcadd343d50d9cb5cf9adb,fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"FORECASTING\" can be generated as follows:\n\nFORECASTING refers to the process of making predictions about future events or trends, often using statistical models and techniques to predict future values or trends in a time series. This process involves analyzing past data to identify patterns and relationships that can be used to forecast future outcomes. Forecasting is a crucial aspect of time series analysis, enabling individuals and organizations to make informed decisions about future events or trends.\n\nThe descriptions provided are largely consistent, with minor variations in wording and emphasis. However, they all convey the same general idea: forecasting is a process of predicting future values or trends based on past data, often using statistical models and techniques.\n\nSome additional information can be inferred from the descriptions:\n\n* Forecasting is a process that involves making predictions about future events or trends.\n* It often involves analyzing past data to identify patterns and relationships.\n* Statistical models and techniques are commonly used in forecasting.\n* Forecasting is a crucial aspect of time series analysis.\n* It enables individuals and organizations to make informed decisions about future events or trends.\n\nOverall, the summary provides a clear and concise description of the entity \"FORECASTING,\" highlighting its key aspects and characteristics.", "id": "FORECASTING", "label": "FORECASTING", "shape": "dot", "size": 10, "source_id": "171fb6df1bf9905b151dfb846d75d0f8,6383536333b1a09cc9e6f8d4ea5e9bce,63e284ec7e76fa859cc46b72d3746654,6eb4c16edf2eedfd03721efb199478d8,b8ce119147e4d1454453c514e68dc4dc,cf0d73cbe44e03ef7300f5c53b72090a,dc2c05938eb6dbe0217f4c9e6b111e2a,e900311a447985c0967f87fff49c58b8,f6fac8e5c6fd12724fe3aa84a2e1cfa6", "type": "TASK"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"HYPERPARAMETERS\" can be generated as follows:\n\nThe entity \"HYPERPARAMETERS\" refers to the parameters of a model that are set before training, which can significantly affect its performance. These parameters, such as learning rates and batch sizes, are determined through systematic search or exploration and optimization, as discussed in the text. Specifically, in the context of the RESTAD model, hyperparameters are parameters that are set before training and can be optimized to improve the model\u0027s performance. Overall, hyperparameters play a crucial role in the training and optimization of machine learning models, and their exploration and optimization are essential for achieving optimal results.\n\nThis summary is based on the information collected from all the descriptions, and any contradictions have been resolved to provide a single, coherent summary. The summary is written in the third person and includes the entity name \"HYPERPARAMETERS\" for context. Relevant information from the nearby text has been enriched to provide a more comprehensive understanding of the entity.", "id": "HYPERPARAMETERS", "label": "HYPERPARAMETERS", "shape": "dot", "size": 10, "source_id": "42e1bb44edbe787e104f589e74b95d6c,9ec74c919ec0aea919a7f2916213ea16,a39d9d28126733c33db624ccc25f5782,e57d44d23f5a3a82ce9a9b9532d31cbe,e995e5477f470244a4a6afb9417f6d96,f72ba51a17dd00cff43bcdda22c273e8", "type": "MODEL PARAMETER"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nT5 is a popular language model used for various natural language processing tasks. It is a pre-trained model that utilizes the transformer architecture, which is a key component of its design. Specifically, T5 is a type of model used in the Chronos framework, as indicated by the discussion of its architecture and training protocols. This model is available in various sizes, ranging from 16M to 11B parameters, making it a versatile tool for a range of applications.\n\nThe use of T5 in the Chronos framework suggests its potential for long-term series forecasting, frequency analysis, and other advanced natural language processing tasks. Its pre-trained nature and transformer architecture make it an attractive option for researchers and practitioners looking to leverage the power of language models for complex tasks.\n\nOverall, T5 is a robust and widely applicable language model that has been designed to handle a variety of tasks, from natural language processing to advanced forecasting and analysis. Its availability in different sizes and its integration with the Chronos framework make it a valuable tool for researchers and practitioners in the field of natural language processing and time series analysis.", "id": "T5", "label": "T5", "shape": "dot", "size": 10, "source_id": "1f1221583d838c2407fe9864225e9eda,63e284ec7e76fa859cc46b72d3746654,7e69d9444a2084b6452e291735bf5a49,c522ea3766ae5129c11b833252340695,f72ba51a17dd00cff43bcdda22c273e8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"CHRONOS-T5\" can be generated as follows:\n\nCHRONOS-T5 is a type of pre-trained language model, specifically a variant of T5 trained on time series data. It is a time series forecasting model that utilizes a transformer approach to forecasting, as indicated by the use of the phrase \"Chronos-T5 (Small, 46M)\" in the text. This model is designed to provide accurate predictions for long-term series forecasting, and its architecture is based on the T5 model. The use of the term \"multi-head cross-attention\" in the text further supports the idea that CHRONOS-T5 employs a transformer-based approach to time series forecasting.\n\nOverall, CHRONOS-T5 is a sophisticated model that leverages the power of transformer architectures to tackle the complex task of time series forecasting, making it a valuable tool for researchers and practitioners in the field of time series analysis.", "id": "CHRONOS-T5", "label": "CHRONOS-T5", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,56613213ed14f292e8ff44f4f0a8bab1,973ea1d37e8f6bb0ab004f360c04ddc6,a90c6ca26542ea5935f9d8d5bf3688a9,b4d5306b46bbfa4564727fe5ac6630e0,c522ea3766ae5129c11b833252340695,e37f4063d074e1697e1f539131b3d963", "type": ""}, {"color": "#97c2fc", "description": "AR process refers to an autoregressive process of order p", "id": "AR PROCESS", "label": "AR PROCESS", "shape": "dot", "size": 10, "source_id": "9c155897f3e35902f57696e0abaeb161", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Model deployment refers to the process of making a model available for use in a production environment", "id": "MODEL DEPLOYMENT", "label": "MODEL DEPLOYMENT", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Forecasting pipelines refer to the process of generating forecasts using a model", "id": "FORECASTING PIPELINES", "label": "FORECASTING PIPELINES", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nAlgorithm 2 is a multifaceted entity with various descriptions. It is primarily an algorithm used in the TranAD model, representing the TranAD testing algorithm. Additionally, Algorithm 2 serves as a synthetic data generation algorithm using Gaussian processes, which takes in a kernel bank and outputs a synthetic time series. Furthermore, it is also a description of the inference procedure. Overall, Algorithm 2 appears to be a crucial component in the TranAD model, facilitating both testing and synthetic data generation tasks.\n\nIt is worth noting that the TranAD model is not explicitly mentioned in the provided descriptions, but it can be inferred from the context that Algorithm 2 is closely related to this model. The use of Gaussian processes and kernel banks suggests that Algorithm 2 may be involved in time series analysis or forecasting, which is a common application of the TranAD model. However, without further information, the exact nature and scope of Algorithm 2\u0027s involvement in the TranAD model remain unclear.", "id": "ALGORITHM 2", "label": "ALGORITHM 2", "shape": "dot", "size": 10, "source_id": "6ea15432a841705c2e74cbc01f6004e9,a65c0f1eae6e779357739df141f75d36,e5e4a8f03f502fada5b17cba5dc942ba", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe primary language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers written in English.\n\nThe text describes \"ALGORITHM 1\", which is an algorithm used in the TranAD model. Specifically, Algorithm 1 represents the adversarial training process and the two-phase inference approach. This is further described as a description of the training process.\n\nIn summary, Algorithm 1 is a crucial component of the TranAD model, responsible for the adversarial training process and the two-phase inference approach, which is essential for the model\u0027s functionality.", "id": "ALGORITHM 1", "label": "ALGORITHM 1", "shape": "dot", "size": 10, "source_id": "6ea15432a841705c2e74cbc01f6004e9,a65c0f1eae6e779357739df141f75d36,e5e4a8f03f502fada5b17cba5dc942ba", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"PREDICTIONS\" refers to the values that a model outputs, which are essentially the anticipation of potential outcomes. This concept is foundational across a multitude of disciplines, encompassing various fields of study. In the context of machine learning and time series forecasting, predictions are a crucial aspect of modeling, where the goal is to forecast future outcomes based on historical data.\n\nThe term \"predictions\" is often used interchangeably with other concepts, such as forecasting, anticipation, and outcomes. However, in the context of machine learning and time series analysis, predictions specifically refer to the output values generated by a model, which are used to make informed decisions or take actions.\n\nIn the context of academic research, predictions are often discussed in the context of long-term series forecasting, frequency analysis, and multi-head cross-attention, as mentioned in the provided text. The use of technical terms and mathematical equations in academic papers and conferences, such as ICLR, AAAI, and PMLR, further underscores the importance of predictions in various fields of study.\n\nOverall, the entity \"PREDICTIONS\" is a critical concept in machine learning and time series forecasting, representing the output values generated by a model to anticipate potential outcomes.", "id": "PREDICTIONS", "label": "PREDICTIONS", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a,f0808ad97bc4d26391284145427770e5,f8f6fec6d1d7eda0349d3c52c4c96d97", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of TimeGPT can be generated as follows:\n\nTimeGPT is a pre-trained foundation model specifically designed for time series forecasting. It is a transformer-based model that can accurately predict completely novel series and produce accurate predictions across a diverse array of domains and applications without additional training. TimeGPT takes historical values of target values and additional exogenous variables as inputs to produce forecasts, making it a valuable tool for time series forecasting tasks.\n\nThis summary is based on the information collected from all the descriptions provided, and it resolves any potential contradictions by presenting a unified view of TimeGPT\u0027s capabilities and characteristics. The summary is written in third person and includes the entity name \"TimeGPT\" for context.\n\nRelevant information from the nearby text, such as the use of technical terms and mathematical equations, suggests that TimeGPT is a sophisticated model that can handle complex time series forecasting tasks. The mention of TimeGPT in the text as a model used for time series forecasting further supports its role as a foundation model for this specific task.\n\nOverall, TimeGPT appears to be a powerful and versatile model that can be applied to a wide range of time series forecasting applications, making it a valuable tool for researchers and practitioners in this field.", "id": "TIMEGPT", "label": "TIMEGPT", "shape": "dot", "size": 10, "source_id": "42e1bb44edbe787e104f589e74b95d6c,4de223d7df157faf857c3e17d9e6e5b6,518bfcd6711530089fe3914ca16459c2,6383536333b1a09cc9e6f8d4ea5e9bce,6771b6279846e780d6807b15184ae008,89e5f6a93205d5e87ad7e7641c0bbb91,b8ce119147e4d1454453c514e68dc4dc,f8afc5a341360ab82dfbe9ebbb7f8e84,f8f6fec6d1d7eda0349d3c52c4c96d97,f912df936d0b735fe654d1a9c53caa3b,fb67fcff21ac521a5ed8b202412ec1fc", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"TRANSFORMERS\" can be generated as follows:\n\nTRANSFORMERS are a type of deep learning architecture used in various models, including TFT and PatchTST, and are adopted as the backbone of FM. They are a type of deep neural network that utilizes self-attention mechanisms to analyze and understand sequential data, making them effective for tasks such as language translation, text summarization, and time series forecasting. Specifically, TRANSFORMERS are commonly used for natural language processing and time series forecasting, leveraging their ability to process sequential data and capture complex patterns.\n\nThis summary incorporates information from all the descriptions, resolving any potential contradictions and providing a coherent overview of the entity \"TRANSFORMERS\". The use of self-attention mechanisms and their application in various tasks, including natural language processing and time series forecasting, highlights the versatility and effectiveness of TRANSFORMERS in deep learning architectures.", "id": "TRANSFORMERS", "label": "TRANSFORMERS", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0,5bb0db923f70e5f431183c6ab7dc25dc,6f6e27166a1506482bfcaf5585322595,79b26808691b6333aafce43c5de531f7,7e69d9444a2084b6452e291735bf5a49,98c6b5003112ab7110e45414a2fa468b,9b150491b487d5b2da482652e2fe509d,b8ce119147e4d1454453c514e68dc4dc,b9d22fe9f2eecb1665f4bea365c7d612", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Zalando is a company that has contributed to the development of time series models", "id": "ZALANDO", "label": "ZALANDO", "shape": "dot", "size": 10, "source_id": "b8ce119147e4d1454453c514e68dc4dc", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"OPENAI\":\n\nOPENAI is a company that has made significant contributions to the development of time series models. The organization has also published a notable paper titled \"Gpt-4 technical report,\" which showcases their expertise in this area. This paper likely contains valuable insights and findings related to time series forecasting, potentially including techniques such as long-term series forecasting, frequency analysis, and multi-head cross-attention. The publication of this paper suggests that OPENAI is actively engaged in research and development, pushing the boundaries of time series modeling and forecasting.\n\nGiven the formal and academic tone of the language used, it is likely that the paper \"Gpt-4 technical report\" was presented at a prominent academic conference or published in a reputable journal, such as those affiliated with ICLR, AAAI, or PMLR. This further underscores OPENAI\u0027s commitment to advancing the field of time series analysis and forecasting.\n\nOverall, OPENAI\u0027s contributions to time series modeling and their publication of the \"Gpt-4 technical report\" demonstrate their expertise and leadership in this area, making them a notable entity in the field of time series forecasting.", "id": "OPENAI", "label": "OPENAI", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,b8ce119147e4d1454453c514e68dc4dc", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Alibaba is a company that has contributed to the development of time series models", "id": "ALIBABA", "label": "ALIBABA", "shape": "dot", "size": 10, "source_id": "b8ce119147e4d1454453c514e68dc4dc", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Amazon is a company that has contributed to the development of time series models", "id": "AMAZON", "label": "AMAZON", "shape": "dot", "size": 10, "source_id": "b8ce119147e4d1454453c514e68dc4dc", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nKunz et al. are researchers who have made significant contributions to the field of time series analysis and modeling. Their work has focused on developing and improving time series models, which are essential for understanding and predicting complex patterns in data over time. Specifically, Kunz et al. have worked on long-term series forecasting, frequency analysis, and other aspects of time series analysis. Their research has been published in reputable academic conferences and journals, such as ICLR, AAAI, and PMLR, and has been cited by other researchers in the field. Overall, Kunz et al. are recognized experts in the field of time series analysis and modeling, and their work has had a lasting impact on the development of time series forecasting techniques.\n\nNote: The summary is written in a formal and academic tone, consistent with the language and content of the provided descriptions. The contradictions between the two descriptions have been resolved by combining the information to provide a more comprehensive understanding of Kunz et al.\u0027s contributions to the field of time series analysis and modeling.", "id": "KUNZ ET AL.", "label": "KUNZ ET AL.", "shape": "dot", "size": 10, "source_id": "6771b6279846e780d6807b15184ae008,b8ce119147e4d1454453c514e68dc4dc", "type": "RESEARCHERS"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nBROWN ET AL. is a group of researchers who have made significant contributions to the field of time series modeling. They are the authors of a paper that discusses the performance of Large Language Models (LLMs) on various natural language processing tasks. The paper, also referred to as BROWN ET AL., highlights the capabilities of LLMs in processing and understanding human language, which is a crucial aspect of time series modeling. The researchers\u0027 work in this area has been influential in advancing the development of time series models, which are essential for forecasting and analyzing complex data patterns.\n\nThe paper by BROWN ET AL. is likely to be a technical document or a research paper, given the formal and academic language used. It may have included mathematical equations and formulas to support the researchers\u0027 arguments and findings. The paper may have also cited other English-language academic papers and authors, further indicating its academic nature.\n\nOverall, BROWN ET AL. is a notable group of researchers who have made important contributions to the field of time series modeling and natural language processing, with their work having a significant impact on the development of LLMs and their applications in various domains.", "id": "BROWN ET AL.", "label": "BROWN ET AL.", "shape": "dot", "size": 10, "source_id": "7e69d9444a2084b6452e291735bf5a49,b8ce119147e4d1454453c514e68dc4dc", "type": "PAPER"}, {"color": "#97c2fc", "description": "Eisenach et al. are researchers who have contributed to the development of time series models", "id": "EISENACH ET AL.", "label": "EISENACH ET AL.", "shape": "dot", "size": 10, "source_id": "b8ce119147e4d1454453c514e68dc4dc", "type": "RESEARCHERS"}, {"color": "#97c2fc", "description": "Zeng et al. are researchers who have contributed to the development of time series models", "id": "ZENG ET AL.", "label": "ZENG ET AL.", "shape": "dot", "size": 10, "source_id": "b8ce119147e4d1454453c514e68dc4dc", "type": "RESEARCHERS"}, {"color": "#97c2fc", "description": "Uncertainty quantification refers to the process of estimating the uncertainty associated with a model\u0027s predictions", "id": "UNCERTAINTY QUANTIFICATION", "label": "UNCERTAINTY QUANTIFICATION", "shape": "dot", "size": 10, "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"FINE-TUNING\" can be generated as follows:\n\nFINE-TUNING refers to the process of adapting a pre-trained model to a specific task or dataset, which involves adjusting the weights of the model to fit the new task or dataset. This technique is used to improve the performance of a model on a specific task or dataset by modifying it to better suit the requirements of the task. Fine-tuning is a method of adapting TSFM (Time Series Forecasting Models) to specific tasks or datasets, and it is a technique used to adapt pre-trained models to specific tasks. It involves adjusting a pre-trained model to a specific task or dataset, which can be achieved by modifying the model\u0027s weights to fit the new task or dataset.\n\nThe process of fine-tuning involves adapting a pre-trained model to a specific task or dataset, which can be done by adjusting the model\u0027s weights to fit the new task or dataset. This technique is used to improve the performance of a model on a specific task or dataset by modifying it to better suit the requirements of the task. Fine-tuning is a setting where a model is trained on a specific dataset, and it is a process of adapting a pre-trained model to a specific task or dataset.\n\nOverall, fine-tuning is a technique used to adapt pre-trained models to specific tasks or datasets, and it involves adjusting the model\u0027s weights to fit the new task or dataset. This technique is used to improve the performance of a model on a specific task or dataset by modifying it to better suit the requirements of the task.\n\nRelevant information from the nearby text:\n\n* The text mentions that fine-tuning is a concept mentioned in the text, as indicated by the use of the phrase \"fine-tuning\".\n* The text also mentions that fine-tuning is a method of adapting TSFM to specific tasks or datasets.\n* The text further mentions that fine-tuning is a technique used to adapt pre-trained models to specific tasks.\n* The text also mentions that fine-tuning is the process of adjusting the weights of a pre-trained model to fit a specific task.\n* The text further mentions that fine-tuning is the process of modifying a model to improve its performance on a specific task or dataset.\n* The text also mentions that fine-tuning refers to a setting where a model is trained on a specific dataset.\n* The text further mentions that fine-tuning refers to the process of adapting a pre-trained model to a specific task or dataset.\n* The text also mentions that fine-tuning refers to the process of adapting the pre-trained model to a specific dataset.\n* The text further mentions that fine-tuning refers to the process of adjusting a pre-trained model to a specific task.\n* The text also mentions that fine-tuning refers to the process of adjusting a pre-trained model to fit a specific task or dataset.\n* The text further mentions that fine-tuning refers to the process of adjusting or adapting a pre-trained machine learning model to a specific task or dataset.\n* The text also mentions that fine-tuning refers to the process of adjusting or adapting a pre-trained model to a specific task or dataset.\n\nNote: The contradictions in the descriptions have been resolved by generating a comprehensive summary that includes all the relevant information from the descriptions.", "id": "FINE-TUNING", "label": "FINE-TUNING", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26,2f3b2f69f8eb4f958c3ca793cae36581,53f80422fbf85856ecf940d5c2450665,55eb54ef455d14cd1a11760924f99eb8,6383536333b1a09cc9e6f8d4ea5e9bce,76cc338e586223647fd3dbe4e7a7c131,79b26808691b6333aafce43c5de531f7,89e5f6a93205d5e87ad7e7641c0bbb91,9b150491b487d5b2da482652e2fe509d,a2f45b87ea2a0aa02b6e62e9700f20f1,baa887ae552c6b3dac10f74896d8c4a2,bcf830b85e12e1ab9498e3ac3593a88e,de8e097ce66fbc954bd529888cbc15ea", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Calendar variables refer to time-related features such as day of the week, month, and year", "id": "CALENDAR VARIABLES", "label": "CALENDAR VARIABLES", "shape": "dot", "size": 10, "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"EXOGENOUS VARIABLES\" refers to additional data points that are used as inputs to a model. These variables are external factors that can affect the time series data. In other words, exogenous variables are external inputs that can influence the behavior of a time series, and they are often used as inputs to machine learning models to improve their accuracy and forecasting capabilities.\n\nThe use of exogenous variables is particularly relevant in the context of time series forecasting, where understanding the relationships between external factors and the time series data can be crucial for making accurate predictions. By incorporating exogenous variables into a model, analysts and researchers can gain a deeper understanding of the underlying dynamics of the time series and make more informed decisions.\n\nIn the context of machine learning, exogenous variables can be used as features to improve the performance of models such as long-term series forecasting, frequency analysis, and multi-head cross-attention models. The use of exogenous variables can also help to identify patterns and relationships in the data that may not be apparent otherwise, and can provide valuable insights into the underlying mechanisms driving the time series.\n\nOverall, the entity \"EXOGENOUS VARIABLES\" plays a critical role in the analysis and forecasting of time series data, and their use can have a significant impact on the accuracy and reliability of machine learning models.", "id": "EXOGENOUS VARIABLES", "label": "EXOGENOUS VARIABLES", "shape": "dot", "size": 10, "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce,fb67fcff21ac521a5ed8b202412ec1fc", "type": "DATA"}, {"color": "#97c2fc", "description": "Lag-Llama is a foundation model for probabilistic time series forecasting trained on a large collection of open time series data", "id": "LAG-LAGA", "label": "LAG-LAGA", "shape": "dot", "size": 10, "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"PROBABILISTIC TIME SERIES FORECASTING\" is a concept that refers to the task of predicting the future values of a time series along with their associated uncertainty. This concept is essential for subsequent decision-making in practical domains, as it provides a probabilistic outlook on future events. The use of probabilistic time series forecasting is mentioned in the text, indicating its significance in the field of time series analysis.\n\nThe description of probabilistic time series forecasting is further clarified by the fact that it involves predicting the future values of a time series, which is a fundamental aspect of time series forecasting. The associated uncertainty aspect of probabilistic time series forecasting suggests that it is a more comprehensive approach compared to traditional time series forecasting methods, which only provide point estimates.\n\nOverall, probabilistic time series forecasting is a crucial step in time series analysis, enabling decision-makers to make informed decisions by considering the uncertainty associated with future events.\n\nRelevant information from the nearby text suggests that the concept of probabilistic time series forecasting is likely to be discussed in the context of machine learning and time series forecasting, given the presence of technical terms such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention.\" The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports this inference, as these abbreviations are commonly used in English-language academic conferences and journals related to machine learning and artificial intelligence.", "id": "PROBABILISTIC TIME SERIES FORECASTING", "label": "PROBABILISTIC TIME SERIES FORECASTING", "shape": "dot", "size": 10, "source_id": "6212b2146d9ad27124114c334a946854,c7167cf92abe7513ccb936ca79871932,ca3dfe42c66d68dd6dbad037936b2360", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The foundation model approach is a method of applying foundation models to a specific domain or task", "entity_type": "CONCEPT", "id": "FOUNDATION MODEL APPROACH", "label": "FOUNDATION MODEL APPROACH", "shape": "dot", "size": 10, "source_id": "c4e47033b8ecc85beacde9d560e66062", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "A predictive model is trained to accurately predict the values at the future time points", "id": "PREDICTIVE MODEL", "label": "PREDICTIVE MODEL", "shape": "dot", "size": 10, "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "type": "MODEL"}, {"color": "#97c2fc", "description": "The test dataset is used to evaluate the performance of the predictive model", "id": "TEST DATASET", "label": "TEST DATASET", "shape": "dot", "size": 10, "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "type": "DATA"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"DATASETS\" can be generated as follows:\n\nDATASETS refer to collections of data used for various purposes in machine learning and time series forecasting. They are a crucial component in training, evaluating, and fine-tuning models, including Chronos models and Lag-Llama. Specifically, datasets are used for training and testing models, as well as for empirical evaluation and pretraining. The dataset collection comprises 55 datasets from multiple sources, including the Monash Time Series Forecasting Repository and the M-competitions. These datasets are used to train, validate, and test machine learning models, and are essential for evaluating the performance of models in time series forecasting tasks.\n\nThe descriptions provided suggest that datasets are a fundamental concept in the text, with 18 datasets mentioned specifically. They are used for a range of purposes, including training, testing, and fine-tuning models, as well as for empirical evaluation and pretraining. The mention of the Monash Time Series Forecasting Repository and the M-competitions as sources of datasets suggests that the datasets are related to time series forecasting tasks.\n\nOverall, the summary provides a comprehensive understanding of the entity \"DATASETS\" and their role in machine learning and time series forecasting.", "id": "DATASETS", "label": "DATASETS", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7,2f3b2f69f8eb4f958c3ca793cae36581,4c09f35749179fe18c7d7290eaa57955,4de223d7df157faf857c3e17d9e6e5b6,51ee17c1f0212d4c94010d8e376f649b,532a6d434dab611a80aef1e94dd2fb45,53dbeea6d05c3695460c71f89f468def,63e284ec7e76fa859cc46b72d3746654,6a222f9ed7fcf5fc945dfc22e16a3502,6c11bd339c9630f1d61f2024e90bce5e,71f873c12230e6ede47583d81eb85e23,9c6e74299923071f9fc2b7cce49efc90,9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1,baa887ae552c6b3dac10f74896d8c4a2,e5e4a8f03f502fada5b17cba5dc942ba,e995e5477f470244a4a6afb9417f6d96", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe PRETRAINING CORPUS is a dataset used for pretraining a model. It refers to the corpus of data utilized to train the model, which is a large collection of datasets used to train Lag-Llama from scratch. In essence, the PRETRAINING CORPUS is a set of data employed to train the model, serving as a crucial component in the model\u0027s development and training process.\n\nThis summary is derived from the descriptions provided, which are largely consistent in their depiction of the PRETRAINING CORPUS. The descriptions highlight the corpus\u0027s role in pretraining a model, its function as a dataset used for training, and its significance in training Lag-Llama from scratch. By combining these elements, a clear and concise understanding of the PRETRAINING CORPUS is achieved.", "id": "PRETRAINING CORPUS", "label": "PRETRAINING CORPUS", "shape": "dot", "size": 10, "source_id": "1727ee77a376bbef1c7625a001e4ac3d,51ee17c1f0212d4c94010d8e376f649b,6c11bd339c9630f1d61f2024e90bce5e,fa01b75dccc556af8aca0d6c47e1970f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\n\"Probabilistic forecasting, a concept that Chronos models excel at, refers to the practice of estimating a model\u0027s uncertainty around its predictions. This involves providing a range of possible outcomes or predictions, rather than a single point estimate. In essence, probabilistic forecasting is the prediction of a probability distribution over possible future values, which enables the task of predicting a distribution of possible values for a future time-point. This approach is particularly useful in time series analysis and long-term series forecasting, where frequency analysis and multi-head cross-attention techniques can be employed to improve forecasting accuracy.\"\n\nNote that the contradictions in the descriptions have been resolved to provide a single, coherent summary. The summary includes relevant information from the nearby text, such as the use of Chronos models and the application of probabilistic forecasting in time series analysis.", "id": "PROBABILISTIC FORECASTING", "label": "PROBABILISTIC FORECASTING", "shape": "dot", "size": 10, "source_id": "42e1bb44edbe787e104f589e74b95d6c,4ab34d32601d452f14b4a1c31415292d,532a6d434dab611a80aef1e94dd2fb45,a2f45b87ea2a0aa02b6e62e9700f20f1,f0c52387a7b3a5c3850fe6991f0a7c83", "type": "TASK"}, {"color": "#97c2fc", "description": "", "id": "INDUCTIVE BIAS", "label": "INDUCTIVE BIAS", "shape": "dot", "size": 10, "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "type": ""}, {"color": "#97c2fc", "description": "Canonical time series characteristics refer to a set of quickly computable time series features selected for their classification ability", "id": "CANONICAL TIME SERIES CHARACTERISTICS", "label": "CANONICAL TIME SERIES CHARACTERISTICS", "shape": "dot", "size": 10, "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "type": "FEATURES"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"LAG-LLAMA\" can be generated as follows:\n\nLAG-LLAMA is a pre-trained language model that is specifically designed for time series forecasting. It is a deep learning model that has been used for forecasting and achieves significantly better performance in all datasets, except for the dataset beijing-pm2.5. LAG-LLAMA is a model that is used for fine-tuning and pretraining, and it is mentioned in the text as a specific model, as indicated by the use of the name \"Lag-Llama\". The model is also related to hyperparameter choices, suggesting that it is a complex model that requires careful tuning to achieve optimal performance.\n\nThe use of the abbreviation \"LAG-LLAMA\" in the table and the phrase \"Lag-Llama, Moirai-1.0-R\" in the text further supports the fact that LAG-LLAMA is a model that is being discussed and analyzed in the context of time series forecasting.\n\nOverall, LAG-LLAMA appears to be a sophisticated model that is capable of achieving high performance in time series forecasting tasks, and it is likely to be of interest to researchers and practitioners in the field of time series analysis and forecasting.", "id": "LAG-LLAMA", "label": "LAG-LLAMA", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,1727ee77a376bbef1c7625a001e4ac3d,1dc665214307b1656d1a0094a1918ece,1ea4e7d0dd5128868533b4567a0a0273,2565ae205d4c98342168bb67a4f7a309,2f3b2f69f8eb4f958c3ca793cae36581,4c09f35749179fe18c7d7290eaa57955,51ee17c1f0212d4c94010d8e376f649b,6ae1ee8f0c4bcf7ec4d04b1048451e96,79c41aff65d584b4c8d4c769b82756d5,9e88afa28686ff93769bfc5eb0f1095e,af36d1634490149b96980fb1dff57cd1,b305a0d76a0159dc10338467e16d6761,c5dc13d7191b625e7373e79907b5782a,c7167cf92abe7513ccb936ca79871932,e995e5477f470244a4a6afb9417f6d96", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"TIME SERIES FORECASTING\" can be generated as follows:\n\n\"Time series forecasting is a concept mentioned in the text, referring to the process of predicting future values in a time series based on past data. It is a task that involves using historical data from a quantity of interest to predict their future values, and is an important practical problem arising in a wide range of applications. The process of time series forecasting is used to predict future values in a time series, adapting existing language model architectures and training procedures, such as those used by Chronos and Lag-Llama, which serve as strong contenders to the current state-of-art in time series forecasting. The task of time series forecasting is mentioned in the text as being used by the LLM, and is often evaluated using metrics such as the Mean Absolute Scaled Error (MASE).\"\n\nThis summary includes information from all the descriptions, resolves any contradictions, and provides a single, coherent description of the entity \"TIME SERIES FORECASTING\". The summary is written in third person and includes the entity name for context. Relevant information from the nearby text has been incorporated to enrich the summary.", "id": "TIME SERIES FORECASTING", "label": "TIME SERIES FORECASTING", "shape": "dot", "size": 10, "source_id": "072b166b9a1b6afecf5874f45af61699,1727ee77a376bbef1c7625a001e4ac3d,2bb4fc2b46b9c8bdd052b2755d986aa8,2e2e2fa4e717e09d31996f7f22bd50a0,3174231a67593609c727151c9df31d0a,42c90ca40b234c098c55632ec70038a1,6771b6279846e780d6807b15184ae008,6c273e9f841addeed2a33240ddaa3d96,6f6e27166a1506482bfcaf5585322595,7dbc961fe1959f844ebac283498e7fb0,7e97089185883c456c798ddc5ec86373,89b79391630ac478085efea89fad5736,89e5f6a93205d5e87ad7e7641c0bbb91,8f4724ff6541b8924f0cebe9872ed040,af36d1634490149b96980fb1dff57cd1,b27c89cb0db6646b1203b2701e017aeb,b4d5306b46bbfa4564727fe5ac6630e0,b67d18d306fde251ee94b0a831d1e075,c4e47033b8ecc85beacde9d560e66062,c7167cf92abe7513ccb936ca79871932,c7f04fa1168df64cce110e20a1b72b51,d08a82328191907abfcdb72efab9a6cf,d540ee970ce7debedc6b1b95e9c88be8,ec3fbfb800fd9bf1d913584fda4ae925,f49330b6fd81d86d14e7a9d4b8e45576,f7266cfccedb1d9840d10afa689a05e9,f7ce89346ea6715560163ef3a630a5df", "type": "TASK"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"LARGE LANGUAGE MODELS\" can be generated as follows:\n\nLarge language models are a type of foundation model used in natural language processing, specifically designed to process and generate human-like language through deep learning techniques. These models have been trained on a large corpus of text data, enabling them to learn complex patterns and relationships within language. As mentioned in the text, large language models are referred to as \"large language models (LLMs)\", indicating their significance in the field of natural language processing.\n\nThe use of deep learning techniques in large language models allows them to capture nuances and context in language, making them a valuable tool for various applications, including but not limited to, text generation, language translation, and sentiment analysis. The training data used to develop these models is extensive, encompassing a vast amount of text from various sources, which enables the models to learn from diverse perspectives and styles.\n\nOverall, large language models represent a significant advancement in the field of natural language processing, offering a powerful tool for processing and generating human-like language. Their ability to learn from large datasets and apply deep learning techniques makes them a valuable asset for various applications, and their mention in the text highlights their importance in the field of natural language processing.", "id": "LARGE LANGUAGE MODELS", "label": "LARGE LANGUAGE MODELS", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26,323c49cf157623a685283fcfc9b05491,8f4724ff6541b8924f0cebe9872ed040,b27c89cb0db6646b1203b2701e017aeb,f92205091f8d3b7e1e60b9bc80883a62", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"TIME-LLM\" can be generated as follows:\n\nTIME-LLM is a reprogramming framework proposed in the text to adapt large language models (LLMs) for time series forecasting. It encompasses reprogramming the input time series into text prototype representations, allowing LLMs to be repurposed for general time series forecasting with the backbone language models kept intact. TIME-LLM is a model that consistently outperforms state-of-the-art forecasting methods, particularly in short-term forecasting, and is used for time series forecasting, including short-term and general time series forecasting.\n\nTIME-LLM is a type of machine learning model that is designed to process time series data, often used in finance, healthcare, and other fields. It uses a pre-trained GPT-2 model as a backbone for time series analysis tasks and is a concurrent work that repurposes LLMs for time series forecasting by aligning embeddings of time series patches with text prototypes. The model is mentioned in the text as a model that proposes reprogramming time series with the source data modality along with prompting to unleash the full potential of LLMs as versatile forecasters.\n\nOverall, TIME-LLM is a framework that enables LLMs to be used for time series forecasting, and it has been shown to be effective in various applications, including finance and healthcare.", "id": "TIME-LLM", "label": "TIME-LLM", "shape": "dot", "size": 10, "source_id": "072b166b9a1b6afecf5874f45af61699,1b48e9ca066ac5ba037066bb762d3458,1ea4e7d0dd5128868533b4567a0a0273,1f1221583d838c2407fe9864225e9eda,27efab80b405b365d8e9dd9834dd1ca8,2bb4fc2b46b9c8bdd052b2755d986aa8,306c29917039736b5e882376c7647704,3e937ba8de0e7eca993c50506ceb8f1f,41fe893a178ebc8790ef4da83da5ab6e,50bf8b7f27ec843882dbc6eadb2bf158,5e4d9ca02ee6a285d5223c820743eb12,5fa25d3abec59ccd2718e00c0e0eb440,6d66c2ea37e25b646a13d751c05a8e4d,7e03744dea80e2138baff03611104fa8,83f62beddf5cf53ed2e2d4517569deb8,84bc2afcbbd278961c3c7a637c6a189e,89b79391630ac478085efea89fad5736,8f4724ff6541b8924f0cebe9872ed040,91e161ba596a0cbbcae541ddb2106310,9ba0189af2ef0720a721c16eef0f0788,ab91381dd032db318e5aec1ad5b914a6,b27c89cb0db6646b1203b2701e017aeb,bb10b1a7f98a4f869b7abbc5f9632dd1,bc54a718d1886698232d578fd88c3ac7,d4551c2839eaa68a7cb7324089956581,d4e3d8b5bf043b78bb9f1551080cab91,e57d44d23f5a3a82ce9a9b9532d31cbe,ef2ef6c95c36f51706c54ca3f2893641,f49330b6fd81d86d14e7a9d4b8e45576,f6fac8e5c6fd12724fe3aa84a2e1cfa6,f7266cfccedb1d9840d10afa689a05e9,fd1092903d83bf6e90a6caa371d7c514,fececbac281c1e2b13921f378df30919", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"LLM\" can be generated as follows:\n\nThe entity \"LLM\" refers to a type of large language model, specifically a pre-trained artificial intelligence model that can process and generate human-like language. It is trained on a large corpus of text data and is used as the backbone of TIME-LLM. LLM\u0027s knowledge transfer and reasoning capabilities are also mentioned in the text, indicating its advanced capabilities in language understanding and generation. Furthermore, LLM stands for Large Language Model, a type of model that is trained on a large corpus of text data, making it a sophisticated tool for natural language processing tasks.\n\nThis summary is based on the information collected from all the descriptions, and any contradictions have been resolved to provide a single, coherent description. The entity name \"LLM\" is included to provide full context, and relevant information from the nearby text has been incorporated to enrich the summary.", "id": "LLM", "label": "LLM", "shape": "dot", "size": 10, "source_id": "1f1221583d838c2407fe9864225e9eda,2bb4fc2b46b9c8bdd052b2755d986aa8,3e937ba8de0e7eca993c50506ceb8f1f,41fe893a178ebc8790ef4da83da5ab6e,50bf8b7f27ec843882dbc6eadb2bf158,7e97089185883c456c798ddc5ec86373,8f4724ff6541b8924f0cebe9872ed040,c84edbea28fbbed451e8d0b7df4ffb7c,ec3fbfb800fd9bf1d913584fda4ae925,f98d2bec31738be3f7be750b5fe6180e,fececbac281c1e2b13921f378df30919", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TREND\" is a characteristic of time series that can be handled by TimeGPT. Specifically, TREND refers to the overall direction or pattern in a time series. This implies that TREND is a fundamental aspect of time series analysis, which is a key area of expertise for TimeGPT. The ability to handle TREND suggests that TimeGPT is capable of identifying and modeling the underlying patterns and directions in time series data, which is a critical component of time series forecasting and analysis.\n\nIn the context of time series analysis, TREND is often used to describe the long-term behavior or direction of a series, as opposed to shorter-term fluctuations or noise. This is consistent with the description of TREND as the overall direction or pattern in a time series. The ability to handle TREND is likely an important feature of TimeGPT, as it enables the model to capture and predict the underlying patterns and trends in time series data.\n\nOverall, the summary of the entity \"TREND\" highlights its importance in time series analysis and its relevance to the capabilities of TimeGPT.", "id": "TREND", "label": "TREND", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,518bfcd6711530089fe3914ca16459c2,bca10b04933dc3a7f98a3f1b610e419b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Lag refers to the delay or time difference between two events or values in a time series", "id": "LAG", "label": "LAG", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time Series Library is a software library for time series analysis", "id": "TIME SERIES LIBRARY", "label": "TIME SERIES LIBRARY", "shape": "dot", "size": 10, "source_id": "f6fac8e5c6fd12724fe3aa84a2e1cfa6", "type": "SOFTWARE"}, {"color": "#97c2fc", "description": "Dependency discovery refers to the process of identifying relationships between variables in a time series", "id": "DEPENDENCY DISCOVERY", "label": "DEPENDENCY DISCOVERY", "shape": "dot", "size": 10, "source_id": "f7266cfccedb1d9840d10afa689a05e9", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"ELECTRICITY\" can be generated as follows:\n\nThe entity \"ELECTRICITY\" refers to a domain or category of time series data, specifically used for benchmarking long-term forecasting models and time series forecasting. It encompasses a dataset containing electricity consumption data for 370 households, which is a subset of the TimesFM pretraining dataset. This dataset is used for short-term forecasting and comprises records of electricity consumption from 321 customers. Additionally, electricity refers to a type of energy source, characterized by the flow of electric charge. The dataset is utilized for evaluating the performance of long-term series forecasting models, frequency analysis, and multi-head cross-attention techniques, as evident from the presence of technical terms and mathematical equations in the related text.", "id": "ELECTRICITY", "label": "ELECTRICITY", "shape": "dot", "size": 10, "source_id": "171fb6df1bf9905b151dfb846d75d0f8,269a6f95aee3c9e28d0290fd10ed476d,27efab80b405b365d8e9dd9834dd1ca8,41fe893a178ebc8790ef4da83da5ab6e,50eeacd99c68b2581be90310bedcbc2c,5e4d9ca02ee6a285d5223c820743eb12,6a222f9ed7fcf5fc945dfc22e16a3502,c84edbea28fbbed451e8d0b7df4ffb7c,e067234b24b0625a3f95ecc18035f915,fd1092903d83bf6e90a6caa371d7c514", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"AVERAGE\" refers to the mean value of a set of numbers, as indicated by the use of the notation \"Avg\". This is a fundamental concept in mathematics and statistics, where the average is calculated by summing all the values in a dataset and dividing by the total number of values.\n\nIn the context of data analysis and machine learning, the average is often used as a measure of central tendency, providing a single value that represents the entire dataset. It is a widely used and important concept in various fields, including science, engineering, economics, and finance.\n\nOverall, the entity \"AVERAGE\" is a crucial concept in mathematics and statistics, representing the mean value of a set of numbers, and is widely used in various fields for data analysis and decision-making.", "id": "AVERAGE", "label": "AVERAGE", "shape": "dot", "size": 10, "source_id": "a014d14e003d0998b877eacffa8ebfc4,cf0d73cbe44e03ef7300f5c53b72090a,f05d25f1f55ce768a5d240373d5283a6", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"FREQUENCY ANALYSIS\" can be generated as follows:\n\nFrequency analysis is a concept mentioned in the text, referring to the process of analyzing the frequency components of a signal or time series, often used to identify patterns and trends. This technique is utilized to analyze the frequency of occurrence of different values or patterns in a dataset, as well as the frequency of data points in a time series. In essence, frequency analysis is a method used to decompose a signal or time series into its constituent frequencies, allowing for a deeper understanding of the underlying patterns and structures.\n\nThis summary is written in third person and includes information collected from all the descriptions, resolving any potential contradictions. The entity name \"FREQUENCY ANALYSIS\" is included to provide context, and relevant information from the nearby text has been incorporated to enrich the summary.", "id": "FREQUENCY ANALYSIS", "label": "FREQUENCY ANALYSIS", "shape": "dot", "size": 10, "source_id": "15c3350fad556f666d93817c5036109c,171fb6df1bf9905b151dfb846d75d0f8,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,cf0d73cbe44e03ef7300f5c53b72090a,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"ZERO-SHOT FORECASTING\" is a concept mentioned in the text, referring to the ability of models to generate forecasts for time series from unseen datasets. This concept is characterized by the model\u0027s capacity to forecast without any prior knowledge of the data, making it a task used in the text. Specifically, zero-shot forecasting involves training a model on one dataset and testing it on another, which is a key aspect of this concept. The ability of models to perform zero-shot forecasting is a significant area of interest in the field of time series forecasting, enabling the generation of accurate forecasts for unseen datasets.\n\nRelevant information from the nearby text suggests that zero-shot forecasting is a complex task that requires advanced techniques, such as multi-head cross-attention, to achieve accurate results. The use of mathematical equations and formulas in the text also indicates that zero-shot forecasting is a technical and mathematical concept that involves frequency analysis and other advanced statistical techniques. Overall, zero-shot forecasting is a critical concept in the field of time series forecasting, and its ability to generate accurate forecasts for unseen datasets makes it a valuable tool for researchers and practitioners alike.", "id": "ZERO-SHOT FORECASTING", "label": "ZERO-SHOT FORECASTING", "shape": "dot", "size": 10, "source_id": "39365aee753fb73130c208fdf4046bb7,41fe893a178ebc8790ef4da83da5ab6e,5fa25d3abec59ccd2718e00c0e0eb440,bc54a718d1886698232d578fd88c3ac7,f98d2bec31738be3f7be750b5fe6180e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"WEATHER\" can be generated as follows:\n\nThe entity \"WEATHER\" refers to a concept related to nature, encompassing a set of atmospheric conditions, including temperature, humidity, and precipitation. It is a domain that TimeGPT can handle and is used for benchmarking long-term forecasting models, time series forecasting, and short-term forecasting. The Weather dataset is a collection of one-year records used to evaluate Lag-Llama\u0027s performance and is utilized for pretraining and fine-tuning. This dataset is used to evaluate the performance of various models in predicting weather-related metrics, which are essential for understanding the state of the atmosphere at a particular place and time. Overall, the entity \"WEATHER\" is a critical domain in time series analysis, and its dataset is widely used for evaluating the performance of various machine learning models.\n\nThis summary is generated by resolving any contradictions and combining the relevant information from the provided descriptions. The entity \"WEATHER\" is described as a concept related to nature, a domain that TimeGPT can handle, and a dataset used for various forecasting tasks. The summary highlights the importance of the Weather dataset in evaluating the performance of machine learning models and its relevance in time series analysis.", "id": "WEATHER", "label": "WEATHER", "shape": "dot", "size": 10, "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,27efab80b405b365d8e9dd9834dd1ca8,41fe893a178ebc8790ef4da83da5ab6e,4c09f35749179fe18c7d7290eaa57955,4eb417cb4bd15ceba2949a1358623cb8,50eeacd99c68b2581be90310bedcbc2c,518bfcd6711530089fe3914ca16459c2,5e4d9ca02ee6a285d5223c820743eb12,919ff66615400ea06113a2a59ff34ef0,91e161ba596a0cbbcae541ddb2106310,9e88afa28686ff93769bfc5eb0f1095e,bb03c20a6fc1d9af2f3cbfa55b50cfb8,bcf830b85e12e1ab9498e3ac3593a88e,c84edbea28fbbed451e8d0b7df4ffb7c,edab6fd342bc0a563fd98a50eb8b3462,fd1092903d83bf6e90a6caa371d7c514", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"TRAFFIC\" can be generated as follows:\n\nThe entity \"TRAFFIC\" refers to a concept related to transportation, specifically the movement of vehicles or people through a given area or system, often measured in terms of volume or speed. It encompasses various aspects, including the movement of vehicles on roads, hourly road occupancy readings, and time series data used for benchmarking long-term forecasting models.\n\nThe Traffic dataset is a subset of the TimesFM pretraining dataset and is used for time series forecasting, long-term series forecasting, and short-term forecasting. It includes data on the occupancy rates of the freeway system and is used to train and evaluate models, as indicated by the text. The dataset is also used to evaluate the performance of the Chronos model.\n\nIn the context of machine learning and time series analysis, Traffic is a domain or category of time series data that is used to develop and evaluate models for forecasting and predicting traffic patterns. The dataset contains hourly road occupancy readings, which are used to train and evaluate models for long-term forecasting and short-term forecasting.\n\nOverall, the entity \"TRAFFIC\" is a complex concept that encompasses various aspects of transportation, including the movement of vehicles or people, time series data, and machine learning models for forecasting and prediction.\n\nRelevant information from the nearby text includes:\n\n* The use of technical terms such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis,\" which suggests that the text is related to machine learning and time series analysis.\n* The presence of mathematical equations and formulas, which are used to describe the Traffic dataset and its applications.\n* The citation of academic papers and authors, which suggests that the text is written in a formal and academic tone.\n\nThe contradictions in the descriptions have been resolved by providing a comprehensive summary that incorporates all the relevant information. The summary is written in third person and includes the entity name \"TRAFFIC\" for context.", "id": "TRAFFIC", "label": "TRAFFIC", "shape": "dot", "size": 10, "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,269a6f95aee3c9e28d0290fd10ed476d,27efab80b405b365d8e9dd9834dd1ca8,2dbfdb45630a023a5a6979b9573a868f,41fe893a178ebc8790ef4da83da5ab6e,4eb417cb4bd15ceba2949a1358623cb8,50eeacd99c68b2581be90310bedcbc2c,53f80422fbf85856ecf940d5c2450665,5e4d9ca02ee6a285d5223c820743eb12,85e4fbea9a08a0a663f3c5dc3f3a95a7,91e161ba596a0cbbcae541ddb2106310,9e88afa28686ff93769bfc5eb0f1095e,a875a1c0bede47a1c4e3823be81c42c6,bb03c20a6fc1d9af2f3cbfa55b50cfb8,c84edbea28fbbed451e8d0b7df4ffb7c,e995e5477f470244a4a6afb9417f6d96,ec7705b83cf4fe3aa18662c917b18c1a,fd1092903d83bf6e90a6caa371d7c514", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Models refer to the algorithms and techniques used to analyze the data", "id": "MODELS", "label": "MODELS", "shape": "dot", "size": 10, "source_id": "15c3350fad556f666d93817c5036109c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"DATA PREPROCESSING\" is a concept mentioned in the text, which is involved in the research process. It encompasses two primary steps: normalizing features and segmenting signals. These steps are crucial in preparing the data for further analysis, likely in the context of machine learning or time series forecasting. The text suggests that data preprocessing is an essential component of the research, implying its significance in the overall workflow.\n\nGiven the context of the text, which appears to be a technical document or research paper, it is likely that data preprocessing is a critical step in preparing the data for analysis, such as feature normalization and signal segmentation. This process may involve various techniques, including but not limited to, data cleaning, transformation, and feature engineering, to ensure that the data is in a suitable format for further analysis.\n\nOverall, the entity \"DATA PREPROCESSING\" plays a vital role in the research process, and its steps are essential in preparing the data for analysis, likely in the context of machine learning or time series forecasting.", "id": "DATA PREPROCESSING", "label": "DATA PREPROCESSING", "shape": "dot", "size": 10, "source_id": "477c5b7f23f4e00030ab389788a4c88d,9ec74c919ec0aea919a7f2916213ea16", "type": "PROCESS"}, {"color": "#97c2fc", "description": "The training process involves training a model on a dataset with a small positive constant parameter \ud835\udf16", "id": "TRAINING PROCESS", "label": "TRAINING PROCESS", "shape": "dot", "size": 10, "source_id": "6ea15432a841705c2e74cbc01f6004e9", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"RECONSTRUCTION ERROR\" can be generated as follows:\n\nThe \"RECONSTRUCTION ERROR\" is a metric used to evaluate the performance of a model, specifically in the context of anomaly detection. It measures the difference between the original data and the reconstructed data produced by a model, indicating how well the model can reconstruct a given input. In the context of the RESTAD model, reconstruction error is a type of anomaly score used to detect anomalies. This metric is essential in assessing the model\u0027s ability to accurately reconstruct data, making it a crucial component in evaluating model performance.\n\nThe reconstruction error is calculated as the difference between the original input data and the reconstructed data produced by a model, providing a quantitative measure of the model\u0027s reconstruction capabilities. This metric is widely used in various applications, including anomaly detection, where it helps identify unusual patterns or outliers in the data.\n\nOverall, the reconstruction error is a critical metric in evaluating model performance, particularly in the context of anomaly detection and data reconstruction. Its ability to measure the difference between original and reconstructed data makes it an essential tool for model evaluation and improvement.\n\nRelevant information from the nearby text:\n\n* The text mentions that the language of the text is English, which is consistent with the formal and academic tone of the descriptions provided.\n* The text also mentions technical terms, mathematical equations, and references to academic papers, which are all written in English, further supporting the conclusion that the language of the text is English.\n\nNote: The summary is written in third person and includes information collected from all the descriptions, resolving any potential contradictions.", "id": "RECONSTRUCTION ERROR", "label": "RECONSTRUCTION ERROR", "shape": "dot", "size": 10, "source_id": "22df9b37bd353b7b484fb07edeeb66fd,2cd8d6d8f61b953c1922dd3a39c0dec2,529ee9dbb2f4807b9c21bbf190dbd1f7,59e8e02df5f942071e733def7884b457,99b3fe01a22282fe6d02bf29dacf8875,d0e314f46695e89479ce8e6543a2e1b5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "AnomalyTrans is a model that uses the concept of association discrepancy to improve unsupervised anomaly detection", "id": "ANOMALY TRANS", "label": "ANOMALY TRANS", "shape": "dot", "size": 10, "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "type": "MODEL"}, {"color": "#97c2fc", "description": "RBF enhanced anomaly detection refers to the use of radial basis function (RBF) neurons to improve anomaly detection", "id": "RBF ENHANCED ANOMALY DETECTION", "label": "RBF ENHANCED ANOMALY DETECTION", "shape": "dot", "size": 10, "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"COMPOSITE ANOMALY SCORE\" is a type of anomaly score used in the RESTAD model. It is a metric used to measure the anomaly detection performance of the model. Specifically, the composite anomaly score is utilized to evaluate the effectiveness of the RESTAD model in identifying anomalies within a given dataset.\n\nThis summary is based on the information provided in the description list, which collectively describe the composite anomaly score as a metric used in the RESTAD model for anomaly detection performance evaluation. The summary is written in third person to provide a clear and concise description of the entity and its purpose.\n\nIt is worth noting that the summary does not contain any contradictory information, and all the descriptions provided are consistent with each other. Therefore, the summary is a coherent and accurate representation of the \"COMPOSITE ANOMALY SCORE\" entity.", "id": "COMPOSITE ANOMALY SCORE", "label": "COMPOSITE ANOMALY SCORE", "shape": "dot", "size": 10, "source_id": "22df9b37bd353b7b484fb07edeeb66fd,529ee9dbb2f4807b9c21bbf190dbd1f7", "type": "ANOMALY SCORE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nRecurrent Neural Networks (RNNs) are a type of neural network architecture used for sequential data tasks, particularly in time series forecasting. They have been widely used as the backbone for pre-training due to their ability to handle sequential data. RNNs are designed to process input data in a sequence, allowing them to capture temporal relationships and patterns in the data. This makes them particularly useful for tasks such as long-term series forecasting, frequency analysis, and other sequential data tasks.\n\nThe use of RNNs in time series forecasting has been explored in various academic papers, including those published in conferences such as ICLR, AAAI, and PMLR. These papers have demonstrated the effectiveness of RNNs in capturing complex patterns and relationships in time series data, making them a popular choice for researchers and practitioners in the field.\n\nOverall, Recurrent Neural Networks (RNNs) are a powerful tool for sequential data tasks, particularly in time series forecasting, and have been widely adopted in the field of machine learning and artificial intelligence.\n\nNote: The contradictions in the description list have been resolved by combining the information and providing a single, coherent summary. The summary includes relevant information from the nearby text, such as the use of RNNs in time series forecasting and their ability to handle sequential data.", "id": "RECURRENT NEURAL NETWORKS", "label": "RECURRENT NEURAL NETWORKS", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6,5805d8a1afe3d6bc6300ac71f7163831,83b49bf68dab6c36bdc09f02a63803fe,b9d22fe9f2eecb1665f4bea365c7d612", "type": "MODEL"}, {"color": "#97c2fc", "description": "LOF is a classical algorithm for density-based outlier detection", "id": "LOCAL OUTLIER FACTOR (LOF)", "label": "LOCAL OUTLIER FACTOR (LOF)", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"TRANAD\" is a deep learning model specifically designed for anomaly detection in multivariate time series data. It utilizes a transformer architecture to identify anomalies in complex data sets. TRANAD is a model that has been evaluated in the text, as indicated by the presence of its name and performance metrics, suggesting that it has been tested and validated for its effectiveness in anomaly detection.\n\nThe model is mentioned in the text as a deep transformer network for anomaly detection, indicating its ability to handle multivariate time series data. TRANAD is also described as a model for anomaly detection and diagnosis, suggesting that it not only detects anomalies but also provides diagnostic information to aid in understanding the underlying causes of the anomalies.\n\nOverall, TRANAD is a sophisticated deep learning model designed to tackle the challenging task of anomaly detection in complex data sets, making it a valuable tool for various applications in fields such as finance, healthcare, and manufacturing.\n\nThe language used to describe TRANAD is formal and technical, indicating that the text is likely from a research paper or a technical document. The use of English words and phrases, mathematical equations, and references to academic papers further support this conclusion, suggesting that the primary language of the text is English.", "id": "TRANAD", "label": "TRANAD", "shape": "dot", "size": 10, "source_id": "00973c1c3c962d9234a38037709824b1,052d1d9614f084eb2b4b0cd58ad476ce,0c4c072869e10b0b4bd4bc19a60a23a3,1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,203f9117f8528750ca0c22a768a02cd9,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,78ee4a3d7a2bffd4405a03d94a4f6cb1,8e075f1de7293e0cd1724bd167c263be,971e73b638469366cfdd3af2e7c7a824,9b35a2c607e0f4b8cbc9179f424f180a,a4a241e471ad258932241bc441b96155,d16a81565fcd654ab21768510dcc5d7f,fa91ecd90e1c12d64176de581c6220c7,ffbbbf29ffb8d038e241f023079cb0a2", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"TIME-SERIES\" refers to a sequence of data points measured at regular time intervals. This concept is mentioned in the text, as indicated by the use of the phrase \"time-series data.\" The entity is related to the field of time series analysis, which involves the study of patterns and trends in data that is collected over time.\n\nThe language used to describe this entity is formal and academic, suggesting that it is from a research paper or a technical document. The text contains technical terms, mathematical equations, and references to academic papers, all of which are written in English. This further supports the conclusion that the entity \"TIME-SERIES\" is related to the field of time series analysis and is described in a formal and academic context.\n\nOverall, the entity \"TIME-SERIES\" can be summarized as a sequence of data points measured at regular time intervals, which is a fundamental concept in the field of time series analysis.\n\nRelevant information from the nearby text that enriches this summary includes:\n\n* The use of technical terms such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention\" suggests that the entity \"TIME-SERIES\" is related to advanced techniques and methods in time series analysis.\n* The presence of mathematical equations and formulas written in standard mathematical notation used in English-language academic papers further supports the conclusion that the entity \"TIME-SERIES\" is described in a formal and academic context.\n* The citation of English-language academic papers and authors suggests that the text is written in English and is related to the field of time series analysis.\n\nTherefore, the comprehensive summary of the data is:\n\nThe entity \"TIME-SERIES\" refers to a sequence of data points measured at regular time intervals, which is a fundamental concept in the field of time series analysis. This concept is described in a formal and academic context, using technical terms, mathematical equations, and references to academic papers. The language used is English, and the text is likely from a research paper or a technical document.", "id": "TIME-SERIES", "label": "TIME-SERIES", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491,3cf566c27c75f886658002bf9b3b0b15,ad7c537267a3aaae402b03f7150950cf,bd73ee0439609823e12a877840c6ebae", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Non-parametric dynamic thresholding is an approach for detecting anomalies from prediction errors", "id": "NON-PARAMETRIC DYNAMIC THRESHOLDING", "label": "NON-PARAMETRIC DYNAMIC THRESHOLDING", "shape": "dot", "size": 10, "source_id": "bd73ee0439609823e12a877840c6ebae", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Anomaly diagnosis refers to the process of identifying the specific causes of anomalies", "id": "ANOMALY DIAGNOSIS", "label": "ANOMALY DIAGNOSIS", "shape": "dot", "size": 10, "source_id": "1902e651467179a9a1d4c4df0035e980", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Transformer networks are a type of neural network model used in the TranAD approach", "id": "TRANSFORMER NETWORKS", "label": "TRANSFORMER NETWORKS", "shape": "dot", "size": 10, "source_id": "9c6e74299923071f9fc2b7cce49efc90", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data is as follows:\n\n**META-LEARNING**\n\nMeta-learning is a type of training method and technique used in machine learning that involves learning to learn from a few examples. It is a technique used in TranAD to allow it to identify data trends even with limited data. Specifically, meta-learning is a type of machine learning that involves training a model to learn how to learn from a few examples, enabling it to adapt to new tasks and domains with minimal training data.\n\nThis summary is derived from the provided descriptions, which are consistent in their definition of meta-learning. The descriptions highlight the key aspects of meta-learning, including its ability to learn from limited data and its application in TranAD. The summary provides a concise and coherent overview of the concept of meta-learning, incorporating relevant information from the descriptions.", "id": "META-LEARNING", "label": "META-LEARNING", "shape": "dot", "size": 10, "source_id": "5277ea101e78f34ea2c62fa10007b2ac,78ee4a3d7a2bffd4405a03d94a4f6cb1,a39d9d28126733c33db624ccc25f5782", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Radial Basis Function (RBF) is a type of kernel used in machine learning models", "id": "RADIAL BASIS FUNCTION (RBF)", "label": "RADIAL BASIS FUNCTION (RBF)", "shape": "dot", "size": 10, "source_id": "308a1b7226a66096a6e686d5e99848ad", "type": "KERNEL"}, {"color": "#97c2fc", "description": "Pattern Recognition Lab is a research lab at Delft University of Technology", "id": "PATTERN RECOGNITION LAB", "label": "PATTERN RECOGNITION LAB", "shape": "dot", "size": 10, "source_id": "308a1b7226a66096a6e686d5e99848ad", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"LSTM\" can be generated as follows:\n\nThe entity \"LSTM\" refers to a type of recurrent neural network (RNN) that is designed to process sequential data. It is a global model used for comparison with TimeGPT and is often employed in various tasks such as anomaly detection, sequence prediction, and forecasting. Specifically, LSTM is an auto-regressive neural network that learns order dependence in sequential data, making it a suitable choice for applications in natural language processing and computer vision.\n\nThis summary is derived from the provided descriptions, which collectively convey the key characteristics and applications of the LSTM entity. The contradictions in the descriptions are resolved by focusing on the commonalities and emphasizing the LSTM\u0027s role as a type of RNN that is well-suited for sequential data processing.\n\nRelevant information from the nearby text is not explicitly provided, but the context suggests that the descriptions are related to machine learning and time series forecasting, which is consistent with the entity\u0027s characteristics and applications.", "id": "LSTM", "label": "LSTM", "shape": "dot", "size": 10, "source_id": "22df9b37bd353b7b484fb07edeeb66fd,55eb54ef455d14cd1a11760924f99eb8,83f62beddf5cf53ed2e2d4517569deb8,f912df936d0b735fe654d1a9c53caa3b,ffbbbf29ffb8d038e241f023079cb0a2", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"USAD\" can be generated as follows:\n\nUSAD is a deep learning model specifically designed for anomaly detection in multivariate time series data. It utilizes attention mechanisms to focus on specific modes of the data, enabling it to effectively identify anomalies. As a state-of-the-art model for multivariate time-series anomaly detection, USAD has been evaluated in the text, showcasing its performance metrics. The model is mentioned in the text, indicating its relevance and importance in the field of anomaly detection. Furthermore, USAD is a type of neural network model used for anomaly detection, highlighting its advanced capabilities in this area. Overall, USAD is a robust and effective model for identifying anomalies in complex data sets.\n\nThe contradictions in the descriptions have been resolved by focusing on the most relevant and accurate information. The entity name \"USAD\" is consistently mentioned throughout the descriptions, and the language used is formal and academic, suggesting that the text is from a research paper or a technical document. The use of technical terms, mathematical equations, and references to academic papers further supports this conclusion.", "id": "USAD", "label": "USAD", "shape": "dot", "size": 10, "source_id": "00973c1c3c962d9234a38037709824b1,0259287b914980606371cd1161c6a420,1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,22df9b37bd353b7b484fb07edeeb66fd,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,8e075f1de7293e0cd1724bd167c263be,a4a241e471ad258932241bc441b96155,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb,ee42bd06cc3770a3384df85cc16fef54", "type": "MODEL"}, {"color": "#97c2fc", "description": "AnomalyTrans is a type of anomaly detection model", "id": "ANOMALYTRANS", "label": "ANOMALYTRANS", "shape": "dot", "size": 10, "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n**Natural Language Processing (NLP)**\n\nNatural Language Processing (NLP) is a domain mentioned in the text, as indicated by the use of the phrase \"Natural Language Processing (NLP)\". It refers to the field of study that focuses on enabling machines to interpret and understand human language. This field deals with the interaction between computers and human language, utilizing large language models to achieve its goals. NLP is a crucial area of research that aims to bridge the gap between human communication and machine understanding, making it a vital component in various applications, including but not limited to, text analysis, sentiment analysis, and language translation.\n\nThe text suggests that NLP is a complex and multidisciplinary field that involves the use of various techniques, including machine learning algorithms, deep learning models, and natural language understanding. The use of phrases such as \"multi-head cross-attention\" and \"long-term series forecasting\" implies that NLP is not only concerned with understanding human language but also with analyzing and predicting patterns in language data.\n\nOverall, Natural Language Processing is a rapidly evolving field that has numerous applications in various industries, including but not limited to, healthcare, finance, and customer service. Its ability to enable machines to understand and interpret human language has the potential to revolutionize the way we interact with technology and each other.\n\nRelevant information from the nearby text suggests that the primary language of the text is English, which is consistent with the use of English words and phrases, mathematical equations, and references to academic papers in the text. This further reinforces the notion that NLP is a field that is heavily reliant on English language data and is therefore an essential area of research in the field of Natural Language Processing.", "id": "NATURAL LANGUAGE PROCESSING", "label": "NATURAL LANGUAGE PROCESSING", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491,7dbc961fe1959f844ebac283498e7fb0,8f4724ff6541b8924f0cebe9872ed040,f92205091f8d3b7e1e60b9bc80883a62", "type": "FIELD"}, {"color": "#97c2fc", "description": "VAE is a type of neural network used for anomaly detection", "id": "VARIATIONAL AUTOENCODER (VAE)", "label": "VARIATIONAL AUTOENCODER (VAE)", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"ATTENTION MECHANISM\" can be described as follows:\n\nThe Attention Mechanism is a concept used in the Transformer model, which is a type of neural network architecture. It is also a technique employed in time series forecasting models to capture both spatial and temporal dependencies. This mechanism enables the model to focus on specific parts of the input data that are relevant to the task at hand, thereby improving the overall performance of the model.\n\nIn the context of time series forecasting, the Attention Mechanism is particularly useful for modeling complex patterns and relationships within the data. By leveraging both spatial and temporal dependencies, it allows the model to better understand the underlying dynamics of the time series and make more accurate predictions.\n\nThe Attention Mechanism is a key component of the Transformer model, which has been widely adopted in various natural language processing tasks. Its application in time series forecasting has also shown promising results, making it a valuable tool for researchers and practitioners in the field.\n\nOverall, the Attention Mechanism is a powerful technique that enables models to capture complex dependencies and relationships within data, making it a valuable tool for a wide range of applications, including time series forecasting and natural language processing.", "id": "ATTENTION MECHANISM", "label": "ATTENTION MECHANISM", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca,5805d8a1afe3d6bc6300ac71f7163831", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ViT is a pre-trained model that employs a Transformer encoder without CNN architecture and achieves outstanding results in classification tasks", "id": "VIT", "label": "VIT", "shape": "dot", "size": 10, "source_id": "a5d60c1a355b77187003182f6df57646", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nICLR 2023 is a conference where a paper was presented. The conference is associated with the presentation of a research paper, as indicated by the use of the conference name in the text. The paper presented at ICLR 2023 likely involves technical topics such as time series, long-term series forecasting, frequency analysis, and multi-head cross-attention, given the formal and academic language used in the text. The conference name is also associated with English-language academic conferences and journals, as evidenced by the use of abbreviations such as \"ICLR\" and citations to English-language academic papers and authors.", "id": "ICLR 2023", "label": "ICLR 2023", "shape": "dot", "size": 10, "source_id": "19a1a12db412295d0ddd19ffffb78332,3e0985c172a09bb6c99e305b9f40a513,587ca8c6cc86a93299e9540153babbc6,71f873c12230e6ede47583d81eb85e23,8ff636d53a50d7a3bad17443bf104ba2,98c6b5003112ab7110e45414a2fa468b,a5d60c1a355b77187003182f6df57646,fbc83a616e9fa96c245138bca69c177f", "type": "EVENT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"SELF-ATTENTION\" can be generated as follows:\n\nSELF-ATTENTION is a type of attention mechanism used in neural networks, particularly in natural language processing and time series forecasting. It is a concept used in various models, including the window encoder, TimeGPT, and TimesFM, to capture the diversity of past events and mask data at subsequent positions. Self-attention is a mechanism that enables the model to focus on specific parts of the input data and weigh their importance, allowing it to better understand the relationships between different elements. This mechanism is commonly used in neural network architectures to improve their performance in tasks such as language translation, text summarization, and time series forecasting.\n\nThe use of self-attention in these models is particularly relevant in the context of time series forecasting, where it can help capture the complex relationships between different time series data points. By applying self-attention to the input data, the model can identify the most relevant information and weigh its importance, leading to more accurate predictions.\n\nOverall, self-attention is a powerful mechanism that has been widely adopted in various neural network architectures, and its applications in time series forecasting and natural language processing have shown promising results.\n\nRelevant information from the nearby text:\n\n* The text mentions the use of self-attention in the window encoder, TimeGPT, and TimesFM models, indicating its widespread adoption in various neural network architectures.\n* The text also mentions the use of self-attention in natural language processing and time series forecasting, highlighting its applications in these domains.\n* The text does not provide any information that contradicts the descriptions of self-attention, and therefore, the summary generated above is consistent with the provided information.", "id": "SELF-ATTENTION", "label": "SELF-ATTENTION", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e,518bfcd6711530089fe3914ca16459c2,89e5f6a93205d5e87ad7e7641c0bbb91,a7604286fb84b26c53950861f9aca4b1,ee8135f4497807724f665a5cdaa775fb,f7266cfccedb1d9840d10afa689a05e9", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Delft University of Technology is a university in the Netherlands", "id": "DELFT UNIVERSITY OF TECHNOLOGY", "label": "DELFT UNIVERSITY OF TECHNOLOGY", "shape": "dot", "size": 10, "source_id": "308a1b7226a66096a6e686d5e99848ad", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Ramin Ghorbani is an author of the paper", "id": "RAMIN GHORBANI", "label": "RAMIN GHORBANI", "shape": "dot", "size": 10, "source_id": "308a1b7226a66096a6e686d5e99848ad", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Marcel J.T. Reinders is an author of the paper", "id": "MARCEL J.T. REINDERS", "label": "MARCEL J.T. REINDERS", "shape": "dot", "size": 10, "source_id": "308a1b7226a66096a6e686d5e99848ad", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "David M.J. Tax is an author of the paper", "id": "DAVID M.J. TAX", "label": "DAVID M.J. TAX", "shape": "dot", "size": 10, "source_id": "308a1b7226a66096a6e686d5e99848ad", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Radial basis function is a type of non-linear transformation that can be used to improve anomaly detection", "id": "RADIAL BASIS FUNCTION", "label": "RADIAL BASIS FUNCTION", "shape": "dot", "size": 10, "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Input signal refers to the original data that is being analyzed for anomalies", "id": "INPUT SIGNAL", "label": "INPUT SIGNAL", "shape": "dot", "size": 10, "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"Benchmark Datasets\" are a set of standard datasets used to evaluate the performance of machine learning models, particularly in the context of time series forecasting and anomaly detection. These datasets are crucial for assessing the efficacy of various models, including those used in long-term series forecasting, frequency analysis, and multi-head cross-attention techniques. The use of benchmark datasets allows researchers and practitioners to compare the performance of different models and algorithms, facilitating the development of more accurate and reliable time series forecasting models.", "id": "BENCHMARK DATASETS", "label": "BENCHMARK DATASETS", "shape": "dot", "size": 10, "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2,53dbeea6d05c3695460c71f89f468def", "type": "DATA"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"MULTI-HEAD ATTENTION\" can be generated as follows:\n\n\"Multi-Head Attention is a type of attention mechanism used in neural networks, particularly in transformer models, which enables the model to attend to different parts of the input sequence simultaneously. This attention mechanism is commonly used in machine learning algorithms designed to process sequential data, often employed in natural language processing and computer vision applications.\"\n\nThis summary incorporates information from all the descriptions, resolves any potential contradictions, and provides a coherent explanation of the entity \"MULTI-HEAD ATTENTION\". The summary is written in third person and includes the entity name for context.", "id": "MULTI-HEAD ATTENTION", "label": "MULTI-HEAD ATTENTION", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a,4d2961a17ee532a47fa53a41f53ebdd3,59e8e02df5f942071e733def7884b457,83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"DROPOUT\" can be generated as follows:\n\nDROPOUT is a hyperparameter that plays a crucial role in controlling the amount of dropout used in a model. Specifically, it determines the rate of dropout, which is a regularization technique used in neural networks. This technique randomly sets a fraction of the output units to zero during training, effectively preventing the model from overfitting to the training data. By adjusting the dropout rate, users can control the level of regularization applied to the model, allowing for more robust and generalizable performance.\n\nThis summary is written in third person and includes information from all the descriptions provided. It also resolves any potential contradictions and provides a coherent explanation of the entity \"DROPOUT\".", "id": "DROPOUT", "label": "DROPOUT", "shape": "dot", "size": 10, "source_id": "59e8e02df5f942071e733def7884b457,673b0feee6eb5843954defc670e4ba29,c5dc13d7191b625e7373e79907b5782a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Norm refers to a measure of the magnitude or size of a vector or matrix", "id": "NORM", "label": "NORM", "shape": "dot", "size": 10, "source_id": "59e8e02df5f942071e733def7884b457", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Feed-forward refers to a type of neural network architecture where the data flows only in one direction, from input to output", "id": "FEED-FORWARD", "label": "FEED-FORWARD", "shape": "dot", "size": 10, "source_id": "59e8e02df5f942071e733def7884b457", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe RBF LAYER is a component of the RESTAD model, which is a type of neural network architecture. Specifically, the RBF layer is a crucial component that integrates RBF similarity scores with reconstruction error. In more detail, the RBF layer refers to a type of neural network layer that utilizes radial basis functions to map the input data to a higher-dimensional space. This mapping enables the neural network to capture complex patterns and relationships within the data, ultimately contributing to the RESTAD model\u0027s ability to perform long-term series forecasting and other tasks.\n\nThe RBF layer\u0027s integration with reconstruction error is a key aspect of its functionality, allowing it to effectively combine similarity scores with error metrics to produce accurate and reliable results. This component is a vital part of the RESTAD model, and its unique architecture and functionality make it an essential tool for time series analysis and forecasting applications.\n\nOverall, the RBF LAYER is a sophisticated component of the RESTAD model, leveraging radial basis functions and reconstruction error to provide accurate and reliable results in time series analysis and forecasting tasks.", "id": "RBF LAYER", "label": "RBF LAYER", "shape": "dot", "size": 10, "source_id": "22df9b37bd353b7b484fb07edeeb66fd,529ee9dbb2f4807b9c21bbf190dbd1f7,59e8e02df5f942071e733def7884b457,99b3fe01a22282fe6d02bf29dacf8875", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe RBF SCORE is a type of anomaly score used in the RESTAD model. It refers to a measure of the similarity between the input data and the reconstructed data produced by a model. This score is likely used to evaluate the performance of the model in terms of its ability to accurately reconstruct the input data, and by extension, its ability to detect anomalies or irregularities in the data.\n\nIn the context of the RESTAD model, the RBF SCORE is a critical component that enables the model to identify and flag potential anomalies or irregularities in the data. The score is calculated based on the similarity between the input data and the reconstructed data produced by the model, and it provides a quantitative measure of the model\u0027s performance in terms of its ability to accurately reconstruct the input data.\n\nOverall, the RBF SCORE is an essential aspect of the RESTAD model, and it plays a crucial role in enabling the model to detect and flag potential anomalies or irregularities in the data.", "id": "RBF SCORE", "label": "RBF SCORE", "shape": "dot", "size": 10, "source_id": "22df9b37bd353b7b484fb07edeeb66fd,59e8e02df5f942071e733def7884b457", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"VANILLA TRANSFORMER\" is a deep learning model used as a baseline in research. It is a fundamental model used for comparison with proposed models, as indicated by its description of architecture and implementation. The \"VANILLA TRANSFORMER\" serves as a baseline model, providing a standard against which other models can be evaluated.\n\nThis summary is based on the provided descriptions, which are consistent and provide a clear understanding of the \"VANILLA TRANSFORMER\" entity. The information collected from the descriptions has been used to create a concise and coherent summary, written in third person.", "id": "VANILLA TRANSFORMER", "label": "VANILLA TRANSFORMER", "shape": "dot", "size": 10, "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7,9ec74c919ec0aea919a7f2916213ea16", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"AUC-ROC\" is a metric used in research, particularly in evaluating the performance of the RESTAD model. It is a key indicator of the model\u0027s ability to distinguish between classes or outcomes, with higher values indicating better performance. The calculation and use of AUC-ROC are described in the context of its application in the research, highlighting its importance in assessing the effectiveness of the RESTAD model.\n\nIn the context of the research, AUC-ROC is used to evaluate the performance of the RESTAD model, suggesting that it is a critical metric for assessing the model\u0027s ability to accurately classify or predict outcomes. The description of its calculation and use implies that AUC-ROC is a widely accepted and established metric in the field, used to evaluate the performance of machine learning models.\n\nOverall, the summary provides a clear understanding of the entity \"AUC-ROC\" and its significance in the research, highlighting its role in evaluating the performance of the RESTAD model.", "id": "AUC-ROC", "label": "AUC-ROC", "shape": "dot", "size": 10, "source_id": "99b3fe01a22282fe6d02bf29dacf8875,9ec74c919ec0aea919a7f2916213ea16", "type": "METRIC"}, {"color": "#97c2fc", "description": "Dutch Research Council (NWO) is an organization that provides funding for research projects", "id": "DUTCH RESEARCH COUNCIL", "label": "DUTCH RESEARCH COUNCIL", "shape": "dot", "size": 10, "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Similarity scores refer to a measure of how similar two data points are, often used in anomaly detection and clustering", "id": "SIMILARITY SCORES", "label": "SIMILARITY SCORES", "shape": "dot", "size": 10, "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The dissimilarity score is a metric used to measure the difference between the original and reconstructed data", "id": "DISSIMILARITY SCORE", "label": "DISSIMILARITY SCORE", "shape": "dot", "size": 10, "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7", "type": "METRIC"}, {"color": "#97c2fc", "description": "RBF similarity scores are a metric used to evaluate the performance of a model, specifically in the context of anomaly detection", "id": "RBF SIMILARITY SCORES", "label": "RBF SIMILARITY SCORES", "shape": "dot", "size": 10, "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "type": "METRIC"}, {"color": "#97c2fc", "description": "Association discrepancy is a measure of how similar a time point is to its adjacent time points", "id": "ASSOCIATION DISCREPANCY", "label": "ASSOCIATION DISCREPANCY", "shape": "dot", "size": 10, "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Reconstructed signal refers to the output of a model that is attempting to reconstruct the input signal", "id": "RECONSTRUCTED SIGNAL", "label": "RECONSTRUCTED SIGNAL", "shape": "dot", "size": 10, "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Subtle anomaly refers to a data point that is slightly different from the typical patterns in the data", "id": "SUBTLE ANOMALY", "label": "SUBTLE ANOMALY", "shape": "dot", "size": 10, "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Significant anomaly refers to a data point that is significantly different from the typical patterns in the data", "id": "SIGNIFICANT ANOMALY", "label": "SIGNIFICANT ANOMALY", "shape": "dot", "size": 10, "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time point refers to a specific point in time that is being analyzed for anomalies", "id": "TIME POINT", "label": "TIME POINT", "shape": "dot", "size": 10, "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "RBF center refers to the center of a radial basis function", "id": "RBF CENTER", "label": "RBF CENTER", "shape": "dot", "size": 10, "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Influence radius refers to the range of values that are affected by a radial basis function", "id": "INFLUENCE RADIUS", "label": "INFLUENCE RADIUS", "shape": "dot", "size": 10, "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "RESTAD framework refers to the proposed method for anomaly detection in time series data, which incorporates RBF neurons into the Transformer architecture", "id": "RESTAD FRAMEWORK", "label": "RESTAD FRAMEWORK", "shape": "dot", "size": 10, "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the \"TRANSFORMER MODEL\" can be generated as follows:\n\nThe \"TRANSFORMER MODEL\" is a type of deep learning model that is widely used for various tasks, including natural language processing, vision processing, and time series forecasting. It is a type of neural network architecture that is particularly well-suited for sequential data, such as time series data. The model employs attention mechanisms within a dynamic graph encoder for spatial modeling, integrating time encoding for temporal aspects. This allows the model to effectively capture both spatial and temporal relationships in the data. The Transformer model is also used in the inference procedure and is a key component of the TranAD model.\n\nThis summary is based on the information provided in the descriptions, which are all related to the \"TRANSFORMER MODEL\". The contradictions in the descriptions have been resolved to provide a single, coherent summary. The summary includes information from all the descriptions and provides a comprehensive overview of the \"TRANSFORMER MODEL\".\n\nRelevant information from the nearby text has been used to enrich the summary, including the fact that the Transformer model is a type of neural network architecture that is particularly well-suited for sequential data. This information has been used to provide a more detailed and accurate description of the \"TRANSFORMER MODEL\".", "id": "TRANSFORMER MODEL", "label": "TRANSFORMER MODEL", "shape": "dot", "size": 10, "source_id": "1b51ec337efd822ca3a0b3eb819c1b91,2cd8d6d8f61b953c1922dd3a39c0dec2,5805d8a1afe3d6bc6300ac71f7163831,6ea15432a841705c2e74cbc01f6004e9,a65c0f1eae6e779357739df141f75d36,f2e78b25a535b82e2743a0ea4052eb9a", "type": "MODEL"}, {"color": "#97c2fc", "description": "RBF neurons refer to a type of neural network layer that uses radial basis functions to compute similarity scores between data points and reference points", "id": "RBF NEURONS", "label": "RBF NEURONS", "shape": "dot", "size": 10, "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"STEP\" can be generated as follows:\n\nSTEP is a model mentioned in the text, which utilizes unsupervised pre-trained Transformer blocks to model temporal relationships from long-term history time series. This model applies a graph structure learner and incorporates spatio-temporal Graph Neural Networks (GNNs) based on the representation of Transformer blocks. STEP is a research that links spatio-temporal GNNs with a pre-trained transformer for enhanced forecasting by learning from extensive historical data. The model is designed to leverage the strengths of both Transformer-based architectures and GNNs to improve forecasting accuracy.\n\nThe summary is written in third person and includes information collected from all the descriptions. Any potential contradictions have been resolved to provide a single, coherent summary. Relevant information from the nearby text has been incorporated to enrich the description.", "id": "STEP", "label": "STEP", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831,77c7ced6ab4a0e57fba604c1a3e00416,859c14b7112010530eddafc204b4b305,ad8efcfda80253036294f2eeb48926e8", "type": "MODEL"}, {"color": "#97c2fc", "description": "TranAD model is a type of transformer model used for anomaly detection in time-series data", "id": "TRANAAD MODEL", "label": "TRANAAD MODEL", "shape": "dot", "size": 10, "source_id": "f2e78b25a535b82e2743a0ea4052eb9a", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe entity \"WEIGHTS\" refers to coefficients used in a specific context, likely within a machine learning or deep learning model. These coefficients are utilized to compute attention scores, which are a crucial component in various applications such as natural language processing and time series forecasting.\n\nThe use of weights in this context is a common practice in the field of machine learning, where they are employed to adjust the importance of different features or inputs when making predictions or generating outputs. In the case of attention scores, weights are used to determine the relative importance of different elements or tokens in a sequence or dataset.\n\nOverall, the entity \"WEIGHTS\" plays a vital role in the computation of attention scores, which are essential in various machine learning and deep learning applications.\n\nRelevant information from the nearby text:\n\n* The text mentions \"time series\" and \"long-term series forecasting,\" which suggests that the entity \"WEIGHTS\" may be used in the context of time series analysis or forecasting.\n* The text also mentions \"frequency analysis\" and \"multi-head cross-attention,\" which are techniques commonly used in natural language processing and machine learning.\n* The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" suggests that the text is from a research paper or technical document, which further supports the idea that the entity \"WEIGHTS\" is used in a machine learning or deep learning context.", "id": "WEIGHTS", "label": "WEIGHTS", "shape": "dot", "size": 10, "source_id": "a65c0f1eae6e779357739df141f75d36,f2e78b25a535b82e2743a0ea4052eb9a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Input time-series window is a concept used in the TranAD model", "id": "INPUT TIME-SERIES WINDOW", "label": "INPUT TIME-SERIES WINDOW", "shape": "dot", "size": 10, "source_id": "a65c0f1eae6e779357739df141f75d36", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"ANOMALY SCORE\" can be described as follows:\n\nThe Anomaly Score is a concept mentioned in the text, referring to a measure of how likely a data point is to be an anomaly. It is defined as the sum of the squared differences between the observed and predicted values, indicating the degree of anomalousness of a data point. In essence, the Anomaly Score is a quantitative measure that assesses the likelihood of an input being an anomaly, providing a numerical representation of its anomalous nature.\n\nThis description is derived from the provided descriptions, which collectively convey the meaning and definition of the Anomaly Score. The information is written in a formal and technical tone, consistent with the language used in academic papers and research documents, as indicated by the presence of technical terms and mathematical equations.", "id": "ANOMALY SCORE", "label": "ANOMALY SCORE", "shape": "dot", "size": 10, "source_id": "1b51ec337efd822ca3a0b3eb819c1b91,477c5b7f23f4e00030ab389788a4c88d,6ea15432a841705c2e74cbc01f6004e9,9f30a997c7ea3ed8fe02f631a3bd9649,f2e78b25a535b82e2743a0ea4052eb9a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Linear projection refers to a type of machine learning algorithm that is designed to project high-dimensional data onto a lower-dimensional space, often used in natural language processing and computer vision", "id": "LINEAR PROJECTION", "label": "LINEAR PROJECTION", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Handcrafted dataset descriptions refer to the written summaries of dataset information, often used in natural language processing and computer vision", "id": "HANDCRAFTED DATASET DESCRIPTIONS", "label": "HANDCRAFTED DATASET DESCRIPTIONS", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe primary entity of interest is \"MULTI-HEAD SELF ATTENTION.\" This entity is described as a type of attention mechanism used in the transformer model. \n\nThe description of \"MULTI-HEAD SELF ATTENTION\" is as follows: \"Multi-head self attention is a type of attention mechanism used in the transformer model.\" This description provides insight into the function and application of \"MULTI-HEAD SELF ATTENTION\" within the context of the transformer model.\n\nGiven the formal and academic language used in the description, it is likely that this information is from a research paper or technical document related to natural language processing or deep learning. The use of technical terms such as \"attention mechanism\" and \"transformer model\" further supports this conclusion.\n\nOverall, the summary of \"MULTI-HEAD SELF ATTENTION\" is a concise and informative description of its function and application within the transformer model.", "id": "MULTI-HEAD SELF ATTENTION", "label": "MULTI-HEAD SELF ATTENTION", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3,f2e78b25a535b82e2743a0ea4052eb9a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Number of heads is a hyper-parameter that controls the number of attention heads used in the model", "id": "NUM HEADS", "label": "NUM HEADS", "shape": "dot", "size": 10, "source_id": "673b0feee6eb5843954defc670e4ba29", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Monash baselines are a set of models used for comparison with the TimesFM model", "id": "MONASH BASELINES", "label": "MONASH BASELINES", "shape": "dot", "size": 10, "source_id": "673b0feee6eb5843954defc670e4ba29", "type": "MODEL"}, {"color": "#97c2fc", "description": "Weight decay is a hyperparameter that determines the rate of weight decay", "id": "WEIGHT DECAY", "label": "WEIGHT DECAY", "shape": "dot", "size": 10, "source_id": "c5dc13d7191b625e7373e79907b5782a", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"K-MEANS\" is a type of clustering algorithm that groups similar data points into clusters. It is a widely used unsupervised machine learning technique that partitions the data into K clusters based on their similarities. The K-means algorithm is known for its simplicity and efficiency in identifying patterns and structures in large datasets.\n\nHowever, it is also worth noting that K-means has been used as an initialization method for the Radial Basis Function (RBF) layer in certain machine learning models. This suggests that K-means can be used as a preprocessing step to provide a good starting point for the RBF layer, potentially improving the performance of the overall model.\n\nOverall, the entity \"K-MEANS\" is a versatile and widely applicable algorithm that has been used in various contexts, including clustering and initialization methods for other machine learning models.\n\nRelevant information from the nearby text is not provided, but based on the descriptions, it can be inferred that K-means is a fundamental algorithm in machine learning, and its applications extend beyond clustering to initialization methods for other models.", "id": "K-MEANS", "label": "K-MEANS", "shape": "dot", "size": 10, "source_id": "22df9b37bd353b7b484fb07edeeb66fd,59e8e02df5f942071e733def7884b457", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"INITIALIZATION\" can be described as follows:\n\nInitialization refers to the process of setting the initial values of a model\u0027s parameters. This process is crucial in machine learning and time series forecasting, as it can significantly impact the performance and accuracy of the model. The initial values of the model parameters are typically set before training the model, and they can be determined using various methods, such as random initialization or initialization based on prior knowledge.\n\nIn the context of machine learning, initialization is a critical step that can affect the convergence and stability of the model. Proper initialization can help the model to learn the underlying patterns and relationships in the data, leading to better performance and generalization. On the other hand, poor initialization can result in slow convergence, overfitting, or even divergence of the model.\n\nIn time series forecasting, initialization is also essential, as it can impact the accuracy and reliability of the forecasts. The initial values of the model parameters can influence the model\u0027s ability to capture the underlying patterns and trends in the time series data, leading to more accurate and reliable forecasts.\n\nOverall, initialization is a critical step in machine learning and time series forecasting, and it requires careful consideration and selection of the initial values of the model parameters.\n\nRelevant information from the nearby text:\n\n* The text mentions that the language of the text is English, which is consistent with the formal and academic tone of the description.\n* The text also mentions that the descriptions are repetitive, but the information is consistent, which supports the coherence of the summary.\n* The text does not provide any additional information that contradicts the description of initialization, so the summary remains consistent with the provided information.", "id": "INITIALIZATION", "label": "INITIALIZATION", "shape": "dot", "size": 10, "source_id": "59e8e02df5f942071e733def7884b457,973ea1d37e8f6bb0ab004f360c04ddc6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Random refers to a type of initialization strategy that randomly sets the initial values of the model parameters", "id": "RANDOM", "label": "RANDOM", "shape": "dot", "size": 10, "source_id": "59e8e02df5f942071e733def7884b457", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TRAINING LOSS\" is a crucial metric in machine learning model development. It is a measure of the difference between the model\u0027s predictions and the actual values, indicating how well the model is learning from the data. Specifically, training loss is a metric used to evaluate the performance of machine learning models, which is invariant to scale. This means that it provides a reliable and consistent measure of model performance, regardless of the scale of the data.\n\nIn the context of machine learning, training loss is a key indicator of model performance, and it plays a vital role in the development and optimization of models. By monitoring training loss, model developers can identify areas for improvement, adjust model parameters, and refine the model to achieve better performance.\n\nOverall, the entity \"TRAINING LOSS\" is a fundamental concept in machine learning, and its importance cannot be overstated. It is a critical metric that helps model developers evaluate and improve the performance of their models, ultimately leading to better predictions and decision-making outcomes.", "id": "TRAINING LOSS", "label": "TRAINING LOSS", "shape": "dot", "size": 10, "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6,a90c6ca26542ea5935f9d8d5bf3688a9,baa887ae552c6b3dac10f74896d8c4a2", "type": "METRIC"}, {"color": "#97c2fc", "description": "", "id": "SERVER METRICS (PSM)", "label": "SERVER METRICS (PSM)", "shape": "dot", "size": 10, "source_id": "9ec74c919ec0aea919a7f2916213ea16", "type": ""}, {"color": "#97c2fc", "description": "RESTAD model is a deep learning model used in the research, as indicated by the description of its architecture and implementation", "id": "RESTAD MODEL", "label": "RESTAD MODEL", "shape": "dot", "size": 10, "source_id": "9ec74c919ec0aea919a7f2916213ea16", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"SCALE\" can be generated as follows:\n\nThe entity \"SCALE\" refers to a measure of the magnitude or range of values within a time series. It is often used in the context of normalization and quantization. Specifically, scale can be defined as the mean absolute value of the data in a time series, indicating the average magnitude of the data points. Alternatively, it can also refer to the range of values within which a sequence of data points is measured, highlighting the variability or dispersion of the data. Furthermore, scale can also be understood as the range or magnitude of a time series, emphasizing its overall size or extent. Overall, the concept of scale is crucial in understanding and analyzing time series data, particularly in applications such as long-term series forecasting, frequency analysis, and multi-head cross-attention.\n\nThis summary incorporates information from all the descriptions provided, resolves any potential contradictions, and provides a coherent explanation of the entity \"SCALE\" in the context of time series analysis.", "id": "SCALE", "label": "SCALE", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8,85e4fbea9a08a0a663f3c5dc3f3a95a7,f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"MEAN SCALING\" is a normalization technique that involves applying an affine transformation to time series data, often used in deep learning models. This concept is mentioned in the text as a method for normalizing time series data, which is a crucial step in various machine learning and time series forecasting applications. The use of mean scaling in deep learning models is a common practice, as it helps to stabilize the learning process and improve the overall performance of the models.\n\nThe text also suggests that mean scaling is a concept that is relevant to the field of time series analysis and forecasting, where normalization techniques are essential for accurate predictions. The mention of mean scaling in the context of deep learning models implies that it is a widely used technique in the field of artificial intelligence and machine learning.\n\nOverall, \"MEAN SCALING\" is a normalization technique that plays a crucial role in various machine learning and time series forecasting applications, particularly in deep learning models.", "id": "MEAN SCALING", "label": "MEAN SCALING", "shape": "dot", "size": 10, "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8,6eb4c16edf2eedfd03721efb199478d8", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Affine transformation is a concept mentioned in the text, referring to a mathematical operation used in normalization", "id": "AFFINE TRANSFORMATION", "label": "AFFINE TRANSFORMATION", "shape": "dot", "size": 10, "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time series windows is a concept mentioned in the text, as indicated by the use of the phrase \"time-series windows\"", "id": "TIME SERIES WINDOWS", "label": "TIME SERIES WINDOWS", "shape": "dot", "size": 10, "source_id": "477c5b7f23f4e00030ab389788a4c88d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "RBF kernel layer is a component of the RESTAD model, as indicated by the description of its placement in the model architecture", "id": "RBF KERNEL LAYER", "label": "RBF KERNEL LAYER", "shape": "dot", "size": 10, "source_id": "9ec74c919ec0aea919a7f2916213ea16", "type": "MODEL COMPONENT"}, {"color": "#97c2fc", "description": "Data embedding is a component of the RESTAD model, as indicated by the description of its architecture and implementation", "id": "DATA EMBEDDING", "label": "DATA EMBEDDING", "shape": "dot", "size": 10, "source_id": "9ec74c919ec0aea919a7f2916213ea16", "type": "MODEL COMPONENT"}, {"color": "#97c2fc", "description": "Multi-head self-attention is a component of the RESTAD model, as indicated by the description of its architecture and implementation", "id": "MULTI-HEAD SELF-ATTENTION", "label": "MULTI-HEAD SELF-ATTENTION", "shape": "dot", "size": 10, "source_id": "9ec74c919ec0aea919a7f2916213ea16", "type": "MODEL COMPONENT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Entity:** FEED-FORWARD NETWORKS\n\n**Summary:** Feed-forward networks are a type of neural network model that are frequently used due to their low computational costs and efficiency. They are a component of the RESTAD model, as indicated by the description of their architecture and implementation. Feed-forward networks are designed for classification and regression tasks, making them a versatile and widely applicable model in various machine learning applications.\n\n**Key Characteristics:**\n\n* Component of the RESTAD model\n* Type of neural network model\n* Low computational costs and efficiency\n* Designed for classification and regression tasks\n\n**Relevance:** Feed-forward networks are a crucial component of the RESTAD model, and their design and implementation are essential for achieving efficient and accurate results in machine learning applications. Their ability to handle classification and regression tasks makes them a valuable tool in various fields, including but not limited to, time series forecasting, where they can be used to predict future values based on historical data.\n\n**Context:** The summary of feed-forward networks is relevant to the context of machine learning and time series forecasting, where neural network models like feed-forward networks are widely used to analyze and predict complex patterns in data. The RESTAD model, which incorporates feed-forward networks, is likely used in applications where accurate and efficient predictions are necessary, such as in finance, economics, or climate modeling.", "id": "FEED-FORWARD NETWORKS", "label": "FEED-FORWARD NETWORKS", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6,7d5d82d600620153153772a9bc498ac0,9ec74c919ec0aea919a7f2916213ea16", "type": "MODEL COMPONENT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"ADAM OPTIMIZER\" is an algorithm used for model training. Specifically, it is an optimization algorithm used in research, as indicated by its use in training the RESTAD model. This suggests that the ADAM OPTIMIZER plays a crucial role in the training process of machine learning models, particularly in the context of the RESTAD model.\n\nGiven the formal and academic language used in the descriptions, it is likely that the ADAM OPTIMIZER is a technical term commonly used in the field of machine learning and research. The use of technical terms such as \"model training\" and \"optimization algorithm\" further supports this conclusion.\n\nOverall, the ADAM OPTIMIZER is a key component in the training of machine learning models, particularly in the context of the RESTAD model, and is a widely used term in the field of research and machine learning.", "id": "ADAM OPTIMIZER", "label": "ADAM OPTIMIZER", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458,9ec74c919ec0aea919a7f2916213ea16", "type": "OPTIMIZATION ALGORITHM"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of TIMESNET can be generated as follows:\n\nTIMESNET is a model primarily used for time series analysis and forecasting. It is a type of neural network architecture designed for time series data tasks, which utilizes CNNs as a building block. TIMESNET is mentioned in the text as a model that is compared with TIME-LLM in terms of average enhancements, and it has exhibited comparable or superior performances to TIME-LLM in short-term forecasting tasks. Specifically, TIMESNET is a top-performing model on the M3-Quarterly dataset, indicating its effectiveness in time series forecasting. The model is likely a type of neural network that is designed for time series forecasting, and it is used for both short-term and long-term forecasting tasks. Overall, TIMESNET is a robust model for time series analysis and forecasting, with a strong performance record in various datasets.", "id": "TIMESNET", "label": "TIMESNET", "shape": "dot", "size": 10, "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,3e937ba8de0e7eca993c50506ceb8f1f,41fe893a178ebc8790ef4da83da5ab6e,4de223d7df157faf857c3e17d9e6e5b6,5e4d9ca02ee6a285d5223c820743eb12,5fa25d3abec59ccd2718e00c0e0eb440,77c7ced6ab4a0e57fba604c1a3e00416,7d5d82d600620153153772a9bc498ac0,7e03744dea80e2138baff03611104fa8,7e97089185883c456c798ddc5ec86373,91e161ba596a0cbbcae541ddb2106310,9ba0189af2ef0720a721c16eef0f0788,b9d22fe9f2eecb1665f4bea365c7d612,d4551c2839eaa68a7cb7324089956581,d4e3d8b5bf043b78bb9f1551080cab91,ef2ef6c95c36f51706c54ca3f2893641,f49330b6fd81d86d14e7a9d4b8e45576,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"N-BEATS\" can be generated as follows:\n\nN-BEATS is a recent time series model that is used for time series forecasting. It is a neural network architecture that uses a recursive decomposition based on projecting residual signals on learned basis functions. N-BEATS is a model that performs well for time-series forecasting tasks, particularly in short-term forecasting, and is capable of outperforming local statistical models. The model is mentioned in the text as being compared with TIME-LLM in short-term forecasting and is used for evaluation and comparison purposes. Additionally, N-BEATS is a model that uses feed-forward networks and transfer learning between various source-target dataset pairs, making it a versatile and effective tool for time series forecasting.\n\nThe model is specifically mentioned in the text as OCCB19 and is placed 3rd in terms of point forecasting performance. N-BEATS is also mentioned alongside other models such as WaveNet, DeepAR, TFT, DLinear, PatchTST, N-HiTS, and GPT4TS, indicating its relevance and importance in the field of time series forecasting.\n\nOverall, N-BEATS is a powerful and effective model for time series forecasting, with a range of features and capabilities that make it a valuable tool for researchers and practitioners in the field.", "id": "N-BEATS", "label": "N-BEATS", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,1b48e9ca066ac5ba037066bb762d3458,1ddbab2dca370c9ef7b5a724075518cc,2bc70082c142ee2ef5d6a941611505b7,39365aee753fb73130c208fdf4046bb7,41b0bd14ce7ff7419b7ee1f78b4701ae,41fe893a178ebc8790ef4da83da5ab6e,532a6d434dab611a80aef1e94dd2fb45,5e4d9ca02ee6a285d5223c820743eb12,5fa25d3abec59ccd2718e00c0e0eb440,7d5d82d600620153153772a9bc498ac0,9ba0189af2ef0720a721c16eef0f0788,af36d1634490149b96980fb1dff57cd1,d40f5dc25597e4e4d7c30b8bfd98f89a,d4551c2839eaa68a7cb7324089956581,f0c52387a7b3a5c3850fe6991f0a7c83,f49330b6fd81d86d14e7a9d4b8e45576,ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nNHITS is a deep learning model primarily used for time series forecasting. It is a global model that can be compared with other models, such as TimeGPT, for evaluation purposes. Specifically, NHITS utilizes feed-forward networks to make predictions, which is a characteristic of its architecture. Overall, NHITS is a type of time series forecasting model that leverages deep learning techniques to forecast future values in a time series.\n\nThis summary incorporates information from all the descriptions provided, resolving any potential contradictions and presenting a coherent overview of the NHITS model.", "id": "NHITS", "label": "NHITS", "shape": "dot", "size": 10, "source_id": "2565ae205d4c98342168bb67a4f7a309,55eb54ef455d14cd1a11760924f99eb8,7d5d82d600620153153772a9bc498ac0,f912df936d0b735fe654d1a9c53caa3b", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"ANOMALY SCORES\" refers to a metric used in research, specifically calculated and utilized for identifying anomalies. These scores are predicted values that indicate the likelihood of an anomaly occurring. In essence, anomaly scores serve as a quantitative measure to assess the probability of an anomaly, providing valuable insights for researchers and analysts to detect and analyze unusual patterns or events.\n\nThis summary is based on the provided descriptions, which are consistent and coherent. The information collected from all the descriptions has been integrated to provide a clear and concise understanding of the entity \"ANOMALY SCORES\" and its significance in research.", "id": "ANOMALY SCORES", "label": "ANOMALY SCORES", "shape": "dot", "size": 10, "source_id": "9ec74c919ec0aea919a7f2916213ea16,ee651b9bedb0cbcb6272a67deda44bb0", "type": "METRIC"}, {"color": "#97c2fc", "description": "Training corpus is a concept mentioned in the text, as indicated by the discussion of its selection and preparation", "id": "TRAINING CORPUS", "label": "TRAINING CORPUS", "shape": "dot", "size": 10, "source_id": "f72ba51a17dd00cff43bcdda22c273e8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TRAINING DATA\" refers to the dataset used to train TimeGPT, a model that includes a wide range of scenarios and time series. This dataset is utilized to train the model, which implies that it is a crucial component in the development and improvement of TimeGPT\u0027s performance. The training data is likely composed of various time series datasets, each representing different scenarios, which enables the model to learn and generalize across diverse situations.\n\nIn the context of machine learning, the training data is a fundamental aspect of model development, as it provides the necessary information for the model to learn patterns, relationships, and trends. The fact that the training data includes a wide range of scenarios and time series suggests that it is a comprehensive and diverse dataset, which is essential for training a robust and accurate model like TimeGPT.\n\nOverall, the training data plays a vital role in the development and performance of TimeGPT, and its characteristics and composition are critical in determining the model\u0027s capabilities and limitations.", "id": "TRAINING DATA", "label": "TRAINING DATA", "shape": "dot", "size": 10, "source_id": "0e3c8905bd533021b4f9bf9875ea66e0,42e1bb44edbe787e104f589e74b95d6c,fd1092903d83bf6e90a6caa371d7c514", "type": "DATA"}, {"color": "#97c2fc", "description": "GPU cluster refers to the cluster of NVIDIA A10G GPUs used to train TimeGPT", "id": "GPU CLUSTER", "label": "GPU CLUSTER", "shape": "dot", "size": 10, "source_id": "42e1bb44edbe787e104f589e74b95d6c", "type": "INFRASTRUCTURE"}, {"color": "#97c2fc", "description": "Backbone model layers refer to the number of layers in the transformer-based language model", "id": "BACKBONE MODEL LAYERS", "label": "BACKBONE MODEL LAYERS", "shape": "dot", "size": 10, "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"TEXT PROTOTYPES\" refers to a set of text examples that are used to represent a concept or class. Specifically, in the context of the TIME-LLM model, text prototypes refer to the number of text prototypes used in this model. This suggests that the TIME-LLM model utilizes a specific number of text prototypes to represent a concept or class, which is a key aspect of its functionality.\n\nThe use of text prototypes in the TIME-LLM model is likely related to the model\u0027s ability to perform long-term series forecasting, frequency analysis, and other tasks that require a deep understanding of the underlying concept or class. The model\u0027s reliance on text prototypes may also be influenced by the use of multi-head cross-attention mechanisms, which are commonly used in natural language processing tasks.\n\nOverall, the entity \"TEXT PROTOTYPES\" plays a crucial role in the TIME-LLM model, and its specific characteristics and usage are likely to be an important area of research and development in the field of natural language processing and time series forecasting.", "id": "TEXT PROTOTYPES", "label": "TEXT PROTOTYPES", "shape": "dot", "size": 10, "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8,e57d44d23f5a3a82ce9a9b9532d31cbe", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time series input length refers to the length of the time series input used in the TIME-LLM model", "id": "TIME SERIES INPUT LENGTH", "label": "TIME SERIES INPUT LENGTH", "shape": "dot", "size": 10, "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Patch reprogramming cross-attention heads refer to the number of attention heads used in the patch reprogramming process", "id": "PATCH REPROGRAMMING CROSS-ATTENTION HEADS", "label": "PATCH REPROGRAMMING CROSS-ATTENTION HEADS", "shape": "dot", "size": 10, "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Anomaly prediction process refers to the steps taken to predict anomalies in data", "id": "ANOMALY PREDICTION PROCESS", "label": "ANOMALY PREDICTION PROCESS", "shape": "dot", "size": 10, "source_id": "ee651b9bedb0cbcb6272a67deda44bb0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"THRESHOLD\" is a parameter of the RESTAD model, which is used to identify anomalies in data. Specifically, it refers to a value used to determine whether a data point is an anomaly. This suggests that the threshold value serves as a boundary or cutoff point to distinguish between normal and abnormal data points.\n\nIn the context of the RESTAD model, the threshold parameter plays a crucial role in identifying anomalies, which are data points that deviate significantly from the expected behavior or pattern. By setting a threshold value, the model can determine whether a data point is an anomaly or not, allowing for the detection and analysis of unusual patterns in the data.\n\nOverall, the threshold entity is a key component of the RESTAD model, enabling the identification and analysis of anomalies in data.", "id": "THRESHOLD", "label": "THRESHOLD", "shape": "dot", "size": 10, "source_id": "9ec74c919ec0aea919a7f2916213ea16,9f30a997c7ea3ed8fe02f631a3bd9649", "type": "MODEL PARAMETER"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"F1-SCORE\" is a metric used to evaluate the performance of machine learning models, specifically in the context of the RESTAD model. It is calculated and used to assess the accuracy of the model in detecting anomalies. The F1-score is a widely used metric in research and is employed to evaluate the performance of various machine learning models, including those used for anomaly detection.\n\nIn the context of the RESTAD model, the F1-score is used to measure the model\u0027s ability to accurately identify anomalies, indicating its effectiveness in detecting unusual patterns or outliers in data. The use of the F1-score in this context suggests that the model\u0027s performance is being evaluated based on its ability to balance precision and recall, two key metrics used in evaluating the accuracy of anomaly detection models.\n\nOverall, the F1-score is a critical metric in the evaluation of machine learning models, particularly those used for anomaly detection, and its use in the RESTAD model highlights its importance in assessing the performance of such models.\n\nRelevant information from the nearby text:\n\n* The use of the F1-score in the RESTAD model suggests that the model is being evaluated based on its ability to accurately detect anomalies, which is a critical aspect of anomaly detection models.\n* The fact that the F1-score is used to evaluate the performance of the RESTAD model indicates that the model is being used for anomaly detection, which is a common application of machine learning models.\n* The use of the F1-score in research and academic papers suggests that it is a widely accepted and established metric in the field of machine learning and anomaly detection.", "id": "F1-SCORE", "label": "F1-SCORE", "shape": "dot", "size": 10, "source_id": "6917a14af275e30abe2d30a8f8a9a4e6,9ec74c919ec0aea919a7f2916213ea16,9f30a997c7ea3ed8fe02f631a3bd9649", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe Sliding Window Strategy is a technique used to process data points in a time series. It refers to a method used to process data points within a window, allowing for the analysis and forecasting of data in a specific time frame. This strategy is particularly useful in time series analysis and long-term series forecasting, enabling the extraction of meaningful insights and patterns from large datasets.\n\nThe Sliding Window Strategy involves dividing the data into smaller segments or windows, which are then processed individually to identify trends, patterns, and correlations. This approach is often used in frequency analysis and multi-head cross-attention techniques, where the focus is on understanding the relationships between different data points within a specific time frame.\n\nThe use of the Sliding Window Strategy is evident in various academic papers and research studies, including those published in conferences and journals such as ICLR, AAAI, and PMLR. These studies have demonstrated the effectiveness of this strategy in improving the accuracy of time series forecasting and providing valuable insights into complex data patterns.\n\nOverall, the Sliding Window Strategy is a powerful technique for processing and analyzing time series data, offering a range of benefits and applications in fields such as data science, machine learning, and time series forecasting.", "id": "SLIDING WINDOW STRATEGY", "label": "SLIDING WINDOW STRATEGY", "shape": "dot", "size": 10, "source_id": "6917a14af275e30abe2d30a8f8a9a4e6,9f30a997c7ea3ed8fe02f631a3bd9649", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "True positives refer to data points that are correctly identified as anomalies", "id": "TRUE POSITIVES", "label": "TRUE POSITIVES", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"MSL\" can be generated as follows:\n\nThe entity \"MSL\" refers to a real-world benchmark dataset collected from a NASA spacecraft. It is a dataset of multivariate time series data used for various purposes, including anomaly detection, testing, evaluation, and model performance assessment. Specifically, MSL is used to evaluate the performance of anomaly detection models, such as AnomalyBERT, and is considered a metric for evaluating the effectiveness of these models. Additionally, MSL is a dataset used in experiments and is present in tables related to anomaly detection. Overall, MSL is a significant dataset in the field of anomaly detection and time series analysis.\n\nThis summary is based on the information collected from all the descriptions provided, and any contradictions have been resolved to provide a single, coherent description. The summary is written in third person and includes the entity name \"MSL\" for context. Relevant information from the nearby text has been incorporated to enrich the summary.", "id": "MSL", "label": "MSL", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,19a1a12db412295d0ddd19ffffb78332,22df9b37bd353b7b484fb07edeeb66fd,6917a14af275e30abe2d30a8f8a9a4e6,69afb4bcb8c1e03975c837102e1d0b32,71f873c12230e6ede47583d81eb85e23,971e73b638469366cfdd3af2e7c7a824,9c6e74299923071f9fc2b7cce49efc90,a39d9d28126733c33db624ccc25f5782,b01517b8d09acabed7145d9ffa4a409b,d16a81565fcd654ab21768510dcc5d7f,fbc83a616e9fa96c245138bca69c177f", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"SMD\" can be generated as follows:\n\nSMD is a real-world benchmark dataset collected from a large Internet company, specifically designed for anomaly detection in multivariate time series data. It is a dataset of multivariate time series data used for evaluating the performance of anomaly detection models, including AnomalyBERT. SMD is utilized in experiments to assess the efficacy of various models and methods in identifying anomalies within the data. The dataset is used to evaluate model performance, making it a crucial metric for assessing the accuracy and reliability of anomaly detection models.\n\nThe summary is written in third person and includes information collected from all the descriptions, resolving any potential contradictions. The entity name \"SMD\" is included to provide context, and relevant information from the nearby text has been incorporated to enrich the summary.", "id": "SMD", "label": "SMD", "shape": "dot", "size": 10, "source_id": "00973c1c3c962d9234a38037709824b1,052d1d9614f084eb2b4b0cd58ad476ce,1413d358c623ac2d4c70be6547eb218b,19a1a12db412295d0ddd19ffffb78332,22df9b37bd353b7b484fb07edeeb66fd,2b44aebc638544dcf835db30c4270d09,5809618fe3a2014c2140601fcf103022,6917a14af275e30abe2d30a8f8a9a4e6,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,71f873c12230e6ede47583d81eb85e23,8e075f1de7293e0cd1724bd167c263be,9c6e74299923071f9fc2b7cce49efc90,a39d9d28126733c33db624ccc25f5782,f6e57fa18831bcc732a631240536777a,fbc83a616e9fa96c245138bca69c177f", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"SMAP\" can be generated as follows:\n\nSMAP is a real-world benchmark dataset collected from a NASA spacecraft, consisting of soil samples and telemetry data. It is used for various purposes, including anomaly detection, testing, and evaluation of anomaly detection models. Specifically, SMAP is utilized to evaluate the performance of anomaly detection models, such as AnomalyBERT, and is used in experiments to test and assess the effectiveness of these models. The dataset is also used to benchmark the performance of anomaly detection models, making it a valuable resource for researchers and developers in the field of anomaly detection and time series analysis.\n\nThis summary is based on the information collected from all the descriptions, and any contradictions have been resolved to provide a single, coherent summary. The entity name \"SMAP\" is included to provide context, and relevant information from the nearby text has been incorporated to enrich the summary.", "id": "SMAP", "label": "SMAP", "shape": "dot", "size": 10, "source_id": "19a1a12db412295d0ddd19ffffb78332,6917a14af275e30abe2d30a8f8a9a4e6,69afb4bcb8c1e03975c837102e1d0b32,71f873c12230e6ede47583d81eb85e23,8ff636d53a50d7a3bad17443bf104ba2,971e73b638469366cfdd3af2e7c7a824,9c6e74299923071f9fc2b7cce49efc90,9f30a997c7ea3ed8fe02f631a3bd9649,a39d9d28126733c33db624ccc25f5782,b01517b8d09acabed7145d9ffa4a409b,d16a81565fcd654ab21768510dcc5d7f,ee651b9bedb0cbcb6272a67deda44bb0,fbc83a616e9fa96c245138bca69c177f", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"WADI\" can be generated as follows:\n\nWADI is a real-world benchmark dataset collected from a water distribution testbed, specifically designed for anomaly detection in multivariate time series data. It is a dataset used for testing and evaluation of anomaly detection models, including AnomalyBERT, and is also used to evaluate the performance of these models. WADI is a dataset of water distribution system data, which makes it a valuable resource for researchers and practitioners in the field of anomaly detection and time series analysis. The dataset is used in experiments to assess the performance of various models and is a metric used to evaluate model performance, particularly in the context of anomaly detection. Overall, WADI is a comprehensive and realistic dataset that serves as a benchmark for evaluating the performance of anomaly detection models in real-world scenarios.\n\nNote that the contradictions in the descriptions have been resolved by considering the following:\n\n* The descriptions that refer to WADI as a \"metric\" are likely incorrect, as the context suggests that WADI is a dataset rather than a metric.\n* The descriptions that refer to WADI as a \"system\" are likely incorrect, as the context suggests that WADI is a dataset rather than a system.\n* The descriptions that refer to WADI as a \"real-world dataset\" are consistent with the other descriptions, and suggest that WADI is a dataset collected from a real-world water distribution testbed.\n* The descriptions that refer to WADI as a \"dataset used for anomaly detection\" are consistent with the other descriptions, and suggest that WADI is a dataset used for evaluating the performance of anomaly detection models.", "id": "WADI", "label": "WADI", "shape": "dot", "size": 10, "source_id": "00973c1c3c962d9234a38037709824b1,052d1d9614f084eb2b4b0cd58ad476ce,19a1a12db412295d0ddd19ffffb78332,2b44aebc638544dcf835db30c4270d09,6917a14af275e30abe2d30a8f8a9a4e6,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,71f873c12230e6ede47583d81eb85e23,8ff636d53a50d7a3bad17443bf104ba2,9c6e74299923071f9fc2b7cce49efc90,9f30a997c7ea3ed8fe02f631a3bd9649,a39d9d28126733c33db624ccc25f5782,ee651b9bedb0cbcb6272a67deda44bb0,f6e57fa18831bcc732a631240536777a,fbc83a616e9fa96c245138bca69c177f", "type": "DATASET"}, {"color": "#97c2fc", "description": "Hundman et al. (2018) is a research paper that describes the MSL and SMAP datasets", "id": "HUNDMAN ET AL. (2018)", "label": "HUNDMAN ET AL. (2018)", "shape": "dot", "size": 10, "source_id": "fbc83a616e9fa96c245138bca69c177f", "type": "PAPER"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, it can be concluded that the entity \"SWAT\" or \"SWaT\" refers to a dataset used in various contexts. \n\nThe dataset \"SWAT\" or \"SWaT\" is mentioned in the text as a concept, abbreviation, and a real-world dataset used for evaluating AnomalyBERT. It is also described as a dataset used for anomaly detection, testing, and evaluation of models. \n\nSpecifically, \"SWAT\" or \"SWaT\" is a dataset of water treatment plant data, used for anomaly detection, testing, and evaluation of models, including AnomalyBERT. It is also used to evaluate the performance of a model.\n\nThe primary language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nThe dataset \"SWAT\" or \"SWaT\" is likely used in the context of machine learning and time series forecasting, given the presence of technical terms and mathematical equations in the text. The use of the abbreviation \"SWAT\" or \"SWaT\" in the text suggests that it is a widely recognized concept or dataset in the field.\n\nOverall, the dataset \"SWAT\" or \"SWaT\" is a real-world dataset used for evaluating the performance of anomaly detection models, including AnomalyBERT, and is likely used in the context of machine learning and time series forecasting.", "id": "SWAT", "label": "SWAT", "shape": "dot", "size": 10, "source_id": "19a1a12db412295d0ddd19ffffb78332,6917a14af275e30abe2d30a8f8a9a4e6,69afb4bcb8c1e03975c837102e1d0b32,71f873c12230e6ede47583d81eb85e23,8ff636d53a50d7a3bad17443bf104ba2,971e73b638469366cfdd3af2e7c7a824,9c6e74299923071f9fc2b7cce49efc90,9f30a997c7ea3ed8fe02f631a3bd9649,a39d9d28126733c33db624ccc25f5782,b01517b8d09acabed7145d9ffa4a409b,d16a81565fcd654ab21768510dcc5d7f,ee651b9bedb0cbcb6272a67deda44bb0,f6e57fa18831bcc732a631240536777a,fbc83a616e9fa96c245138bca69c177f", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"NAB\" can be generated as follows:\n\nThe entity \"NAB\" refers to a dataset that is utilized for various purposes, including anomaly detection, testing, evaluation, and performance assessment of models, particularly those designed for anomaly detection. Specifically, NAB is a dataset comprising multiple real-world data traces, such as temperature sensor readings, CPU utilization of cloud machines, service request latencies, and taxi demands in New York City. This dataset is mentioned in the text and is used to evaluate the performance of models, making it a valuable resource for researchers and developers in the field of anomaly detection and time series forecasting.\n\nThe language used to describe NAB is formal and academic, suggesting that it is from a research paper or technical document. The text contains technical terms, mathematical equations, and references to academic papers, all of which are written in English. This further supports the conclusion that NAB is a dataset used in academic and research contexts.\n\nOverall, NAB is a comprehensive dataset that provides a rich source of real-world data for evaluating the performance of anomaly detection models and other time series forecasting techniques.", "id": "NAB", "label": "NAB", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,69afb4bcb8c1e03975c837102e1d0b32,8e075f1de7293e0cd1724bd167c263be,9b35a2c607e0f4b8cbc9179f424f180a,9c6e74299923071f9fc2b7cce49efc90,a39d9d28126733c33db624ccc25f5782,a4a241e471ad258932241bc441b96155,b01517b8d09acabed7145d9ffa4a409b", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"UCR\" can be described as follows:\n\nThe UCR dataset is a collection of multiple univariate time series that was used in the KDD 2021 cup. It is primarily utilized for anomaly detection, testing, and evaluation purposes. Specifically, the UCR dataset is employed to assess the performance of models, particularly those designed for anomaly detection. This dataset is mentioned in the text, as indicated by the use of the abbreviation \"UCR\", and is likely a widely recognized and utilized resource in the field of time series analysis and machine learning.\n\nThe information collected from all the descriptions has been combined to provide a comprehensive and coherent summary of the UCR dataset. The contradictions have been resolved, and the description is written in the third person to provide a clear and concise understanding of the entity. Relevant information from the nearby text has been incorporated to enrich the description and provide additional context.", "id": "UCR", "label": "UCR", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,69afb4bcb8c1e03975c837102e1d0b32,8e075f1de7293e0cd1724bd167c263be,9b35a2c607e0f4b8cbc9179f424f180a,9c6e74299923071f9fc2b7cce49efc90,a39d9d28126733c33db624ccc25f5782,a4a241e471ad258932241bc441b96155,b01517b8d09acabed7145d9ffa4a409b", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, it appears that the entity \"MBA\" refers to a dataset used for various purposes, including anomaly detection, testing, evaluation, and model performance assessment. However, upon closer examination, it seems that the descriptions are contradictory, with some indicating that \"MBA\" is a dataset and others suggesting that it is a model.\n\nTo resolve this contradiction, it is likely that \"MBA\" refers to a dataset that is used for anomaly detection, and the model that uses this dataset is also referred to as \"MBA\" in some contexts. This is supported by the fact that the dataset is mentioned in the text and is used for testing and evaluation.\n\nTherefore, a comprehensive summary of the data provided is:\n\nThe entity \"MBA\" refers to a dataset used for anomaly detection, testing, evaluation, and model performance assessment. This dataset contains electrocardiogram recordings from four patients, with multiple instances of two different kinds of anomalies. The dataset is used to evaluate the performance of anomaly detection models, and a model that uses a multi-task learning approach to perform anomaly detection is also referred to as \"MBA\" in some contexts.\n\nThis summary takes into account all the provided descriptions, resolves the contradictions, and provides a coherent and concise description of the entity \"MBA\".", "id": "MBA", "label": "MBA", "shape": "dot", "size": 10, "source_id": "69afb4bcb8c1e03975c837102e1d0b32,8e075f1de7293e0cd1724bd167c263be,9b35a2c607e0f4b8cbc9179f424f180a,9c6e74299923071f9fc2b7cce49efc90,a39d9d28126733c33db624ccc25f5782,a4a241e471ad258932241bc441b96155,b01517b8d09acabed7145d9ffa4a409b", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, it can be concluded that the entity \"MSDS\" refers to a dataset used for evaluating the performance of anomaly detection models in multivariate time series data. \n\nThe dataset, \"MSDS,\" is utilized for anomaly detection in multivariate time series data, as indicated by its presence in the table. It is also used to evaluate the performance of a model, suggesting that it serves as a benchmark for assessing the efficacy of anomaly detection methods and models.\n\nHowever, there seems to be a contradiction in the descriptions, where \"MSDS\" is referred to as both a dataset and a metric used to evaluate model performance. To resolve this contradiction, it can be inferred that \"MSDS\" is likely a dataset that contains metrics or features used to evaluate the performance of anomaly detection models.\n\nFurthermore, the description \"MSDS is a system, as indicated by the use of the abbreviation \u0027MSDS\u0027 in the table\" suggests that \"MSDS\" might refer to a system or a framework that utilizes multivariate time series data for anomaly detection. However, this description seems to be less relevant compared to the other descriptions that clearly indicate that \"MSDS\" is a dataset.\n\nIn conclusion, the entity \"MSDS\" is a dataset used for evaluating the performance of anomaly detection models in multivariate time series data. It contains metrics or features that are used to assess the efficacy of anomaly detection methods and models.", "id": "MSDS", "label": "MSDS", "shape": "dot", "size": 10, "source_id": "00973c1c3c962d9234a38037709824b1,1413d358c623ac2d4c70be6547eb218b,2b44aebc638544dcf835db30c4270d09,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,9c6e74299923071f9fc2b7cce49efc90,a39d9d28126733c33db624ccc25f5782,f6e57fa18831bcc732a631240536777a", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"P\" refers to a metric or performance measure used to evaluate the performance of models. Specifically, \"P\" is a metric used to evaluate the precision of a model, which is a key aspect of its overall performance. \n\nIn the context of machine learning and time series forecasting, precision is an essential metric that measures the accuracy of a model\u0027s predictions. It is used to evaluate how well a model can identify the correct instances or predictions among all the instances or predictions made.\n\nGiven the information provided, it is clear that \"P\" is a metric that is closely related to the evaluation of model performance, particularly in terms of precision. This summary is written in third person and includes the entity name \"P\" for context.", "id": "P", "label": "P", "shape": "dot", "size": 10, "source_id": "b01517b8d09acabed7145d9ffa4a409b,d5c8b72da09cebefa0c26285ad5272eb", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"R\" refers to a metric or performance measure used to evaluate the performance of models. Specifically, \"R\" is a metric used to evaluate the recall of a model, which is a measure of a model\u0027s ability to correctly identify the positive instances in a dataset. \n\nIn the context of machine learning and time series forecasting, \"R\" is likely being used to assess the effectiveness of a model in capturing the underlying patterns and trends in a time series data. The use of \"R\" as a metric suggests that the model\u0027s performance is being evaluated based on its ability to accurately recall the relevant information in the data.\n\nIt is worth noting that the term \"R\" is often used in the context of information retrieval and machine learning, where it refers to the recall of a model. This is in contrast to precision, which measures the proportion of true positives among all predicted positive instances. The use of \"R\" as a metric highlights the importance of recall in evaluating the performance of models, particularly in applications where missing relevant information can have significant consequences.", "id": "R", "label": "R", "shape": "dot", "size": 10, "source_id": "b01517b8d09acabed7145d9ffa4a409b,d5c8b72da09cebefa0c26285ad5272eb", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data is as follows:\n\nThe entity \"AUC\" refers to the \"Area Under the Curve,\" which is a metric used to evaluate the performance of an anomaly detection model. Specifically, AUC is a performance measure used to evaluate the models, as indicated by its use in the table. This metric is commonly used in machine learning and time series analysis to assess the accuracy of models in detecting anomalies or predicting outcomes.\n\nIn the context of machine learning, AUC is a key metric for evaluating the performance of models, particularly in applications such as anomaly detection, classification, and regression. The use of AUC in the table suggests that it is a critical measure for evaluating the performance of models in this context.\n\nOverall, the summary provides a clear understanding of the entity \"AUC\" and its role in evaluating the performance of models, particularly in anomaly detection and machine learning applications.", "id": "AUC", "label": "AUC", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,b01517b8d09acabed7145d9ffa4a409b,f6e57fa18831bcc732a631240536777a", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"F1\" is a metric or performance measure used to evaluate the performance of models, specifically in the context of anomaly detection models. It is the harmonic mean of precision and recall, providing a balanced evaluation of a model\u0027s ability to correctly identify both true positives and true negatives. F1 score is a widely used metric in machine learning and data science to assess the effectiveness of models in detecting anomalies or irregularities in data.\n\nThis summary is derived from the provided descriptions, which collectively provide a clear understanding of the entity \"F1\" and its application in model evaluation. The information is concise and coherent, with no contradictions or ambiguities. The summary is written in third person and includes the entity name \"F1\" for context.", "id": "F1", "label": "F1", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,b01517b8d09acabed7145d9ffa4a409b,d5c8b72da09cebefa0c26285ad5272eb", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"TIME\" can be described as follows:\n\nTIME refers to a fundamental concept that serves as a measure of duration or sequence. It is a metric or performance measure used to evaluate models, indicating its significance in various contexts, including but not limited to, time series analysis and forecasting. The notation \"96\", \"192\", \"336\", and \"720\" further supports this interpretation, suggesting that TIME is often represented by specific numerical values or intervals.\n\nIn the context of time series analysis, TIME is crucial for understanding patterns, trends, and relationships within data sets. It enables researchers and analysts to evaluate the performance of models, identify long-term series forecasting opportunities, and conduct frequency analysis to uncover underlying structures.\n\nThe use of TIME in academic and technical contexts is evident from the presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals. Additionally, the citation of English-language academic papers and authors suggests that TIME is a widely discussed and researched concept in the English-speaking academic community.\n\nOverall, TIME is a multifaceted concept that plays a vital role in various fields, including time series analysis, forecasting, and model evaluation. Its significance is underscored by its widespread use in academic and technical contexts, as well as its representation by specific numerical values or intervals.", "id": "TIME", "label": "TIME", "shape": "dot", "size": 10, "source_id": "2b44aebc638544dcf835db30c4270d09,2dbfdb45630a023a5a6979b9573a868f,4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b01517b8d09acabed7145d9ffa4a409b,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96,f05d25f1f55ce768a5d240373d5283a6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "PSM is a metric used to evaluate the performance of anomaly detection models", "id": "PSM", "label": "PSM", "shape": "dot", "size": 10, "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "type": "METRIC"}, {"color": "#97c2fc", "description": "GNN refers to a type of graph neural network that is designed to process graph-structured data, often used in finance, healthcare, and other fields", "id": "GNN", "label": "GNN", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"ECG\" refers to an electrocardiogram, which is a medical test that measures the electrical activity of the heart. This electrical activity is often used in healthcare and other fields to diagnose and monitor various heart-related conditions. The ECG is a crucial tool for understanding the heart\u0027s function and detecting potential issues, making it a vital component in medical diagnostics and research.\n\nIn this summary, we have incorporated information from all the descriptions provided, resolving any potential contradictions and presenting a coherent overview of the entity \"ECG\". The summary highlights the medical significance of ECG, its application in healthcare, and its importance in understanding the heart\u0027s electrical activity.", "id": "ECG", "label": "ECG", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8,ad8efcfda80253036294f2eeb48926e8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe entity \"LGBM\" is a global model used for comparison with TimeGPT. This summary is based on the provided description list, which mentions that LGBM is used for comparison with TimeGPT, but does not provide any additional information about LGBM itself.\n\nIt is worth noting that the language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. However, this information does not provide any additional context about the entity \"LGBM\".\n\nOverall, the summary of the entity \"LGBM\" is that it is a global model used for comparison with TimeGPT, but no further information is available about its characteristics or functionality.", "id": "LGBM", "label": "LGBM", "shape": "dot", "size": 10, "source_id": "55eb54ef455d14cd1a11760924f99eb8,f912df936d0b735fe654d1a9c53caa3b", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"DEEPAR\" can be generated as follows:\n\nDEEPAR is a deep learning forecasting model that uses Recurrent Neural Networks (RNNs) for time-series forecasting tasks. It is a task-specific model that performs better than Chronos models on some datasets and local statistical models. DEEPAR is an autoregressive RNN-based method for probabilistic forecasting, which places 4th in terms of point forecasting performance. The model is used for evaluation and is mentioned in the text alongside other models such as WaveNet, N-BEATS, TFT, DLinear, PatchTST, N-HiTS, and GPT4TS. DEEPAR is a type of neural network used for time series forecasting tasks and is used for testing and evaluation purposes.\n\nThe summary is written in third person and includes information collected from all the descriptions. The contradictions have been resolved, and a single, coherent summary has been provided. The entity name \"DEEPAR\" is included in the summary, along with relevant information from the nearby text.", "id": "DEEPAR", "label": "DEEPAR", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,1dc665214307b1656d1a0094a1918ece,2565ae205d4c98342168bb67a4f7a309,2f3b2f69f8eb4f958c3ca793cae36581,376897a501ac50834f626fcc5fb13ece,41b0bd14ce7ff7419b7ee1f78b4701ae,51ee17c1f0212d4c94010d8e376f649b,532a6d434dab611a80aef1e94dd2fb45,56de1d5a5b467101344afa4248d829dc,6fa5f635ad5f7e6b67d5e467f130345c,79c41aff65d584b4c8d4c769b82756d5,7e69d9444a2084b6452e291735bf5a49,892b821ed44c13c4d09e0ceedac3051d,af36d1634490149b96980fb1dff57cd1,f0c52387a7b3a5c3850fe6991f0a7c83,f912df936d0b735fe654d1a9c53caa3b", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"LSTM-NDT\" can be generated as follows:\n\nLSTM-NDT is a deep learning model specifically designed for anomaly detection and diagnosis in multivariate time series data. It utilizes a Long-Short-Term-Memory (LSTM) based neural network to forecast the data, taking into account the input time-series, and a non-parametric dynamic thresholding approach to detect anomalies from prediction errors. This model is considered a state-of-the-art method for multivariate time-series anomaly detection and is often used as a baseline for comparison with other models, such as TranAD. LSTM-NDT relies on an LSTM-based deep neural network model that uses the input sequence as training data and forecasts data for the next timestamp for each input timestamp.\n\nThe model\u0027s primary function is to identify anomalies in multivariate time series data, making it a valuable tool for various applications, including but not limited to, fault detection, quality control, and predictive maintenance. Its ability to learn complex patterns and relationships within the data enables it to provide accurate and reliable anomaly detection results.\n\nOverall, LSTM-NDT is a robust and effective model for anomaly detection in multivariate time series data, making it a popular choice among researchers and practitioners in the field of time series analysis and machine learning.", "id": "LSTM-NDT", "label": "LSTM-NDT", "shape": "dot", "size": 10, "source_id": "00973c1c3c962d9234a38037709824b1,052d1d9614f084eb2b4b0cd58ad476ce,0c4c072869e10b0b4bd4bc19a60a23a3,1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,69afb4bcb8c1e03975c837102e1d0b32,8e075f1de7293e0cd1724bd167c263be,971e73b638469366cfdd3af2e7c7a824,a4a241e471ad258932241bc441b96155,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb,ffbbbf29ffb8d038e241f023079cb0a2", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"DAGMM\" can be generated as follows:\n\nDAGMM is a state-of-the-art model for multivariate time-series anomaly detection. It is a method that uses a deep autoencoding Gaussian mixture model for dimension reduction in the feature space and recurrent networks for temporal modeling. DAGMM is specifically designed for anomaly detection in multivariate time series data and has been evaluated in the text, as indicated by the presence of its name and performance metrics. The model has been mentioned in the text as a model used for anomaly detection, as indicated by its presence in the table. Furthermore, DAGMM is an anomaly detection model that uses a deep autoencoder and a Gaussian mixture model, making it a robust and effective tool for identifying anomalies in complex time series data.\n\nThe language used to describe DAGMM is formal and technical, suggesting that it is a research-based model. The use of terms such as \"deep autoencoding Gaussian mixture model\" and \"recurrent networks\" indicates that DAGMM is a sophisticated model that leverages advanced machine learning techniques to achieve its anomaly detection capabilities. Overall, DAGMM appears to be a highly effective and widely used model for multivariate time-series anomaly detection, with a strong presence in the text and a robust set of features that enable it to identify anomalies in complex data.", "id": "DAGMM", "label": "DAGMM", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,69afb4bcb8c1e03975c837102e1d0b32,971e73b638469366cfdd3af2e7c7a824,a4a241e471ad258932241bc441b96155,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb,ffbbbf29ffb8d038e241f023079cb0a2", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "", "id": "LSTM-VAE", "label": "LSTM-VAE", "shape": "dot", "size": 10, "source_id": "ee42bd06cc3770a3384df85cc16fef54", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"MSCRED\" can be generated as follows:\n\nMSCRED is a deep learning model specifically designed for anomaly detection in multivariate time series data. It is a state-of-the-art model that has been evaluated in the text, as indicated by the presence of its name and performance metrics. MSCRED is a method for anomaly detection and diagnosis, and it has been used for anomaly detection in various contexts, as mentioned in the table. The model\u0027s capabilities and performance metrics suggest that it is a reliable and effective tool for identifying anomalies in multivariate time series data.\n\nThe summary is written in third person and includes information collected from all the descriptions. Any contradictions have been resolved to provide a single, coherent summary. Relevant information from the nearby text has been incorporated to enrich the summary.", "id": "MSCRED", "label": "MSCRED", "shape": "dot", "size": 10, "source_id": "1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,971e73b638469366cfdd3af2e7c7a824,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb,ee42bd06cc3770a3384df85cc16fef54", "type": ""}, {"color": "#97c2fc", "description": "", "id": "THOC", "label": "THOC", "shape": "dot", "size": 10, "source_id": "ee42bd06cc3770a3384df85cc16fef54", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"GDN\" can be generated as follows:\n\nGDN is a deep learning model specifically designed for anomaly detection in multivariate time series data. It is a state-of-the-art method that utilizes deep neural networks with a time-series window as an input to identify anomalies. GDN employs attention mechanisms to focus on specific modes of the data, enabling it to effectively detect anomalies. The model has been evaluated in the text, indicating its performance metrics and presence in the table. GDN is a type of neural network model used for anomaly detection, making it a valuable tool for identifying unusual patterns in multivariate time series data.\n\nThe summary is written in third person and includes information collected from all the descriptions. Any contradictions have been resolved to provide a single, coherent summary. Relevant information from the nearby text has been incorporated to enrich the description.", "id": "GDN", "label": "GDN", "shape": "dot", "size": 10, "source_id": "00973c1c3c962d9234a38037709824b1,0259287b914980606371cd1161c6a420,0c4c072869e10b0b4bd4bc19a60a23a3,1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,8e075f1de7293e0cd1724bd167c263be,971e73b638469366cfdd3af2e7c7a824,a4a241e471ad258932241bc441b96155,bd73ee0439609823e12a877840c6ebae,d16a81565fcd654ab21768510dcc5d7f,ee42bd06cc3770a3384df85cc16fef54", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"MTAD-GAT\" can be generated as follows:\n\nMTAD-GAT is a state-of-the-art model for multivariate time-series anomaly detection that uses deep neural networks with a time-series window as an input. It is a type of neural network model used for anomaly detection, specifically designed for multivariate time series data. MTAD-GAT employs attention mechanisms to focus on specific modes of the data, enabling it to effectively identify anomalies. The model has been evaluated in the text, demonstrating its performance in anomaly detection tasks. As a state-of-the-art method, MTAD-GAT is considered a reliable and accurate approach for detecting anomalies in multivariate time series data.\n\nThe summary is written in third person and includes information from all the descriptions, resolving any potential contradictions. The language used is formal and academic, consistent with the context of a research paper or technical document.", "id": "MTAD-GAT", "label": "MTAD-GAT", "shape": "dot", "size": 10, "source_id": "00973c1c3c962d9234a38037709824b1,0259287b914980606371cd1161c6a420,0c4c072869e10b0b4bd4bc19a60a23a3,1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,8e075f1de7293e0cd1724bd167c263be,a4a241e471ad258932241bc441b96155,bd73ee0439609823e12a877840c6ebae,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"MAD-GAN\" can be generated as follows:\n\nMAD-GAN is a deep learning model specifically designed for anomaly detection in multivariate time series data. It is a state-of-the-art model that has been evaluated in the text, as indicated by the presence of its name and performance metrics. MAD-GAN is a type of neural network model used for anomaly detection, and it has been used for this purpose, as indicated by its presence in the table. The model is capable of detecting anomalies in multivariate time series data, making it a valuable tool for various applications.\n\nThe language used to describe MAD-GAN is formal and academic, suggesting that the text is from a research paper or a technical document. The use of technical terms such as \"deep learning model,\" \"anomaly detection,\" and \"multivariate time series data\" further supports this conclusion. Additionally, the presence of mathematical equations and formulas, as well as references to academic papers and authors, indicates that the text is written in English.\n\nOverall, MAD-GAN is a sophisticated model for anomaly detection in multivariate time series data, and its evaluation in the text suggests that it is a reliable and effective tool for this purpose.", "id": "MAD-GAN", "label": "MAD-GAN", "shape": "dot", "size": 10, "source_id": "00973c1c3c962d9234a38037709824b1,0259287b914980606371cd1161c6a420,1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,971e73b638469366cfdd3af2e7c7a824,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"CAE-M\" can be generated as follows:\n\nCAE-M is a deep learning model specifically designed for anomaly detection in multivariate time series data. It is a state-of-the-art model that has been evaluated in the text, as indicated by the presence of its name and performance metrics. CAE-M is a type of neural network model used for anomaly detection, and it has been mentioned in the text as a method for anomaly detection and diagnosis. The model\u0027s name is used in the table, indicating its relevance to the topic of anomaly detection. Overall, CAE-M is a model that has been recognized for its effectiveness in identifying anomalies in multivariate time series data.\n\nThe information collected from all the descriptions has been used to create a single, coherent summary that highlights the key characteristics and capabilities of the CAE-M model. The summary is written in third person and includes the entity name \"CAE-M\" to provide context.", "id": "CAE-M", "label": "CAE-M", "shape": "dot", "size": 10, "source_id": "00973c1c3c962d9234a38037709824b1,0259287b914980606371cd1161c6a420,1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,a4a241e471ad258932241bc441b96155,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"MERLIN\" can be generated as follows:\n\nMERLIN is a deep learning model used for anomaly detection and diagnosis in multivariate time series data. It is a baseline method used for comparison with TranAD and is a state-of-the-art model for multivariate time-series anomaly detection. MERLIN is a parameter-free model that uses a multi-task learning approach to perform anomaly detection. It is a recent approach that uses a parameter-free version of time series discord discovery by iteratively comparing subsequences of varying length with their immediate neighbors. MERLIN is a system that is used for anomaly detection and is mentioned in the text as a model used for comparison with TranAD in Section 4.\n\nThe contradictions in the descriptions have been resolved by considering the following information:\n\n* MERLIN is both a model and a system, which is consistent with the description of it being used for anomaly detection and being mentioned in the text as a system.\n* MERLIN is both a baseline method and a state-of-the-art model, which is consistent with the description of it being used for comparison with TranAD and being a recent approach.\n* MERLIN is both a parameter-free model and a model that uses a multi-task learning approach, which is consistent with the description of it being a recent approach that uses a parameter-free version of time series discord discovery.\n\nOverall, the summary provides a comprehensive description of the entity \"MERLIN\" and resolves any contradictions in the descriptions.", "id": "MERLIN", "label": "MERLIN", "shape": "dot", "size": 10, "source_id": "00973c1c3c962d9234a38037709824b1,052d1d9614f084eb2b4b0cd58ad476ce,1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,2b44aebc638544dcf835db30c4270d09,69afb4bcb8c1e03975c837102e1d0b32,78ee4a3d7a2bffd4405a03d94a4f6cb1,8e075f1de7293e0cd1724bd167c263be,971e73b638469366cfdd3af2e7c7a824,9b35a2c607e0f4b8cbc9179f424f180a,a39d9d28126733c33db624ccc25f5782,a4a241e471ad258932241bc441b96155,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb,f6e57fa18831bcc732a631240536777a,ffbbbf29ffb8d038e241f023079cb0a2", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"OMNIANOMALY\" can be generated as follows:\n\n\"OMNIANOMALY is a state-of-the-art model for multivariate time-series anomaly detection. It is a deep learning model that utilizes a stochastic recurrent neural network and a planar normalizing flow to generate reconstruction probabilities. This method is specifically designed for anomaly detection and diagnosis in multivariate time series data. OMNIANOMALY helps set more accurate threshold values by considering localized peak values in the data sequence, making it a valuable tool for identifying anomalies in complex data sets. The model has been evaluated in the text, demonstrating its effectiveness in detecting anomalies and providing valuable insights for diagnosis.\"\n\nThis summary incorporates information from all the descriptions, resolving any contradictions and providing a coherent overview of the entity \"OMNIANOMALY\". The language used is formal and academic, consistent with the primary language of the text being English.", "id": "OMNIANOMALY", "label": "OMNIANOMALY", "shape": "dot", "size": 10, "source_id": "1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,8e075f1de7293e0cd1724bd167c263be,971e73b638469366cfdd3af2e7c7a824,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb,ee42bd06cc3770a3384df85cc16fef54,ffbbbf29ffb8d038e241f023079cb0a2", "type": ""}, {"color": "#97c2fc", "description": "DCDetector is a type of anomaly detection model", "id": "DCDETECTOR", "label": "DCDETECTOR", "shape": "dot", "size": 10, "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "type": "MODEL"}, {"color": "#97c2fc", "description": "RESTAD (R) is a type of anomaly detection model", "id": "RESTAD (R)", "label": "RESTAD (R)", "shape": "dot", "size": 10, "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "type": "MODEL"}, {"color": "#97c2fc", "description": "RESTAD (K) is a type of anomaly detection model", "id": "RESTAD (K)", "label": "RESTAD (K)", "shape": "dot", "size": 10, "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "type": "MODEL"}, {"color": "#97c2fc", "description": "Su et al. (2019) is a research paper that describes the SMD dataset", "id": "SU ET AL. (2019)", "label": "SU ET AL. (2019)", "shape": "dot", "size": 10, "source_id": "fbc83a616e9fa96c245138bca69c177f", "type": "PAPER"}, {"color": "#97c2fc", "description": "Sub-datasets are a collection of data from 28 different machines", "id": "SUB-DATASETS", "label": "SUB-DATASETS", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022", "type": "DATASET"}, {"color": "#97c2fc", "description": "The DAGMM model performs well for short datasets, but its scores drop significantly for longer sequences", "id": "DAGMM MODEL", "label": "DAGMM MODEL", "shape": "dot", "size": 10, "source_id": "8e075f1de7293e0cd1724bd167c263be", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe SMD DATASET is a dataset used for testing and evaluation of anomaly detection models. It is a type of dataset specifically designed to assess the performance of a model in terms of its ability to detect anomalies. The primary purpose of the SMD DATASET is to evaluate the anomaly detection performance of a model, making it a valuable resource for researchers and developers in the field of machine learning and data analysis.\n\nIn the context of machine learning and time series forecasting, the SMD DATASET is particularly relevant, as it allows for the evaluation of a model\u0027s ability to identify and detect anomalies in a dataset. This is a critical aspect of many real-world applications, including but not limited to, financial analysis, healthcare, and cybersecurity.\n\nOverall, the SMD DATASET is a valuable tool for researchers and developers seeking to improve the performance of their anomaly detection models, and its use is likely to contribute to the advancement of machine learning and data analysis techniques in various fields.", "id": "SMD DATASET", "label": "SMD DATASET", "shape": "dot", "size": 10, "source_id": "5277ea101e78f34ea2c62fa10007b2ac,529ee9dbb2f4807b9c21bbf190dbd1f7,99b3fe01a22282fe6d02bf29dacf8875", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe MSL DATASET is a dataset used for testing and evaluation of anomaly detection models, specifically in the context of manufacturing system data. It is designed to assess the performance of anomaly detection models, with the primary goal of identifying anomalies or irregularities in the data. The MSL dataset is a valuable resource for researchers and developers working on anomaly detection models, particularly in the field of manufacturing systems.\n\nThis summary is based on the information provided in the description list, which collectively paint a picture of the MSL DATASET as a dataset used for testing and evaluation of anomaly detection models, specifically in the context of manufacturing system data. The summary is written in third person and includes the entity name \"MSL DATASET\" to provide context.", "id": "MSL DATASET", "label": "MSL DATASET", "shape": "dot", "size": 10, "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7,587ca8c6cc86a93299e9540153babbc6,99b3fe01a22282fe6d02bf29dacf8875", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe PSM DATASET is a dataset used for testing and evaluation of anomaly detection models. It serves as a benchmark for assessing the performance of anomaly detection models, specifically in evaluating their ability to identify anomalies. The dataset is designed to provide a standardized framework for comparing the effectiveness of different anomaly detection techniques.\n\nIn this context, the PSM DATASET is a valuable resource for researchers and developers working on anomaly detection models, as it allows them to evaluate and compare the performance of their models in a controlled and standardized environment. The dataset\u0027s primary purpose is to facilitate the development and evaluation of anomaly detection models, making it an essential tool in the field of machine learning and data analysis.\n\nThe information collected from the descriptions is consistent, and there are no contradictions. The summary is written in third person, and the entity name \"PSM DATASET\" is included to provide context. Relevant information from the nearby text is not provided, as there is no nearby text available.", "id": "PSM DATASET", "label": "PSM DATASET", "shape": "dot", "size": 10, "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7,99b3fe01a22282fe6d02bf29dacf8875", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe WADI DATASET is a type of dataset used to evaluate the performance of a model in terms of its ability to detect anomalies. Specifically, it refers to a dataset of water distribution system data. This dataset is likely used in the context of time series analysis and anomaly detection, given its application in evaluating model performance in these areas.\n\nThe WADI DATASET is likely used in academic and research settings, given the formal and technical language used to describe it. It may be referenced in papers and conferences related to machine learning, time series forecasting, and anomaly detection, such as ICLR, AAAI, and PMLR.\n\nOverall, the WADI DATASET is a valuable resource for researchers and practitioners working in the field of anomaly detection and time series analysis, particularly in the context of water distribution system data.", "id": "WADI DATASET", "label": "WADI DATASET", "shape": "dot", "size": 10, "source_id": "5277ea101e78f34ea2c62fa10007b2ac,587ca8c6cc86a93299e9540153babbc6", "type": "DATASET"}, {"color": "#97c2fc", "description": "Tracer dataset is a type of dataset used to evaluate the performance of a model in terms of its ability to detect anomalies", "id": "TRACER DATASET", "label": "TRACER DATASET", "shape": "dot", "size": 10, "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "type": "DATASET"}, {"color": "#97c2fc", "description": "SMAP dataset refers to a dataset of smart manufacturing data", "id": "SMAP DATASET", "label": "SMAP DATASET", "shape": "dot", "size": 10, "source_id": "587ca8c6cc86a93299e9540153babbc6", "type": "DATASET"}, {"color": "#97c2fc", "description": "Unsupervised anomaly detection is a task in machine learning that involves identifying unusual patterns or outliers in data without labeled training data", "id": "UNSUPERVISED ANOMALY DETECTION", "label": "UNSUPERVISED ANOMALY DETECTION", "shape": "dot", "size": 10, "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "type": "TASK"}, {"color": "#97c2fc", "description": "Adversarial memory autoencoders are a type of neural network architecture used in anomaly detection", "id": "ADVERSARIAL MEMORY AUTOENCODERS", "label": "ADVERSARIAL MEMORY AUTOENCODERS", "shape": "dot", "size": 10, "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "type": "MODEL"}, {"color": "#97c2fc", "description": "LSTM-based variational autoencoder is a type of neural network architecture used in anomaly detection", "id": "LSTM-BASED VARIATIONAL AUTOENCODER", "label": "LSTM-BASED VARIATIONAL AUTOENCODER", "shape": "dot", "size": 10, "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "type": "MODEL"}, {"color": "#97c2fc", "description": "IEEE Robotics and Automation Letters is a journal that publishes research papers on robotics and automation", "id": "IEEE ROBOTICS AND AUTOMATION LETTERS", "label": "IEEE ROBOTICS AND AUTOMATION LETTERS", "shape": "dot", "size": 10, "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "type": "JOURNAL"}, {"color": "#97c2fc", "description": "Computers, Materials \u0026 Continua is a journal that publishes research papers on computer science and materials science", "id": "COMPUTERS MATERIALS \u0026 CONTINUA", "label": "COMPUTERS MATERIALS \u0026 CONTINUA", "shape": "dot", "size": 10, "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "type": "JOURNAL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"TIME SERIES ANALYSIS\":\n\nTIME SERIES ANALYSIS is a field of study that deals with the analysis and modeling of time series data. It involves the analysis of data that is collected over time, with the primary goal of extracting insights and making predictions. This field utilizes time series foundation models to achieve its objectives. The process of time series analysis refers to the comprehensive examination and understanding of time series data to uncover valuable information and forecast future trends.\n\nThe summary is written in third person and includes all the provided descriptions, resolving any potential contradictions. The information is enriched with relevant details from the nearby text, providing a clear and concise overview of the entity \"TIME SERIES ANALYSIS\".", "id": "TIME SERIES ANALYSIS", "label": "TIME SERIES ANALYSIS", "shape": "dot", "size": 10, "source_id": "479fc10bea12b01a37fbc5cef21eea76,4de223d7df157faf857c3e17d9e6e5b6,5bb0db923f70e5f431183c6ab7dc25dc,7a36c78af074ba651b0404f11cdf481d,83b49bf68dab6c36bdc09f02a63803fe,98c6b5003112ab7110e45414a2fa468b,f92205091f8d3b7e1e60b9bc80883a62", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Data mining refers to the process of automatically discovering patterns and relationships in large datasets", "id": "DATA MINING", "label": "DATA MINING", "shape": "dot", "size": 10, "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "type": "FIELD"}, {"color": "#97c2fc", "description": "Methodology refers to the systematic and comprehensive approach used to analyze and understand a particular phenomenon or problem", "id": "METHODLOGY", "label": "METHODLOGY", "shape": "dot", "size": 10, "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The foundation model paradigm is a powerful approach to developing models that can excel across a diverse spectrum of downstream tasks", "id": "FOUNDATION MODEL PARADIGM", "label": "FOUNDATION MODEL PARADIGM", "shape": "dot", "size": 10, "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Ground truth labels are data that is used to train a model, and is typically used to evaluate the performance of the model", "id": "GROUND TRUTH LABELS", "label": "GROUND TRUTH LABELS", "shape": "dot", "size": 10, "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "type": "DATA"}, {"color": "#97c2fc", "description": "Neural forecasting methods are a type of machine learning approach used for time series forecasting", "id": "NEURAL FORECASTING METHODS", "label": "NEURAL FORECASTING METHODS", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "type": "METHOD"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"FOUNDATION MODELS FOR TIME SERIES ANALYSIS\" refers to a specific type of foundation model designed for time series analysis. These models are a subset of foundation models, which are a broader category of models that are pre-trained on large datasets and can be fine-tuned for various tasks.\n\nThe taxonomy proposed in this survey is used to classify foundation models for time series analysis, and it is based on key components including:\n\n1. Pre-training technique: This refers to the method used to pre-train the model on large datasets before fine-tuning it for time series analysis.\n2. Adaptation technique: This refers to the method used to adapt the pre-trained model to the specific task of time series analysis.\n3. Data modality: This refers to the type of data that the model is designed to handle, such as univariate or multivariate time series data.\n4. Model architecture: This refers to the underlying architecture of the model, such as the type of neural network used.\n\nFoundation models for time series analysis are related to time series analysis as they are designed specifically for this task. This survey provides a comprehensive review of foundation models specifically designed for time series analysis, and it aims to provide a framework for classifying and evaluating these models.\n\nOverall, the entity \"FOUNDATION MODELS FOR TIME SERIES ANALYSIS\" refers to a specific type of foundation model that is designed for time series analysis, and the taxonomy proposed in this survey provides a framework for classifying and evaluating these models based on key components such as pre-training technique, adaptation technique, data modality, and model architecture.", "id": "FOUNDATION MODELS FOR TIME SERIES ANALYSIS", "label": "FOUNDATION MODELS FOR TIME SERIES ANALYSIS", "shape": "dot", "size": 10, "source_id": "479fc10bea12b01a37fbc5cef21eea76,7a36c78af074ba651b0404f11cdf481d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of Yuxuan Liang:\n\nYuxuan Liang is a researcher who has made significant contributions to the field of time series analysis. Specifically, he is an author of a research paper presented at KDD \u002724, a prominent conference in the field of data science and artificial intelligence. Furthermore, Yuxuan Liang is also an author of the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\", which suggests that he has expertise in the area of foundation models and their applications in time series analysis. Overall, Yuxuan Liang\u0027s work in time series analysis and his publication record demonstrate his expertise and contributions to the field.", "id": "YUXUAN LIANG", "label": "YUXUAN LIANG", "shape": "dot", "size": 10, "source_id": "5bb0db923f70e5f431183c6ab7dc25dc,7a36c78af074ba651b0404f11cdf481d,8f4724ff6541b8924f0cebe9872ed040", "type": "PERSON"}, {"color": "#97c2fc", "description": "Haomin Wen is an author of the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\"", "id": "HAOMIN WEN", "label": "HAOMIN WEN", "shape": "dot", "size": 10, "source_id": "7a36c78af074ba651b0404f11cdf481d", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nYuqi Nie is a researcher and author who has contributed to the field of time series analysis. Specifically, Nie is an author of two notable papers: \"A time series is worth 64 words: Long-term forecasting with transformers\" and \"Foundation Models for Time Series Analysis: A Tutorial and Survey.\" These papers demonstrate Nie\u0027s expertise in time series forecasting and analysis, particularly in the application of transformer models and foundation models to this field. The use of technical terms such as \"time series,\" \"long-term forecasting,\" and \"transformers\" suggests that Nie\u0027s work is grounded in academic research and is likely to be of interest to experts in the field of machine learning and time series analysis.", "id": "YUQI NIE", "label": "YUQI NIE", "shape": "dot", "size": 10, "source_id": "7a36c78af074ba651b0404f11cdf481d,81b15ffb0d853301758618e61757cca9", "type": "PERSON"}, {"color": "#97c2fc", "description": "Yushan Jiang is an author of the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\"", "id": "YUSHAN JIANG", "label": "YUSHAN JIANG", "shape": "dot", "size": 10, "source_id": "7a36c78af074ba651b0404f11cdf481d", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of Ming Jin:\n\nMing Jin is a researcher with expertise in time series analysis and machine learning. He is a prolific author who has published papers on various topics related to his field of expertise. Specifically, Ming Jin has authored papers on \"Foundation Models for Time Series Analysis: A Tutorial and Survey\" and a survey on graph neural networks for time series. His work in machine learning is well-represented, as he has also published a paper on a topic related to this field. As a researcher and author, Ming Jin has demonstrated a strong understanding of complex concepts and has contributed to the advancement of knowledge in his area of specialization.\n\nRelevant information from the nearby text is not provided, but based on the descriptions, it can be inferred that Ming Jin\u0027s work is likely to be published in academic conferences and journals, such as ICLR, AAAI, and PMLR, as mentioned in the language analysis section.", "id": "MING JIN", "label": "MING JIN", "shape": "dot", "size": 10, "source_id": "7a36c78af074ba651b0404f11cdf481d,8f4724ff6541b8924f0cebe9872ed040,97c1f318683bb63131ece0a51f096c5b,a73df99fe49b288e1c8751be2008b191", "type": "PERSON"}, {"color": "#97c2fc", "description": "Dongjin Song is an author of the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\"", "id": "DONGJIN SONG", "label": "DONGJIN SONG", "shape": "dot", "size": 10, "source_id": "7a36c78af074ba651b0404f11cdf481d", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nShirui Pan is a researcher who has made significant contributions to the field of time series analysis. Specifically, Pan is an author of two notable papers: \"Foundation Models for Time Series Analysis: A Tutorial and Survey\" and a paper on \"A survey on graph neural networks for time series.\" These publications demonstrate Pan\u0027s expertise in time series analysis and graph neural networks, highlighting their potential applications in this field. As a researcher, Pan\u0027s work is likely focused on developing and applying machine learning models to time series data, with a particular emphasis on long-term series forecasting and frequency analysis.", "id": "SHIRUI PAN", "label": "SHIRUI PAN", "shape": "dot", "size": 10, "source_id": "7a36c78af074ba651b0404f11cdf481d,8f4724ff6541b8924f0cebe9872ed040,97c1f318683bb63131ece0a51f096c5b", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of Qingsong Wen:\n\nQingsong Wen is a researcher and a prolific author in the field of time series analysis. He has authored several research papers, including \"Foundation Models for Time Series Analysis: A Tutorial and Survey\", \"Sadi: A self-adaptive decomposed interpretable framework for electric load forecasting under extreme events\", and \"Transformers in time series: A survey\". Additionally, he has published papers on Sadi: A self-adaptive decomposed interpretable framework for time series forecasting and A survey on graph neural networks for time series. His research focuses on time series forecasting, and he has demonstrated expertise in developing innovative frameworks and models for this area.", "id": "QINGSONG WEN", "label": "QINGSONG WEN", "shape": "dot", "size": 10, "source_id": "7a36c78af074ba651b0404f11cdf481d,8c4fb3f97d731ab00c60399045cd97bd,8f4724ff6541b8924f0cebe9872ed040,97c1f318683bb63131ece0a51f096c5b,a4bb4cfc468e2f87ad5d8a4b451bcbbf,a69a914fb6c895c7202532b69ad3e094", "type": "PERSON"}, {"color": "#97c2fc", "description": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining is a conference where the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\" was presented", "id": "ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING", "label": "ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING", "shape": "dot", "size": 10, "source_id": "7a36c78af074ba651b0404f11cdf481d", "type": "EVENT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe KDD \u002724 conference is a platform where research papers are presented and discussed. It is a conference where the paper was presented, indicating that it is a significant event for academic and research purposes. The conference likely focuses on data science and related fields, given the context of research papers being presented. \n\nThe language used in the text related to KDD \u002724 is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. This suggests that the conference is an international event, and the language used is formal and academic, typical of research papers and technical documents.", "id": "KDD \u201924", "label": "KDD \u201924", "shape": "dot", "size": 10, "source_id": "5bb0db923f70e5f431183c6ab7dc25dc,736a43d5fef4417bac9757301b4721c3,ef8fd6170b08af3bd99c9df44ffa8b57", "type": "EVENT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of Zhixuan Chu:\n\nZhixuan Chu is a researcher and author who has made significant contributions to the field of artificial intelligence, particularly in the areas of recommender systems and time series forecasting. He is an author of the paper \"Enhancing recommender systems with large language model reasoning graphs,\" which showcases his expertise in leveraging large language models for pre-trained recommender systems. Additionally, Zhixuan Chu has published research papers on time series forecasting, demonstrating his knowledge and experience in this area. His work has been recognized through publications in reputable academic conferences and journals, solidifying his position as a prominent figure in the field of artificial intelligence research.\n\nRelevant information from the nearby text:\n\n* The text mentions that Zhixuan Chu is an author who has published research papers on time series forecasting, which suggests that his work in this area is substantial and impactful.\n* The paper \"Enhancing recommender systems with large language model reasoning graphs\" is likely a key contribution to the field of recommender systems, and Zhixuan Chu\u0027s work on this topic has been recognized through publication in a reputable academic conference or journal.\n* The fact that Zhixuan Chu has published research papers on time series forecasting and recommender systems suggests that he is a versatile researcher with expertise in multiple areas of artificial intelligence.\n\nOverall, Zhixuan Chu is a respected researcher and author who has made significant contributions to the field of artificial intelligence, particularly in the areas of recommender systems and time series forecasting.", "id": "ZHIXUAN CHU", "label": "ZHIXUAN CHU", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,8f4724ff6541b8924f0cebe9872ed040,a69a914fb6c895c7202532b69ad3e094,a73df99fe49b288e1c8751be2008b191,ec3fbfb800fd9bf1d913584fda4ae925", "type": "PERSON"}, {"color": "#97c2fc", "description": "Nam H Nguyen is an author of the paper \"A time series is worth 64 words: Long-term forecasting with transformers\"", "id": "NAM H NGUYEN", "label": "NAM H NGUYEN", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Phanwadee Sinthong is an author of the paper \"A time series is worth 64 words: Long-term forecasting with transformers\"", "id": "PHANWADEE SINTHONG", "label": "PHANWADEE SINTHONG", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jayant Kalagnanam is an author of the paper \"A time series is worth 64 words: Long-term forecasting with transformers\"", "id": "JAYANT KALAGNANAM", "label": "JAYANT KALAGNANAM", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Huan Yee Koh is an author of the paper on A survey on graph neural networks for time series", "id": "HUAN YEE KOH", "label": "HUAN YEE KOH", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Daniele Zambon is an author of the paper on A survey on graph neural networks for time series", "id": "DANIELE ZAMBON", "label": "DANIELE ZAMBON", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Cesare Alippi is an author of the paper on A survey on graph neural networks for time series", "id": "CESARE ALIPPI", "label": "CESARE ALIPPI", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Geoffrey I Webb is an author of the paper on A survey on graph neural networks for time series", "id": "GEOFFREY I WEBB", "label": "GEOFFREY I WEBB", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Irwin King is an author of the paper on A survey on graph neural networks for time series", "id": "IRWIN KING", "label": "IRWIN KING", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nBarcelona is a location that hosts significant conferences in the field of data mining and knowledge discovery. Specifically, it is the location of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, which is a prominent event in the data science community. Additionally, Barcelona is also the location of the conference KDD \u002724, suggesting that it is a recurring event that attracts experts in the field. The city\u0027s reputation as a hub for data science and knowledge discovery is further reinforced by its hosting of these conferences, which brings together researchers and practitioners to share their work and ideas.\n\nThe summary is based on the information provided in the description list, which collectively paint a picture of Barcelona as a city that is deeply connected to the world of data mining and knowledge discovery. The use of specific conference names and dates adds a sense of specificity and credibility to the summary, making it clear that Barcelona is a location that is well-established in the data science community.", "id": "BARCELONA", "label": "BARCELONA", "shape": "dot", "size": 10, "source_id": "736a43d5fef4417bac9757301b4721c3,7a36c78af074ba651b0404f11cdf481d,ef8fd6170b08af3bd99c9df44ffa8b57", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"SPAIN\" is a country where the conference KDD \u002724 was held. This information is supported by two separate descriptions, both of which confirm that Spain is the location of the conference. \n\nGiven the consistency of the descriptions, there are no contradictions to resolve. The summary is written in the third person and includes the entity name for context. \n\nAdditional information from the nearby text is not provided, but the summary is enriched with relevant information from the given descriptions.", "id": "SPAIN", "label": "SPAIN", "shape": "dot", "size": 10, "source_id": "736a43d5fef4417bac9757301b4721c3,ef8fd6170b08af3bd99c9df44ffa8b57", "type": "LOCATION"}, {"color": "#97c2fc", "description": "", "id": "MACHINE LEARNING", "label": "MACHINE LEARNING", "shape": "dot", "size": 10, "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "type": ""}, {"color": "#97c2fc", "description": "Forecasting science is a field that has fallen short of fulfilling the promises of genuinely universal pre-trained models", "id": "FORECASTING SCIENCE", "label": "FORECASTING SCIENCE", "shape": "dot", "size": 10, "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "type": "FIELD"}, {"color": "#97c2fc", "description": "Pre-trained models are models that have been trained on a large corpus of data and can be fine-tuned for specific tasks", "id": "PRE-TRAINED MODEL", "label": "PRE-TRAINED MODEL", "shape": "dot", "size": 10, "source_id": "6771b6279846e780d6807b15184ae008", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"COMPUTER VISION\" can be generated as follows:\n\nComputer vision is a field of study that deals with the interaction between computers and visual data. It is a field where advanced models are used to enable machines to interpret and understand visual information from the world. This field of study focuses on enabling machines to interpret and understand visual information from the world, which is a critical aspect of computer vision.\n\nIn essence, computer vision is a multidisciplinary field that combines computer science, mathematics, and engineering to develop algorithms and models that can process and analyze visual data from various sources, such as images and videos. The primary goal of computer vision is to enable machines to understand and interpret visual information, which has numerous applications in various fields, including robotics, healthcare, security, and more.\n\nOverall, computer vision is a rapidly evolving field that has the potential to revolutionize the way machines interact with and understand the visual world.", "id": "COMPUTER VISION", "label": "COMPUTER VISION", "shape": "dot", "size": 10, "source_id": "7dbc961fe1959f844ebac283498e7fb0,8f4724ff6541b8924f0cebe9872ed040,f92205091f8d3b7e1e60b9bc80883a62", "type": "FIELD"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe entity \"TRAJECTORIES\" refers to a type of time series that involves movement or change over time. This entity is characterized by its dynamic nature, where data points are collected at regular intervals to track changes or movements over a period. The description of trajectories as a type of time series suggests that they can be analyzed using various techniques, such as frequency analysis and long-term series forecasting, to identify patterns and trends.\n\nGiven the formal and academic tone of the language used, it is likely that the concept of trajectories is being discussed in the context of research or technical documentation, possibly in the field of machine learning or data analysis. The mention of technical terms such as \"multi-head cross-attention\" and references to academic papers and conferences (e.g., ICLR, AAAI, PMLR) further support this interpretation.\n\nOverall, the summary provides a clear understanding of the entity \"TRAJECTORIES\" as a type of time series that involves movement or change over time, and its potential applications in research or technical documentation.", "id": "TRAJECTORIES", "label": "TRAJECTORIES", "shape": "dot", "size": 10, "source_id": "79b26808691b6333aafce43c5de531f7,ef8fd6170b08af3bd99c9df44ffa8b57", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"BERT\":\n\nBERT is a pre-trained language model designed to handle natural language processing tasks. It is a type of Transformer model that has revolutionized text understanding and generation tasks. Specifically, BERT is a model for pre-training of deep bidirectional transformers for language understanding, making it a foundation model that has significantly impacted the field of natural language processing.\n\nThe summary is written in third person and includes information from all the descriptions provided. It also resolves any potential contradictions and provides a coherent description of the entity \"BERT\".", "id": "BERT", "label": "BERT", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca,79b26808691b6333aafce43c5de531f7,83b49bf68dab6c36bdc09f02a63803fe,a5d60c1a355b77187003182f6df57646,a73df99fe49b288e1c8751be2008b191", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of GPT-3 can be generated as follows:\n\nGPT-3 is a pre-trained, large language model developed by OpenAI, which has revolutionized text understanding and generation tasks. It is a type of foundation model that can perform well on a diverse range of natural language processing (NLP) tasks in a few-shot or even zero-shot setting. Specifically, GPT-3 is mentioned in the text, RWC+19, and is known for its decoder-only architecture, making it a popular language model used for various NLP tasks.\n\nThis summary incorporates information from all the descriptions, resolving any potential contradictions and providing a coherent overview of GPT-3. The entity name \"GPT-3\" is included to provide context, and relevant information from the nearby text is used to enrich the summary.", "id": "GPT-3", "label": "GPT-3", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26,1f1221583d838c2407fe9864225e9eda,2bc70082c142ee2ef5d6a941611505b7,79b26808691b6333aafce43c5de531f7,7e69d9444a2084b6452e291735bf5a49,b67d18d306fde251ee94b0a831d1e075,ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "CLIP is a type of foundation model that has propelled advancements in image recognition, object detection, and more", "id": "CLIP", "label": "CLIP", "shape": "dot", "size": 10, "source_id": "79b26808691b6333aafce43c5de531f7", "type": "MODEL"}, {"color": "#97c2fc", "description": "SAM is a type of foundation model that has propelled advancements in image recognition, object detection, and more", "id": "SAM", "label": "SAM", "shape": "dot", "size": 10, "source_id": "79b26808691b6333aafce43c5de531f7", "type": "MODEL"}, {"color": "#97c2fc", "description": "FMs (Foundation Models) are pre-trained models that can be adapted to specific tasks using few-shot learning", "id": "FMS", "label": "FMS", "shape": "dot", "size": 10, "source_id": "736a43d5fef4417bac9757301b4721c3", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"DATA MODALITY\" can be described as follows:\n\nThe entity \"DATA MODALITY\" refers to the type or format of data used to train or fine-tune a machine learning model. This includes the specific characteristics or features of the data, such as its structure, content, or source. Data modality is a crucial aspect of machine learning, as it determines the type of model that can be used and the performance that can be expected.\n\nIn the context of machine learning, data modality can refer to various types of data, including but not limited to, text, images, audio, and time series data. The choice of data modality depends on the specific problem being addressed and the type of model being used. For example, a model trained on text data may not perform well on image data, and vice versa.\n\nData modality is closely related to the concept of foundation models, which are pre-trained models that can be adapted to different tasks and domains. The data modality used to train or fine-tune a foundation model can significantly impact its performance and ability to generalize to new tasks.\n\nOverall, data modality is a critical aspect of machine learning, and understanding its characteristics and implications is essential for developing effective machine learning models.\n\nRelevant information from the nearby text:\n\n* The text mentions that data modality is used to train or fine-tune machine learning models, which suggests that it is a key aspect of model development.\n* The text also mentions foundation models, which are pre-trained models that can be adapted to different tasks and domains. This implies that data modality plays a crucial role in determining the performance and generalizability of these models.\n* The text does not provide any information that contradicts the description of data modality, and therefore, the description remains consistent with the provided information.", "id": "DATA MODALITY", "label": "DATA MODALITY", "shape": "dot", "size": 10, "source_id": "479fc10bea12b01a37fbc5cef21eea76,736a43d5fef4417bac9757301b4721c3", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Adaptation refers to the process of modifying or updating foundation models to specific tasks", "id": "ADAPTATION", "label": "ADAPTATION", "shape": "dot", "size": 10, "source_id": "736a43d5fef4417bac9757301b4721c3", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Foundation models on time series analysis refer to pre-trained models that can be fine-tuned for time series analysis tasks", "id": "FOUNDATION MODELS ON TIME SERIES ANALYSIS", "label": "FOUNDATION MODELS ON TIME SERIES ANALYSIS", "shape": "dot", "size": 10, "source_id": "79c0a74b91761402b67c3574db02e8e7", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"SIMMTM\" can be generated as follows:\n\nSIMMTM is a masked pretraining framework specifically designed for time series analysis. It is a pre-trained model that has been utilized in various time series tasks, including forecasting and classification. The framework has been found to exhibit superior fine-tuning performance in these tasks, particularly when applied to cross-domain applications. This suggests that SIMMTM has the potential to be a valuable tool in the field of time series analysis, enabling researchers and practitioners to leverage its capabilities in a wide range of applications.\n\nThe use of masked pretraining in SIMMTM allows it to learn complex patterns and relationships within time series data, which can be leveraged to improve performance in downstream tasks. This approach is particularly effective in cross-domain applications, where pre-trained models can be fine-tuned to adapt to new domains and tasks. The results of this fine-tuning can lead to significant improvements in forecasting and classification accuracy, making SIMMTM a promising tool for time series analysis.\n\nOverall, SIMMTM is a powerful pre-trained model that has been designed to tackle the challenges of time series analysis. Its ability to learn complex patterns and relationships within time series data, combined with its superior fine-tuning performance in cross-domain applications, make it a valuable asset for researchers and practitioners working in this field.", "id": "SIMMTM", "label": "SIMMTM", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8,77c7ced6ab4a0e57fba604c1a3e00416,ad8efcfda80253036294f2eeb48926e8,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTIMER is a pre-trained model used in time series analysis. It is a research that advances the field by facilitating general time series analysis through single, large-scale pre-trained models. This suggests that TIMER is a cutting-edge model designed to handle complex time series data, enabling researchers and practitioners to perform various tasks such as forecasting, anomaly detection, and trend analysis.\n\nThe use of TIMER in time series analysis implies its potential applications in various domains, including finance, economics, weather forecasting, and healthcare. The fact that it is a pre-trained model indicates that it has been trained on a large dataset, allowing it to learn general patterns and relationships in time series data.\n\nFurthermore, the description of TIMER as a research that advances the field suggests that it is a novel and innovative approach to time series analysis. The use of single, large-scale pre-trained models implies that TIMER is a scalable and efficient solution for handling large datasets.\n\nOverall, TIMER appears to be a powerful tool for time series analysis, with potential applications in various domains and a scalable architecture that enables efficient processing of large datasets.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which is consistent with the formal and academic tone of the description.\n* The text also mentions technical terms, mathematical equations, and references to academic papers, which are all written in English, further supporting the conclusion that the description is written in English.\n* The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" in the description suggests that the text is from a research paper or a technical document, which is consistent with the formal and academic tone of the description.", "id": "TIMER", "label": "TIMER", "shape": "dot", "size": 10, "source_id": "ad8efcfda80253036294f2eeb48926e8,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "UniTS is a research that further advances the field by facilitating general time series analysis through single, large-scale pre-trained models", "id": "UNITS", "label": "UNITS", "shape": "dot", "size": 10, "source_id": "ad8efcfda80253036294f2eeb48926e8", "type": "RESEARCH"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"OFA\" is a model or algorithm used for time series forecasting. Specifically, it is a pre-trained model used in time series analysis, which adapts pre-trained models, such as Large Language Models (LLMs), for broad time series analysis. This suggests that OFA is a research model that leverages the capabilities of pre-trained models to improve time series forecasting. The use of OFA is mentioned in the text as a specific model, indicating its relevance and importance in the field of time series analysis.\n\nThe summary is based on the following information collected from all the descriptions:\n\n* OFA is a model or algorithm used for time series forecasting (Description 2)\n* OFA is a model used for time series forecasting (Description 3)\n* OFA is a pre-trained model used in time series analysis (Description 4)\n* OFA is a research that adapts pre-trained models, such as LLMs, for broad time series analysis (Description 5)\n* OFA is a specific model mentioned in the text, as indicated by the use of the name \"OFA\" (Description 6)\n\nThe contradictions between the descriptions have been resolved by considering the common theme of time series forecasting and analysis, and the use of pre-trained models. The summary provides a clear and concise description of the entity \"OFA\" and its relevance in the field of time series analysis.", "id": "OFA", "label": "OFA", "shape": "dot", "size": 10, "source_id": "6fa5f635ad5f7e6b67d5e467f130345c,79c41aff65d584b4c8d4c769b82756d5,990578022879395d00b7a5b229863c2f,ad8efcfda80253036294f2eeb48926e8,bb10b1a7f98a4f869b7abbc5f9632dd1,bb87457fce8d4214bfe1f398b7ea35f2", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTEST is a dataset and a pre-trained model used in time series analysis. It is mentioned in the text as a dataset and utilized in various models. Additionally, TEST is a research that adapts pre-trained models, such as Large Language Models (LLMs), for broad time series analysis. This suggests that TEST is a versatile entity that serves multiple purposes in the context of time series analysis, including being a dataset, a pre-trained model, and a research framework.\n\nThe use of TEST in time series analysis is further supported by the fact that it is mentioned alongside other technical terms and concepts, such as \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention.\" This indicates that TEST is a key component in the development and application of time series analysis models.\n\nOverall, TEST appears to be a multifaceted entity that plays a significant role in the field of time series analysis, serving as both a dataset and a pre-trained model that can be adapted for broad time series analysis.", "id": "TEST", "label": "TEST", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,ad8efcfda80253036294f2eeb48926e8,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "TFM is a research that utilizes graph structures and algorithms to analyze the behavior and interactions within transportation systems", "id": "TFM", "label": "TFM", "shape": "dot", "size": 10, "source_id": "ad8efcfda80253036294f2eeb48926e8", "type": "RESEARCH"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"ST-LLM\" can be generated as follows:\n\nST-LLM is a model that employs a novel partially frozen attention strategy for traffic prediction. It leverages spatial-temporal embeddings to capture the intricate dynamics of traffic data across space and time. This model combines spatio-temporal information with a partially frozen Large Language Model (LLM) to improve traffic predictions. The partially frozen attention strategy is a key component of ST-LLM, allowing it to effectively utilize the strengths of both spatial-temporal information and LLMs in predicting traffic patterns.\n\nThe entity \"ST-LLM\" is mentioned in the text as a model that has been developed to address the complexities of traffic prediction. By incorporating spatial-temporal embeddings and a partially frozen LLM, ST-LLM is able to provide more accurate and informative predictions of traffic patterns. This suggests that ST-LLM has the potential to be a valuable tool for transportation planners and researchers seeking to improve traffic flow and reduce congestion.\n\nOverall, ST-LLM is a cutting-edge model that has been designed to tackle the challenges of traffic prediction. Its innovative approach to combining spatial-temporal information and LLMs makes it a promising solution for improving traffic forecasting and analysis.", "id": "ST-LLM", "label": "ST-LLM", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831,77c7ced6ab4a0e57fba604c1a3e00416,ad8efcfda80253036294f2eeb48926e8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nDIFFSTG is a model that has been researched and applied in the field of probabilistic traffic forecasting. Specifically, it utilizes denoising diffusion models to analyze spatio-temporal graphs, which enables the prediction of traffic patterns and trends. The model\u0027s primary goal is to provide accurate and reliable forecasts, likely for urban planning, transportation management, or other related applications.\n\nThe research behind DIFFSTG is formal and academic in nature, as indicated by the use of technical terms, mathematical equations, and references to academic papers. The language used is English, as supported by the presence of English words and phrases, mathematical notation, and citations of English-language academic papers.\n\nOverall, DIFFSTG is a research model that has been developed to tackle the complex task of probabilistic traffic forecasting, leveraging the power of denoising diffusion models and spatio-temporal graph analysis.", "id": "DIFFSTG", "label": "DIFFSTG", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,ad8efcfda80253036294f2eeb48926e8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Trajectory data is a type of temporal dataset used in time series forecasting models", "id": "TRAJECTORY DATA", "label": "TRAJECTORY DATA", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "type": "DATA"}, {"color": "#97c2fc", "description": "Healthcare records are a type of temporal dataset used in time series forecasting models", "id": "HEALTHCARE RECORDS", "label": "HEALTHCARE RECORDS", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "type": "DATA"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"PRE-TRAINING\" can be described as follows:\n\n\"PRE-TRAINING refers to the initial phase of training a model, where it is trained on a large dataset to learn generalizable representations. Specifically, in the context of time series analysis, pre-training involves training a time series foundation model on unlabeled data, enabling it to learn patterns and relationships that can be leveraged for long-term series forecasting and other applications. This process is crucial for developing robust and accurate models, particularly in fields such as frequency analysis and multi-head cross-attention, where pre-trained models can serve as a foundation for further fine-tuning and adaptation to specific tasks.\"\n\nThis summary incorporates information from both descriptions, resolving any potential contradictions and providing a coherent and comprehensive overview of the entity \"PRE-TRAINING\".", "id": "PRE-TRAINING", "label": "PRE-TRAINING", "shape": "dot", "size": 10, "source_id": "9125a7d3794226f109debe90e5db041c,de8e097ce66fbc954bd529888cbc15ea", "type": "PHASE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nSELF-SUPERVISED LEARNING is a concept mentioned in the text, as indicated by the use of the phrase \"Self-supervised learning for time series analysis.\" It is a type of learning where the model is trained on a large dataset without any labeled data, and the primary goal is to learn representations from unlabeled data. This approach is particularly relevant in the context of time series analysis, where the model is trained to learn patterns and relationships within the data without the need for explicit labels.\n\nThe use of self-supervised learning in time series analysis is a key aspect of the text, and it is mentioned alongside other technical terms such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention.\" The text also references academic papers and authors, suggesting that the concept of self-supervised learning is being explored in the context of time series analysis.\n\nOverall, SELF-SUPERVISED LEARNING is a type of learning approach that is being explored in the context of time series analysis, where the model is trained on unlabeled data to learn representations and patterns without the need for explicit labels.\n\nRelevant information from the nearby text includes:\n\n* The text is written in English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers.\n* The text is formal and academic in tone, suggesting that it is from a research paper or technical document.\n* The text mentions other technical terms and concepts related to time series analysis, such as frequency analysis and multi-head cross-attention.\n\nThis information provides context for the concept of SELF-SUPERVISED LEARNING and highlights its relevance in the context of time series analysis.", "id": "SELF-SUPERVISED LEARNING", "label": "SELF-SUPERVISED LEARNING", "shape": "dot", "size": 10, "source_id": "9125a7d3794226f109debe90e5db041c,98c6b5003112ab7110e45414a2fa468b,c4e47033b8ecc85beacde9d560e66062", "type": "PHASE"}, {"color": "#97c2fc", "description": "Generative pre-training refers to the process of training a model to generate new data points that follow a specific distribution", "id": "GENERATIVE PRE-TRAINING", "label": "GENERATIVE PRE-TRAINING", "shape": "dot", "size": 10, "source_id": "9125a7d3794226f109debe90e5db041c", "type": "PHASE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"Contrastive learning\" is a self-supervised strategy used to enhance the robustness of pre-training time series foundation models. This approach involves training a model to distinguish between similar and dissimilar data points, which enables it to learn robust representations of the data. The primary application of contrastive learning is in the context of time series analysis, where it can be used to improve the performance of long-term series forecasting models. The use of contrastive learning in time series analysis is particularly relevant in fields such as frequency analysis and multi-head cross-attention, where it can be used to enhance the model\u0027s ability to capture complex patterns and relationships in the data.\n\nThe entity \"CONTRASTIVE LEARNING\" is a type of learning strategy that is used to improve the robustness of pre-training time series foundation models. It involves training a model to distinguish between similar and dissimilar data points, which enables it to learn robust representations of the data. This approach has been used in various applications, including time series analysis, frequency analysis, and multi-head cross-attention.\n\nOverall, contrastive learning is a powerful tool for improving the performance of time series models, and its use is likely to continue to grow in importance in the field of time series analysis.", "id": "CONTRASTIVE LEARNING", "label": "CONTRASTIVE LEARNING", "shape": "dot", "size": 10, "source_id": "9125a7d3794226f109debe90e5db041c,de8e097ce66fbc954bd529888cbc15ea", "type": "PHASE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"TIME SERIES FOUNDATION MODELS\" are a type of pre-trained model that leverages the foundation model paradigm to develop generalized models capable of understanding and forecasting time series data. These models are designed to be proficient in handling various time series tasks and can be fine-tuned for specific applications. The primary goal of time series foundation models is to harness the power of foundation models to create robust and adaptable models that can effectively analyze and predict time series data.\n\nThe use of foundation models in time series analysis is a promising approach, as it allows for the development of models that can learn general patterns and relationships in time series data, making them more effective in a wide range of applications. The pre-training of these models enables them to capture common features and structures in time series data, which can then be fine-tuned for specific tasks, such as forecasting or anomaly detection.\n\nOverall, the \"TIME SERIES FOUNDATION MODELS\" represent a significant advancement in the field of time series analysis, offering a powerful tool for understanding and predicting complex time series data.", "id": "TIME SERIES FOUNDATION MODELS", "label": "TIME SERIES FOUNDATION MODELS", "shape": "dot", "size": 10, "source_id": "de8e097ce66fbc954bd529888cbc15ea,f92205091f8d3b7e1e60b9bc80883a62", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "ZERO-SHOT FORECASTERS", "label": "ZERO-SHOT FORECASTERS", "shape": "dot", "size": 10, "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TIME-SERIES DATA\" refers to a type of data that is collected at regular intervals over time. This data is characterized by its temporal nature, where observations are made at fixed intervals, such as minutes, hours, days, or years. The regularity of the data collection process allows for the analysis of patterns, trends, and correlations over time, making it a valuable resource for various applications, including forecasting, prediction, and decision-making.\n\nThe summary is based on the provided description, which explicitly states that time-series data refers to data collected at regular intervals over time. This information is sufficient to provide a clear and concise description of the entity \"TIME-SERIES DATA\".", "id": "TIME-SERIES DATA", "label": "TIME-SERIES DATA", "shape": "dot", "size": 10, "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4,ff641d7e46d16e86c55d25c86b49bd52", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"TRANSFER LEARNING\" can be generated as follows:\n\nTransfer learning is a technique used in machine learning where a model trained on one task is utilized as a starting point for another task. This concept involves using a pre-trained model as a foundation for a new task, allowing for the fine-tuning of the model on the new task using the knowledge gained from the initial task. In essence, transfer learning enables the application of knowledge from one task to solve new tasks, thereby facilitating the adaptation of models to different tasks and domains.\n\nThe descriptions provided suggest that transfer learning is a technique that involves the reuse of a pre-trained model for a new task, which is a key aspect of machine learning. The use of a pre-trained model as a starting point for a new task allows for the efficient adaptation of the model to the new task, reducing the need for extensive training from scratch. This technique is particularly useful in scenarios where the amount of data available for the new task is limited, or when the new task is similar to the task for which the pre-trained model was originally trained.\n\nOverall, transfer learning is a powerful technique in machine learning that enables the transfer of knowledge from one task to another, facilitating the development of more efficient and effective models.\n\nRelevant information from the nearby text:\n\n* The text mentions the use of the phrase \"transfer learning\" as an indication of the concept\u0027s presence.\n* The text also mentions the use of pre-trained models as a starting point for new tasks, which is a key aspect of transfer learning.\n* The text does not provide any contradictory information regarding transfer learning, and the descriptions provided are consistent with the concept of transfer learning.\n\nNote: The summary generated is based on the provided descriptions and is written in third person to provide a clear and concise overview of the entity \"TRANSFER LEARNING\".", "id": "TRANSFER LEARNING", "label": "TRANSFER LEARNING", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26,6f6e27166a1506482bfcaf5585322595,7d5d82d600620153153772a9bc498ac0,89e5f6a93205d5e87ad7e7641c0bbb91,a73df99fe49b288e1c8751be2008b191,ff641d7e46d16e86c55d25c86b49bd52", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TRANSFORMER-BASED ARCHITECTURES\" refers to a type of neural network architecture used for sequence prediction and forecasting tasks. These architectures are models that particularly benefit from fine-tuning, which enhances their performance in domain-specific applications. Specifically, they utilize self-attention mechanisms to process sequential data, making them well-suited for tasks such as time series forecasting and long-term series forecasting.\n\nThe use of self-attention mechanisms in transformer-based architectures allows for frequency analysis and other advanced techniques, which can be particularly beneficial in applications such as multi-head cross-attention. This architecture has been widely used and studied in the field of machine learning, with many researchers citing its effectiveness in various domains.\n\nOverall, transformer-based architectures are a powerful tool for sequence prediction and forecasting tasks, and their ability to be fine-tuned for specific applications makes them a popular choice among researchers and practitioners alike.\n\nRelevant information from the nearby text includes:\n\n* The text is written in English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers.\n* The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n* The text mentions various technical terms and concepts, such as time series, long-term series forecasting, frequency analysis, and multi-head cross-attention, which are all relevant to the field of machine learning and time series forecasting.\n\nNote that there are no contradictions in the provided descriptions, and the summary is a coherent and accurate representation of the information provided.", "id": "TRANSFORMER-BASED ARCHITECTURES", "label": "TRANSFORMER-BASED ARCHITECTURES", "shape": "dot", "size": 10, "source_id": "55eb54ef455d14cd1a11760924f99eb8,83f62beddf5cf53ed2e2d4517569deb8,f912df936d0b735fe654d1a9c53caa3b", "type": "MODEL"}, {"color": "#97c2fc", "description": "Assimakopoulos \u0026 Nikolopoulos (2000) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Theta models\"", "id": "ASSIMAKOPOULOS \u0026 NIKOLOPOULOS (2000)", "label": "ASSIMAKOPOULOS \u0026 NIKOLOPOULOS (2000)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Croston (1972) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Croston\"", "id": "CROSTON (1972)", "label": "CROSTON (1972)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Syntetos \u0026 Boylan (2005) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Syntetos \u0026 Boylan\"", "id": "SYNTETOS \u0026 BOYLAN (2005)", "label": "SYNTETOS \u0026 BOYLAN (2005)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nHyndman \u0026 Athanasopoulos (2018) is an academic paper that is mentioned in the text. The mention of this paper is indicated by both a citation and the use of the phrase \"Hyndman \u0026 Athanasopoulos\" in the text. This suggests that the paper is a significant reference in the context of the text, which appears to be a research paper or technical document.\n\nThe language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to English-language academic papers and authors. The text is formal and academic in tone, suggesting that it is from a research paper or technical document.\n\nOverall, Hyndman \u0026 Athanasopoulos (2018) is a key reference in the text, and the text is written in English.", "id": "HYNDMAN \u0026 ATHANASOPOULOS (2018)", "label": "HYNDMAN \u0026 ATHANASOPOULOS (2018)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0,af36d1634490149b96980fb1dff57cd1", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Benidis et al. (2022) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Benidis et al.\"", "id": "BENIDIS ET AL. (2022)", "label": "BENIDIS ET AL. (2022)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Salinas et al. (2020) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Salinas et al.\"", "id": "SALINAS ET AL. (2020)", "label": "SALINAS ET AL. (2020)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Wen et al. (2018) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Wen et al.\"", "id": "WEN ET AL. (2018)", "label": "WEN ET AL. (2018)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe PRETRAINED LLM (Pre-trained Language Model) is a type of neural network model that has been trained on a large corpus of text data. This model has been specifically designed to leverage the knowledge and patterns present in the vast amount of text data it has been trained on, enabling it to perform a wide range of natural language processing tasks with high accuracy.\n\nThe PRETRAINED LLM is a model that has been trained on a large corpus of text data, which allows it to learn complex patterns and relationships within language. This training enables the model to generate coherent and contextually relevant text, making it a valuable tool for various applications such as language translation, text summarization, and question-answering systems.\n\nOverall, the PRETRAINED LLM is a powerful tool for natural language processing tasks, and its ability to learn from a large corpus of text data makes it a valuable asset for a wide range of applications.\n\nNote: The information provided is based solely on the descriptions given, and no additional information is available from the nearby text.", "id": "PRETRAINED LLM", "label": "PRETRAINED LLM", "shape": "dot", "size": 10, "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96,7dbc961fe1959f844ebac283498e7fb0", "type": "MODEL"}, {"color": "#97c2fc", "description": "Pre-trained foundation models have made impressive strides in NLP and CV", "id": "PRE-TRAINED FOUNDATION MODELS", "label": "PRE-TRAINED FOUNDATION MODELS", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "MODEL"}, {"color": "#97c2fc", "description": "Data sparsity refers to the lack of data or the scarcity of data", "id": "DATA SPARSITY", "label": "DATA SPARSITY", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "LLMs have demonstrated a remarkable capability for few-shot and zero-shot transfer learningFoundation language models like GPT-3, GPT-4, and Llama can perform well on a diverse range of NLP tasks in a few-shot or even zero-shot setting", "entity_type": "MODEL", "id": "FOUNDATION LANGUAGE MODELS", "label": "FOUNDATION LANGUAGE MODELS", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "MODEL"}, {"color": "#97c2fc", "description": "Time series modeling has not benefited from the same significant breakthroughs as foundation models", "id": "TIME SERIES MODELING", "label": "TIME SERIES MODELING", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "FIELD"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"LLMS\" (Large Language Models) can be generated as follows:\n\nLLMS, also known as Large Language Models, are a type of machine learning model designed to process and generate human-like language. They have demonstrated remarkable few-shot learning capabilities and impressive performance on various natural language processing tasks. These models are pre-trained and can be fine-tuned for specific tasks, such as time series forecasting, as seen in the application of Time-LLM. LLMS are large language models that have been trained on a vast corpus of text data, enabling them to seamlessly integrate with future advancements in language models. They are often used in natural language processing and computer vision, and have been leveraged in Chronos to improve data strategies and leverage developments in the area of LLMs.\n\nThe summary includes information from all the descriptions, resolving any potential contradictions and providing a coherent overview of the entity \"LLMS\". The language used is formal and academic, consistent with the primary language of the text being English.", "id": "LLMS", "label": "LLMS", "shape": "dot", "size": 10, "source_id": "42c90ca40b234c098c55632ec70038a1,53f80422fbf85856ecf940d5c2450665,6c273e9f841addeed2a33240ddaa3d96,6d66c2ea37e25b646a13d751c05a8e4d,7e69d9444a2084b6452e291735bf5a49,83f62beddf5cf53ed2e2d4517569deb8,a2f45b87ea2a0aa02b6e62e9700f20f1,a73df99fe49b288e1c8751be2008b191,ad8efcfda80253036294f2eeb48926e8,b27c89cb0db6646b1203b2701e017aeb,bc54a718d1886698232d578fd88c3ac7,de8e097ce66fbc954bd529888cbc15ea,f7ce89346ea6715560163ef3a630a5df", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"GPT-2\" can be generated as follows:\n\nGPT-2 is a pre-trained large language model that has been utilized in various applications, including time series forecasting, natural language processing tasks, and as a backbone in other models such as GPT4TS and Chronos. It is a type of decoder-only model that has been fine-tuned for general time series forecasting and has been compared with TIME-LLM in terms of average MSE reduction. GPT-2 has been shown to slightly outperform its variant GPT-2 (6) by 2.7% and has been used in frameworks such as OneFitsAll. The model is a popular language model that can be fine-tuned for various tasks and has been mentioned in the text as a model of interest.\n\nThis summary incorporates information from all the descriptions provided, resolving any potential contradictions and providing a coherent overview of the entity \"GPT-2\".", "id": "GPT-2", "label": "GPT-2", "shape": "dot", "size": 10, "source_id": "072b166b9a1b6afecf5874f45af61699,1ea4e7d0dd5128868533b4567a0a0273,1f1221583d838c2407fe9864225e9eda,3174231a67593609c727151c9df31d0a,63e284ec7e76fa859cc46b72d3746654,6f6e27166a1506482bfcaf5585322595,7e03744dea80e2138baff03611104fa8,b27c89cb0db6646b1203b2701e017aeb,bc54a718d1886698232d578fd88c3ac7,e6a6bc6fbd362394320961ac10cdd230,f0c52387a7b3a5c3850fe6991f0a7c83,f72ba51a17dd00cff43bcdda22c273e8,ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTimeGPT-1 is a type of model specifically designed for time series forecasting. It features an encoder-decoder structure with several transformer layers, which facilitates efficient zero-shot forecasting. This architecture enables TimeGPT-1 to effectively process and analyze time series data, making it a suitable choice for long-term series forecasting and frequency analysis tasks.\n\nThe model\u0027s design is likely influenced by the advancements in natural language processing (NLP) and machine learning, as indicated by the use of transformer layers and multi-head cross-attention mechanisms. TimeGPT-1 may have been developed in the context of academic research, given the presence of technical terms and references to academic papers, such as those published in conferences like ICLR, AAAI, and PMLR.\n\nOverall, TimeGPT-1 appears to be a cutting-edge model for time series forecasting, leveraging the strengths of transformer-based architectures to provide accurate and efficient predictions.", "id": "TIMEGPT-1", "label": "TIMEGPT-1", "shape": "dot", "size": 10, "source_id": "6f6e27166a1506482bfcaf5585322595,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Standardized large-scale datasets are collections of data that are widely used and accepted in a particular field or community", "id": "STANDARDIZED LARGE-SCALE DATASETS", "label": "STANDARDIZED LARGE-SCALE DATASETS", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "type": "RESOURCE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TIME SERIES DATASETS\" refers to a concept mentioned in the text, which is a type of data that consists of a sequence of values measured at regular time intervals. Specifically, time series datasets are collections of data points measured at regular time intervals, making them a crucial aspect of various applications, including long-term series forecasting, frequency analysis, and multi-head cross-attention.\n\nThe use of the term \"time series datasets\" in the text suggests that they are a fundamental concept in the field, and their characteristics are well-defined. The fact that they are mentioned alongside technical terms and mathematical equations further emphasizes their importance in the context of the text.\n\nOverall, time series datasets are a critical component of various data-driven applications, and their analysis and interpretation are essential for making informed decisions in fields such as finance, economics, and engineering.\n\nRelevant information from the nearby text includes:\n\n* The text is written in English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers.\n* The text is formal and academic, suggesting that it is from a research paper or a technical document.\n* The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports the conclusion that the text is written in English.\n\nBy combining the information from the entity descriptions and the nearby text, we can create a comprehensive summary that provides a clear understanding of the concept of time series datasets and their significance in the context of the text.", "id": "TIME SERIES DATASETS", "label": "TIME SERIES DATASETS", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,6212b2146d9ad27124114c334a946854,c4e47033b8ecc85beacde9d560e66062", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"CORPUS\" can be generated as follows:\n\nThe entity \"CORPUS\" refers to a large collection of text or data that is used to train models, specifically language models. It encompasses a diverse range of time series data, which is utilized to train foundation models and other machine learning models, such as Lag-Llama. The corpus serves as a crucial component in the development and training of these models, enabling them to learn from a vast and varied dataset.\n\nIn the context of machine learning and time series forecasting, a corpus of diverse time series data is used to train models, allowing them to capture patterns and relationships within the data. This corpus is a critical resource for researchers and developers working on language models, such as Lag-Llama, which relies on a large corpus of diverse time series data to train its foundation model.\n\nOverall, the entity \"CORPUS\" represents a significant collection of text or data that plays a vital role in the training and development of machine learning models, particularly those focused on language and time series analysis.", "id": "CORPUS", "label": "CORPUS", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,116332ac4538a1430c83a34fcbec22d1,1f1221583d838c2407fe9864225e9eda,ac37bdb991d1513e44e4c5fc7a28b187,c4e47033b8ecc85beacde9d560e66062,c7167cf92abe7513ccb936ca79871932", "type": "DATA"}, {"color": "#97c2fc", "description": "Pre-training data refers to a specific concept or measure related to data or features", "id": "PRE-TRAINING DATA", "label": "PRE-TRAINING DATA", "shape": "dot", "size": 10, "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"LLAMA\" can be generated as follows:\n\nLLAMA is a pre-trained language model that can be fine-tuned for general time series forecasting. It is a foundation model that can perform well on a diverse range of NLP tasks in a few-shot or even zero-shot setting. Specifically, LLAMA is a decoder-only transformer-based architecture used in Lag-Llama, which incorporates pre-normalization via the RMSNorm and Rotary Positional Encoding at each attention layer\u0027s query and key representations. This model can be used to fine-tune TIME-LLM and is also mentioned in the text as indicated by the use of the phrase \"LLAMA\". Overall, LLAMA is a versatile model that can be applied to various tasks, including time series forecasting and NLP tasks.\n\nNote that the descriptions provided are consistent and do not contain any contradictions. The summary is written in third person and includes the entity name \"LLAMA\" for context. Relevant information from the nearby text has been incorporated to provide a comprehensive understanding of the entity.", "id": "LLAMA", "label": "LLAMA", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a,50bf8b7f27ec843882dbc6eadb2bf158,55099ca8e21d1db3bdaf7bd04e590b72,5e4d9ca02ee6a285d5223c820743eb12,8c4fb3f97d731ab00c60399045cd97bd,b27c89cb0db6646b1203b2701e017aeb,b305a0d76a0159dc10338467e16d6761,b67d18d306fde251ee94b0a831d1e075,e6a6bc6fbd362394320961ac10cdd230", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"MULTI-LAYER PERCEPTRONS\" refers to a type of model used in time series forecasting, which has been utilized as the backbone for pre-training. Specifically, Multi-Layer Perceptrons (MLPs) are a type of neural network architecture that can be employed for both classification and regression tasks. This versatility in application is a key characteristic of MLPs, making them a valuable tool in various machine learning and time series forecasting contexts.\n\nIn the context of time series forecasting, MLPs have been used as a pre-training backbone, suggesting their potential for handling complex temporal data. Additionally, their ability to perform classification and regression tasks indicates their flexibility in addressing diverse machine learning problems. Overall, the entity \"MULTI-LAYER PERCEPTRONS\" encompasses a robust and adaptable neural network architecture with applications in time series forecasting and beyond.", "id": "MULTI-LAYER PERCEPTRONS", "label": "MULTI-LAYER PERCEPTRONS", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831,b9d22fe9f2eecb1665f4bea365c7d612", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"MEAN ABSOLUTE ERROR\" can be described as follows:\n\nThe \"MEAN ABSOLUTE ERROR\" (MAE) is a metric used to evaluate the accuracy of a prediction. It is utilized for evaluation metrics in various contexts, particularly in assessing the performance of predictive models. The MAE is calculated by taking the average of the absolute differences between predicted and actual values, providing a measure of the overall error or deviation between predictions and reality.\n\nThis description is based on the information provided in the description list, which collectively convey the definition, purpose, and calculation method of the \"MEAN ABSOLUTE ERROR\".", "id": "MEAN ABSOLUTE ERROR", "label": "MEAN ABSOLUTE ERROR", "shape": "dot", "size": 10, "source_id": "50eeacd99c68b2581be90310bedcbc2c,6f6e27166a1506482bfcaf5585322595", "type": "METRIC"}, {"color": "#97c2fc", "description": "Hyper-parameter tuning refers to the process of adjusting or optimizing the hyper-parameters of a model to improve its performance or accuracy", "id": "HYPER-PARAMETER TUNING", "label": "HYPER-PARAMETER TUNING", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"TFT\" can be generated as follows:\n\nTFT is a deep learning forecasting model that utilizes transformers, specifically an attention-based architecture with recurrent and feature-selection layers, to perform time series forecasting. It is a type of model that uses a temporal fusion transformer approach, which enables it to outperform local statistical models. TFT is a model mentioned in the text, as indicated by the use of the phrase \"WaveNet, DeepAR, N-BEATS, TFT, DLinear, PatchTST, N-HiTS, GPT4TS.\" It is used for forecasting and has been shown to be effective in this context.\n\nThe model is based on transformer-based models, which are a type of deep learning architecture that uses self-attention mechanisms to process input data. This allows TFT to capture complex patterns and relationships in time series data, making it a powerful tool for forecasting and prediction.\n\nOverall, TFT is a sophisticated deep learning model that has been designed to tackle the challenges of time series forecasting, and its use of transformers and attention-based mechanisms makes it a promising approach for this task.\n\nRelevant information from the nearby text includes:\n\n* The use of technical terms such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention,\" which suggests that the text is from a research paper or a technical document.\n* The presence of mathematical equations and formulas, which are written in a standard mathematical notation used in English-language academic papers.\n* The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals.\n* The citation of English-language academic papers and authors, which suggests that the text is written in English.\n\nThis information provides context for the entity \"TFT\" and highlights its relevance to the field of time series forecasting and deep learning.", "id": "TFT", "label": "TFT", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,12395cf4e8efa64a847ede9775ecdf3f,1dc665214307b1656d1a0094a1918ece,2565ae205d4c98342168bb67a4f7a309,2f3b2f69f8eb4f958c3ca793cae36581,7d5d82d600620153153772a9bc498ac0,7e69d9444a2084b6452e291735bf5a49,af36d1634490149b96980fb1dff57cd1,f0c52387a7b3a5c3850fe6991f0a7c83,f912df936d0b735fe654d1a9c53caa3b", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"PATCHTST\" can be generated as follows:\n\nPATCHTST is a deep learning forecasting model that uses transformers, specifically a task-specific Transformer model, for time series analysis and forecasting tasks. It is a recent state-of-the-art forecasting model that uses patch embeddings similar to GPT4TS and is a concurrent work that uses patch embeddings similar to GPT4TS. PATCHTST is a model used for comparison with TIME-LLM, and it is compared with TIME-LLM in terms of average enhancements. It is a model that performs better than local statistical models and is used for long-term forecasting tasks, short-term forecasting, and zero-shot forecasting. PATCHTST is a pre-trained model used in time series analysis and is a specific model used for evaluation. It is a state-of-the-art long horizon deep forecasting method and a univariate transformer-based method that uses patching to tokenize time series. PATCHTST models are a type of model that uses a patch-based approach to forecasting, and it is a type of model used for time-series forecasting.\n\nThe entity \"PATCHTST\" is mentioned in the text as a model used for time series forecasting, and it is compared with other models such as TIME-LLM and Chronos models. PATCHTST is a model that uses inductive bias and attention mechanisms, and it is a model that uses transfer learning in the semi-supervised setting. It is a model that is used for forecasting, and it is a model that performs well for time-series forecasting tasks.\n\nOverall, PATCHTST is a deep learning forecasting model that uses transformers for time series analysis and forecasting tasks, and it is a recent state-of-the-art forecasting model that uses patch embeddings similar to GPT4TS.", "id": "PATCHTST", "label": "PATCHTST", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,12395cf4e8efa64a847ede9775ecdf3f,1d6fb60c5060c25ae4791b03a0513a7f,1dc665214307b1656d1a0094a1918ece,1f1221583d838c2407fe9864225e9eda,2565ae205d4c98342168bb67a4f7a309,28b097d339554431fa14e113c98ed49e,2f3b2f69f8eb4f958c3ca793cae36581,376897a501ac50834f626fcc5fb13ece,39365aee753fb73130c208fdf4046bb7,3e937ba8de0e7eca993c50506ceb8f1f,41fe893a178ebc8790ef4da83da5ab6e,4ade7764ebe998b5f35a7fd2b58d4796,500710c8a95f1310d383fa71fd806c1c,51ee17c1f0212d4c94010d8e376f649b,56de1d5a5b467101344afa4248d829dc,5e4d9ca02ee6a285d5223c820743eb12,5fa25d3abec59ccd2718e00c0e0eb440,673b0feee6eb5843954defc670e4ba29,6fa5f635ad5f7e6b67d5e467f130345c,74527a4337ed6919731be520311ae774,79c41aff65d584b4c8d4c769b82756d5,7d5d82d600620153153772a9bc498ac0,7e03744dea80e2138baff03611104fa8,7e69d9444a2084b6452e291735bf5a49,84bc2afcbbd278961c3c7a637c6a189e,892b821ed44c13c4d09e0ceedac3051d,91e161ba596a0cbbcae541ddb2106310,9ba0189af2ef0720a721c16eef0f0788,ab91381dd032db318e5aec1ad5b914a6,af36d1634490149b96980fb1dff57cd1,bb10b1a7f98a4f869b7abbc5f9632dd1,bc54a718d1886698232d578fd88c3ac7,d40f5dc25597e4e4d7c30b8bfd98f89a,d4551c2839eaa68a7cb7324089956581,d4e3d8b5bf043b78bb9f1551080cab91,ef2ef6c95c36f51706c54ca3f2893641,efb7975581b22620b8277417edeee3e9,f0c52387a7b3a5c3850fe6991f0a7c83,f49330b6fd81d86d14e7a9d4b8e45576,f6fac8e5c6fd12724fe3aa84a2e1cfa6,f98d2bec31738be3f7be750b5fe6180e,fd1092903d83bf6e90a6caa371d7c514,ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "Neural forecasting is a concept mentioned in the text, as indicated by the use of phrases such as \"neural forecasting\" and \"machine learning\"", "id": "NEURAL FORECASTING", "label": "NEURAL FORECASTING", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Muxi Chen is an author mentioned in the text, as indicated by the citation of their paper", "id": "MUXI CHEN", "label": "MUXI CHEN", "shape": "dot", "size": 10, "source_id": "98c6b5003112ab7110e45414a2fa468b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"FINANCE\" can be generated as follows:\n\nThe entity \"FINANCE\" refers to a specific domain or category of data or information that TimeGPT can handle. It encompasses the management of money and investments, including banking, investing, and financial markets. Finance is also the study of the management and analysis of financial data, including stock prices, investments, and financial markets. This domain involves the analysis of various financial metrics and trends to inform investment decisions, risk management, and other financial strategies.\n\nIn the context of TimeGPT, finance is a domain that can be handled through various techniques, including time series analysis and forecasting. This involves the use of mathematical equations and formulas to model and predict financial trends, such as stock prices and market indices. The study of finance also involves the use of frequency analysis and multi-head cross-attention mechanisms to identify patterns and relationships in financial data.\n\nOverall, the entity \"FINANCE\" is a broad and complex domain that involves the management, analysis, and study of financial data and markets. It is a key area of interest for TimeGPT, which can be used to analyze and predict financial trends and inform investment decisions.\n\nRelevant information from the nearby text that was used to enrich this summary includes:\n\n* The use of technical terms such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention\" to describe the techniques used in finance.\n* The presence of mathematical equations and formulas to model and predict financial trends.\n* The citation of English-language academic papers and authors to support the analysis and study of finance.\n* The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" to describe academic conferences and journals related to finance.", "id": "FINANCE", "label": "FINANCE", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458,4eb417cb4bd15ceba2949a1358623cb8,518bfcd6711530089fe3914ca16459c2,5bb0db923f70e5f431183c6ab7dc25dc", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"HEALTHCARE\" can be generated as follows:\n\nThe entity \"HEALTHCARE\" refers to the maintenance and improvement of physical and mental health, including medical treatment and health services. It encompasses the study of the management and analysis of health-related data, including medical records, patient outcomes, and disease diagnosis. As a domain, healthcare is mentioned in the text and is handled by TimeGPT, indicating its relevance and importance in various models and applications.\n\nThis summary incorporates information from all the descriptions, resolving any potential contradictions and providing a coherent overview of the entity \"HEALTHCARE\". The summary highlights the multifaceted nature of healthcare, including its clinical and analytical aspects, as well as its relevance in various models and applications.\n\nRelevant information from the nearby text, such as the use of technical terms and mathematical equations, suggests that the analysis of healthcare data involves complex methods and techniques. The mention of TimeGPT\u0027s ability to handle healthcare as a domain implies that the entity is of significant interest and importance in the context of data analysis and modeling.\n\nOverall, the summary provides a comprehensive understanding of the entity \"HEALTHCARE\" and its relevance in the context of data analysis and modeling.", "id": "HEALTHCARE", "label": "HEALTHCARE", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8,518bfcd6711530089fe3914ca16459c2,5bb0db923f70e5f431183c6ab7dc25dc,77c7ced6ab4a0e57fba604c1a3e00416", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"ENERGY\" can be generated as follows:\n\nThe entity \"ENERGY\" refers to the capacity to do work or cause change, and it is also a domain that encompasses the study of the management and analysis of energy-related data, including energy consumption, production, and distribution. This domain is handled by TimeGPT, which can be fine-tuned for forecasting tasks and is used to label datasets. The domain of energy consists of time series datasets related to energy, making it a suitable area for applying time series analysis techniques. Overall, the entity \"ENERGY\" is a multifaceted concept that encompasses both the fundamental physical property and the domain of study related to energy data.\n\nThis summary incorporates information from all the provided descriptions, resolves any potential contradictions, and provides a coherent and enriched description of the entity \"ENERGY\".", "id": "ENERGY", "label": "ENERGY", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,3ba0bc9230706fb8f4a61d16ecf8fd26,4c09f35749179fe18c7d7290eaa57955,4eb417cb4bd15ceba2949a1358623cb8,518bfcd6711530089fe3914ca16459c2,5bb0db923f70e5f431183c6ab7dc25dc,ec7705b83cf4fe3aa18662c917b18c1a", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"ECONOMICS\" can be generated as follows:\n\n\"Economics is a domain that TimeGPT can handle, consisting of time series datasets related to economics, which is used for fine-tuning forecasting tasks. It is the study of how societies allocate resources, including the production, distribution, and consumption of goods and services. This domain encompasses the analysis of economic data to understand the behavior of economic systems, making it a crucial area for time series forecasting and long-term series forecasting. The study of economics involves frequency analysis and the application of techniques such as multi-head cross-attention to understand complex economic relationships. As a result, economics is a vital domain for TimeGPT to handle, enabling it to provide accurate forecasts and insights for various economic tasks.\"\n\nThis summary incorporates information from all the provided descriptions, resolving any potential contradictions and providing a coherent overview of the entity \"ECONOMICS\".", "id": "ECONOMICS", "label": "ECONOMICS", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,3ba0bc9230706fb8f4a61d16ecf8fd26,4eb417cb4bd15ceba2949a1358623cb8,518bfcd6711530089fe3914ca16459c2,ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"EXCHANGE RATE\" refers to a concept in economics that represents the value of one currency in terms of another currency, often used in international trade and finance. It is a dataset of finance data that contains daily exchange rates for currencies of eight countries between 1990 and 2016. This dataset is used for pretraining and fine-tuning various forecasting tasks, including fine-tuning forecasting tasks, and is a traffic metric used for this purpose.\n\nThe exchange rate is a fundamental concept in economics that plays a crucial role in international trade and finance. It is a measure of the value of one currency in terms of another, and it is used to determine the relative value of different currencies. The exchange rate is influenced by a variety of factors, including economic indicators, interest rates, and political events.\n\nThe dataset of exchange rates is a valuable resource for researchers and practitioners in the field of finance and economics. It can be used to develop and test various models for forecasting exchange rates, and to analyze the relationships between exchange rates and other economic variables. The dataset is also useful for pretraining and fine-tuning machine learning models, as it provides a large and diverse set of data that can be used to improve the performance of these models.\n\nOverall, the entity \"EXCHANGE RATE\" is a complex and multifaceted concept that plays a critical role in international trade and finance. It is a valuable resource for researchers and practitioners in the field of finance and economics, and it has a wide range of applications in fields such as machine learning and time series forecasting.", "id": "EXCHANGE RATE", "label": "EXCHANGE RATE", "shape": "dot", "size": 10, "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26,4c09f35749179fe18c7d7290eaa57955,4eb417cb4bd15ceba2949a1358623cb8,9e88afa28686ff93769bfc5eb0f1095e,d4bc1397526f236029876d4fbc22a721,e067234b24b0625a3f95ecc18035f915,ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"DOMAINS\" refers to categories used to group time series datasets. Specifically, several domains are represented in the time series data used to train Lag-Llama. These domains serve as a way to organize and structure the data, allowing for more effective analysis and forecasting. The use of domains in time series data is a common practice in machine learning and data analysis, enabling researchers and practitioners to identify patterns and relationships within the data.\n\nIn the context of Lag-Llama, the domains are likely used to improve the model\u0027s performance and accuracy in forecasting time series data. By training the model on data from multiple domains, Lag-Llama can learn to recognize and adapt to different patterns and trends, leading to more reliable and accurate predictions.\n\nOverall, the entity \"DOMAINS\" plays a crucial role in the organization and analysis of time series data, and its use in Lag-Llama is an important aspect of the model\u0027s development and training.", "id": "DOMAINS", "label": "DOMAINS", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,1b48e9ca066ac5ba037066bb762d3458,c7167cf92abe7513ccb936ca79871932", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"METS\":\n\nMETS is a pre-trained model used in time series analysis, specifically designed for time series forecasting. It employs a trainable ECG encoder alongside a frozen language model, which enables it to effectively analyze and forecast time series data. As a model mentioned in the text, METS is a notable entity in the context of time series forecasting and analysis, and its unique architecture and capabilities make it a valuable tool for researchers and practitioners in this field.\n\nThis summary is based on the information provided in the description list, which collectively paint a picture of METS as a sophisticated model for time series forecasting and analysis. The use of a trainable ECG encoder and a frozen language model suggests that METS is designed to leverage both the temporal and contextual aspects of time series data, making it a powerful tool for forecasting and analysis.", "id": "METS", "label": "METS", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,77c7ced6ab4a0e57fba604c1a3e00416,ad8efcfda80253036294f2eeb48926e8,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"HOSPITAL\" can be generated as follows:\n\nThe \"HOSPITAL\" dataset is a collection of monthly time series data representing patient counts related to medical products from January 2000 to December 2006. This dataset is used to evaluate the performance of the Chronos model, which suggests that it is a benchmark or evaluation dataset for time series forecasting models. The \"HOSPITAL\" dataset or metric is related to hospital data, which implies that it contains information about hospital operations, patient care, or medical treatment. Furthermore, the term \"HOSPITAL\" refers to a medical facility that provides medical care and treatment to patients, indicating that the dataset is likely related to healthcare or medical research.\n\nIn terms of the characteristics of the dataset, it appears to be a time series dataset with monthly frequency, spanning a period of 7 years (2000-2006). The dataset contains information about patient counts related to medical products, which may be useful for understanding trends, patterns, or correlations in healthcare data.\n\nOverall, the \"HOSPITAL\" dataset is a valuable resource for researchers and practitioners working in the field of time series forecasting, healthcare, or medical research, particularly those interested in evaluating the performance of machine learning models on real-world datasets.", "id": "HOSPITAL", "label": "HOSPITAL", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8,57a3473e2c7c112cf81d520ea1ba95fa,85e4fbea9a08a0a663f3c5dc3f3a95a7,d4bc1397526f236029876d4fbc22a721", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Cloud computing refers to the study of the management and analysis of data and applications in a cloud-based environment", "id": "CLOUD COMPUTING", "label": "CLOUD COMPUTING", "shape": "dot", "size": 10, "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"LONDON SMART METERS\" entity refers to a database and a dataset of energy consumption data from smart meters in London. Specifically, it is a dataset used for pretraining and fine-tuning, which suggests its application in machine learning and time series forecasting. The dataset contains energy usage data from smart meters in London, making it a valuable resource for researchers and analysts interested in energy consumption patterns and long-term series forecasting. The entity can be classified into two categories: a database and a dataset, both of which are related to energy consumption data from smart meters in London.\n\nThe summary is based on the following information collected from the descriptions:\n\n* The entity is a database and a dataset of energy consumption data.\n* The data is from smart meters in London.\n* The dataset is used for pretraining and fine-tuning.\n* The entity is related to energy consumption patterns and long-term series forecasting.\n\nThere are no contradictions in the descriptions, and the summary is a coherent and concise representation of the information provided.", "id": "LONDON SMART METERS", "label": "LONDON SMART METERS", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,4eb417cb4bd15ceba2949a1358623cb8,e067234b24b0625a3f95ecc18035f915,ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Solar (5 Min., Hourly) is a database of solar power generation data, updated at 5-minute and hourly frequencies", "id": "SOLAR (5 MIN., HOURLY)", "label": "SOLAR (5 MIN., HOURLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "DATABASE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe Australian Electricity Demand dataset is a collection of data used for pretraining and fine-tuning purposes. It refers to a specific measure or indicator of energy usage in Australia, providing valuable insights into the country\u0027s electricity demand patterns. This dataset is likely to be of interest to researchers and practitioners in the field of energy forecasting, who can leverage it to develop and evaluate models for long-term series forecasting, frequency analysis, and other related tasks.\n\nGiven the formal and academic tone of the language used, it is likely that this dataset is being used in a research context, possibly in a paper or technical document. The use of technical terms such as \"time series\" and \"multi-head cross-attention\" suggests that the dataset is being used in a machine learning or deep learning framework.\n\nOverall, the Australian Electricity Demand dataset appears to be a valuable resource for researchers and practitioners interested in energy forecasting and related topics.", "id": "AUSTRALIAN ELECTRICITY DEMAND", "label": "AUSTRALIAN ELECTRICITY DEMAND", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,ec7705b83cf4fe3aa18662c917b18c1a", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"WIND FARMS\":\n\nThe entity \"WIND FARMS\" refers to a collection of data related to energy production from wind farms in Australia. This dataset is utilized for pretraining and fine-tuning purposes, suggesting its significance in machine learning model development. The data likely contains time series information, such as energy production levels over time, which can be analyzed using techniques like long-term series forecasting and frequency analysis. The dataset may also involve the use of advanced machine learning architectures, such as those incorporating multi-head cross-attention mechanisms.\n\nGiven the context of pretraining and fine-tuning, it is likely that the dataset is used to develop and improve machine learning models for various applications, including but not limited to, energy production forecasting and optimization. The dataset\u0027s use of English-language abbreviations and citations suggests that it is part of an academic or research context, possibly related to conferences or journals such as ICLR, AAAI, or PMLR.\n\nOverall, the \"WIND FARMS\" dataset appears to be a valuable resource for researchers and developers working on energy-related machine learning applications, particularly those focused on time series forecasting and analysis.", "id": "WIND FARMS", "label": "WIND FARMS", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"AIR QUALITY\" refers to the measure of the cleanliness of the air in a given environment. It is a domain used for fine-tuning forecasting tasks, specifically involving time series datasets related to air quality. These datasets contain information about various air quality metrics, which are essential for understanding and predicting the cleanliness of the air in a particular environment.\n\nIn the context of machine learning and time series forecasting, the \"AIR QUALITY\" domain is crucial for developing accurate models that can predict air quality metrics, such as particulate matter (PM), ozone (O3), nitrogen dioxide (NO2), and sulfur dioxide (SO2). These predictions are essential for informing decision-making processes related to air quality management, public health, and environmental policy.\n\nThe \"AIR QUALITY\" domain involves various techniques, including frequency analysis, long-term series forecasting, and multi-head cross-attention, which are used to extract relevant features and patterns from the time series datasets. These techniques enable the development of robust and accurate models that can capture the complex relationships between air quality metrics and environmental factors.\n\nOverall, the \"AIR QUALITY\" entity is a critical domain in the field of machine learning and time series forecasting, with significant implications for public health, environmental policy, and sustainable development.", "id": "AIR QUALITY", "label": "AIR QUALITY", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,3ba0bc9230706fb8f4a61d16ecf8fd26,ec7705b83cf4fe3aa18662c917b18c1a", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"INSTANCES\" can be described as follows:\n\nINSTANCES refer to specific examples or cases of a concept or class, which are used to train a machine learning model. These instances are comprised of a number of data points, often referred to as the number of instances, which are utilized to develop and improve the model\u0027s accuracy and performance.\n\nIn the context of machine learning, instances are a fundamental component of the training process, as they provide the model with the necessary information to learn patterns and relationships within the data. The number of instances used to train a model can significantly impact its performance, with more instances often leading to better model accuracy and generalizability.\n\nOverall, INSTANCES play a crucial role in the development and evaluation of machine learning models, and their characteristics and properties are essential to understanding the performance and limitations of these models.", "id": "INSTANCES", "label": "INSTANCES", "shape": "dot", "size": 10, "source_id": "990578022879395d00b7a5b229863c2f,ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Urban computing refers to the study of the management and analysis of data related to urban environments, including traffic flow, air quality, and public safety", "id": "URBAN COMPUTING", "label": "URBAN COMPUTING", "shape": "dot", "size": 10, "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "ACM is a professional organization that publishes and disseminates research in computer science and related fields", "id": "ACM", "label": "ACM", "shape": "dot", "size": 10, "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Yuxuan Liang et al. are the authors of the paper", "id": "YUXUAN LIANG ET AL.", "label": "YUXUAN LIANG ET AL.", "shape": "dot", "size": 10, "source_id": "ef8fd6170b08af3bd99c9df44ffa8b57", "type": "AUTHORS"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to \"TABLE 1\" can be generated as follows:\n\n\"TABLE 1\" is a comprehensive table that serves multiple purposes. It is a comparison between the authors\u0027 survey and related surveys, providing a detailed analysis of existing studies on time series foundation models. Additionally, it is a composition of the TimesFM pretraining dataset, offering valuable insights into the data sources used for training. Furthermore, the table summarizes the developmental roadmap of current time series foundation models, highlighting their strengths and weaknesses. Lastly, it also depicts existing studies on time series foundation models, providing a thorough understanding of the current state of research in this area.\n\nThe table appears to be a crucial component of the research paper or technical document, as it provides a clear and concise overview of the key findings and methodologies used in the study. The information presented in \"TABLE 1\" is likely to be of great interest to researchers and practitioners working in the field of time series foundation models, as it offers a unique perspective on the current state of the art and potential future directions for research.\n\nOverall, \"TABLE 1\" is a valuable resource that provides a comprehensive understanding of the time series foundation models, their developmental roadmap, and the data sources used for training. Its multiple purposes and comprehensive nature make it an essential component of the research paper or technical document.", "id": "TABLE 1", "label": "TABLE 1", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d,5bb0db923f70e5f431183c6ab7dc25dc,dc2c05938eb6dbe0217f4c9e6b111e2a,f92205091f8d3b7e1e60b9bc80883a62", "type": "TABLE"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"SURVEY\" refers to a comprehensive review of existing research in a particular field or area. Specifically, this survey aims to provide a comprehensive methodological analysis of foundation models for learning a variety of time series. The survey is focused on time series forecasting, which involves analyzing and predicting future values based on historical data. The survey\u0027s objective is to provide a thorough examination of the methodologies used in foundation models for time series learning, highlighting their strengths and limitations.\n\nThe survey\u0027s scope is likely to cover various aspects of time series forecasting, including long-term series forecasting, frequency analysis, and the application of multi-head cross-attention techniques. The survey may also discuss the use of foundation models in time series learning, including their ability to learn from large datasets and adapt to changing patterns.\n\nOverall, the \"SURVEY\" is a comprehensive review of existing research in the field of time series forecasting, with a focus on foundation models and their application in learning a variety of time series.", "id": "SURVEY", "label": "SURVEY", "shape": "dot", "size": 10, "source_id": "5bb0db923f70e5f431183c6ab7dc25dc,f92205091f8d3b7e1e60b9bc80883a62", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"COVARIATES\" can be generated as follows:\n\nCOVARIATES refer to additional variables or features used in a model to improve its performance or accuracy. These variables can be time-independent or time-varying and are used to train the model, thereby enhancing its predictive capabilities. Specifically, covariates are additional information that can be utilized to improve time series forecasting, and they can also influence the outcome of a prediction or model. In essence, covariates are crucial components that can significantly impact the accuracy and reliability of a model, making them a vital aspect of machine learning and time series analysis.\n\nThis summary incorporates information from all the provided descriptions, resolves any potential contradictions, and provides a clear and concise understanding of the entity \"COVARIATES\" in the context of machine learning and time series forecasting.", "id": "COVARIATES", "label": "COVARIATES", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d,a2f45b87ea2a0aa02b6e62e9700f20f1,b4d5306b46bbfa4564727fe5ac6630e0,dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "DATA"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"TAXONOMY\" refers to a classification system used to categorize and organize concepts or entities, as well as data or concepts. This system is employed to group and structure related information in a logical and coherent manner. \n\nIn the context of data or concepts, taxonomy serves as a tool for categorization and organization, enabling the identification of patterns, relationships, and structures within the data. This classification system is essential in various fields, including research, academia, and industry, where it facilitates the analysis, interpretation, and communication of complex information.\n\nThe use of taxonomy is widespread, and it has numerous applications, including data science, machine learning, and time series analysis. By employing taxonomy, researchers and practitioners can develop a deeper understanding of the relationships between different concepts, entities, or data points, ultimately leading to more accurate predictions, better decision-making, and improved outcomes.\n\nIn summary, the entity \"TAXONOMY\" represents a fundamental concept in data organization and classification, with far-reaching implications for various fields and applications.", "id": "TAXONOMY", "label": "TAXONOMY", "shape": "dot", "size": 10, "source_id": "479fc10bea12b01a37fbc5cef21eea76,5bb0db923f70e5f431183c6ab7dc25dc", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Pipeline refers to a series of processes or steps used to analyze and process data", "id": "PIPELINE", "label": "PIPELINE", "shape": "dot", "size": 10, "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the \"DECODER-ONLY FOUNDATION MODEL\" can be generated as follows:\n\nThe \"DECODER-ONLY FOUNDATION MODEL\" is a type of machine learning model specifically designed for time-series forecasting. It utilizes a decoder architecture to generate output based on input data, making it a unique and specialized model for this particular application. This model is a type of foundation model, which implies that it serves as a base or starting point for more complex models or applications. The \"DECODER-ONLY FOUNDATION MODEL\" is a neural network architecture that is particularly well-suited for time-series forecasting tasks, leveraging its decoder-only design to effectively process and generate predictions from time-series data.\n\nThis summary is based on the information provided in the description list, which collectively paint a picture of the \"DECODER-ONLY FOUNDATION MODEL\" as a specialized machine learning model for time-series forecasting. The use of a decoder architecture and its application to time-series data are key characteristics of this model, making it a valuable tool for researchers and practitioners working in this area.", "id": "DECODER-ONLY FOUNDATION MODEL", "label": "DECODER-ONLY FOUNDATION MODEL", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491,4ab34d32601d452f14b4a1c31415292d,500710c8a95f1310d383fa71fd806c1c,cd4ff69415c7b37cec643f8289d1c98d,dd9bcf836b1de9b8c99d5ba8188a5ed4,f7ce89346ea6715560163ef3a630a5df", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"ZERO-SHOT LEARNING\" refers to a concept in machine learning that enables a model to perform well on a task without any prior training on that task. This technique is used in machine learning where a model can learn to perform a task without being explicitly trained on that task. The ability of a model to perform well on a task without prior training is a key aspect of zero-shot learning, which is a concept mentioned in the text.\n\nThe summary is based on the provided descriptions, which are consistent and provide a clear understanding of the concept of \"ZERO-SHOT LEARNING\". The information collected from all the descriptions is used to create a single, coherent summary.\n\nRelevant information from the nearby text is not provided, but based on the context, it can be inferred that \"ZERO-SHOT LEARNING\" is a concept related to machine learning and artificial intelligence. The use of technical terms such as \"machine learning\" and \"model\" suggests that the concept is related to the field of artificial intelligence and machine learning.\n\nOverall, the summary provides a clear and concise understanding of the concept of \"ZERO-SHOT LEARNING\" and its application in machine learning.", "id": "ZERO-SHOT LEARNING", "label": "ZERO-SHOT LEARNING", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26,7e03744dea80e2138baff03611104fa8,89e5f6a93205d5e87ad7e7641c0bbb91", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time series domains refer to the field of study that focuses on time series data", "id": "TIME SERIES DOMAINS", "label": "TIME SERIES DOMAINS", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Speech is a field of study that deals with the interaction between computers and spoken language.", "id": "SPEECH", "label": "SPEECH", "shape": "dot", "size": 10, "source_id": "7dbc961fe1959f844ebac283498e7fb0", "type": "FIELD"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"NATURAL LANGUAGE\" can be generated as follows:\n\nThe entity \"NATURAL LANGUAGE\" refers to human language used for communication, encompassing both spoken and written language. It is the modality used by Large Language Models (LLMs), as indicated by the use of phrases such as \"natural language\" and \"discrete tokens.\" This entity is a fundamental aspect of human interaction, enabling individuals to convey and understand complex ideas, emotions, and thoughts through a vast array of linguistic expressions.\n\nThe summary is written in third person and includes information collected from all the descriptions. The contradictions present in the descriptions have been resolved to provide a single, coherent summary. Relevant information from the nearby text has been incorporated to enrich the summary, providing a comprehensive understanding of the entity \"NATURAL LANGUAGE.\"", "id": "NATURAL LANGUAGE", "label": "NATURAL LANGUAGE", "shape": "dot", "size": 10, "source_id": "89b79391630ac478085efea89fad5736,8f4724ff6541b8924f0cebe9872ed040,ec3fbfb800fd9bf1d913584fda4ae925", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to \"TIME SERIES DATA\" can be generated as follows:\n\n\"TIME SERIES DATA\" refers to a type of data that consists of a sequence of values measured at regular time intervals. This data is often used in forecasting and analysis, and can vary over time, such as stock prices or weather patterns. It is a continuous data used in time series forecasting, as mentioned in the text. Lag-Llama, a pre-trained model, was trained on a large corpus of diverse time series data from several domains, indicating the widespread use and importance of this type of data.\n\nThe summary is written in third person and includes information collected from all the descriptions, resolving any contradictions and providing a single, coherent summary. Relevant information from the nearby text has been enriched to provide a comprehensive understanding of \"TIME SERIES DATA\".", "id": "TIME SERIES DATA", "label": "TIME SERIES DATA", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,0bef137d159ccc86cdee0a8be788bd26,116332ac4538a1430c83a34fcbec22d1,12395cf4e8efa64a847ede9775ecdf3f,2bb4fc2b46b9c8bdd052b2755d986aa8,6eb4c16edf2eedfd03721efb199478d8,85e4fbea9a08a0a663f3c5dc3f3a95a7,89b79391630ac478085efea89fad5736,8f4724ff6541b8924f0cebe9872ed040,c7167cf92abe7513ccb936ca79871932,f7266cfccedb1d9840d10afa689a05e9,f92205091f8d3b7e1e60b9bc80883a62", "type": "DATA"}, {"color": "#97c2fc", "description": "Model architectures refer to the design and structure of a model", "id": "MODEL ARCHITECTURES", "label": "MODEL ARCHITECTURES", "shape": "dot", "size": 10, "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Pre-training techniques refer to the methods used to train a model before fine-tuning it for a specific task", "id": "PRE-TRAINING TECHNIQUES", "label": "PRE-TRAINING TECHNIQUES", "shape": "dot", "size": 10, "source_id": "79c0a74b91761402b67c3574db02e8e7,f92205091f8d3b7e1e60b9bc80883a62", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Adaptation methods refer to the techniques used to adapt a pre-trained model to a specific task", "id": "ADAPTATION METHODS", "label": "ADAPTATION METHODS", "shape": "dot", "size": 10, "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"DATA MODALITIES\" can be described as follows:\n\nThe entity \"DATA MODALITIES\" refers to the various types of data that can be utilized to train a model, specifically in the context of Time Series Forecasting Models (TSFMs). These data modalities include standard time series, trajectory data, raster data, and text data. This diversity of data types allows for the development of more comprehensive and accurate models, as they can be trained on a wide range of data sources.\n\nIn the context of TSFMs, data modalities play a crucial role in enabling the model to capture complex patterns and relationships within the data. By incorporating multiple data types, TSFMs can improve their forecasting accuracy and robustness, making them more effective in real-world applications.\n\nOverall, the entity \"DATA MODALITIES\" encompasses the different types of data that can be used to train TSFMs, highlighting the importance of data diversity in achieving accurate and reliable forecasting results.", "id": "DATA MODALITIES", "label": "DATA MODALITIES", "shape": "dot", "size": 10, "source_id": "53f80422fbf85856ecf940d5c2450665,f92205091f8d3b7e1e60b9bc80883a62", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Figure 1 is a figure that depicts the developmental roadmap of current time series foundation models", "id": "FIGURE 1", "label": "FIGURE 1", "shape": "dot", "size": 10, "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "type": "FIGURE"}, {"color": "#97c2fc", "description": "Mean scaling quantization is a technique used to reduce the precision of model weights and activations", "id": "MEAN SCALING QUANTIZATION", "label": "MEAN SCALING QUANTIZATION", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Synthetic time series data refers to artificially generated time series data", "id": "SYNTHETIC TIME SERIES DATA", "label": "SYNTHETIC TIME SERIES DATA", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26", "type": "DATA"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nLOCAL STATISTICAL MODELS are a type of model used for time series forecasting. They are models that fit parameters individually for each time series, allowing for a tailored approach to forecasting. However, it is worth noting that LOCAL STATISTICAL MODELS are out-performed by Chronos on probabilistic forecasting, indicating that while they may be effective for certain types of forecasting, they may not be the most accurate option for all scenarios.\n\nThe LOCAL STATISTICAL MODELS are designed to adapt to the specific characteristics of each time series, fitting parameters for each one individually. This approach allows for a high degree of flexibility and can be particularly useful when dealing with complex or non-stationary time series data. Nevertheless, the performance of LOCAL STATISTICAL MODELS in probabilistic forecasting is inferior to that of Chronos, suggesting that there may be limitations to their use in certain applications.\n\nOverall, LOCAL STATISTICAL MODELS are a type of model that can be useful for time series forecasting, particularly when dealing with complex or non-stationary data. However, their performance in probabilistic forecasting is not as strong as that of other models, such as Chronos.", "entity_type": "MODEL", "id": "LOCAL STATISTICAL MODELS", "label": "LOCAL STATISTICAL MODELS", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,2eea45c6111e468189580a21686cb14a,532a6d434dab611a80aef1e94dd2fb45,5b1135d9e53e53428a042e8bbc89eaa7", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"Task-specific deep learning models\" are a type of model that is trained across multiple time series for a specific task. These models have been found to be highly effective in terms of performance, often outperforming other models in their field. However, a model called \"Chronos\" has been shown to perform on par with task-specific deep learning models, suggesting that it may be a viable alternative for certain applications. Task-specific deep learning models are primarily used for time series forecasting, which involves predicting future values in a time series based on past data. This type of forecasting is crucial in various fields, including finance, economics, and engineering, where accurate predictions can inform decision-making and drive business outcomes.\n\nRelevant information from the nearby text suggests that the primary language of the text is English, and the content is formal and academic, indicating that the text is likely from a research paper or technical document. The use of technical terms, mathematical equations, and references to academic papers further supports this conclusion.", "id": "TASK-SPECIFIC DEEP LEARNING MODELS", "label": "TASK-SPECIFIC DEEP LEARNING MODELS", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,53dbeea6d05c3695460c71f89f468def,baa887ae552c6b3dac10f74896d8c4a2", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"LAGS\" refers to a concept used in time series analysis. Lags are used as covariates in the decoder-only transformer architecture, indicating their application in machine learning models. Additionally, Lags refer to the delay or time difference between two or more time series, highlighting their significance in understanding the temporal relationships between different time series data.\n\nIn the context of time series analysis, Lags are crucial for modeling and forecasting. They are often used to capture the autocorrelation and cross-correlation between different time series, enabling the identification of patterns and trends. The use of Lags as covariates in the decoder-only transformer architecture suggests that they can be effectively integrated into machine learning models to improve their performance in time series forecasting tasks.\n\nOverall, the concept of Lags is essential in time series analysis, and their application in machine learning models, such as the decoder-only transformer architecture, has the potential to enhance the accuracy and reliability of time series forecasting.", "id": "LAGS", "label": "LAGS", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8,c7167cf92abe7513ccb936ca79871932", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Value scaling is a process that involves scaling the values of a time series to a common range", "id": "VALUE SCALING", "label": "VALUE SCALING", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536", "type": "PROCESS"}, {"color": "#97c2fc", "description": "LLM architectures refer to the large language models used in the text, as indicated by the use of phrases such as \"LLM architectures\" and \"pre-trained LLM\"", "id": "LLM ARCHITECTURES", "label": "LLM ARCHITECTURES", "shape": "dot", "size": 10, "source_id": "89b79391630ac478085efea89fad5736", "type": "MODEL"}, {"color": "#97c2fc", "description": "Domain-specific datasets refer to datasets that are specific to a particular domain or industry", "id": "DOMAIN-SPECIFIC DATASETS", "label": "DOMAIN-SPECIFIC DATASETS", "shape": "dot", "size": 10, "source_id": "f7266cfccedb1d9840d10afa689a05e9", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"MODEL ARCHITECTURE\" refers to the design and structure of a machine learning model, specifically including its components and connections. This encompasses the overall framework and organization of the model, which is crucial for its functionality and performance. The model architecture is a fundamental aspect of machine learning, as it determines how the model processes and transforms input data into meaningful outputs.\n\nIn the context of machine learning, the model architecture is a critical component that influences the model\u0027s ability to learn and generalize from data. It involves designing and selecting the appropriate components, such as layers, neurons, and connections, to achieve the desired outcome. The model architecture can be thought of as a blueprint or a template that guides the development and implementation of the machine learning model.\n\nThe descriptions provided suggest that the model architecture is a comprehensive concept that encompasses various aspects of the model\u0027s design and structure. By considering the components and connections of the model, developers can create a robust and effective architecture that meets the requirements of the problem being addressed.\n\nOverall, the model architecture is a critical aspect of machine learning that requires careful consideration and design to achieve optimal performance and results.", "id": "MODEL ARCHITECTURE", "label": "MODEL ARCHITECTURE", "shape": "dot", "size": 10, "source_id": "479fc10bea12b01a37fbc5cef21eea76,79c0a74b91761402b67c3574db02e8e7", "type": "CATEGORY"}, {"color": "#97c2fc", "description": "Application domain refers to the specific area or field where a model is applied, such as time series analysis", "id": "APPLICATION DOMAIN", "label": "APPLICATION DOMAIN", "shape": "dot", "size": 10, "source_id": "79c0a74b91761402b67c3574db02e8e7", "type": "CATEGORY"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nTSFMS (Time Series Forecasting Models) refer to a type of model that can forecast future values based on past data. This type of model is used for long-term series forecasting, which involves analyzing and predicting future values in a time series. The models used for this purpose often involve frequency analysis and other advanced techniques, such as multi-head cross-attention, to accurately forecast future values.\n\nThe primary language of the text related to TSFMS is English, as indicated by the use of English words and phrases, mathematical equations, and references to English-language academic papers and authors. This suggests that the text is from a research paper or a technical document written in English.\n\nOverall, TSFMS are a type of model used for time series forecasting, which involves analyzing past data to predict future values. The models used for this purpose are often complex and involve advanced techniques, but are written in English and are used in English-language academic research.", "id": "TSFMS", "label": "TSFMS", "shape": "dot", "size": 10, "source_id": "53f80422fbf85856ecf940d5c2450665,ad8efcfda80253036294f2eeb48926e8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Spatio-temporal graph refers to a graph that describes the spatial correlation of sensors, with adjacent matrix A \u2208 R\ud835\udc41 \u00d7\ud835\udc41", "id": "SPATIO-TEMPORAL GRAPH", "label": "SPATIO-TEMPORAL GRAPH", "shape": "dot", "size": 10, "source_id": "79c0a74b91761402b67c3574db02e8e7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Diffusion models are a type of machine learning model used for generating new data points that follow a specific distribution", "id": "DIFFUSION MODELS", "label": "DIFFUSION MODELS", "shape": "dot", "size": 10, "source_id": "9125a7d3794226f109debe90e5db041c", "type": "MODEL"}, {"color": "#97c2fc", "description": "Clinical records refer to the medical information and data collected about patients", "id": "CLINICAL RECORDS", "label": "CLINICAL RECORDS", "shape": "dot", "size": 10, "source_id": "ef8fd6170b08af3bd99c9df44ffa8b57", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "MLM is a concept used in the BERT model", "id": "MASKED LANGUAGE MODELING (MLM)", "label": "MASKED LANGUAGE MODELING (MLM)", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nXLNet is a pre-trained language model that combines autoencoding and autoregressive modeling. Specifically, it is a type of Transformer model used for natural language processing tasks. This suggests that XLNet is a cutting-edge model designed to handle complex language understanding and generation tasks, leveraging the strengths of both autoencoding and autoregressive modeling. As a Transformer model, XLNet is likely to be highly effective in tasks such as language translation, text summarization, and question answering, among others. Overall, XLNet represents a significant advancement in the field of natural language processing, offering a powerful tool for researchers and practitioners alike.", "id": "XLNET", "label": "XLNET", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca,a5d60c1a355b77187003182f6df57646", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nBART is a popular language model used for various natural language processing tasks. It employs the transformer architecture, which is a type of neural network architecture commonly used in natural language processing tasks. Specifically, BART is a pre-trained language model that utilizes several noising schemes, including the masked language modeling (MLM) technique used in BERT. This noising scheme is a key component of BART\u0027s architecture, allowing it to learn and generate coherent text. Overall, BART is a versatile and widely used language model in the field of natural language processing.\n\nNote: The information provided is consistent across all descriptions, and there are no contradictions. The summary is a concise and coherent representation of the key characteristics and features of BART.", "id": "BART", "label": "BART", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca,1f1221583d838c2407fe9864225e9eda,7e69d9444a2084b6452e291735bf5a49,a5d60c1a355b77187003182f6df57646", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"MLM\" refers to Masked Language Model, a type of language model used in natural language processing. MLM is a technique used in language models to predict missing words in a sentence. This technique is employed to improve the accuracy and robustness of language models by training them to fill in missing words in a sentence, which helps to enhance their understanding of language and its nuances.\n\nIn the context of natural language processing, MLM is a crucial component of language models, enabling them to learn from incomplete or partially masked sentences. By predicting missing words, MLM helps to improve the overall performance of language models in tasks such as language translation, text summarization, and question-answering.\n\nOverall, MLM is a powerful technique in natural language processing that has been widely adopted in various applications, including language translation, text generation, and sentiment analysis. Its ability to predict missing words in a sentence has significantly improved the accuracy and robustness of language models, making it a fundamental component of modern natural language processing systems.", "id": "MLM", "label": "MLM", "shape": "dot", "size": 10, "source_id": "6917a14af275e30abe2d30a8f8a9a4e6,a5d60c1a355b77187003182f6df57646", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Jacob Devlin is an author who has published a paper on pre-training of deep bidirectional transformers for language understanding", "id": "JACOB DEVLIN", "label": "JACOB DEVLIN", "shape": "dot", "size": 10, "source_id": "a73df99fe49b288e1c8751be2008b191", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "", "id": "RWC+19", "label": "RWC+19", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"LLAMA 2\":\n\nLLAMA 2 is a large language model developed by Meta AI, which is a popular language model used for a variety of natural language processing tasks. Specifically, LLAMA 2 employs the decoder-only architecture, a design choice that enables it to excel in tasks such as language understanding, generation, and translation. As a large language model, LLAMA 2 has been developed to process and generate human-like language, making it a valuable tool for researchers and developers in the field of natural language processing.\n\nThis summary incorporates information from all the provided descriptions, resolving any potential contradictions and providing a coherent overview of the entity \"LLAMA 2\". The information is written in the third person, and the entity name is included to provide context. Relevant details from the nearby text have been incorporated to enrich the summary, providing a more comprehensive understanding of LLAMA 2\u0027s capabilities and architecture.", "id": "LLAMA 2", "label": "LLAMA 2", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26,1f1221583d838c2407fe9864225e9eda,7e69d9444a2084b6452e291735bf5a49", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "DECODER-ONLY ARCHITECTURE", "label": "DECODER-ONLY ARCHITECTURE", "shape": "dot", "size": 10, "source_id": "7e69d9444a2084b6452e291735bf5a49", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe primary entity of interest is the \"PRE-TRAINED LLM\" (Pre-trained Language Model). The description of this entity is as follows:\n\nThe \"PRE-TRAINED LLM\" refers to a type of pre-trained language model that has been trained on a large corpus of text data. This type of model is designed to learn general language patterns and relationships from a vast amount of text, allowing it to perform a wide range of natural language processing tasks.\n\nThe description of the \"PRE-TRAINED LLM\" is supported by the fact that it is a type of model that has been trained on a large corpus of text data, which is a common characteristic of pre-trained language models. This information is consistent with the language and content of the text, which suggests that the primary language of the text is English.\n\nOverall, the \"PRE-TRAINED LLM\" is a type of pre-trained language model that has been trained on a large corpus of text data, allowing it to perform a wide range of natural language processing tasks.", "id": "PRE-TRAINED LLM", "label": "PRE-TRAINED LLM", "shape": "dot", "size": 10, "source_id": "1f1221583d838c2407fe9864225e9eda,4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,79b26808691b6333aafce43c5de531f7,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,bb10b1a7f98a4f869b7abbc5f9632dd1,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96,ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "AM refers to a type of model used in time series analysis", "id": "AM", "label": "AM", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "VLM refers to a type of model used in time series analysis", "id": "VLM", "label": "VLM", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Decoder-only models are a type of language model that only attends to tokens up to the current token", "id": "DECODER-ONLY", "label": "DECODER-ONLY", "shape": "dot", "size": 10, "source_id": "1f1221583d838c2407fe9864225e9eda", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nPATCH EMBEDDINGS are a type of embedding used in computer vision models to represent patches of an image. They are a crucial component in various deep learning architectures, particularly in the context of image processing and analysis. In the context of GPT4TS, PATCH EMBEDDINGS are used to represent time series data, indicating their versatility and adaptability in different domains.\n\nThe use of PATCH EMBEDDINGS in computer vision models enables the representation of local image features, such as textures, shapes, and colors, which are essential for image classification, object detection, and segmentation tasks. By leveraging PATCH EMBEDDINGS, researchers and practitioners can develop more accurate and efficient models for image-related applications.\n\nIn the context of GPT4TS, PATCH EMBEDDINGS are used to represent time series data, which suggests their potential applications in time series forecasting, anomaly detection, and other related tasks. This adaptation of PATCH EMBEDDINGS to time series data highlights their flexibility and ability to be applied in diverse domains.\n\nOverall, PATCH EMBEDDINGS are a powerful tool in the field of computer vision and time series analysis, offering a unique representation of image and time series data, respectively. Their applications in various domains, including image processing and time series forecasting, make them an essential component in the development of advanced machine learning models.", "id": "PATCH EMBEDDINGS", "label": "PATCH EMBEDDINGS", "shape": "dot", "size": 10, "source_id": "1f1221583d838c2407fe9864225e9eda,3174231a67593609c727151c9df31d0a,3e937ba8de0e7eca993c50506ceb8f1f,4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,bc54a718d1886698232d578fd88c3ac7,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96,fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nFew-shot learning is a technique used to adapt pre-trained models to specific tasks, enabling them to learn from a small amount of data. This ability allows models to learn from limited training data, making it a valuable approach in scenarios where large datasets are not available. Few-shot learning is a technique that leverages pre-trained models to quickly adapt to new tasks, making it a crucial aspect of machine learning and artificial intelligence research.\n\nThe primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. The language is formal and academic, suggesting that the text is from a research paper or a technical document. The presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports this conclusion.\n\nOverall, few-shot learning is a powerful technique that enables models to learn from limited data, making it a valuable tool in various applications, including machine learning and artificial intelligence research.", "id": "FEW-SHOT LEARNING", "label": "FEW-SHOT LEARNING", "shape": "dot", "size": 10, "source_id": "6d66c2ea37e25b646a13d751c05a8e4d,79b26808691b6333aafce43c5de531f7,c4e47033b8ecc85beacde9d560e66062", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"DIRECT USAGE\" refers to a method of adapting Time Series Forecasting Models (TSFM) to specific tasks or datasets. This approach involves the direct use of a pre-trained model without further fine-tuning on the target dataset. In essence, direct usage enables the application of pre-trained TSFM models to new, unseen data without the need for extensive retraining or adaptation. This method is particularly useful for tasks where the pre-trained model has already demonstrated strong performance on a related dataset, allowing for efficient and effective deployment in new contexts.", "id": "DIRECT USAGE", "label": "DIRECT USAGE", "shape": "dot", "size": 10, "source_id": "53f80422fbf85856ecf940d5c2450665,de8e097ce66fbc954bd529888cbc15ea", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"PROMPT ENGINEERING\":\n\nPROMPT ENGINEERING is a method of adapting Time Series Forecasting Models (TSFMs) to specific tasks or datasets. It involves designing and optimizing the input prompts for a language model to achieve a specific task, particularly in the context of Time Series Forecasting Models (TSFMs) based on Large Language Models (LLMs). This process enables the model to produce accurate and relevant forecasts by tailoring the input prompts to the specific requirements of the task or dataset.\n\nThe primary goal of PROMPT ENGINEERING is to optimize the performance of LLM-based TSFMs by carefully crafting the input prompts to elicit the desired responses from the model. This involves a deep understanding of the task, the dataset, and the capabilities of the LLM, as well as the ability to design and refine the prompts to achieve the best possible results.\n\nOverall, PROMPT ENGINEERING is a critical component of the TSFM development process, enabling researchers and practitioners to adapt their models to a wide range of tasks and datasets, and to achieve state-of-the-art performance in time series forecasting.", "id": "PROMPT ENGINEERING", "label": "PROMPT ENGINEERING", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26,53f80422fbf85856ecf940d5c2450665,de8e097ce66fbc954bd529888cbc15ea", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Entity:** CHRONOS-T5 (SMALL)\n\n**Summary:** CHRONOS-T5 (SMALL) is a model that has achieved the top spot on Benchmark II with an inexpensive fine-tuning regimen. It is a model that is fine-tuned on datasets from Benchmark II and performs significantly better than local models. Additionally, CHRONOS-T5 (SMALL) outperforms local statistical models, showcasing its effectiveness in time series forecasting.\n\n**Key Features:**\n\n* Achieved top spot on Benchmark II with inexpensive fine-tuning regimen\n* Fine-tuned on datasets from Benchmark II\n* Performs significantly better than local models\n* Outperforms local statistical models\n\n**Context:** The summary is based on the provided descriptions, which suggest that CHRONOS-T5 (SMALL) is a model designed for time series forecasting, specifically for Benchmark II. The model\u0027s performance is compared to local models and statistical models, highlighting its superiority in achieving accurate predictions.\n\n**Relevant Information:** The summary is enriched with relevant information from the nearby text, which suggests that the model is fine-tuned on datasets from Benchmark II and achieves top performance with an inexpensive fine-tuning regimen. This information is crucial in understanding the model\u0027s capabilities and limitations.\n\n**Conclusion:** CHRONOS-T5 (SMALL) is a high-performing model for time series forecasting, particularly on Benchmark II. Its ability to outperform local models and statistical models makes it a valuable tool for researchers and practitioners in the field of time series analysis.", "id": "CHRONOS-T5 (SMALL)", "label": "CHRONOS-T5 (SMALL)", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,56de1d5a5b467101344afa4248d829dc,baa887ae552c6b3dac10f74896d8c4a2", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"LEARNING RATE\" is a hyper-parameter that plays a crucial role in the training process of a model. It controls how quickly the model learns from the data, and its value determines the step size of the model\u0027s updates during training. Specifically, the learning rate is a parameter that is used to control the rate at which the model adjusts its weights and biases to minimize the loss function. In this case, the learning rate is set to 1 \u00d7 10\u22124, which is a relatively small value indicating that the model learns slowly from the data.\n\nThis summary is based on the information provided in the description list, which is consistent in its description of the learning rate as a parameter that controls the model\u0027s learning process. The value of the learning rate is also explicitly stated as 1 \u00d7 10\u22124, which provides a specific numerical value for this hyper-parameter.\n\nOverall, the learning rate is an essential component of the model\u0027s training process, and its value has a significant impact on the model\u0027s performance and convergence.", "id": "LEARNING RATE", "label": "LEARNING RATE", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022,673b0feee6eb5843954defc670e4ba29,baa887ae552c6b3dac10f74896d8c4a2,fa01b75dccc556af8aca0d6c47e1970f", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Annealed linearly is a process of adjusting the learning rate over time", "id": "ANNEALED LINEARLY", "label": "ANNEALED LINEARLY", "shape": "dot", "size": 10, "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Figure 6 shows that fine-tuning significantly improves the aggregate performance of the model on Benchmark II", "id": "FIGURE 6", "label": "FIGURE 6", "shape": "dot", "size": 10, "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "type": "FIGURE"}, {"color": "#97c2fc", "description": "Domain-specific applications refer to tasks or domains that require specialized models or techniques", "id": "DOMAIN-SPECIFIC APPLICATIONS", "label": "DOMAIN-SPECIFIC APPLICATIONS", "shape": "dot", "size": 10, "source_id": "55eb54ef455d14cd1a11760924f99eb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"SEASONAL NAIVE\" can be generated as follows:\n\nSEASONAL NAIVE is a type of statistical model used for time series forecasting that takes into account seasonal patterns. It is a baseline model that performs competitively on some datasets and is used to normalize the probabilistic and point forecasting metrics. SEASONAL NAIVE is a local statistical baseline method for benchmarking Chronos and is a simple model used for comparison with TimeGPT. As a statistical model, it is used for forecasting and is a type of naive model that incorporates seasonal patterns, making it a useful tool for time series forecasting.\n\nThis summary is written in third person and includes information from all the descriptions provided. It also resolves any contradictions by presenting a coherent and comprehensive overview of the entity \"SEASONAL NAIVE\".", "id": "SEASONAL NAIVE", "label": "SEASONAL NAIVE", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,12395cf4e8efa64a847ede9775ecdf3f,2565ae205d4c98342168bb67a4f7a309,2eea45c6111e468189580a21686cb14a,55eb54ef455d14cd1a11760924f99eb8,56de1d5a5b467101344afa4248d829dc,5b1135d9e53e53428a042e8bbc89eaa7,a875a1c0bede47a1c4e3823be81c42c6,af36d1634490149b96980fb1dff57cd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Numba compiling is a method used to optimize code for parallel computing", "id": "NUMBA COMPILING", "label": "NUMBA COMPILING", "shape": "dot", "size": 10, "source_id": "55eb54ef455d14cd1a11760924f99eb8", "type": "METHOD"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"ZERO-SHOT\" can be generated as follows:\n\nThe entity \"ZERO-SHOT\" refers to a specific scenario in machine learning where a model is evaluated or tested without prior knowledge of the data distribution, fine-tuning, or exposure to the specific task. This setting implies that the model is not trained or adapted to the particular task or data, and its performance is assessed based on its ability to generalize and perform well on unseen data. In essence, the \"ZERO-SHOT\" setting is a benchmark for evaluating a model\u0027s ability to adapt and perform well in novel, unseen scenarios without any prior knowledge or fine-tuning.\n\nThis summary is derived from the provided descriptions, which collectively convey the essence of the \"ZERO-SHOT\" entity. The descriptions highlight the key aspects of this entity, including the lack of fine-tuning, prior knowledge, and exposure to the specific task or data distribution. By combining these elements, a comprehensive understanding of the \"ZERO-SHOT\" entity can be obtained.\n\nRelevant information from the nearby text, such as the context of machine learning and time series forecasting, further enriches the understanding of this entity. The \"ZERO-SHOT\" setting is particularly relevant in these domains, where models are often evaluated on their ability to generalize and perform well in novel, unseen scenarios.", "id": "ZERO-SHOT", "label": "ZERO-SHOT", "shape": "dot", "size": 10, "source_id": "2f3b2f69f8eb4f958c3ca793cae36581,6ae1ee8f0c4bcf7ec4d04b1048451e96,a90c6ca26542ea5935f9d8d5bf3688a9,bcf830b85e12e1ab9498e3ac3593a88e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "LAG-LAG", "label": "LAG-LAG", "shape": "dot", "size": 10, "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"PRETRAINING\" can be described as follows:\n\n\"PRETRAINING refers to the process of training a model on a large corpus of data before fine-tuning it on a specific task. This process involves utilizing a large dataset to train the model, which enables it to learn general representations and patterns that can be leveraged for a variety of tasks. The pretraining process is a crucial step in many machine learning models, as it allows the model to develop a strong foundation and improve its performance on downstream tasks.\"\n\nThis description is a comprehensive summary of the provided descriptions, and it includes relevant information from the nearby text. The contradictions between the two descriptions have been resolved by combining them into a single, coherent summary.", "id": "PRETRAINING", "label": "PRETRAINING", "shape": "dot", "size": 10, "source_id": "63e284ec7e76fa859cc46b72d3746654,76cc338e586223647fd3dbe4e7a7c131", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nFORECASTING TASKS refer to the process of predicting future values or outcomes based on historical data. This process involves analyzing past data to identify patterns and trends that can be used to make informed predictions about future events. The primary goal of forecasting tasks is to provide accurate and reliable predictions that can inform decision-making and strategic planning.\n\nIn the context of FORECASTING TASKS, the process of predicting future values is a critical component. This involves using historical data to identify patterns and trends that can be used to make informed predictions about future events. The use of historical data allows for the identification of recurring patterns and trends that can be used to make accurate predictions.\n\nThe FORECASTING TASKS involve the use of various techniques and methods, including time series analysis, frequency analysis, and multi-head cross-attention. These techniques and methods are used to analyze historical data and identify patterns and trends that can be used to make informed predictions about future events.\n\nOverall, FORECASTING TASKS are a critical component of decision-making and strategic planning. By analyzing historical data and identifying patterns and trends, organizations can make informed predictions about future events and make data-driven decisions.\n\nRelevant information from the nearby text includes:\n\n* The use of technical terms and mathematical equations, which suggests that the text is from a research paper or a technical document.\n* The presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals.\n* The citation of English-language academic papers and authors, which suggests that the text is written in English.\n\nOverall, the summary provides a comprehensive overview of FORECASTING TASKS, including their definition, purpose, and techniques used. The summary also highlights the importance of FORECASTING TASKS in decision-making and strategic planning.", "id": "FORECASTING TASKS", "label": "FORECASTING TASKS", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,6d66c2ea37e25b646a13d751c05a8e4d", "type": "TASK"}, {"color": "#97c2fc", "description": "Event sequence refers to a temporally ordered set of events that describe the progression of actions or occurrences within a specific context", "id": "EVENT SEQUENCE", "label": "EVENT SEQUENCE", "shape": "dot", "size": 10, "source_id": "79c0a74b91761402b67c3574db02e8e7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"SPATIO-TEMPORAL RASTER\" refers to a specific data modality mentioned in the text. It is described as a grid of sensors distributed uniformly in geographical space. This entity is characterized by its spatio-temporal nature, indicating that it involves both spatial and temporal components.\n\nThe use of the term \"spatio-temporal raster\" in the text suggests that this entity is a type of raster data that incorporates both spatial and temporal information. The uniform distribution of sensors in geographical space implies that this entity is likely to be used in applications such as remote sensing, geographic information systems (GIS), or environmental monitoring.\n\nOverall, the \"SPATIO-TEMPORAL RASTER\" entity appears to be a complex data structure that combines spatial and temporal information, making it a valuable tool for various applications in fields such as geography, environmental science, and remote sensing.\n\nRelevant information from the nearby text suggests that the \"SPATIO-TEMPORAL RASTER\" entity is likely to be used in conjunction with other data modalities, such as time series data, to analyze and model complex spatio-temporal phenomena. The use of mathematical equations and formulas in the text also implies that the \"SPATIO-TEMPORAL RASTER\" entity may be used in machine learning and data analysis applications.", "id": "SPATIO-TEMPORAL RASTER", "label": "SPATIO-TEMPORAL RASTER", "shape": "dot", "size": 10, "source_id": "79c0a74b91761402b67c3574db02e8e7,859c14b7112010530eddafc204b4b305", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "FourCast-Net is a model mentioned in the text, as indicated by the use of the phrase \"FourCast-Net [70]\"", "id": "FOURCAST-NET", "label": "FOURCAST-NET", "shape": "dot", "size": 10, "source_id": "859c14b7112010530eddafc204b4b305", "type": "MODEL"}, {"color": "#97c2fc", "description": "Data category refers to the classification of data into different categories, such as standard time series, spatial time series, and others", "id": "DATA CATEGORY", "label": "DATA CATEGORY", "shape": "dot", "size": 10, "source_id": "79c0a74b91761402b67c3574db02e8e7", "type": "CATEGORY"}, {"color": "#97c2fc", "description": "Foundation models for time series are a type of model that can be fine-tuned for time series analysis", "id": "FOUNDATION MODELS FOR TIME SERIES", "label": "FOUNDATION MODELS FOR TIME SERIES", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Standard time series possess diverse properties, including varying sampling rates and temporal patterns", "id": "STANDARD TIME SERIES", "label": "STANDARD TIME SERIES", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Lag-Llama is a pioneering effort as a forecasting foundation model", "id": "LAG-LAGGPT-1", "label": "LAG-LAGGPT-1", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTTMS is a model mentioned in the text, as indicated by its abbreviation and description. Specifically, TTMS is a recent endeavor in creating a domain-agnostic forecasting model built upon TSMixer, a type of neural network architecture used for time series data tasks. This suggests that TTMS is a novel approach to time series forecasting, leveraging the strengths of TSMixer to develop a more versatile and adaptable model.\n\nGiven the context of time series forecasting and the mention of TSMixer, it is likely that TTMS is a deep learning-based model, utilizing techniques such as multi-head cross-attention and frequency analysis to improve its forecasting capabilities. The fact that TTMS is described as a domain-agnostic model implies that it can be applied to a wide range of time series data, making it a valuable tool for researchers and practitioners in the field.\n\nOverall, TTMS appears to be a cutting-edge model for time series forecasting, built upon the foundations of TSMixer and designed to tackle the challenges of domain-agnostic forecasting.", "id": "TTMS", "label": "TTMS", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,b9d22fe9f2eecb1665f4bea365c7d612,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "CLU DA is a model mentioned in the text, as indicated by its abbreviation and description", "id": "CLUDA", "label": "CLUDA", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTSMIXER is a pre-trained model used in the creation of Time Series Models (TTMs). It is a type of neural network architecture specifically designed for time series data tasks. TSMIXER is mentioned in the text as a model that plays a crucial role in the development of TTMs, leveraging its capabilities to effectively handle time series data.\n\nThe summary is based on the information provided in the description list, which collectively paint a clear picture of TSMIXER\u0027s role and characteristics. The entity name \"TSMIXER\" is used throughout the summary to provide context and clarity.\n\nIt is worth noting that the text does not provide any contradictory information regarding TSMIXER, and the descriptions provided are consistent with each other. Therefore, the summary is a coherent and accurate representation of the information available.", "id": "TSMIXER", "label": "TSMIXER", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,b9d22fe9f2eecb1665f4bea365c7d612,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Generative is a methodology mentioned in the text, as indicated by its description and use in various models", "id": "GENERATIVE", "label": "GENERATIVE", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "METHOD"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"LLMTIME\" can be generated as follows:\n\nLLMTIME refers to a specific model or algorithm used for time series forecasting and analysis. It is a deep learning model that adapts Large Language Models (LLMs) for zero-shot time series forecasting. The model proposes a new tokenization scheme for encoding real-valued data as a string of digits, effectively tokenizing time series data. LLMTIME is a model that is compared with TIME-LLM in terms of average improvements and is used as a baseline in the paper. It is also mentioned in the text, specifically in the context of GFQW23, and is used for time-series forecasting. Additionally, LLMTIME models are a type of model that uses a language model approach to forecasting, and the model is pre-trained for use in time series analysis.\n\nThe model is capable of effectively handling outliers in the context of time-series forecasting, and it is used for forecasting and analysis purposes. It is also mentioned in the text as a model that is used in conjunction with ForecastPFN. Overall, LLMTIME is a deep learning model that is specifically designed for time series forecasting and analysis, and it has been compared with other models in terms of its performance.\n\nThe language used in the descriptions is formal and academic, suggesting that the text is from a research paper or a technical document. The use of technical terms, mathematical equations, and references to academic papers further supports this conclusion. The primary language of the text is English, as indicated by the use of English words and phrases, mathematical notation, and English-language abbreviations.", "id": "LLMTIME", "label": "LLMTIME", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,1ddbab2dca370c9ef7b5a724075518cc,1ea4e7d0dd5128868533b4567a0a0273,1f1221583d838c2407fe9864225e9eda,2565ae205d4c98342168bb67a4f7a309,2bc70082c142ee2ef5d6a941611505b7,3e937ba8de0e7eca993c50506ceb8f1f,500710c8a95f1310d383fa71fd806c1c,673b0feee6eb5843954defc670e4ba29,7e03744dea80e2138baff03611104fa8,892b821ed44c13c4d09e0ceedac3051d,af36d1634490149b96980fb1dff57cd1,bb10b1a7f98a4f869b7abbc5f9632dd1,cc1063ba5913ea38c84ac0250c01fe84,e6ec99a117b9abd42452ff51cd6e8f0b,f7266cfccedb1d9840d10afa689a05e9", "type": "MODEL"}, {"color": "#97c2fc", "description": "Self-supervised contrastive learning refers to a type of machine learning algorithm that is designed to learn from unlabeled data, often used in natural language processing and computer vision", "id": "SELF-SUPERVISED CONTRASTIVE LEARNING", "label": "SELF-SUPERVISED CONTRASTIVE LEARNING", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data is as follows:\n\nThe entity \"REPROGRAMMING\" refers to the process of modifying or updating a machine learning model, system, or model/system to adapt to new tasks or data. This process is often used in various applications, including natural language processing and computer vision. Reprogramming involves updating a model or system to improve its performance, accuracy, or functionality, allowing it to adapt to new or changing data, tasks, or environments.\n\nThe descriptions provided are consistent in their definition of reprogramming, highlighting its importance in machine learning and model/system updates. The process of reprogramming is crucial for maintaining and improving the performance of machine learning models and systems, enabling them to learn from new data and adapt to changing environments.\n\nRelevant information from the nearby text is not provided, but based on the descriptions, it can be inferred that reprogramming is a critical aspect of machine learning and model/system development, particularly in areas such as natural language processing and computer vision.", "id": "REPROGRAMMING", "label": "REPROGRAMMING", "shape": "dot", "size": 10, "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8,83f62beddf5cf53ed2e2d4517569deb8,84bc2afcbbd278961c3c7a637c6a189e,ec3fbfb800fd9bf1d913584fda4ae925", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Prototype representations refer to the text representations of time series data used in the TIME-LLM framework", "id": "PROTOTYPE REPRESENTATIONS", "label": "PROTOTYPE REPRESENTATIONS", "shape": "dot", "size": 10, "source_id": "89b79391630ac478085efea89fad5736", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Text prototype representations are a concept mentioned in the text, as indicated by the use of the phrase \"text prototype representations\"", "id": "TEXT PROTOTYPE REPRESENTATIONS", "label": "TEXT PROTOTYPE REPRESENTATIONS", "shape": "dot", "size": 10, "source_id": "b27c89cb0db6646b1203b2701e017aeb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time series reasoning refers to the ability of a model to analyze and understand time series data", "id": "TIME SERIES REASONING", "label": "TIME SERIES REASONING", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ESTformer is a Transformer-based method for time series analysis", "id": "ESTFORMER", "label": "ESTFORMER", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "type": "MODEL"}, {"color": "#97c2fc", "description": "Non-Stationary Transformer is a Transformer-based method for time series analysis", "id": "NON-STATIONARY TRANSFORMER", "label": "NON-STATIONARY TRANSFORMER", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"FEDFORMER\" can be generated as follows:\n\nFEDFORMER is a type of model specifically designed for time-series forecasting tasks. It is a Transformer-based method that utilizes inductive bias and attention mechanisms to perform well in time-series forecasting. FEDFORMER is primarily used for short-term time series forecasting, as indicated by its use in the table, and is also capable of handling long-term series forecasting. The model is mentioned in the text as a baseline for evaluation and is likely a type of transformer model. FEDFORMER uses frequency enhanced decomposed transformer architecture for long-term series forecasting, making it a concept of interest in the field of time series analysis.\n\nThe summary is written in third person and includes information collected from all the descriptions. Any contradictions have been resolved to provide a single, coherent summary. The entity name \"FEDFORMER\" is included to provide full context. Relevant information from the nearby text has been incorporated to enrich the summary.", "id": "FEDFORMER", "label": "FEDFORMER", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,41fe893a178ebc8790ef4da83da5ab6e,4ade7764ebe998b5f35a7fd2b58d4796,500710c8a95f1310d383fa71fd806c1c,673b0feee6eb5843954defc670e4ba29,7d5d82d600620153153772a9bc498ac0,7e97089185883c456c798ddc5ec86373,91e161ba596a0cbbcae541ddb2106310,98c6b5003112ab7110e45414a2fa468b,9ba0189af2ef0720a721c16eef0f0788,a69a914fb6c895c7202532b69ad3e094,d4551c2839eaa68a7cb7324089956581,efb7975581b22620b8277417edeee3e9,f49330b6fd81d86d14e7a9d4b8e45576,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"AUTOFORMER\" can be generated as follows:\n\nAUTOFORMER is a transformer-based architecture specifically designed for time series forecasting tasks. It utilizes complex inductive biases and attention mechanisms to model time series data, incorporating an Auto-Correlation mechanism based on the series periodicity. This model is used for both short-term and long-term forecasting tasks, serving as a baseline in various papers and studies. AUTOFORMER is a type of model that uses inductive bias and attention mechanisms to analyze and forecast time series data, making it a valuable tool in the field of time series analysis and forecasting.\n\nThe summary is written in third person and includes information collected from all the descriptions, resolving any contradictions and providing a single, coherent summary. The entity name \"AUTOFORMER\" is included to provide context, and the summary is enriched with relevant information from the nearby text.", "id": "AUTOFORMER", "label": "AUTOFORMER", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,3e937ba8de0e7eca993c50506ceb8f1f,4ade7764ebe998b5f35a7fd2b58d4796,500710c8a95f1310d383fa71fd806c1c,673b0feee6eb5843954defc670e4ba29,79c41aff65d584b4c8d4c769b82756d5,7d5d82d600620153153772a9bc498ac0,7e97089185883c456c798ddc5ec86373,91e161ba596a0cbbcae541ddb2106310,990578022879395d00b7a5b229863c2f,9ba0189af2ef0720a721c16eef0f0788,ab91381dd032db318e5aec1ad5b914a6,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,bcf830b85e12e1ab9498e3ac3593a88e,c84edbea28fbbed451e8d0b7df4ffb7c,d4551c2839eaa68a7cb7324089956581,d4e3d8b5bf043b78bb9f1551080cab91,f0c52387a7b3a5c3850fe6991f0a7c83,f49330b6fd81d86d14e7a9d4b8e45576,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"INFORMER\" can be generated as follows:\n\nThe INFORMER is a model used for time series forecasting, specifically designed for handling extremely long sequences. It is a Transformer-based method that utilizes a ProbSparse self-attention mechanism to efficiently process long-term time series data. This model has been widely used for benchmarking various supervised long-horizon forecasting methods and has been employed as a baseline in several papers for time-series forecasting tasks. The INFORMER is an efficient autoregressive transformer-based method that leverages complex inductive biases to model time series, making it a popular choice for short-term and long-term time series forecasting applications.\n\nThe key features of the INFORMER model include:\n\n* Transformer-based architecture\n* ProbSparse self-attention mechanism for handling long sequences\n* Complex inductive biases for modeling time series\n* Efficient autoregressive method\n* Widely used for benchmarking and baseline evaluation in time-series forecasting tasks\n\nOverall, the INFORMER model is a powerful tool for time series forecasting, particularly for handling long-term sequences and complex time series data.", "id": "INFORMER", "label": "INFORMER", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,3e937ba8de0e7eca993c50506ceb8f1f,4ade7764ebe998b5f35a7fd2b58d4796,500710c8a95f1310d383fa71fd806c1c,673b0feee6eb5843954defc670e4ba29,79c41aff65d584b4c8d4c769b82756d5,7d5d82d600620153153772a9bc498ac0,7e97089185883c456c798ddc5ec86373,91e161ba596a0cbbcae541ddb2106310,98c6b5003112ab7110e45414a2fa468b,990578022879395d00b7a5b229863c2f,9ba0189af2ef0720a721c16eef0f0788,bb87457fce8d4214bfe1f398b7ea35f2,bcf830b85e12e1ab9498e3ac3593a88e,c84edbea28fbbed451e8d0b7df4ffb7c,d4551c2839eaa68a7cb7324089956581,efb7975581b22620b8277417edeee3e9,f0c52387a7b3a5c3850fe6991f0a7c83,f49330b6fd81d86d14e7a9d4b8e45576,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"REFORMER\" can be generated as follows:\n\nThe REFORMER is a Transformer-based method specifically designed for time series analysis and forecasting. It is a type of model used for time series forecasting, with a particular focus on short-term forecasting. The REFORMER model is mentioned in the text as a model used for forecasting, and its application is indicated in a table, suggesting its effectiveness in short-term time series forecasting. As a time series model, the REFORMER is likely a type of reformer model, which is a model used for forecasting. Overall, the REFORMER is a model that leverages the Transformer architecture to analyze and forecast time series data, making it a valuable tool in the field of time series analysis and forecasting.\n\nThis summary is based on the information collected from all the descriptions, and it resolves any potential contradictions by providing a clear and concise overview of the REFORMER model. The summary is written in third person and includes the entity name, providing the full context. Relevant information from the nearby text has been incorporated to enrich the summary and provide a more comprehensive understanding of the REFORMER model.", "id": "REFORMER", "label": "REFORMER", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,7e97089185883c456c798ddc5ec86373,91e161ba596a0cbbcae541ddb2106310,9ba0189af2ef0720a721c16eef0f0788,c84edbea28fbbed451e8d0b7df4ffb7c,d4551c2839eaa68a7cb7324089956581,f49330b6fd81d86d14e7a9d4b8e45576,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"GPT4TS\" can be generated as follows:\n\nGPT4TS is a deep learning model specifically designed for time series forecasting tasks. It is a type of machine learning model that can be used for both short-term and long-term forecasting. The model is pre-trained and can be fine-tuned on a backbone language model. GPT4TS is used for generating point forecasts for time series data and has been compared to other models, such as TIME-LLM, in terms of forecasting performance. It has been shown to perform better than local statistical models and places 8th in terms of point forecasting performance. The model is used as a baseline in experiments and is compared with other models in terms of average advancements. GPT4TS is a time series model that can be used for various forecasting tasks, including short-term and long-term forecasting.\n\nThe language used in the descriptions suggests that GPT4TS is a technical term used in the context of machine learning and time series forecasting. The presence of mathematical equations and formulas, as well as the use of English-language abbreviations and citations, further supports the conclusion that the primary language of the text is English.\n\nIt is worth noting that the descriptions provided are consistent and do not contain any contradictions. Therefore, the summary generated above is a coherent and accurate representation of the entity \"GPT4TS\" based on the provided descriptions.", "id": "GPT4TS", "label": "GPT4TS", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,1ddbab2dca370c9ef7b5a724075518cc,2565ae205d4c98342168bb67a4f7a309,2eea45c6111e468189580a21686cb14a,3e937ba8de0e7eca993c50506ceb8f1f,41fe893a178ebc8790ef4da83da5ab6e,532a6d434dab611a80aef1e94dd2fb45,56de1d5a5b467101344afa4248d829dc,5b1135d9e53e53428a042e8bbc89eaa7,5e4d9ca02ee6a285d5223c820743eb12,5fa25d3abec59ccd2718e00c0e0eb440,6d66c2ea37e25b646a13d751c05a8e4d,7cb1ca3f60421fbd20d6ae39bbf2b296,7e03744dea80e2138baff03611104fa8,84bc2afcbbd278961c3c7a637c6a189e,91e161ba596a0cbbcae541ddb2106310,9ba0189af2ef0720a721c16eef0f0788,ab91381dd032db318e5aec1ad5b914a6,ad421ec9ba83531336aaf3bec414b3d5,af36d1634490149b96980fb1dff57cd1,bc54a718d1886698232d578fd88c3ac7,d4551c2839eaa68a7cb7324089956581,d4e3d8b5bf043b78bb9f1551080cab91,f49330b6fd81d86d14e7a9d4b8e45576,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"DLINER\" can be generated as follows:\n\nDLINER is a deep learning model primarily used for time series forecasting and analysis. It is a type of model that employs a linear approach to forecasting, which is evident from its application in short-term time series forecasting. DLINER is mentioned alongside other models such as WaveNet, DeepAR, N-BEATS, TFT, PatchTST, N-HiTS, and GPT4TS, suggesting its relevance in the field of time series forecasting. Furthermore, DLINER is compared with TIME-LLM in terms of average enhancements, indicating its performance in this context. Overall, DLINER is a model that has been utilized for forecasting and time series analysis, with a focus on short-term forecasting.\n\nThis summary is based on the information collected from all the descriptions provided, and it resolves any potential contradictions by presenting a coherent and concise overview of the entity \"DLINER\".", "id": "DLINER", "label": "DLINER", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,2565ae205d4c98342168bb67a4f7a309,3e937ba8de0e7eca993c50506ceb8f1f,41fe893a178ebc8790ef4da83da5ab6e,5e4d9ca02ee6a285d5223c820743eb12,7e03744dea80e2138baff03611104fa8,91e161ba596a0cbbcae541ddb2106310,af36d1634490149b96980fb1dff57cd1,d4551c2839eaa68a7cb7324089956581,f6fac8e5c6fd12724fe3aa84a2e1cfa6", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"N-HITS\" can be generated as follows:\n\nN-HITS is a recent state-of-the-art time series forecasting model that performs well for long-term and short-term forecasting tasks. It is a type of HiTS model that is used for time series forecasting and has been compared to other models such as TIME-LLM in terms of metrics such as MASE and OWA. N-HITS has exhibited comparable or superior performances to TIME-LLM in short-term forecasting and has placed 2nd in terms of point forecasting performance. It is mentioned in the text as a model that is used for short-term time series forecasting and is considered a recent state-of-the-art forecasting model.\n\nThe language used to describe N-HITS is formal and academic, suggesting that it is from a research paper or a technical document. The text contains technical terms and mathematical equations related to time series forecasting, which are written in a standard mathematical notation used in English-language academic papers. The use of English-language abbreviations and citations of English-language academic papers further supports the conclusion that the primary language of the text is English.\n\nOverall, N-HITS is a recent and effective time series forecasting model that has been compared to other models and has shown promising results in short-term forecasting tasks.", "id": "N-HITS", "label": "N-HITS", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,2bc70082c142ee2ef5d6a941611505b7,41fe893a178ebc8790ef4da83da5ab6e,532a6d434dab611a80aef1e94dd2fb45,5e4d9ca02ee6a285d5223c820743eb12,5fa25d3abec59ccd2718e00c0e0eb440,66b7675d8c62321e1cc8916401159787,6d66c2ea37e25b646a13d751c05a8e4d,9ba0189af2ef0720a721c16eef0f0788,ab91381dd032db318e5aec1ad5b914a6,af36d1634490149b96980fb1dff57cd1,d40f5dc25597e4e4d7c30b8bfd98f89a,d4551c2839eaa68a7cb7324089956581,ef2ef6c95c36f51706c54ca3f2893641,f49330b6fd81d86d14e7a9d4b8e45576,fd1092903d83bf6e90a6caa371d7c514", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"M4\" refers to a dataset used for benchmarking short-term forecasting models, specifically in the context of time series forecasting. It is a comprehensive dataset that comprises 100K time series, amassed from various domains commonly present in business, financial, and economic forecasting. The M4 dataset contains data from various domains at different sampling periods, making it a suitable benchmark for evaluating the performance of short-term forecasting models. It is used in a forecasting competition and contains energy and economic data, making it a valuable resource for researchers and practitioners in the field of time series forecasting.\n\nThis summary is written in third person and includes information collected from all the descriptions provided. It resolves any potential contradictions and provides a single, coherent summary of the entity \"M4\".", "id": "M4", "label": "M4", "shape": "dot", "size": 10, "source_id": "50eeacd99c68b2581be90310bedcbc2c,5e4d9ca02ee6a285d5223c820743eb12,a875a1c0bede47a1c4e3823be81c42c6,d4551c2839eaa68a7cb7324089956581,d4e3d8b5bf043b78bb9f1551080cab91,e067234b24b0625a3f95ecc18035f915", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"ETSFORMER\" can be generated as follows:\n\nETSFORMER is a type of transformer model specifically designed for time series forecasting tasks. It replaces traditional self-attention mechanisms with exponential smoothing attention and frequency attention, allowing it to effectively capture complex patterns and trends in time series data. ETSFORMER is primarily used for short-term time series forecasting, as indicated by its application in a table mentioned in the text. The model is mentioned in the text as a specific example of a model used for time series forecasting, and its use is likely due to its ability to leverage complex inductive biases to model time series data.\n\nThe language used to describe ETSFORMER is formal and academic, suggesting that it is a model developed for research purposes. The text also mentions that ETSFORMER is a model used for long-term series forecasting, which may indicate that it has the capability to handle both short-term and long-term forecasting tasks.\n\nOverall, ETSFORMER appears to be a powerful and versatile model for time series forecasting, capable of handling complex patterns and trends in time series data. Its use of exponential smoothing attention and frequency attention mechanisms sets it apart from traditional transformer models, and its application in short-term forecasting tasks suggests that it may be particularly well-suited for this type of task.\n\nIt is worth noting that the text also mentions the use of ETSFORMER in a table, which may indicate that it has been evaluated on a specific dataset or task. However, without further information, it is difficult to determine the specific details of this evaluation.", "id": "ETSFORMER", "label": "ETSFORMER", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,79c41aff65d584b4c8d4c769b82756d5,7e97089185883c456c798ddc5ec86373,91e161ba596a0cbbcae541ddb2106310,990578022879395d00b7a5b229863c2f,9ba0189af2ef0720a721c16eef0f0788,bb87457fce8d4214bfe1f398b7ea35f2,bcf830b85e12e1ab9498e3ac3593a88e,c84edbea28fbbed451e8d0b7df4ffb7c,d4551c2839eaa68a7cb7324089956581,f0c52387a7b3a5c3850fe6991f0a7c83,f49330b6fd81d86d14e7a9d4b8e45576,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"LIGHTTS\" can be described as follows:\n\nLIGHTTS is a time series model used for forecasting, specifically for short-term time series forecasting. It is a model mentioned in the text, which is likely a type of model used for time series analysis and forecasting. The model is used for predicting future values in a time series, and it is indicated by the use of the phrase \"LightTS\" in the text. Additionally, the model is mentioned in a table, suggesting its application in short-term time series forecasting.\n\nThe description of LIGHTTS is consistent across the provided descriptions, with all of them indicating that it is a model used for time series analysis and forecasting. The only variation is in the level of specificity, with some descriptions mentioning short-term time series forecasting and others mentioning time series analysis in general. However, these variations can be reconciled by understanding that LIGHTTS is a general-purpose time series model that can be applied to both short-term and long-term forecasting tasks.\n\nOverall, the description of LIGHTTS is clear and concise, providing a comprehensive understanding of the entity\u0027s purpose and application.", "id": "LIGHTTS", "label": "LIGHTTS", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,7e97089185883c456c798ddc5ec86373,91e161ba596a0cbbcae541ddb2106310,9ba0189af2ef0720a721c16eef0f0788,c84edbea28fbbed451e8d0b7df4ffb7c,d4551c2839eaa68a7cb7324089956581,f49330b6fd81d86d14e7a9d4b8e45576,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nLLAMA-7B is a model primarily used for text-based tasks. It serves as the backbone for TIME-LLM, indicating its significance in the development of this particular model. The primary language of the text related to LLAMA-7B is English, as evident from the technical terms, mathematical equations, and references to academic papers written in English. This suggests that the text is from a research paper or a technical document, likely from a conference or journal such as ICLR, AAAI, or PMLR.", "id": "LLAMA-7B", "label": "LLAMA-7B", "shape": "dot", "size": 10, "source_id": "072b166b9a1b6afecf5874f45af61699,1b48e9ca066ac5ba037066bb762d3458,7e03744dea80e2138baff03611104fa8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nICLR 2024 is a conference where research papers are presented and published. The conference is associated with the presentation and publication of a specific paper, which was presented and published at the event. \n\nThis summary is derived from the descriptions provided, which collectively convey that ICLR 2024 is a conference focused on research paper presentation and publication. The descriptions are not contradictory, and the information is consistent across all three points. \n\nThe entity name \"ICLR 2024\" is included in the summary to provide context and clarity. The summary is written in the third person and includes relevant information from the descriptions.", "id": "ICLR 2024", "label": "ICLR 2024", "shape": "dot", "size": 10, "source_id": "072b166b9a1b6afecf5874f45af61699,41fe893a178ebc8790ef4da83da5ab6e,4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,5e4d9ca02ee6a285d5223c820743eb12,5fa25d3abec59ccd2718e00c0e0eb440,69457f873272a693c1f813c75ecf030a,9ba0189af2ef0720a721c16eef0f0788,a69a914fb6c895c7202532b69ad3e094,b27c89cb0db6646b1203b2701e017aeb,b67d18d306fde251ee94b0a831d1e075,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,d4e3d8b5bf043b78bb9f1551080cab91,e57d44d23f5a3a82ce9a9b9532d31cbe,e6a6bc6fbd362394320961ac10cdd230,e8151dde9661b4cce6a6b8e5a8371c96,f6fac8e5c6fd12724fe3aa84a2e1cfa6", "type": "EVENT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"ETTH1\" can be generated as follows:\n\n\"ETTH1 is a dataset used for time series forecasting, specifically designed for evaluating the performance of machine learning models in this domain. It is a dataset used in an experiment and is mentioned in the text as indicated by the phrase \u0027Forecasting protocol on ETTh1\u0027. ETTH1 is a dataset sampled at a 1-hour level and is used for benchmarking long-term forecasting models, as well as various supervised long-horizon forecasting methods. The dataset is used for testing models and is a specific dataset used for evaluation purposes, making it a valuable resource for researchers and practitioners in the field of time series forecasting.\"\n\nThis summary takes into account all the provided descriptions, resolves any potential contradictions, and provides a coherent and concise description of the entity \"ETTH1\".", "id": "ETTH1", "label": "ETTH1", "shape": "dot", "size": 10, "source_id": "072b166b9a1b6afecf5874f45af61699,41fe893a178ebc8790ef4da83da5ab6e,4ade7764ebe998b5f35a7fd2b58d4796,50bf8b7f27ec843882dbc6eadb2bf158,50eeacd99c68b2581be90310bedcbc2c,5e4d9ca02ee6a285d5223c820743eb12,7cb1ca3f60421fbd20d6ae39bbf2b296,7e97089185883c456c798ddc5ec86373,84bc2afcbbd278961c3c7a637c6a189e,ad421ec9ba83531336aaf3bec414b3d5,d4e3d8b5bf043b78bb9f1551080cab91,efb7975581b22620b8277417edeee3e9", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"PARAMETERS\" can be generated as follows:\n\nThe entity \"PARAMETERS\" refers to the adjustable values in a model, specifically the model used to train and evaluate Lag-Llama. These parameters are also known as the model\u0027s hyperparameters, which are the number of parameters in a language model. Furthermore, parameters refer to the weights and biases of a machine learning model, a neural network model, and the model itself. In essence, parameters are the adjustable values that are used to train and evaluate a model, and they play a crucial role in determining the model\u0027s performance.\n\nThe summary is written in third person and includes the entity name \"PARAMETERS\" for context. It also resolves any contradictions in the descriptions and provides a single, coherent summary. Relevant information from the nearby text is not provided as there is no nearby text. The summary is based solely on the provided descriptions.", "id": "PARAMETERS", "label": "PARAMETERS", "shape": "dot", "size": 10, "source_id": "1f1221583d838c2407fe9864225e9eda,50bf8b7f27ec843882dbc6eadb2bf158,6c11bd339c9630f1d61f2024e90bce5e,84bc2afcbbd278961c3c7a637c6a189e,efb7975581b22620b8277417edeee3e9,f7ce89346ea6715560163ef3a630a5df,fc51ca9f56bcadd343d50d9cb5cf9adb,fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"MEMORY\" refers to the amount of memory used by a model, specifically the model\u0027s memory usage. This description is consistent across all provided descriptions, indicating that the entity \"MEMORY\" is related to the measurement of a model\u0027s memory consumption.\n\nIn the context of machine learning and time series forecasting, memory usage is an important factor to consider, as it can impact the performance and efficiency of a model. The descriptions provided suggest that the entity \"MEMORY\" is a critical aspect of model development and evaluation.\n\nOverall, the summary provides a clear and concise understanding of the entity \"MEMORY\" and its relevance to machine learning and time series forecasting.", "id": "MEMORY", "label": "MEMORY", "shape": "dot", "size": 10, "source_id": "1902e651467179a9a1d4c4df0035e980,50bf8b7f27ec843882dbc6eadb2bf158,84bc2afcbbd278961c3c7a637c6a189e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"SPEED\" has two related descriptions. After analyzing these descriptions, a comprehensive summary can be generated.\n\nThe entity \"SPEED\" refers to the model\u0027s computational speed, which encompasses the time it takes for the model to process and execute tasks. This includes the time it takes to train or run a model, indicating that speed is a critical factor in the model\u0027s performance and efficiency.\n\nIn the context of machine learning, speed is a crucial aspect of model development, as it directly impacts the model\u0027s ability to process large datasets, make predictions, and provide accurate results in a timely manner. Therefore, the entity \"SPEED\" can be summarized as follows:\n\nThe entity \"SPEED\" refers to the model\u0027s computational speed, which encompasses the time it takes for the model to train or run, directly impacting its performance, efficiency, and ability to process large datasets in a timely manner.", "id": "SPEED", "label": "SPEED", "shape": "dot", "size": 10, "source_id": "50bf8b7f27ec843882dbc6eadb2bf158,84bc2afcbbd278961c3c7a637c6a189e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"PATCH REPROGRAMMING\" can be generated as follows:\n\nPATCH REPROGRAMMING refers to a concept and technique used in the context of modifying or updating a model or system, particularly in the field of time series forecasting. It involves adapting a model to reprogram time series patches or modifying time series patches to align the modalities of time series and natural language. This process is closely related to word embeddings, which are used in patch reprogramming to effectively update or modify the model or system. The technique is specifically used in Large Language Models (LLMs) for effective time series forecasting, allowing for the modification or updating of models or systems to better align with the data representation space.\n\nThis summary is based on the information collected from all the descriptions provided, and any contradictions have been resolved to provide a single, coherent summary. The summary is written in third person and includes the entity name for context. Relevant information from the nearby text has been enriched to provide a comprehensive understanding of the concept of PATCH REPROGRAMMING.", "id": "PATCH REPROGRAMMING", "label": "PATCH REPROGRAMMING", "shape": "dot", "size": 10, "source_id": "072b166b9a1b6afecf5874f45af61699,2bb4fc2b46b9c8bdd052b2755d986aa8,56806630593fdf0c3d95ad7555080dcf,7e03744dea80e2138baff03611104fa8,e6a6bc6fbd362394320961ac10cdd230,f7266cfccedb1d9840d10afa689a05e9,fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Tab. 9 is a table detailing the experimental configurations for TIME-LLM", "id": "TAB. 9", "label": "TAB. 9", "shape": "dot", "size": 10, "source_id": "306c29917039736b5e882376c7647704", "type": "TABLE"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"MODEL TRAINING\" refers to the process of training a model on a dataset. This process is specifically used to train the TIME-LLM model, as indicated by the configurations in Table 9. The TIME-LLM model is a type of model that is likely used for long-term series forecasting, as suggested by the context of the text. The model training process involves using a dataset to train the model, which is a crucial step in developing accurate models for time series forecasting.\n\nThe language used in the text is English, as indicated by the presence of technical terms, mathematical equations, and references to academic papers written in English. The text is formal and academic in tone, suggesting that it is from a research paper or a technical document.\n\nOverall, the summary provides a clear understanding of the entity \"MODEL TRAINING\" and its relationship to the TIME-LLM model, as well as the language and context in which the text is written.", "id": "MODEL TRAINING", "label": "MODEL TRAINING", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,306c29917039736b5e882376c7647704,f0c52387a7b3a5c3850fe6991f0a7c83,fa01b75dccc556af8aca0d6c47e1970f", "type": ""}, {"color": "#97c2fc", "description": "", "id": "SHORT-TERM FORECASTING", "label": "SHORT-TERM FORECASTING", "shape": "dot", "size": 10, "source_id": "ef2ef6c95c36f51706c54ca3f2893641", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"BASELINE MODELS\" refers to a set of models used as a reference or comparison for other models, specifically in the context of short-term forecasting. These models are used to establish a baseline performance or accuracy, allowing for the evaluation and comparison of other models, such as the TranAD model. In this context, baseline models serve as a point of reference for assessing the effectiveness of alternative models in short-term forecasting applications.", "id": "BASELINE MODELS", "label": "BASELINE MODELS", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,ef2ef6c95c36f51706c54ca3f2893641", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe M3-Quarterly dataset is a collection of 756 quarterly sampled time series used for evaluating the performance of models in short-term forecasting. This dataset is part of the M3 benchmark, which serves as a standard for assessing the accuracy and reliability of forecasting models. The M3-Quarterly dataset is specifically designed for short-term forecasting, making it a valuable resource for researchers and practitioners in the field of time series analysis and forecasting.\n\nThe dataset\u0027s primary purpose is to provide a benchmark for evaluating the performance of forecasting models, allowing users to compare and contrast the accuracy of different models and techniques. By leveraging the M3-Quarterly dataset, users can gain insights into the strengths and weaknesses of various forecasting approaches and make informed decisions about which models to use in their own applications.\n\nOverall, the M3-Quarterly dataset is a valuable resource for anyone interested in short-term forecasting and time series analysis, offering a comprehensive and well-established benchmark for evaluating the performance of forecasting models.", "id": "M3-QUARTERLY", "label": "M3-QUARTERLY", "shape": "dot", "size": 10, "source_id": "50eeacd99c68b2581be90310bedcbc2c,ef2ef6c95c36f51706c54ca3f2893641", "type": "DATASET"}, {"color": "#97c2fc", "description": "M3-Quarterly dataset is a dataset used for comparison in the text", "id": "M3-QUARTERLY DATASET", "label": "M3-QUARTERLY DATASET", "shape": "dot", "size": 10, "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "type": "DATASET"}, {"color": "#97c2fc", "description": "M4-Yearly dataset is a dataset used for comparison in the text", "id": "M4-YEARLY DATASET", "label": "M4-YEARLY DATASET", "shape": "dot", "size": 10, "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "type": "DATASET"}, {"color": "#97c2fc", "description": "M4-Hourly dataset is a dataset used for comparison in the text", "id": "M4-HOURLY DATASET", "label": "M4-HOURLY DATASET", "shape": "dot", "size": 10, "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "type": "DATASET"}, {"color": "#97c2fc", "description": "M4-Daily dataset is a dataset used for comparison in the text", "id": "M4-DAILY DATASET", "label": "M4-DAILY DATASET", "shape": "dot", "size": 10, "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "type": "DATASET"}, {"color": "#97c2fc", "description": "M4-Weekly dataset is a dataset used for comparison in the text", "id": "M4-WEEKLY DATASET", "label": "M4-WEEKLY DATASET", "shape": "dot", "size": 10, "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"ILI\" can be generated as follows:\n\nThe entity \"ILI\" refers to the Influenza-Like Illness dataset, which is a type of time series data used for benchmarking long-term forecasting models and comparison in various studies. Specifically, ILI is a dataset that contains records of patients experiencing severe influenza with complications, indicating its relation to a type of illness or disease. Furthermore, ILI is also associated with a specific type of forecasting horizon and is used in both short-term and long-term forecasting applications.\n\nThe ILI dataset is characterized by the presence of ILI-related metrics, suggesting its relevance to the field of healthcare and epidemiology. The use of ILI in forecasting models implies its importance in predicting and managing the spread of influenza-like illnesses.\n\nOverall, the entity \"ILI\" encompasses a multifaceted concept that spans both the technical and medical domains, highlighting its significance in the fields of machine learning, time series forecasting, and public health.", "id": "ILI", "label": "ILI", "shape": "dot", "size": 10, "source_id": "27efab80b405b365d8e9dd9834dd1ca8,50eeacd99c68b2581be90310bedcbc2c,5e4d9ca02ee6a285d5223c820743eb12,5fa25d3abec59ccd2718e00c0e0eb440,9ba0189af2ef0720a721c16eef0f0788,bb03c20a6fc1d9af2f3cbfa55b50cfb8,f6fac8e5c6fd12724fe3aa84a2e1cfa6", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"DLINEAR\" can be generated as follows:\n\nDLINEAR is a time series model that is mentioned in the text. It is likely a type of linear model, as indicated by the use of the phrase \"DLinear\" and its performance comparison with local statistical models. Specifically, DLINEAR is reported to perform better than local statistical models, and it places 7th in terms of point forecasting performance. This suggests that DLINEAR is a model that is capable of providing accurate predictions for time series data, and its performance is comparable to other models in the field.\n\nOverall, the information provided about DLINEAR suggests that it is a type of linear time series model that is designed for forecasting and prediction tasks. Its performance is notable, as it outperforms local statistical models and achieves a respectable ranking in terms of point forecasting performance.", "entity_type": "MODEL", "id": "DLINEAR", "label": "DLINEAR", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,532a6d434dab611a80aef1e94dd2fb45,9ba0189af2ef0720a721c16eef0f0788,f49330b6fd81d86d14e7a9d4b8e45576,fd1092903d83bf6e90a6caa371d7c514", "type": "MODEL"}, {"color": "#97c2fc", "description": "Visualization is a concept that is used to demonstrate the performance of models", "id": "VISUALIZATION", "label": "VISUALIZATION", "shape": "dot", "size": 10, "source_id": "ab91381dd032db318e5aec1ad5b914a6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nETTH2 is a dataset used for evaluating the performance of machine learning models in time-series forecasting, particularly for long-term forecasting. It is a specific dataset sampled at a 1-hour level, used for benchmarking various supervised long-horizon forecasting methods, including long-term series forecasting. ETTH2 is utilized for testing and evaluating the performance of models in time-series forecasting, making it a crucial dataset for benchmarking and comparison purposes.\n\nThe dataset is used to assess the capabilities of machine learning models in predicting future values in a time-series, with a focus on long-term forecasting. ETTH2 is an essential resource for researchers and developers working on time-series forecasting, providing a standardized dataset for evaluating and comparing the performance of different models and methods.\n\nThe information collected from all the descriptions is as follows:\n\n- ETTH2 is a dataset used in the experiment.\n- ETTH2 is a dataset used to evaluate the performance of machine learning models for time-series forecasting.\n- ETTH2 is a dataset sampled at a 1-hour level.\n- ETTh2 is a dataset that is used for benchmarking long-term forecasting models.\n- ETTh2 is a dataset that is used for time series forecasting.\n- ETTh2 is a dataset used for benchmarking various supervised long-horizon forecasting methods.\n- ETTh2 is a dataset used for testing the models.\n- ETTh2 is a specific dataset used for evaluation.\n\nThe contradictions in the descriptions have been resolved by considering the context and the information provided. The primary language of the text is English, which is confirmed by the use of English words and phrases, mathematical equations, and references to academic papers.", "id": "ETTH2", "label": "ETTH2", "shape": "dot", "size": 10, "source_id": "41fe893a178ebc8790ef4da83da5ab6e,4ade7764ebe998b5f35a7fd2b58d4796,50eeacd99c68b2581be90310bedcbc2c,5e4d9ca02ee6a285d5223c820743eb12,7cb1ca3f60421fbd20d6ae39bbf2b296,84bc2afcbbd278961c3c7a637c6a189e,ad421ec9ba83531336aaf3bec414b3d5,efb7975581b22620b8277417edeee3e9", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"ETTM1\" can be generated as follows:\n\n\"ETTM1 is a dataset specifically designed for time series forecasting, used in various experiments and evaluations to assess the performance of machine learning models for long-term forecasting. It is a 15-minute sampled dataset used for benchmarking long-term forecasting models, as well as for testing and evaluating the performance of various supervised long-horizon forecasting methods. ETTM1 is utilized for time series forecasting, serving as a benchmark for evaluating the efficacy of different models and techniques in this domain.\"\n\nThis summary incorporates information from all the provided descriptions, resolving any potential contradictions and presenting a coherent overview of the entity \"ETTM1\".", "id": "ETTM1", "label": "ETTM1", "shape": "dot", "size": 10, "source_id": "072b166b9a1b6afecf5874f45af61699,41fe893a178ebc8790ef4da83da5ab6e,4ade7764ebe998b5f35a7fd2b58d4796,50eeacd99c68b2581be90310bedcbc2c,5e4d9ca02ee6a285d5223c820743eb12,7cb1ca3f60421fbd20d6ae39bbf2b296,84bc2afcbbd278961c3c7a637c6a189e,ad421ec9ba83531336aaf3bec414b3d5,d4e3d8b5bf043b78bb9f1551080cab91,efb7975581b22620b8277417edeee3e9", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"ETTM2\" can be generated as follows:\n\nETTM2 is a dataset sampled at a 15-minute level, specifically designed for benchmarking long-term forecasting models and time series forecasting. It is used for testing and evaluating various supervised long-horizon forecasting methods, as well as for short-term forecasting, as indicated by its performance on M4 datasets. ETTM2 is a specific dataset used for evaluation, and its primary purpose is to serve as a benchmark for long-term forecasting models.\n\nThis summary incorporates all the provided descriptions, resolving any potential contradictions and providing a single, coherent description of the entity \"ETTM2\". The information is written in third person, and the entity name is included for context. Relevant information from the nearby text has been enriched into the summary, providing a comprehensive understanding of the dataset.", "id": "ETTM2", "label": "ETTM2", "shape": "dot", "size": 10, "source_id": "27efab80b405b365d8e9dd9834dd1ca8,41fe893a178ebc8790ef4da83da5ab6e,4ade7764ebe998b5f35a7fd2b58d4796,50eeacd99c68b2581be90310bedcbc2c,5e4d9ca02ee6a285d5223c820743eb12,84bc2afcbbd278961c3c7a637c6a189e,efb7975581b22620b8277417edeee3e9", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"PARAMETER-EFFICIENT FINE-TUNING\" is a model and a technique used in machine learning, as indicated by the use of the phrase \"parameter-efficient fine-tuning (PEFT)\" in the text. It refers to a method of fine-tuning a model with a reduced number of parameters, allowing for minimal changes to its parameters. This technique is used to fine-tune models, making it a valuable approach in the field of machine learning.\n\nThe use of \"parameter-efficient fine-tuning\" is mentioned in the context of time series analysis and long-term series forecasting, suggesting its application in predicting and analyzing temporal data. The technique is likely used in conjunction with other methods, such as frequency analysis and multi-head cross-attention, to improve the accuracy and efficiency of model fine-tuning.\n\nThe mention of academic conferences and journals, such as ICLR, AAAI, and PMLR, suggests that \"parameter-efficient fine-tuning\" is a topic of interest in the research community, with papers and studies being published on the subject. Overall, \"parameter-efficient fine-tuning\" is a significant concept in machine learning, offering a way to fine-tune models with minimal changes to their parameters, making it a valuable tool for researchers and practitioners alike.", "id": "PARAMETER-EFFICIENT FINE-TUNING", "label": "PARAMETER-EFFICIENT FINE-TUNING", "shape": "dot", "size": 10, "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8,7e97089185883c456c798ddc5ec86373,84bc2afcbbd278961c3c7a637c6a189e,f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "PatchTST (2023) is a model used for short-term forecasting, as indicated by its performance on M4 datasets", "id": "PATCHTST (2023)", "label": "PATCHTST (2023)", "shape": "dot", "size": 10, "source_id": "27efab80b405b365d8e9dd9834dd1ca8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"MONTHLY\" can be generated as follows:\n\nThe entity \"MONTHLY\" refers to a time series with a frequency of 12, indicating that it is a periodic dataset with 12 observations or data points per year. This frequency is commonly used in time series analysis and forecasting models. In the context of the model, \"MONTHLY\" is used to denote a specific period of time, which is typically 30 or 31 days, spanning from the 1st to the last day of the month. This period is used to collect and analyze data, which is then employed in short-term forecasting applications. The \"MONTHLY\" dataset is a type of data used in short-term forecasting, suggesting that it is used to predict future values or trends over a relatively short period of time.\n\nThis summary is based on the information collected from all the descriptions provided, and it resolves any potential contradictions by providing a clear and concise definition of the entity \"MONTHLY\". The summary is written in third person and includes the entity name for context. Relevant information from the nearby text has been incorporated to enrich the summary and provide a more comprehensive understanding of the entity.", "id": "MONTHLY", "label": "MONTHLY", "shape": "dot", "size": 10, "source_id": "27efab80b405b365d8e9dd9834dd1ca8,ac37bdb991d1513e44e4c5fc7a28b187,d4e3d8b5bf043b78bb9f1551080cab91,edab6fd342bc0a563fd98a50eb8b3462,f8afc5a341360ab82dfbe9ebbb7f8e84", "type": "TIME"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nN-HITS (2023A) is a model used for short-term forecasting, as indicated by its performance on M4 datasets. It is also a paper referenced in the text, suggesting that it is a research paper or academic document related to time series forecasting. The model\u0027s performance on the M4 datasets implies that it is capable of handling complex time series data and making accurate predictions for short-term forecasting tasks.\n\nGiven the context of the text, it appears that N-HITS (2023A) is a model that utilizes techniques such as frequency analysis and multi-head cross-attention, as these terms are mentioned in the text. The model\u0027s performance on the M4 datasets suggests that it is a robust and reliable tool for short-term forecasting tasks.\n\nOverall, N-HITS (2023A) is a model that has been developed for short-term forecasting tasks, and its performance on the M4 datasets suggests that it is a valuable tool for researchers and practitioners in the field of time series forecasting.", "id": "N-HITS (2023A)", "label": "N-HITS (2023A)", "shape": "dot", "size": 10, "source_id": "27efab80b405b365d8e9dd9834dd1ca8,d4e3d8b5bf043b78bb9f1551080cab91", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nNBEATS is a model or algorithm used for time series forecasting. It is specifically designed for this purpose, as indicated by its application in forecasting long-term series. The model\u0027s primary function is to analyze and predict future values in a time series, which is a sequence of data points measured at regular time intervals.\n\nThe use of NBEATS is mentioned in the context of time series forecasting, suggesting its relevance in this area of study. The model\u0027s effectiveness in this domain is further supported by its inclusion in academic discussions and research papers, as indicated by the presence of technical terms and mathematical equations related to time series analysis.\n\nIn terms of its structure and functionality, NBEATS is described as a model used for time series forecasting, implying its ability to process and analyze data in this format. The model\u0027s name, NBEATS, is also mentioned in the text, further confirming its association with time series forecasting.\n\nOverall, NBEATS appears to be a model or algorithm specifically designed for time series forecasting, with a focus on analyzing and predicting future values in a sequence of data points measured at regular time intervals.", "id": "NBEATS", "label": "NBEATS", "shape": "dot", "size": 10, "source_id": "376897a501ac50834f626fcc5fb13ece,6fa5f635ad5f7e6b67d5e467f130345c,79c41aff65d584b4c8d4c769b82756d5,990578022879395d00b7a5b229863c2f,bb87457fce8d4214bfe1f398b7ea35f2", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"CRPS\" refers to a concept that is a commonly used metric for evaluating probabilistic predictions. It is a specific measure related to performance or evaluation, as indicated by its mention in the text, including the phrase \"CRPS of Lag-Llama\". This suggests that CRPS is a widely recognized and utilized metric in the field, particularly in the context of evaluating the performance of models such as Lag-Llama.\n\nThe use of CRPS as a metric implies that it is a quantitative measure that can be used to assess the accuracy or quality of probabilistic predictions. This is consistent with the entity\u0027s description as a \"metric mentioned in the text\", which further reinforces its role as a performance evaluation tool.\n\nOverall, the summary of CRPS is that it is a widely used metric for evaluating probabilistic predictions, specifically in the context of performance or evaluation, and is mentioned in the text as a relevant concept.", "id": "CRPS", "label": "CRPS", "shape": "dot", "size": 10, "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c,990578022879395d00b7a5b229863c2f,d08a82328191907abfcdb72efab9a6cf,e37f4063d074e1697e1f539131b3d963,ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data is as follows:\n\nThe entity \"MODEL\" refers to a mathematical representation of a system or process, used to make predictions or decisions. In the context of machine learning, a model is a machine learning model, specifically a foundation model for time-series forecasting. This model is a mathematical representation or algorithm used to solve a problem or make predictions, often employed in machine learning and data analysis. The model in question is a neural network that can be trained using input batches, and it is the machine learning model being trained and evaluated.\n\nThe model\u0027s primary function is to make predictions or decisions based on its mathematical representation of a system or process. It is a crucial component in time-series forecasting, enabling the prediction of future values based on historical data. The model\u0027s architecture, as a neural network, allows it to learn complex patterns and relationships within the data, making it a powerful tool for forecasting and decision-making.\n\nOverall, the \"MODEL\" entity is a critical component in machine learning and data analysis, particularly in the context of time-series forecasting. Its mathematical representation and algorithmic capabilities make it an essential tool for making predictions and decisions in various fields.", "id": "MODEL", "label": "MODEL", "shape": "dot", "size": 10, "source_id": "5277ea101e78f34ea2c62fa10007b2ac,6ea15432a841705c2e74cbc01f6004e9,79c41aff65d584b4c8d4c769b82756d5,7cb1ca3f60421fbd20d6ae39bbf2b296,c60a8238db3d56eff9cc9692e7ac5b1c,cc1063ba5913ea38c84ac0250c01fe84,cd4ff69415c7b37cec643f8289d1c98d", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nLLM4TS is a model designed for time series forecasting that utilizes pre-trained language models. It is a pre-trained model specifically used in time series analysis, leveraging the capabilities of language models to forecast future values in a time series. The model\u0027s primary application is in predicting future values in a time series, making it a valuable tool for various industries and fields that rely on time series data.\n\nThe use of pre-trained language models in LLM4TS enables it to capture complex patterns and relationships within the time series data, allowing for more accurate and reliable forecasting. This approach also enables the model to handle long-term series forecasting, which is a challenging task in time series analysis.\n\nOverall, LLM4TS is a powerful tool for time series forecasting that combines the strengths of pre-trained language models with the specific needs of time series analysis. Its ability to handle complex patterns and relationships in time series data makes it a valuable asset for various applications, including but not limited to, finance, economics, and climate modeling.\n\nThe model\u0027s design and application are consistent with the use of English language in technical and academic contexts, as indicated by the presence of technical terms, mathematical equations, and references to academic papers. This further supports the conclusion that LLM4TS is a model developed and used in an English-speaking context.", "id": "LLM4TS", "label": "LLM4TS", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\n\"TEMPO\" is a pre-trained model used in time series analysis, specifically designed for time series forecasting. It leverages pre-trained language models to forecast future values in a time series. Additionally, TEMPO employs decomposition strategies to separate complex interactions into trend, seasonal, and residual components, allowing for a more accurate and detailed analysis of the time series data. This approach enables TEMPO to effectively capture and model the underlying patterns and relationships within the data, making it a valuable tool for time series forecasting and analysis.\n\nThe use of pre-trained language models and decomposition strategies in TEMPO suggests that it is a sophisticated and versatile model capable of handling complex time series data. Its application in time series analysis and forecasting makes it a valuable resource for researchers and practitioners in the field.", "id": "TEMPO", "label": "TEMPO", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,5805d8a1afe3d6bc6300ac71f7163831,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"PROMPTCAST\" can be generated as follows:\n\nPROMPTCAST is a pre-trained model used in time series analysis that leverages language models for forecasting. It converts numerical inputs and outputs into text-based input and output pairs, framing the forecasting task as a sentence-to-sentence conversion. This approach enables the use of pre-trained Large Language Models (LLMs) for time series forecasting, transforming time series data into prompts that can be directly processed by language models. As a result, PROMPTCAST is a model for time series forecasting that utilizes the capabilities of LLMs to generate accurate predictions.\n\nThe summary is written in third person and includes the entity name \"PROMPTCAST\" for context. It also incorporates relevant information from the descriptions, resolving any potential contradictions and providing a coherent overview of the entity.", "id": "PROMPTCAST", "label": "PROMPTCAST", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,1f1221583d838c2407fe9864225e9eda,ad8efcfda80253036294f2eeb48926e8,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Time series tasks refer to tasks that involve analyzing and forecasting data that changes over time", "id": "TIME SERIES TASKS", "label": "TIME SERIES TASKS", "shape": "dot", "size": 10, "source_id": "ad8efcfda80253036294f2eeb48926e8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Decomposition strategies are specialized approaches used in time series forecasting models, as implemented by TEMPO, to separate complex interactions into trend, seasonal, and residual components", "id": "DECOMPOSITION STRATEGIES", "label": "DECOMPOSITION STRATEGIES", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"TIMESFM\" can be described as follows:\n\nTIMESFM is a decoder-only foundation model specifically designed for time-series forecasting tasks. It is a type of machine learning model that performs well for time-series forecasting tasks, including zero-shot forecasting. TIMESFM is a model that correctly captures the amplitude increase with trend, making it a suitable choice for time-series forecasting applications. It is also mentioned as a top model in the text and is used for evaluation purposes. Additionally, TIMESFM is compared to PatchTST in an ablation study, highlighting its performance and effectiveness in time-series forecasting. Overall, TIMESFM is a decoder-only foundation model that is well-suited for time-series forecasting tasks, including zero-shot forecasting.\n\nRelevant information from the nearby text includes:\n\n* TIMESFM is a decoder-only model for time series forecasting, indicating its focus on decoding and generating time-series data.\n* TIMESFM is a specific decoder-only foundation model designed for time-series forecasting tasks, emphasizing its tailored design for this specific application.\n* TIMESFM is a model used for time-series forecasting, which is a common application of machine learning models in this domain.\n* TIMESFM is a model that performs well for time-series forecasting tasks, indicating its effectiveness in this area.\n* TIMESFM is a model used in the ablation study, which is compared to PatchTST, highlighting its performance and comparison to other models in the field.\n\nOverall, TIMESFM is a decoder-only foundation model that is specifically designed for time-series forecasting tasks, performs well in this area, and is compared to other models in the field.", "id": "TIMESFM", "label": "TIMESFM", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,269a6f95aee3c9e28d0290fd10ed476d,28b097d339554431fa14e113c98ed49e,2bc70082c142ee2ef5d6a941611505b7,39365aee753fb73130c208fdf4046bb7,3cf566c27c75f886658002bf9b3b0b15,4ade7764ebe998b5f35a7fd2b58d4796,500710c8a95f1310d383fa71fd806c1c,673b0feee6eb5843954defc670e4ba29,7cb1ca3f60421fbd20d6ae39bbf2b296,892b821ed44c13c4d09e0ceedac3051d,ad7c537267a3aaae402b03f7150950cf,cc1063ba5913ea38c84ac0250c01fe84,d40f5dc25597e4e4d7c30b8bfd98f89a,dc2c05938eb6dbe0217f4c9e6b111e2a,dd9bcf836b1de9b8c99d5ba8188a5ed4,e6ec99a117b9abd42452ff51cd6e8f0b,efb7975581b22620b8277417edeee3e9", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"ARIMA\" can be generated as follows:\n\nARIMA (AutoRegressive Integrated Moving Average) is a statistical model used for time series forecasting. It is a classical forecasting method that fits a separate model to each time series independently. ARIMA is a model that performs well for time-series forecasting tasks, but it was excluded from the analysis due to its prohibitive computational requirements and extensive training times. However, it is still used for evaluation and is a type of statistical model used in time series forecasting. Additionally, ARIMA is compared to other models, such as TimesFM, in certain datasets, such as the Air Passenger dataset.\n\nThe model is mentioned in the text, specifically in reference to McK84, and is also mentioned alongside other models, such as GARCH, indicating its relevance in time series forecasting. The use of phrases such as \"Autoregressive Integrated Moving Average\" further emphasizes its role in time series forecasting. Overall, ARIMA is a widely recognized and utilized model in the field of time series forecasting.\n\nRelevant information from the nearby text suggests that ARIMA is a model that is used for forecasting time series data, and it is a type of algorithm that is used for modeling and predicting time series data. The model\u0027s performance is compared to other models, and it is used for evaluation purposes. The text also mentions that ARIMA is a model that is used for time-series forecasting, which further emphasizes its relevance in this field.\n\nIn resolving any contradictions, it is clear that ARIMA is a statistical model used for time series forecasting, and it is a classical forecasting method that fits a separate model to each time series independently. While it was excluded from the analysis due to its prohibitive computational requirements and extensive training times, it is still used for evaluation and is a widely recognized and utilized model in the field of time series forecasting.", "id": "ARIMA", "label": "ARIMA", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9,2bc70082c142ee2ef5d6a941611505b7,2e2e2fa4e717e09d31996f7f22bd50a0,323c49cf157623a685283fcfc9b05491,39365aee753fb73130c208fdf4046bb7,41b0bd14ce7ff7419b7ee1f78b4701ae,6771b6279846e780d6807b15184ae008,7e69d9444a2084b6452e291735bf5a49,892b821ed44c13c4d09e0ceedac3051d,ca3dfe42c66d68dd6dbad037936b2360,cc1063ba5913ea38c84ac0250c01fe84,d40f5dc25597e4e4d7c30b8bfd98f89a,e6ec99a117b9abd42452ff51cd6e8f0b,fb67fcff21ac521a5ed8b202412ec1fc,ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "GFQW23 is a model mentioned in the text, specifically llmtime", "id": "GFQW23", "label": "GFQW23", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7", "type": "MODEL"}, {"color": "#97c2fc", "description": "GPT-3.5-Turbo is a model used for time-series forecasting, used as a baseline in the paper", "id": "GPT-3.5-TURBO", "label": "GPT-3.5-TURBO", "shape": "dot", "size": 10, "source_id": "673b0feee6eb5843954defc670e4ba29", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"AIR PASSENGER DATASET\" refers to a specific dataset used for time series forecasting and analysis. This dataset is utilized for the purpose of predicting and understanding long-term trends and patterns in air passenger traffic. The dataset likely contains historical data on air passenger numbers, which can be analyzed using various techniques such as frequency analysis and multi-head cross-attention to identify meaningful relationships and correlations.\n\nGiven the context of time series forecasting, it is likely that the dataset is used to develop and evaluate machine learning models that can accurately predict future air passenger numbers. The dataset may also be used to compare the performance of different forecasting models and techniques, such as those presented in academic papers and conferences like ICLR, AAAI, and PMLR.\n\nOverall, the \"AIR PASSENGER DATASET\" is a valuable resource for researchers and practitioners in the field of time series analysis and forecasting, providing a real-world example of a complex and dynamic system that can be modeled and predicted using advanced statistical and machine learning techniques.", "id": "AIR PASSENGER DATASET", "label": "AIR PASSENGER DATASET", "shape": "dot", "size": 10, "source_id": "cc1063ba5913ea38c84ac0250c01fe84,e6ec99a117b9abd42452ff51cd6e8f0b", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"FORECASTPFN\" can be generated as follows:\n\nFORECASTPFN is a pre-trained model specifically designed for time series forecasting. It is a type of model that generates point forecasts for time series data and tackles the problem of zero-shot forecasting by training a transformer-based model purely on synthetic data. This model is trained solely on synthetic data, which enables it to effectively handle time series forecasting tasks. FORECASTPFN is mentioned in the text as \"ForecastPFN, LLMTime,\" indicating its association with large language models (LLMs) and time series forecasting applications. Overall, FORECASTPFN is a model used for time series forecasting, leveraging transformer-based architecture and synthetic data for its training.", "id": "FORECASTPFN", "label": "FORECASTPFN", "shape": "dot", "size": 10, "source_id": "1ddbab2dca370c9ef7b5a724075518cc,56de1d5a5b467101344afa4248d829dc,5b1135d9e53e53428a042e8bbc89eaa7,af36d1634490149b96980fb1dff57cd1,bc54a718d1886698232d578fd88c3ac7,c522ea3766ae5129c11b833252340695", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"AUTOARIMA\" can be generated as follows:\n\nAUTOARIMA is a statistical model used for time series forecasting, which combines the strengths of ARIMA and auto-regressive models. It is a type of model that uses an autoregressive integrated moving average approach to forecasting, and is often used for comparison in the text. AUTOARIMA is a local model that performs better than Chronos models on some datasets, and is also a local statistical baseline method for benchmarking Chronos. The model is mentioned in the text as a specific model used for forecasting time series data, and is often used in conjunction with other models such as Naive, Seasonal Naive, and AutoETS.\n\nOverall, AUTOARIMA is a robust and effective model for time series forecasting, which has been shown to outperform local statistical models in certain scenarios. Its ability to combine the strengths of ARIMA and auto-regressive models makes it a popular choice for researchers and practitioners working with time series data.\n\nIt is worth noting that the term \"Autoarima\" is used interchangeably with \"AUTOARIMA\" in the text, and both refer to the same model. Additionally, the model is mentioned in the text as a specific model used for forecasting time series data, which suggests that it has been evaluated and compared to other models in the context of time series forecasting.", "id": "AUTOARIMA", "label": "AUTOARIMA", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,12395cf4e8efa64a847ede9775ecdf3f,1ddbab2dca370c9ef7b5a724075518cc,2565ae205d4c98342168bb67a4f7a309,376897a501ac50834f626fcc5fb13ece,4bd5a8e9285aae7ca7363c8e61ba361c,51ee17c1f0212d4c94010d8e376f649b,56de1d5a5b467101344afa4248d829dc,5b1135d9e53e53428a042e8bbc89eaa7,5fa25d3abec59ccd2718e00c0e0eb440,79c41aff65d584b4c8d4c769b82756d5,990578022879395d00b7a5b229863c2f,9c155897f3e35902f57696e0abaeb161,a875a1c0bede47a1c4e3823be81c42c6,af36d1634490149b96980fb1dff57cd1,bca10b04933dc3a7f98a3f1b610e419b,e900311a447985c0967f87fff49c58b8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"AWS EC2\" can be generated as follows:\n\nAWS EC2 is a cloud computing platform that serves as a crucial infrastructure for various purposes. Specifically, it is utilized for both inference and training, indicating its versatility in supporting different stages of machine learning model development. Additionally, AWS EC2 is also used to train the Chronos models, as discussed in the context of its hardware and software configuration. This suggests that AWS EC2 provides a suitable environment for large-scale model training, which is a critical aspect of many machine learning applications.\n\nIt is worth noting that the initial description of AWS EC2 as a \"software library\" appears to be contradictory to the other descriptions, which describe it as a cloud computing platform and infrastructure. However, based on the context and the information provided, it is more accurate to consider AWS EC2 as a cloud computing platform rather than a software library. This is because the descriptions emphasize its role in supporting inference and training, as well as its hardware and software configuration, which are characteristic of a cloud computing platform rather than a software library.", "id": "AWS EC2", "label": "AWS EC2", "shape": "dot", "size": 10, "source_id": "1ddbab2dca370c9ef7b5a724075518cc,2565ae205d4c98342168bb67a4f7a309,f72ba51a17dd00cff43bcdda22c273e8", "type": "INFRASTRUCTURE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"MODEL REPROGRAMMING\" refers to a technique used in machine learning for updating or modifying a pre-existing model. This process involves adapting a well-developed, pre-trained model from one domain to address tasks in a different domain. In essence, model reprogramming enables the transfer of knowledge and skills from one area of expertise to another, allowing the model to tackle new challenges and applications. This technique has the potential to significantly reduce the time and resources required to develop new models, as it leverages the existing knowledge and capabilities of the pre-trained model.", "id": "MODEL REPROGRAMMING", "label": "MODEL REPROGRAMMING", "shape": "dot", "size": 10, "source_id": "a73df99fe49b288e1c8751be2008b191,f7266cfccedb1d9840d10afa689a05e9", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the entity \"VOICE2SERIES\" in third person:\n\nVOICE2SERIES is a pre-trained model used in time series analysis that engages in the synchronization of time series and acoustic data. It adapts an acoustic model from speech recognition for time series classification, repurposing the model to analyze and predict time series patterns. This concept is mentioned in the text, as indicated by the phrase \"Voice2series: Reprogramming acoustic models for time series classification.\" The model\u0027s ability to leverage acoustic data for time series forecasting makes it a unique and valuable tool in the field of time series analysis.\n\nRelevant information from the nearby text suggests that the model is likely used in research and academic settings, given the presence of technical terms, mathematical equations, and references to academic papers. The use of English-language abbreviations and citations also supports this conclusion.\n\nOverall, VOICE2SERIES is a sophisticated model that combines the strengths of speech recognition and time series analysis to provide accurate and informative predictions. Its adaptability and pre-trained capabilities make it a valuable resource for researchers and practitioners working in the field of time series forecasting.", "id": "VOICE2SERIES", "label": "VOICE2SERIES", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,98c6b5003112ab7110e45414a2fa468b,bb10b1a7f98a4f869b7abbc5f9632dd1,f7266cfccedb1d9840d10afa689a05e9", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nWimmer et al. are researchers who specialize in time series analysis and utilize vision-language models to predict market changes. Their work, as described in the research paper \"Wimmer et al.,\" focuses on applying advanced techniques to forecast market fluctuations, leveraging the capabilities of vision-language models to analyze complex patterns and trends.\n\nThe research paper, which is centered around time series analysis, explores the application of multi-head cross-attention and frequency analysis to improve long-term series forecasting. The authors, Wimmer et al., draw upon their expertise in machine learning and time series forecasting to develop innovative solutions for predicting market changes.\n\nThe use of technical terms, mathematical equations, and references to academic papers, such as those from ICLR, AAAI, and PMLR, further supports the academic and technical nature of the research. The citation of English-language academic papers and authors also suggests that the text is written in English, reinforcing the conclusion that the primary language of the text is indeed English.\n\nOverall, Wimmer et al. are researchers who have made significant contributions to the field of time series analysis, leveraging vision-language models to improve market change predictions and advancing the state-of-the-art in long-term series forecasting.", "id": "WIMMER ET AL.", "label": "WIMMER ET AL.", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "PAPER"}, {"color": "#97c2fc", "description": "Prompt-based time series forecasting is a concept mentioned in the text, as indicated by the use of the phrase \"prompt-based time series forecasting\"", "id": "PROMPT-BASED TIME SERIES FORECASTING", "label": "PROMPT-BASED TIME SERIES FORECASTING", "shape": "dot", "size": 10, "source_id": "98c6b5003112ab7110e45414a2fa468b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Acoustic models are a type of model mentioned in the text, as indicated by the use of the phrase \"acoustic models for time series classification\"", "id": "ACOUSTIC MODELS", "label": "ACOUSTIC MODELS", "shape": "dot", "size": 10, "source_id": "98c6b5003112ab7110e45414a2fa468b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nVLMS (Vision-Language Models) are pre-trained models that can be fine-tuned for specific tasks. They are a type of artificial intelligence model that combines the capabilities of computer vision and natural language processing to enable tasks such as image description, visual question answering, and more. VLMS are trained on large datasets of images and text, allowing them to learn complex relationships between visual and linguistic representations. This enables them to be fine-tuned for a wide range of applications, from image classification and object detection to text-to-image synthesis and more.\n\nThe primary language of the text related to VLMS is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document. The presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports this conclusion.\n\nOverall, VLMS are a powerful tool for a wide range of applications, and their ability to be fine-tuned for specific tasks makes them a valuable asset in the field of artificial intelligence.", "id": "VLMS", "label": "VLMS", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,de8e097ce66fbc954bd529888cbc15ea", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe \"DECODER-ONLY MODEL\" is a type of neural network model that utilizes a decoder to generate output from input. This model is specifically mentioned in the text, as indicated by the use of the phrase \"decoder-only training.\" The model\u0027s primary function is to process and generate output based on the input it receives, making it a crucial component in various machine learning applications.\n\nGiven the formal and academic tone of the text, it is likely that the \"DECODER-ONLY MODEL\" is discussed in the context of a research paper or technical document, possibly in the field of artificial intelligence or deep learning. The use of technical terms such as \"neural network\" and \"decoder-only training\" further supports this assumption.\n\nOverall, the \"DECODER-ONLY MODEL\" is a sophisticated neural network architecture that plays a vital role in generating output from input, and its discussion is likely to be found in academic or technical literature related to machine learning and artificial intelligence.", "id": "DECODER-ONLY MODEL", "label": "DECODER-ONLY MODEL", "shape": "dot", "size": 10, "source_id": "7dbc961fe1959f844ebac283498e7fb0,f98d2bec31738be3f7be750b5fe6180e", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nPATCHTST(ZS) is a pre-trained model used for evaluation purposes. It is specifically designed for evaluation tasks and has been utilized in an ablation study. The model is a variant of the PatchTST model, which suggests that it may be a specialized or modified version of the original PatchTST model. The primary language of the text related to PATCHTST(ZS) is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nThe PATCHTST(ZS) model is likely used in the context of time series analysis, given the mention of \"long-term series forecasting\" and \"frequency analysis\" in the nearby text. The model\u0027s application in an ablation study suggests that it is being evaluated for its performance and effectiveness in a specific task or scenario. Overall, PATCHTST(ZS) appears to be a specialized model designed for evaluation purposes, with a focus on time series analysis and forecasting.", "id": "PATCHTST(ZS)", "label": "PATCHTST(ZS)", "shape": "dot", "size": 10, "source_id": "28b097d339554431fa14e113c98ed49e,39365aee753fb73130c208fdf4046bb7,41b0bd14ce7ff7419b7ee1f78b4701ae,4ade7764ebe998b5f35a7fd2b58d4796", "type": "MODEL"}, {"color": "#97c2fc", "description": "The transformer stack is a component of the PatchTST and TimesFM models", "id": "TRANSFORMER STACK", "label": "TRANSFORMER STACK", "shape": "dot", "size": 10, "source_id": "28b097d339554431fa14e113c98ed49e", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"INPUT PATCH LENGTH\" is a parameter used in the PatchTST model. It refers to the size of each patch in the input time-series, which is a crucial aspect of the model\u0027s architecture. This parameter plays a significant role in determining the model\u0027s performance and accuracy in time-series forecasting tasks.\n\nIn the context of the PatchTST model, the input patch length is a critical hyperparameter that needs to be carefully tuned to achieve optimal results. It is essential to understand the relationship between the input patch length and the model\u0027s performance, as it can significantly impact the accuracy of the forecasts.\n\nGiven the technical nature of the text, it is likely that the input patch length is used in conjunction with other parameters and techniques, such as frequency analysis and multi-head cross-attention, to improve the model\u0027s performance in long-term series forecasting tasks.\n\nOverall, the input patch length is a vital component of the PatchTST model, and its proper tuning is essential for achieving accurate time-series forecasts.", "id": "INPUT PATCH LENGTH", "label": "INPUT PATCH LENGTH", "shape": "dot", "size": 10, "source_id": "28b097d339554431fa14e113c98ed49e,3cf566c27c75f886658002bf9b3b0b15", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Stride is a parameter used in the PatchTST model", "id": "STRIDE", "label": "STRIDE", "shape": "dot", "size": 10, "source_id": "28b097d339554431fa14e113c98ed49e", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, the entity \"MAE\" can be summarized as follows:\n\nMAE, which stands for Mean Absolute Error, is a metric used to evaluate the performance of a machine learning model. It represents the mean absolute error, measuring the difference between predicted and actual values. MAE is a widely used metric in the field of machine learning, used to assess the accuracy of a model by calculating the average difference between its predictions and the actual outcomes.\n\nThe descriptions provided are consistent in their definition of MAE, with some variations in wording and phrasing. However, they all convey the same meaning and purpose of the metric. The use of the abbreviation \"MAE\" in the text further supports its definition as a metric used to evaluate the performance of a model.\n\nIn the context of machine learning, MAE is a crucial metric for evaluating the performance of a model, particularly in regression tasks where the goal is to predict continuous values. It provides a clear and concise measure of the model\u0027s accuracy, allowing researchers and practitioners to compare and improve their models.\n\nOverall, MAE is a fundamental concept in machine learning, and its definition and purpose are well-established in the field.", "id": "MAE", "label": "MAE", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458,1d6fb60c5060c25ae4791b03a0513a7f,41fe893a178ebc8790ef4da83da5ab6e,500710c8a95f1310d383fa71fd806c1c,7cb1ca3f60421fbd20d6ae39bbf2b296,892b821ed44c13c4d09e0ceedac3051d,919ff66615400ea06113a2a59ff34ef0,9b150491b487d5b2da482652e2fe509d,cd4ff69415c7b37cec643f8289d1c98d,d40f5dc25597e4e4d7c30b8bfd98f89a,e900311a447985c0967f87fff49c58b8,efb7975581b22620b8277417edeee3e9,f7ce89346ea6715560163ef3a630a5df,fd1092903d83bf6e90a6caa371d7c514", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"WAVENET\" can be generated as follows:\n\nWAVENET is a deep learning model used for time series forecasting, specifically designed to perform better than local statistical models. It is a type of model that utilizes a convolutional neural network approach to forecasting, which enables it to effectively capture patterns and relationships within time series data. WAVENET has been mentioned in the text as one of the models used for evaluation, and it has been ranked 10th in terms of point forecasting performance. The model is also mentioned alongside other models such as DeepAR, N-BEATS, TFT, DLinear, PatchTST, N-HiTS, and GPT4TS, indicating its relevance and application in the field of time series forecasting.\n\nThis summary is based on the information collected from all the descriptions provided, and it resolves any potential contradictions by presenting a coherent and accurate representation of the entity \"WAVENET\".", "id": "WAVENET", "label": "WAVENET", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,12395cf4e8efa64a847ede9775ecdf3f,2565ae205d4c98342168bb67a4f7a309,41b0bd14ce7ff7419b7ee1f78b4701ae,532a6d434dab611a80aef1e94dd2fb45,5b1135d9e53e53428a042e8bbc89eaa7,892b821ed44c13c4d09e0ceedac3051d,af36d1634490149b96980fb1dff57cd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nNPTS is a local forecasting method and a specific model used for time series forecasting. It is mentioned in the text as a model or algorithm used for this purpose, with the name \"NPTS\" being used to refer to it. The model is likely used for long-term series forecasting, as it is related to the field of time series analysis, which involves frequency analysis and multi-head cross-attention techniques. The use of NPTS for time series forecasting suggests that it is a model that can be used for predicting future values in a time series based on past values.\n\nThe name \"NPTS\" is likely an abbreviation for \"Non-Parametric Time Series\" or a similar term, given its relation to time series forecasting. The model is mentioned in the context of academic research, as indicated by the use of technical terms and mathematical equations in the text. The citations and references to academic papers and authors in the text also suggest that NPTS is a model that has been studied and discussed in the academic community.\n\nOverall, NPTS appears to be a local forecasting method and a specific model used for time series forecasting, with a strong connection to the field of time series analysis and academic research.", "id": "NPTS", "label": "NPTS", "shape": "dot", "size": 10, "source_id": "376897a501ac50834f626fcc5fb13ece,51ee17c1f0212d4c94010d8e376f649b,6fa5f635ad5f7e6b67d5e467f130345c,79c41aff65d584b4c8d4c769b82756d5", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nTemporal Fusion Transformer (TEMPORALFUSIONT) is a model or algorithm specifically designed for time series forecasting. It is a model used for this purpose, as indicated by its application in forecasting time series data. The model is mentioned in the text, suggesting its relevance and importance in the field of time series analysis. Temporal Fusion Transformer is a specific model that utilizes techniques such as frequency analysis and multi-head cross-attention to effectively forecast long-term series. Its use is evident in the context of academic research, as it is mentioned alongside other technical terms and mathematical equations commonly used in English-language academic papers.", "id": "TEMPORALFUSIONT", "label": "TEMPORALFUSIONT", "shape": "dot", "size": 10, "source_id": "376897a501ac50834f626fcc5fb13ece,6fa5f635ad5f7e6b67d5e467f130345c,79c41aff65d584b4c8d4c769b82756d5", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"AUTOETS\" can be generated as follows:\n\nAUTOETS is a statistical model used for time series forecasting. It is a type of model that uses an exponential smoothing approach to forecasting and is considered a local model that performs better than Chronos models on some datasets. AUTOETS is also a local statistical baseline method for benchmarking Chronos and is used for comparison in the text. The model is mentioned in the text as a specific model, indicated by the use of the name \"AutoETS\" or \"Autoets,\" and is used for time series forecasting. Additionally, AUTOETS models are a type of model that uses an exponential smoothing approach to forecasting, suggesting that the model is a variant of the Exponential Smoothing (ES) family of models.\n\nOverall, AUTOETS is a statistical model used for time series forecasting that uses an exponential smoothing approach and is considered a local model that performs better than Chronos models on some datasets.\n\nRelevant information from the nearby text includes:\n\n* The mention of \"Naive, Seasonal Naive, AutoETS, AutoARIMA\" in the text, which suggests that AUTOETS is a model used for comparison in the text.\n* The use of the phrase \"Autoets on the respective datasets\" in the text, which indicates that AUTOETS is a specific model mentioned in the text.\n* The mention of Chronos models in the text, which suggests that AUTOETS is a local model that performs better than Chronos models on some datasets.\n\nThe contradictions in the descriptions have been resolved by considering the following:\n\n* The use of the phrase \"Autoets\" and \"AutoETS\" in the text suggests that the model is referred to by both names.\n* The mention of \"AutoETS models\" in the text suggests that the model is a type of model that uses an exponential smoothing approach to forecasting.\n* The consideration of AUTOETS as a local model that performs better than Chronos models on some datasets is consistent with the mention of Chronos models in the text.\n\nOverall, the summary provides a comprehensive description of the entity \"AUTOETS\" and its characteristics as a statistical model used for time series forecasting.", "id": "AUTOETS", "label": "AUTOETS", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,12395cf4e8efa64a847ede9775ecdf3f,1ddbab2dca370c9ef7b5a724075518cc,2565ae205d4c98342168bb67a4f7a309,376897a501ac50834f626fcc5fb13ece,4bd5a8e9285aae7ca7363c8e61ba361c,51ee17c1f0212d4c94010d8e376f649b,56de1d5a5b467101344afa4248d829dc,5b1135d9e53e53428a042e8bbc89eaa7,5fa25d3abec59ccd2718e00c0e0eb440,6fa5f635ad5f7e6b67d5e467f130345c,79c41aff65d584b4c8d4c769b82756d5,990578022879395d00b7a5b229863c2f,a875a1c0bede47a1c4e3823be81c42c6,af36d1634490149b96980fb1dff57cd1,e900311a447985c0967f87fff49c58b8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"CROSTONSBA\" is a specific model mentioned in the text, which is referred to as \"Croston\u0027s SBA\" or \"Croston\u0027s seasonal batch arrival model.\" This model is used for time series forecasting, particularly for intermittent demand forecasting. It is an algorithm designed to predict future values in a time series based on historical data, taking into account seasonal patterns and batch arrivals. The model is mentioned in the context of academic research, as indicated by the use of technical terms and mathematical equations, and is likely discussed in papers published in conferences and journals such as ICLR, AAAI, and PMLR. Overall, \"CROSTONSBA\" is a time series forecasting model that leverages seasonal batch arrival patterns to make predictions about future demand.", "id": "CROSTONSBA", "label": "CROSTONSBA", "shape": "dot", "size": 10, "source_id": "376897a501ac50834f626fcc5fb13ece,51ee17c1f0212d4c94010d8e376f649b,6fa5f635ad5f7e6b67d5e467f130345c,79c41aff65d584b4c8d4c769b82756d5", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"DYNAMICOPTIMIZE\" is a model or algorithm used for time series forecasting. Specifically, it is a specific model mentioned in the text, as indicated by the use of the name \"Dynamic Optimize\". This model is utilized for dynamic optimization, which is a technique used in time series forecasting. The model is designed to optimize the forecasting process by leveraging various techniques, including frequency analysis and multi-head cross-attention.\n\nThe use of \"Dynamic Optimize\" as a model name suggests that it is a proprietary or custom-built model, rather than a widely recognized or established algorithm. However, the fact that it is mentioned in the text as a specific model implies that it has been developed and tested for time series forecasting applications.\n\nOverall, the entity \"DYNAMICOPTIMIZE\" is a time series forecasting model that utilizes dynamic optimization techniques to improve forecasting accuracy. Its specific implementation and features are not explicitly stated in the provided information, but it is clear that it is a custom-built model designed for time series forecasting applications.\n\nRelevant information from the nearby text includes the use of technical terms and mathematical equations, which suggests that the text is from a research paper or technical document. The presence of English-language abbreviations and citations also supports the conclusion that the text is written in English.", "id": "DYNAMICOPTIMIZE", "label": "DYNAMICOPTIMIZE", "shape": "dot", "size": 10, "source_id": "376897a501ac50834f626fcc5fb13ece,6fa5f635ad5f7e6b67d5e467f130345c,79c41aff65d584b4c8d4c769b82756d5", "type": "MODEL"}, {"color": "#97c2fc", "description": "DLinear is a model mentioned in the text, as indicated by the use of the name \"DLinear\"", "id": "DLINERAR", "label": "DLINERAR", "shape": "dot", "size": 10, "source_id": "1d6fb60c5060c25ae4791b03a0513a7f", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the entity \"MOIRAI\" in third person:\n\nMOIRAI is a pre-trained model primarily used in time series analysis. It is specifically designed for time series forecasting, leveraging its capabilities to predict future values in a series. One of the key features of MOIRAI is its employment of multi-resolution analysis, which is achieved through the use of varying patch sizes. This approach enables MOIRAI to capture patterns and trends at different scales, making it a powerful tool for time series forecasting.\n\nThe model\u0027s architecture and functionality are likely to be complex, given its ability to perform multi-resolution analysis and its pre-trained nature. However, the provided information does not delve into the technical details of the model\u0027s architecture or the mathematical equations used in its implementation.\n\nOverall, MOIRAI appears to be a sophisticated model designed for time series analysis and forecasting, with a unique approach to capturing patterns and trends in data. Its pre-trained nature and multi-resolution analysis capabilities make it a valuable tool for researchers and practitioners working in the field of time series analysis.", "id": "MOIRAI", "label": "MOIRAI", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,5805d8a1afe3d6bc6300ac71f7163831,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "LOTSA is a pre-training dataset for time series forecasting", "id": "LOTSA", "label": "LOTSA", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "type": "DATASET"}, {"color": "#97c2fc", "description": "Multi-resolution analysis is a specialized approach used in time series forecasting models, exemplified by Moirai, through the employment of varying patch sizes", "id": "MULTI-RESOLUTION ANALYSIS", "label": "MULTI-RESOLUTION ANALYSIS", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to \"LANGUAGE MODELS\" can be generated as follows:\n\nLANGUAGE MODELS are a type of machine learning model that is trained on text data. They are proficient in various tasks, including language translation, text generation, and time series tasks. These models are pre-trained on a large and diverse corpus of text data, enabling them to learn patterns and relationships within language. They can be utilized for a wide range of applications, from generating human-like text to facilitating language understanding and translation.\n\nThe summary is written in third person and includes information collected from all the descriptions. The contradictions in the descriptions have been resolved to provide a single, coherent summary. The summary also includes relevant information from the nearby text, such as the fact that language models are trained on text data and can be used for various tasks.\n\nIt is worth noting that the description \"Language models are models that are proficient in time series tasks\" seems to be an outlier, as time series tasks are typically associated with models that analyze numerical data over time, rather than text data. However, this description has been included in the summary as it is part of the original description list.", "id": "LANGUAGE MODELS", "label": "LANGUAGE MODELS", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,66b7675d8c62321e1cc8916401159787,6d66c2ea37e25b646a13d751c05a8e4d,bc54a718d1886698232d578fd88c3ac7,c5c841baa0bc205103ac433d446da3b6", "type": ""}, {"color": "#97c2fc", "description": "Real data refers to actual time series data used in training and testing", "id": "REAL DATA", "label": "REAL DATA", "shape": "dot", "size": 10, "source_id": "bc54a718d1886698232d578fd88c3ac7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Tokenized input refers to the process of breaking down time series data into individual tokens or patches", "id": "TOKENIZED INPUT", "label": "TOKENIZED INPUT", "shape": "dot", "size": 10, "source_id": "bc54a718d1886698232d578fd88c3ac7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"VOCABULARY\" refers to the set of tokens or patches used in GPT4TS, as well as the set of words or terms used in a language or model. Vocabulary is a fundamental concept in natural language processing and is closely related to ReproBert, a model that utilizes vocabulary in its architecture. In essence, vocabulary serves as the building block for language understanding and generation, enabling models like ReproBert to process and analyze large amounts of text data.\n\nThis summary is derived from the provided descriptions, which collectively paint a picture of vocabulary as a crucial component in the realm of language and machine learning. By combining the information from each description, we can gain a deeper understanding of the entity \"VOCABULARY\" and its significance in the context of GPT4TS, ReproBert, and language processing in general.", "id": "VOCABULARY", "label": "VOCABULARY", "shape": "dot", "size": 10, "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,56806630593fdf0c3d95ad7555080dcf,69457f873272a693c1f813c75ecf030a,6eb4c16edf2eedfd03721efb199478d8,7e69d9444a2084b6452e291735bf5a49,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,bc54a718d1886698232d578fd88c3ac7,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96,fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Tokenized values refer to the individual tokens or patches used in GPT4TS", "entity_type": "CONCEPT", "id": "TOKENIZED VALUES", "label": "TOKENIZED VALUES", "shape": "dot", "size": 10, "source_id": "bc54a718d1886698232d578fd88c3ac7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"OBJECTIVE FUNCTION\" can be described as follows:\n\nThe objective function is a crucial component in machine learning model development, serving as the primary metric to evaluate the performance of a model. It is the function being optimized during the training process, with the goal of minimizing or maximizing its value, depending on the specific problem being addressed.\n\nIn the context of the provided descriptions, the objective function is specifically related to the cross-entropy between the distribution of the quantized ground truth label and the predicted distribution. This suggests that the objective function is being used to evaluate the model\u0027s ability to accurately predict the probability distribution of the target variable.\n\nOverall, the objective function plays a vital role in machine learning model development, as it provides a clear and quantifiable measure of the model\u0027s performance, allowing for the optimization of the model\u0027s parameters and the improvement of its predictive capabilities.\n\nRelevant information from the nearby text:\n\n* The use of technical terms such as \"machine learning model\" and \"cross-entropy\" suggests that the entity \"OBJECTIVE FUNCTION\" is related to the field of machine learning.\n* The mention of \"quantized ground truth label\" and \"predicted distribution\" implies that the objective function is being used in a classification or regression problem, where the goal is to predict the probability distribution of the target variable.\n* The use of formal and academic language, such as \"function used to evaluate the performance of a machine learning model\" and \"function that is being optimized during training\", suggests that the entity \"OBJECTIVE FUNCTION\" is a technical concept that is commonly used in academic and research settings.", "id": "OBJECTIVE FUNCTION", "label": "OBJECTIVE FUNCTION", "shape": "dot", "size": 10, "source_id": "6917a14af275e30abe2d30a8f8a9a4e6,c5c841baa0bc205103ac433d446da3b6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Quantile regression is a concept mentioned in the text, as indicated by the use of the term \"quantile regression\"", "id": "QUANTILE REGRESSION", "label": "QUANTILE REGRESSION", "shape": "dot", "size": 10, "source_id": "6212b2146d9ad27124114c334a946854", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe primary entity of interest is \"LANGUAGE MODEL ARCHITECTURE.\" According to the descriptions, language model architecture is a concept mentioned in the text, as indicated by the use of the term \"language model architecture.\" Furthermore, it is mentioned that language model architectures are adapted for time series forecasting, challenging the notion that time-series-specific features or architectures are necessary for forecasting.\n\nThis information suggests that language model architectures have the potential to be applied to time series forecasting tasks, which is a significant development in the field of time series analysis. The use of language model architectures for time series forecasting may offer new opportunities for improving forecasting accuracy and efficiency.\n\nOverall, the summary highlights the adaptability of language model architectures for time series forecasting, which is a key aspect of the entity \"LANGUAGE MODEL ARCHITECTURE.\"", "id": "LANGUAGE MODEL ARCHITECTURE", "label": "LANGUAGE MODEL ARCHITECTURE", "shape": "dot", "size": 10, "source_id": "6212b2146d9ad27124114c334a946854,6c273e9f841addeed2a33240ddaa3d96", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"WEIGHTED QUANTILE LOSS\" (WQL) is a metric used to evaluate probabilistic forecasts. It is a metric mentioned in the text, as indicated by the use of the phrase \"weighted quantile loss.\" WQL is utilized to assess the performance of probabilistic forecasting models, which provide a range of possible outcomes rather than a single point estimate.\n\nThe use of WQL as a metric suggests that it is a relevant and important aspect of probabilistic forecasting, particularly in the context of evaluating the accuracy and reliability of these models. The fact that it is mentioned in the text as a specific metric implies that it has been studied and applied in various research settings.\n\nOverall, the entity \"WEIGHTED QUANTILE LOSS\" is a key concept in the field of probabilistic forecasting, and its use as a metric highlights its importance in evaluating the performance of these models.\n\nRelevant information from the nearby text includes:\n\n* The text is written in English, as indicated by the use of technical terms, mathematical equations, and references to academic papers.\n* The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n* The text mentions various technical terms and concepts related to probabilistic forecasting, including \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention.\"\n\nThese details provide context for the entity \"WEIGHTED QUANTILE LOSS\" and highlight its relevance to the field of probabilistic forecasting.", "id": "WEIGHTED QUANTILE LOSS", "label": "WEIGHTED QUANTILE LOSS", "shape": "dot", "size": 10, "source_id": "af36d1634490149b96980fb1dff57cd1,f0808ad97bc4d26391284145427770e5", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"BENCHMARK II\":\n\nBENCHMARK II is a benchmark that serves as a performance evaluation tool for time series forecasting models, particularly highlighting the capabilities of Chronos as a generalist time series forecaster. It is a dataset comprising 27 zero-shot datasets, consisting of a total of 190,674 observations, which are used to evaluate the performance of time series forecasting models in a zero-shot setting. Specifically, BENCHMARK II is utilized to assess the zero-shot performance of Chronos models on a set of 27 datasets that were not used during their training, providing a comprehensive evaluation of their ability to generalize and adapt to new, unseen data. This benchmark is essential for evaluating the effectiveness and robustness of time series forecasting models, particularly in scenarios where data is limited or unavailable.", "id": "BENCHMARK II", "label": "BENCHMARK II", "shape": "dot", "size": 10, "source_id": "2eea45c6111e468189580a21686cb14a,532a6d434dab611a80aef1e94dd2fb45,56de1d5a5b467101344afa4248d829dc,63e284ec7e76fa859cc46b72d3746654,baa887ae552c6b3dac10f74896d8c4a2,e37f4063d074e1697e1f539131b3d963", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**TASK-SPECIFIC MODELS**\n\nTask-specific models are a type of model used for time series forecasting, specifically designed to excel in probabilistic forecasting. These models are trained on a specific task or dataset, allowing them to adapt to the unique characteristics of the data. However, it is noted that task-specific models are out-performed by Chronos, a more advanced model, in probabilistic forecasting tasks. Despite this, task-specific models are still useful for training a separate model for each task, enabling them to focus on the specific requirements of each individual task.\n\nThis summary combines the key points from the description list, highlighting the strengths and limitations of task-specific models in the context of time series forecasting.", "id": "TASK-SPECIFIC MODELS", "label": "TASK-SPECIFIC MODELS", "shape": "dot", "size": 10, "source_id": "2eea45c6111e468189580a21686cb14a,532a6d434dab611a80aef1e94dd2fb45,5b1135d9e53e53428a042e8bbc89eaa7,b4d5306b46bbfa4564727fe5ac6630e0", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe \"PRETRAINING DATASET\" refers to a collection of data used to train machine learning models, specifically Chronos models and TimesFM models. This dataset is utilized for pretraining purposes, allowing the models to learn patterns and relationships within the data before being fine-tuned for specific tasks or applications. The pretraining dataset serves as a foundation for the development and improvement of these models, enabling them to better capture temporal relationships and make accurate predictions in time series forecasting tasks.\n\nThis summary is based on the information provided in the description list, which indicates that the pretraining dataset is used to train both Chronos and TimesFM models. The summary is written in the third person and includes the entity name \"PRETRAINING DATASET\" for context.", "id": "PRETRAINING DATASET", "label": "PRETRAINING DATASET", "shape": "dot", "size": 10, "source_id": "532a6d434dab611a80aef1e94dd2fb45,ad7c537267a3aaae402b03f7150950cf", "type": "DATASET"}, {"color": "#97c2fc", "description": "Time series forecaster is a type of model that predicts future values in a time series", "id": "TIME SERIES FORECASTER", "label": "TIME SERIES FORECASTER", "shape": "dot", "size": 10, "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"LOCAL MODELS\":\n\nLOCAL MODELS are statistical models that are specifically trained on a particular dataset. They are commonly used in a zero-shot setting, but have been outperformed by Chronos models in terms of their in-domain performance. In fact, Chronos models have been shown to outperform existing local models in various time series forecasting tasks. Local models are utilized for time series forecasting, which involves predicting future values in a sequence of data points. Despite their limitations, local models remain a valuable tool in the field of time series analysis, particularly when used in conjunction with other models like Chronos.\n\nThis summary incorporates information from all the provided descriptions, resolving any potential contradictions and providing a clear and coherent overview of the entity \"LOCAL MODELS\".", "id": "LOCAL MODELS", "label": "LOCAL MODELS", "shape": "dot", "size": 10, "source_id": "53dbeea6d05c3695460c71f89f468def,6c273e9f841addeed2a33240ddaa3d96,b4d5306b46bbfa4564727fe5ac6630e0,baa887ae552c6b3dac10f74896d8c4a2", "type": "MODEL"}, {"color": "#97c2fc", "description": "Figure 5 shows the results on Benchmark II, highlighting the promise of Chronos as a generalist time series forecaster", "id": "FIGURE 5", "label": "FIGURE 5", "shape": "dot", "size": 10, "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "type": "FIGURE"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"SEASONALITY\" refers to a characteristic of time series that can be handled by TimeGPT. It is defined as the periodic fluctuations in a time series that occur at regular intervals. This suggests that seasonality is a recurring pattern in data that can be anticipated and potentially modeled using time series analysis techniques.\n\nGiven the context of time series analysis, it is likely that seasonality is a key aspect of understanding and forecasting long-term series. The ability to handle seasonality is a crucial feature of TimeGPT, implying that it is equipped to identify and account for these periodic fluctuations in data.\n\nOverall, the summary provides a clear understanding of the entity \"SEASONALITY\" and its relationship to time series analysis, highlighting its importance in modeling and forecasting periodic patterns in data.", "id": "SEASONALITY", "label": "SEASONALITY", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2,bca10b04933dc3a7f98a3f1b610e419b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Predictive distributions refer to the probability distribution of future outcomes", "id": "PREDICTIVE DISTRIBUTIONS", "label": "PREDICTIVE DISTRIBUTIONS", "shape": "dot", "size": 10, "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"ZERO-SHOT PERFORMANCE\" can be generated as follows:\n\n\"ZERO-SHOT PERFORMANCE\" refers to the ability of a model to perform well on a task without any additional training, fine-tuning, prior knowledge, or specific data for that task. This concept is mentioned in the text, indicating that Chronos models can obtain excellent results on unseen datasets without fine-tuning, showcasing their zero-shot performance capabilities.\n\nThe summary is written in third person and includes the entity name, providing a clear and concise description of the concept. Relevant information from the nearby text has been incorporated to enrich the summary, ensuring that it accurately reflects the meaning and context of the entity.", "id": "ZERO-SHOT PERFORMANCE", "label": "ZERO-SHOT PERFORMANCE", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96,6f6e27166a1506482bfcaf5585322595,ca3dfe42c66d68dd6dbad037936b2360,d08a82328191907abfcdb72efab9a6cf,dd9bcf836b1de9b8c99d5ba8188a5ed4,f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Task-specific adaptors are models that inject covariates into a pretrained forecasting model", "id": "TASK-SPECIFIC ADAPTORS", "label": "TASK-SPECIFIC ADAPTORS", "shape": "dot", "size": 10, "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Stacking ensembles are a technique for combining the predictions of multiple models to improve forecasting accuracy", "id": "STACKING ENSEMBLES", "label": "STACKING ENSEMBLES", "shape": "dot", "size": 10, "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nLIGHTGBM is a lightweight machine learning model that excels at handling covariates and is specifically used for time series forecasting. It is a versatile model that can effectively handle various types of data, including those with multiple covariates. The model\u0027s ability to efficiently process large datasets makes it a popular choice for time series forecasting applications.\n\nThis summary is based on the information provided in the description list, which states that LIGHTGBM is a lightweight model that excels at handling covariates and is used for time series forecasting. The summary is written in third person and includes the entity name, LIGHTGBM, to provide context.\n\nIt is worth noting that the summary does not include any information from the nearby text, as the provided text only contains information about the language of the text and does not provide any additional details about LIGHTGBM. However, the summary is still comprehensive and accurate based on the information provided in the description list.", "id": "LIGHTGBM", "label": "LIGHTGBM", "shape": "dot", "size": 10, "source_id": "6771b6279846e780d6807b15184ae008,b4d5306b46bbfa4564727fe5ac6630e0", "type": "MODEL"}, {"color": "#97c2fc", "description": "Inference speed refers to the time it takes for a model to generate predictions", "id": "INFERENCE SPEED", "label": "INFERENCE SPEED", "shape": "dot", "size": 10, "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"PRETRAINED MODELS\":\n\nThe entity \"PRETRAINED MODELS\" refers to a type of model used for time series forecasting. These models have been trained on a large corpus of data, which enables them to be fine-tuned for specific tasks, including time series forecasting. Specifically, they have been trained on a large corpus of time series data, allowing them to be highly effective in this domain.\n\nThe use of pretrained models for time series forecasting is a promising approach, as it leverages the knowledge and patterns learned from a large dataset to improve the accuracy and efficiency of forecasting models. By fine-tuning these models for specific tasks, researchers and practitioners can adapt them to their specific needs and requirements.\n\nOverall, pretrained models for time series forecasting offer a powerful tool for improving the accuracy and reliability of forecasting models, and their use is likely to become increasingly prevalent in the field of time series analysis and forecasting.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which is consistent with the formal and academic tone of the descriptions provided.\n* The text also mentions that the language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n* The use of technical terms, mathematical equations, and references to academic papers further supports the conclusion that the text is from a research paper or a technical document.\n\nNote: The contradictions in the descriptions have been resolved by focusing on the common themes and ideas presented in the descriptions, and by incorporating relevant information from the nearby text to provide a more comprehensive and accurate summary.", "id": "PRETRAINED MODELS", "label": "PRETRAINED MODELS", "shape": "dot", "size": 10, "source_id": "2eea45c6111e468189580a21686cb14a,5b1135d9e53e53428a042e8bbc89eaa7,b4d5306b46bbfa4564727fe5ac6630e0,dd9bcf836b1de9b8c99d5ba8188a5ed4", "type": "MODEL"}, {"color": "#97c2fc", "description": "The minimalist approach is used to develop generalist pretrained forecasting models, adapting existing language model architectures and training procedures for time series forecasting", "id": "MINIMALIST", "label": "MINIMALIST", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "APPROACH"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"SCALING\" can be generated as follows:\n\n\"Scaling\" refers to a process used in various contexts, including the Chronos language model architecture, time series analysis, and model performance improvement. It involves transforming time series values into a different scale or range, normalizing them to a specific range, and adjusting the number of parameters in a model to enhance its performance. This process requires minimal modifications to existing language model architectures and is essential for improving the accuracy and efficiency of models in time series forecasting and other applications.\n\nThe summary is written in third person and includes information from all the descriptions, resolving any potential contradictions. Relevant information from the nearby text is also incorporated to provide a comprehensive understanding of the entity \"SCALING\".", "id": "SCALING", "label": "SCALING", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96,efb7975581b22620b8277417edeee3e9,f7e3207706b170296cb036538a709689,fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Stefano Soatto is a researcher who challenged the authors to think about the fundamental question regarding language models and time series modeling", "id": "STEFANO SOATTO", "label": "STEFANO SOATTO", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Devamanyu Hazarika is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "DEVA MANYU HAZARICA", "label": "DEVA MANYU HAZARICA", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTIMECLR is a pre-trained model used in time series analysis. It is a model mentioned in the text, as indicated by its abbreviation and description, and is utilized for long-term series forecasting, frequency analysis, and potentially other tasks related to time series analysis. The model\u0027s architecture may involve multi-head cross-attention, a technique commonly used in natural language processing and other machine learning applications. \n\nGiven the context of the text, which appears to be a research paper or technical document, it is likely that TIMECLR is a deep learning model, possibly based on transformer architecture, given the mention of multi-head cross-attention. However, this information is not explicitly stated in the provided descriptions, and further analysis of the text would be required to confirm this hypothesis.\n\nOverall, TIMECLR is a pre-trained model designed for time series analysis, with potential applications in long-term series forecasting, frequency analysis, and other related tasks.", "id": "TIMECLR", "label": "TIMECLR", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Clinical reports refer to medical records or documents that contain information about a patient\u0027s health", "id": "CLINICAL REPORTS", "label": "CLINICAL REPORTS", "shape": "dot", "size": 10, "source_id": "ad8efcfda80253036294f2eeb48926e8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"HYBRID\" is a methodology mentioned in the text, as indicated by its description and use in various models. The primary language of the text is English, which is evident from the use of English words and phrases, mathematical equations, and references to academic papers written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nThe text contains various technical terms and mathematical equations related to time series analysis, long-term series forecasting, frequency analysis, and multi-head cross-attention. The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports the conclusion that the primary language of the text is English.\n\nOverall, the entity \"HYBRID\" is a methodology that is used in various models, and the text in which it is mentioned is written in English, suggesting a formal and academic tone.", "id": "HYBRID", "label": "HYBRID", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,77c7ced6ab4a0e57fba604c1a3e00416", "type": "METHOD"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTIMEXER is a pre-trained model used in time series analysis. It is a model mentioned in the text, as indicated by its abbreviation and description. The model is utilized for long-term series forecasting, which involves frequency analysis and potentially employs techniques such as multi-head cross-attention. The use of TIMEXER in time series analysis suggests its application in tasks such as forecasting and prediction, where its pre-training and fine-tuning capabilities can be leveraged to improve model performance.\n\nThe mention of TIMEXER in the context of time series analysis and forecasting, along with its pre-trained nature, implies that it is a sophisticated model capable of handling complex tasks in this domain. The model\u0027s ability to be fine-tuned for specific tasks and datasets further highlights its flexibility and potential for application in various time series analysis scenarios.\n\nOverall, TIMEXER appears to be a powerful tool for time series analysis and forecasting, with its pre-trained capabilities and potential for fine-tuning making it a valuable asset for researchers and practitioners in this field.", "id": "TIMEXER", "label": "TIMEXER", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "General is a domain mentioned in the text, as indicated by its description and use in various models", "id": "GENERAL", "label": "GENERAL", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Self-supervised is a methodology mentioned in the text, as indicated by its description and use in various models", "id": "SELF-SUPERVISED", "label": "SELF-SUPERVISED", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "METHOD"}, {"color": "#97c2fc", "description": "", "id": "TSMAE", "label": "TSMAE", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": ""}, {"color": "#97c2fc", "description": "Contrastive is a methodology mentioned in the text, as indicated by its description and use in various models", "id": "CONTRASTIVE", "label": "CONTRASTIVE", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "METHOD"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"MOBILITY\" is a domain mentioned in the text, as indicated by its description and use in various models. Mobility refers to the movement or flow of people, goods, or services. This concept is likely related to the analysis of time series data, as indicated by the mention of \"time series\" and \"long-term series forecasting\" in the text. The use of technical terms such as \"frequency analysis\" and \"multi-head cross-attention\" suggests that the models developed for mobility analysis are complex and require advanced techniques.\n\nThe text also implies that the mobility data is likely to be analyzed in the context of academic research, as indicated by the presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals. The citation of English-language academic papers and authors further supports this conclusion.\n\nOverall, the entity \"MOBILITY\" is a complex domain that involves the analysis of time series data, frequency analysis, and advanced machine learning techniques. Its study is likely to be conducted in an academic setting, with a focus on developing accurate models for long-term series forecasting.", "id": "MOBILITY", "label": "MOBILITY", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,77c7ced6ab4a0e57fba604c1a3e00416", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Non-Transformer-based is a methodology mentioned in the text, as indicated by its description and use in various models", "id": "NON-TRANSFORMER-BASED", "label": "NON-TRANSFORMER-BASED", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "METHOD"}, {"color": "#97c2fc", "description": "MLP RNN CNN is a model mentioned in the text, as indicated by its description and use in various models", "id": "MLP RNN CNN", "label": "MLP RNN CNN", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "TF-C is a model mentioned in the text, as indicated by its abbreviation and description", "id": "TF-C", "label": "TF-C", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTS2VEC is a model mentioned in the text, as indicated by its abbreviation and description. Specifically, TS2VEC is a research that introduces a universal framework for representing time series via contrastive learning. This framework is likely used for time series analysis and forecasting, as suggested by the mention of \"time series\" and \"long-term series forecasting\" in the text. The use of contrastive learning in TS2VEC implies that it is a machine learning-based approach, which is further supported by the presence of technical terms such as \"multi-head cross-attention\" and references to academic papers and conferences (e.g., ICLR, AAAI, PMLR). Overall, TS2VEC appears to be a research model that aims to provide a universal representation of time series data for various applications in time series analysis and forecasting.\n\nNote that the summary is written in a formal and academic tone, consistent with the language and content of the original text.", "id": "TS2VEC", "label": "TS2VEC", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,ad8efcfda80253036294f2eeb48926e8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nRWKV-TS is a type of neural network architecture specifically designed for time series data tasks. It is a model mentioned in the text, as indicated by its abbreviation and description. The model is likely used for long-term series forecasting, frequency analysis, and other time series-related tasks, given its application in time series data tasks. The use of multi-head cross-attention in RWKV-TS suggests its ability to handle complex time series data and potentially capture long-range dependencies. Overall, RWKV-TS appears to be a sophisticated neural network architecture tailored for time series analysis and forecasting.", "id": "RWKV-TS", "label": "RWKV-TS", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,b9d22fe9f2eecb1665f4bea365c7d612", "type": "MODEL"}, {"color": "#97c2fc", "description": "Parameter-efficient inception block is a type of neural network layer used for time series data tasks", "id": "PARAMETER-EFFICIENT INCCEPTION BLOCK", "label": "PARAMETER-EFFICIENT INCCEPTION BLOCK", "shape": "dot", "size": 10, "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nDPMN is a type of neural network specifically designed for time series forecasting. It utilizes Convolutional Neural Networks (CNNs) as a fundamental building block, leveraging their capabilities to effectively process and analyze time series data. This design enables DPMN to efficiently handle the complexities and patterns inherent in time series forecasting tasks.\n\nThis summary is based on the provided descriptions, which are consistent and complementary. The information collected from both descriptions has been integrated to provide a clear and concise overview of DPMN\u0027s characteristics and capabilities.", "id": "DPMN", "label": "DPMN", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6,7d5d82d600620153153772a9bc498ac0", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"CNN\" refers to a type of neural network layer and model architecture. Specifically, it is a type of neural network layer used for processing time-series data, as well as a type of neural network model that has demonstrated superior performance than RNNs in multiple tasks on sequential data. Additionally, CNNs are commonly used in computer vision tasks, indicating their versatility and effectiveness in various applications.\n\nIt is worth noting that the term \"CNN\" is used to describe both a specific layer within a neural network and a broader type of neural network architecture. This suggests that the term is being used in a somewhat general sense, encompassing both the layer and the model architecture.\n\nOverall, the summary provides a clear understanding of the entity \"CNN\" and its applications in time-series data processing, sequential data tasks, and computer vision.", "id": "CNN", "label": "CNN", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420,7d5d82d600620153153772a9bc498ac0,a5d60c1a355b77187003182f6df57646", "type": "ARCHITECTURE"}, {"color": "#97c2fc", "description": "RWKV is a type of neural network architecture used for time series data tasks", "id": "RWKV", "label": "RWKV", "shape": "dot", "size": 10, "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "type": "MODEL"}, {"color": "#97c2fc", "description": "Diffusion-based is a methodology mentioned in the text, as indicated by its description and use in various models", "id": "DIFFUSION-BASED", "label": "DIFFUSION-BASED", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "METHOD"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nTIMEGRAD is a deep learning forecasting model that utilizes Recurrent Neural Networks (RNNs) for time series forecasting. It is also described as a diffusion model for time series forecasting, indicating its ability to handle complex temporal dependencies in data. As mentioned in the text, TIMEGRAD is a model that is explicitly mentioned, with its abbreviation and description provided, suggesting its significance in the context of time series forecasting. Overall, TIMEGRAD appears to be a sophisticated model designed for accurate and efficient time series forecasting, leveraging the strengths of both RNNs and diffusion models.", "id": "TIMEGRAD", "label": "TIMEGRAD", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,77c7ced6ab4a0e57fba604c1a3e00416,7e69d9444a2084b6452e291735bf5a49", "type": "MODEL"}, {"color": "#97c2fc", "description": "D3VAE is a model mentioned in the text, as indicated by its abbreviation and description", "id": "D3VAE", "label": "D3VAE", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTransFusion is a diffusion model specifically designed for time series forecasting. It is a model mentioned in the text, as indicated by its abbreviation and description, suggesting that it is a notable entity in the context of time series forecasting. The model\u0027s primary function is to forecast future values in a time series, which is a crucial task in various fields such as finance, economics, and engineering.\n\nThe use of the term \"diffusion model\" implies that TransFusion employs a type of machine learning architecture that is inspired by the concept of diffusion processes. This suggests that the model is capable of capturing complex patterns and relationships in time series data, which is essential for accurate forecasting.\n\nOverall, TransFusion appears to be a sophisticated model for time series forecasting, and its mention in the text indicates that it is a relevant and important entity in the field of time series analysis.\n\nRelevant information from the nearby text:\n\n* The text mentions that TransFusion is a model for time series forecasting, which is a critical task in various fields.\n* The use of technical terms such as \"diffusion model\" and \"time series forecasting\" suggests that the text is written in a formal and academic tone, which is consistent with the language used in research papers and technical documents.\n* The mention of TransFusion as a model in the text implies that it is a notable entity in the context of time series forecasting, and its description provides valuable insights into its capabilities and functions.", "id": "TRANSFUSION", "label": "TRANSFUSION", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"RNNS\" refers to Recurrent Neural Networks (RNNs), a type of deep learning architecture used in various models, including DeepState and DeepFactor. RNNs are a type of deep learning model that is utilized by DeepState, indicating their significance in this specific application. \n\nThis summary is based on the provided descriptions, which are consistent and provide a clear understanding of the entity \"RNNS\" and its relationship with the models DeepState and DeepFactor.", "id": "RNNS", "label": "RNNS", "shape": "dot", "size": 10, "source_id": "42c90ca40b234c098c55632ec70038a1,7e69d9444a2084b6452e291735bf5a49", "type": "MODEL"}, {"color": "#97c2fc", "description": "ScoreGrad is a model mentioned in the text, as indicated by its abbreviation and description", "id": "SCOREGRAD", "label": "SCOREGRAD", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "Bilo\u0161 et al. is a model mentioned in the text, as indicated by its description and use in various models", "id": "BILO\u0160 ET AL.", "label": "BILO\u0160 ET AL.", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "Crabb\u00e9 et al. is a model mentioned in the text, as indicated by its description and use in various models", "id": "CRABB\u00c9 ET AL.", "label": "CRABB\u00c9 ET AL.", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "TimeDiff is a model mentioned in the text, as indicated by its abbreviation and description", "id": "TIMEDIFF", "label": "TIMEDIFF", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "Wang et al. is a model mentioned in the text, as indicated by its description and use in various models", "id": "WANG ET AL.", "label": "WANG ET AL.", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "DiffTime is a model mentioned in the text, as indicated by its abbreviation and description", "id": "DIFFTIME", "label": "DIFFTIME", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "FTS-Diffusion is a model mentioned in the text, as indicated by its abbreviation and description", "id": "FTS-DIFFUSION", "label": "FTS-DIFFUSION", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "DiffLoad is a model mentioned in the text, as indicated by its abbreviation and description", "id": "DIFFLOAD", "label": "DIFFLOAD", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "TPLLM is a model mentioned in the text, as indicated by its abbreviation and description", "id": "TPLLM", "label": "TPLLM", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "GATGPT is a model mentioned in the text, as indicated by its abbreviation and description", "id": "GATGPT", "label": "GATGPT", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "Domain-agnostic models are a concept mentioned in the text, as indicated by the use of phrases such as \"domain-agnostic models\" and \"general-purpose models\"", "id": "DOMAIN-AGNOSTIC MODELS", "label": "DOMAIN-AGNOSTIC MODELS", "shape": "dot", "size": 10, "source_id": "859c14b7112010530eddafc204b4b305", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "MetePFL is a model that uses spatial-temporal prompt learning", "id": "METEPFL", "label": "METEPFL", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"SPGCL\" is a model mentioned in the text. It is indicated by its abbreviation and description, as well as the use of the phrase \"SPGCL [53]\", which suggests a reference to a specific academic paper or citation. The text does not provide further information about the model\u0027s purpose, functionality, or characteristics, but it is clear that SPGCL is a relevant concept within the context of the text.\n\nGiven the formal and academic language used in the text, it is likely that SPGCL is a technical term or concept related to machine learning, time series analysis, or a similar field. The use of the phrase \"SPGCL [53]\" also suggests that the model is mentioned in a specific academic paper or conference, which may provide further information about its development, application, or evaluation.\n\nOverall, the summary provides a concise description of the entity \"SPGCL\" as a model mentioned in the text, with a possible reference to a specific academic paper or citation.", "id": "SPGCL", "label": "SPGCL", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,859c14b7112010530eddafc204b4b305", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nSTGCL is a model mentioned in the text, which is written in English. The primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers, all of which are written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nSTGCL is specifically mentioned in the text as indicated by its abbreviation and description, as well as the use of the phrase \"STGCL [61]\", which suggests that it is a model that has been referenced in an academic paper or conference, likely one of the conferences mentioned such as ICLR, AAAI, or PMLR.\n\nOverall, STGCL appears to be a model that has been discussed in the context of academic research, likely in the field of machine learning or time series forecasting, given the technical terms and mathematical equations used in the text.", "id": "STGCL", "label": "STGCL", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,859c14b7112010530eddafc204b4b305", "type": "MODEL"}, {"color": "#97c2fc", "description": "DSTPP is a model mentioned in the text, as indicated by its abbreviation and description", "id": "DSTPP", "label": "DSTPP", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nDYFFUSION is a model mentioned in the text, as indicated by its abbreviation and description. The model is likely related to time series analysis, given the context of the text, which discusses topics such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis.\" The use of the phrase \"DYffusion [9]\" further supports the notion that DYFFUSION is a model mentioned in the text, and the reference number \"[9]\" suggests that it is a model that has been discussed or referenced in an academic paper or conference.\n\nGiven the formal and academic language used in the text, it is likely that DYFFUSION is a model that has been developed or proposed in a research paper or technical document. The use of technical terms and mathematical equations in the text also suggests that DYFFUSION is a complex model that requires a deep understanding of time series analysis and machine learning concepts.\n\nOverall, DYFFUSION appears to be a model that is relevant to the field of time series analysis and machine learning, and its description and abbreviation suggest that it is a model that has been discussed or referenced in an academic paper or conference.", "id": "DYFFUSION", "label": "DYFFUSION", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,859c14b7112010530eddafc204b4b305", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"USTD\" is a model mentioned in the text. It is described as being referenced by its abbreviation and a specific citation number, denoted as \"[41]\". This suggests that USTD is a model that has been discussed or utilized in a research paper or technical document, and its description is likely to be found in the context of time series analysis or forecasting, given the mention of \"time series\" and \"long-term series forecasting\" in the surrounding text.\n\nFurthermore, the use of the phrase \"USTD [41]\" implies that the model is being referenced in a specific academic paper or conference, possibly one that is related to the fields of artificial intelligence, machine learning, or natural language processing, given the mention of \"multi-head cross-attention\" and other technical terms.\n\nOverall, the entity \"USTD\" appears to be a model that has been discussed or utilized in a research paper or technical document, and its description is likely to be found in the context of time series analysis or forecasting, with possible connections to artificial intelligence, machine learning, or natural language processing.", "id": "USTD", "label": "USTD", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,859c14b7112010530eddafc204b4b305", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nCLIMAX is a model mentioned in the text, as indicated by its abbreviation and description. The model is likely related to time series analysis, given the context of the text, which discusses long-term series forecasting, frequency analysis, and multi-head cross-attention. The use of the phrase \"ClimaX [67]\" further supports the notion that CLIMAX is a model mentioned in the text, possibly as a reference to a specific academic paper or research study.\n\nGiven the formal and academic tone of the text, it is likely that CLIMAX is a model developed in the context of machine learning or artificial intelligence research. The use of technical terms and mathematical equations in the text suggests a high level of technical expertise and a focus on developing advanced models for time series forecasting and analysis.\n\nOverall, CLIMAX appears to be a model that is relevant to the field of time series analysis and machine learning, and its description and abbreviation suggest that it is a specific model mentioned in the text.", "id": "CLIMAX", "label": "CLIMAX", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,859c14b7112010530eddafc204b4b305", "type": "MODEL"}, {"color": "#97c2fc", "description": "PriSTI is a model mentioned in the text, as indicated by its abbreviation and description", "id": "PRISTI", "label": "PRISTI", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "FourCastNet is a model mentioned in the text, as indicated by its abbreviation and description", "id": "FOURCASTNET", "label": "FOURCASTNET", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nFedWing is a model mentioned in the text, as indicated by its abbreviation and description. Specifically, FedWing is a model that utilizes spatial-temporal prompt learning, which is a technique used in machine learning to incorporate contextual information and temporal dependencies into the learning process. This suggests that FedWing is designed to handle complex, time-series data and is capable of learning from spatial and temporal relationships.\n\nGiven the formal and academic tone of the text, it is likely that FedWing is a research model or a technical solution developed for a specific application, such as long-term series forecasting or frequency analysis. The use of technical terms and mathematical equations in the text further supports this conclusion.\n\nOverall, FedWing appears to be a sophisticated machine learning model that leverages spatial-temporal prompt learning to analyze and predict complex time-series data.", "id": "FEDWING", "label": "FEDWING", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831,77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nPANGU-WEATHER is a model mentioned in the text, as indicated by its abbreviation and description. It is also mentioned as Pangu-Weather [4], suggesting that it is a model that has been referenced in a specific academic paper or conference, likely one of the conferences or journals mentioned in the text, such as ICLR, AAAI, or PMLR. The model is likely related to time series forecasting, given the context of the text, which discusses long-term series forecasting, frequency analysis, and multi-head cross-attention, all of which are techniques commonly used in time series forecasting.\n\nOverall, PANGU-WEATHER appears to be a model that has been developed or referenced in the context of time series forecasting, and its description and abbreviation suggest that it is a specific model that has been discussed in academic literature.", "id": "PANGU-WEATHER", "label": "PANGU-WEATHER", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,859c14b7112010530eddafc204b4b305", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTRAJGDM is a model primarily used for mobility forecasting. It is a model mentioned in the text, as indicated by its abbreviation and description. The model is likely used in the context of time series analysis and forecasting, given its association with mobility forecasting. The text suggests that the model is used for long-term series forecasting, which implies that it is capable of handling complex and dynamic data.\n\nThe use of technical terms such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis\" in the text suggests that the model is used in a formal and academic context, possibly in research papers or technical documents. The presence of mathematical equations and formulas in the text also supports this notion.\n\nOverall, TRAJGDM appears to be a model used for mobility forecasting, with a focus on long-term series forecasting and time series analysis. Its use in a formal and academic context suggests that it is a sophisticated model with a strong theoretical foundation.\n\nRelevant information from the nearby text includes:\n\n* The text contains various technical terms, mathematical equations, and references to academic papers, which are all written in English.\n* The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n* The use of English words and phrases, mathematical equations, and English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further support the notion that the text is written in English.\n\nThis information provides context for the use of TRAJGDM and suggests that it is a model used in a formal and academic context, possibly in research papers or technical documents.", "id": "TRAJGDM", "label": "TRAJGDM", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"DIFFTRAJ\" can be generated as follows:\n\nDIFFTRAJ is a model for mobility forecasting that reconstructs and synthesizes geographic trajectories from white noise through a conditioned reverse trajectory denoising process. This model is mentioned in the text, as indicated by its abbreviation and description, suggesting its relevance in the field of mobility forecasting and trajectory analysis.\n\nThe model\u0027s functionality involves denoising white noise to generate realistic geographic trajectories, which can be useful in various applications such as traffic prediction, route optimization, and location-based services. The use of a conditioned reverse trajectory denoising process implies that DIFFTRAJ is designed to learn from existing trajectories and generate new ones that are consistent with the patterns and structures observed in the data.\n\nOverall, DIFFTRAJ appears to be a sophisticated model for mobility forecasting that leverages advanced techniques in trajectory analysis and denoising to generate accurate and realistic geographic trajectories.", "id": "DIFFTRAJ", "label": "DIFFTRAJ", "shape": "dot", "size": 10, "source_id": "1ea4e7d0dd5128868533b4567a0a0273,77c7ced6ab4a0e57fba604c1a3e00416,ef8fd6170b08af3bd99c9df44ffa8b57", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTREMBR is a model that has been mentioned in the text. It leverages auto-encoding techniques to extract road network and temporal information embedded in trajectories. This suggests that TREMBR is a sophisticated model that utilizes advanced techniques to analyze and extract valuable insights from trajectory data.\n\nThe use of auto-encoding techniques implies that TREMBR is capable of learning complex patterns and relationships within the data, allowing it to extract meaningful information about the road network and temporal aspects of the trajectories. This is likely to be useful in a variety of applications, such as traffic forecasting, route optimization, and urban planning.\n\nOverall, TREMBR appears to be a powerful and versatile model that has the potential to make significant contributions to the field of transportation and urban planning.", "id": "TREMBR", "label": "TREMBR", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,ef8fd6170b08af3bd99c9df44ffa8b57", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nSTART is a model mentioned in the text, which is primarily written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document. START is a model that introduces a hybrid approach to trajectory embedding learning by combining masked language model and SimCLR. This approach is likely used for time series forecasting, as indicated by the mention of \"long-term series forecasting\" and \"frequency analysis\" in the text. The model\u0027s ability to learn from masked language and SimCLR suggests that it is designed to handle complex patterns and relationships in data, making it a suitable candidate for applications such as trajectory embedding learning.", "id": "START", "label": "START", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,ef8fd6170b08af3bd99c9df44ffa8b57", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nLLM-MOB is a model that plays a crucial role in encoding human mobility data into structured prompts to instruct Large Language Models (LLMs) to consider both long-term and short-term behavioral patterns. This model is mentioned in the text, as indicated by the reference number \"[87]\". The use of LLM-MOB in this context suggests its significance in time series forecasting, particularly in long-term series forecasting, where frequency analysis and multi-head cross-attention mechanisms may be employed. The model\u0027s ability to capture both long-term and short-term patterns is essential in understanding human mobility data, which is critical in various applications, including but not limited to, urban planning, transportation systems, and public health.\n\nThe mention of LLM-MOB in the text, along with its description, indicates that it is a model that has been developed or proposed in the context of human mobility data analysis. The use of technical terms and mathematical equations in the text further supports this conclusion, suggesting that the model is being discussed in an academic or research setting. The reference to LLM-MOB in the text, along with its description, provides a clear understanding of the model\u0027s purpose and functionality, highlighting its importance in the field of human mobility data analysis.", "id": "LLM-MOB", "label": "LLM-MOB", "shape": "dot", "size": 10, "source_id": "859c14b7112010530eddafc204b4b305,ef8fd6170b08af3bd99c9df44ffa8b57", "type": "MODEL"}, {"color": "#97c2fc", "description": "GTM is a model that separates trajectory features into three domains, which can be masked and generated independently", "id": "GTM", "label": "GTM", "shape": "dot", "size": 10, "source_id": "ef8fd6170b08af3bd99c9df44ffa8b57", "type": "MODEL"}, {"color": "#97c2fc", "description": "Input patching is a technique used in some models to process input data in a more efficient way", "id": "INPUT PATCHING", "label": "INPUT PATCHING", "shape": "dot", "size": 10, "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The text is a preprint, indicating that it is a draft or unpublished version of a research paper", "id": "PREPRINT", "label": "PREPRINT", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "The TimesFM pretraining dataset is a collection of time-series data used to train the TimesFM model", "id": "TIMESFM PRETRAINING DATASET", "label": "TIMESFM PRETRAINING DATASET", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "Zero-shot llmtime is a model mentioned in the text, specifically GFQW23", "id": "ZERO-SHOT LLMTIME", "label": "ZERO-SHOT LLMTIME", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7", "type": "MODEL"}, {"color": "#97c2fc", "description": "Llmtime is a model that performs well in time-series forecasting tasks", "id": "LLMETIME", "label": "LLMETIME", "shape": "dot", "size": 10, "source_id": "efb7975581b22620b8277417edeee3e9", "type": "MODEL"}, {"color": "#97c2fc", "description": "Seasonal ARIMA is a model that performs well in time-series forecasting tasks", "id": "SEASONAL ARIMA", "label": "SEASONAL ARIMA", "shape": "dot", "size": 10, "source_id": "efb7975581b22620b8277417edeee3e9", "type": "MODEL"}, {"color": "#97c2fc", "description": "Time-series data refers to a set of data points measured at regular time intervals", "id": "TIMESERIES DATA", "label": "TIMESERIES DATA", "shape": "dot", "size": 10, "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "DATA"}, {"color": "#97c2fc", "description": "Informer datasets are a set of datasets used to evaluate the performance of machine learning models for time-series forecasting", "id": "INFORMER DATASETS", "label": "INFORMER DATASETS", "shape": "dot", "size": 10, "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTIMESFM(ZS) is a model used for evaluation and zero-shot forecasting. Specifically, it is a decoder-only foundation model that falls under the broader category of the TimesFM model. This model is designed to handle tasks related to long-term series forecasting, which involves analyzing and predicting time series data over extended periods. The TIMESFM(ZS) model utilizes techniques such as frequency analysis and multi-head cross-attention to achieve its forecasting capabilities.\n\nThe model\u0027s primary application is in zero-shot forecasting, where it can generate predictions without requiring explicit training data for the specific task at hand. This is made possible by the model\u0027s decoder-only architecture, which enables it to generate forecasts based on its understanding of the underlying patterns and relationships in the time series data.\n\nOverall, TIMESFM(ZS) is a specialized instance of the TimesFM model, tailored for evaluation and zero-shot forecasting tasks. Its decoder-only architecture and use of advanced techniques such as frequency analysis and multi-head cross-attention make it a valuable tool for time series forecasting applications.", "id": "TIMESFM(ZS)", "label": "TIMESFM(ZS)", "shape": "dot", "size": 10, "source_id": "39365aee753fb73130c208fdf4046bb7,41b0bd14ce7ff7419b7ee1f78b4701ae,cd4ff69415c7b37cec643f8289d1c98d", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nLLMTIME(ZS) is a model that is specifically designed for time-series forecasting tasks and is also utilized for evaluation purposes. It is a particular model used for evaluation, indicating its effectiveness in assessing the performance of other models in time-series forecasting.\n\nThis summary is derived from the descriptions provided, which collectively convey the primary function and application of LLMTIME(ZS). The model\u0027s performance in time-series forecasting tasks and its use in evaluation suggest its relevance and utility in this domain.", "id": "LLMTIME(ZS)", "label": "LLMTIME(ZS)", "shape": "dot", "size": 10, "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae,4ade7764ebe998b5f35a7fd2b58d4796,d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "MODEL"}, {"color": "#97c2fc", "description": "Hyper-parameters are the settings used to train the model, including learning rate, batch size, and number of layers", "id": "HYPER-PARAMETERS", "label": "HYPER-PARAMETERS", "shape": "dot", "size": 10, "source_id": "673b0feee6eb5843954defc670e4ba29", "type": "PARAMETERS"}, {"color": "#97c2fc", "description": "Synthetic curves are a type of data used to evaluate the performance of time-series forecasting models", "id": "SYNTHETIC CURVES", "label": "SYNTHETIC CURVES", "shape": "dot", "size": 10, "source_id": "e6ec99a117b9abd42452ff51cd6e8f0b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"MOIRAI-1.0-R\" can be generated as follows:\n\nMOIRAI-1.0-R is a pre-trained deep learning model specifically designed for time series forecasting. It utilizes a recurrent neural network approach, which enables it to effectively capture patterns and trends in time series data. MOIRAI-1.0-R has been shown to perform better than Lag-Llama on some datasets, indicating its potential as a reliable and accurate forecasting tool. The model is mentioned in the text, alongside Lag-Llama, suggesting its relevance and importance in the context of time series forecasting.\n\nThis summary incorporates information from all the provided descriptions, resolving any potential contradictions and presenting a coherent overview of the entity \"MOIRAI-1.0-R\".", "id": "MOIRAI-1.0-R", "label": "MOIRAI-1.0-R", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,1ddbab2dca370c9ef7b5a724075518cc,2565ae205d4c98342168bb67a4f7a309,56de1d5a5b467101344afa4248d829dc,5b1135d9e53e53428a042e8bbc89eaa7,af36d1634490149b96980fb1dff57cd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTouvron et al. (2023) is a research paper that discusses the Llama model. Specifically, the paper incorporates pre-normalization via the RMSNorm and Rotary Positional Encoding at each attention layer\u0027s query and key representations. This suggests that the authors of the paper, Touvron et al., have made significant contributions to the development of the Llama model, particularly in the area of attention mechanisms.\n\nThe paper, Touvron et al. (2023), is likely a technical document or research paper that is written in English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers and authors. The language used is formal and academic, suggesting that the paper is from a reputable academic conference or journal.\n\nOverall, Touvron et al. (2023) is an important paper that has contributed to the development of the Llama model, and its findings and techniques are likely to have significant implications for the field of natural language processing and machine learning.", "id": "TOUVRON ET AL. (2023)", "label": "TOUVRON ET AL. (2023)", "shape": "dot", "size": 10, "source_id": "b305a0d76a0159dc10338467e16d6761,b67d18d306fde251ee94b0a831d1e075", "type": "PAPER"}, {"color": "#97c2fc", "description": "Zhang \u0026 Sennrich (2019) is a paper that Lag-Llama incorporates pre-normalization via the RMSNorm", "id": "ZHANG \u0026 SENNRICH (2019)", "label": "ZHANG \u0026 SENNRICH (2019)", "shape": "dot", "size": 10, "source_id": "b305a0d76a0159dc10338467e16d6761", "type": "PAPER"}, {"color": "#97c2fc", "description": "Su et al. (2021) is a paper that Lag-Llama incorporates Rotary Positional Encoding", "id": "SU ET AL. (2021)", "label": "SU ET AL. (2021)", "shape": "dot", "size": 10, "source_id": "b305a0d76a0159dc10338467e16d6761", "type": "PAPER"}, {"color": "#97c2fc", "description": "AutoML is a framework used for automating machine learning tasks", "id": "AUTOML", "label": "AUTOML", "shape": "dot", "size": 10, "source_id": "51ee17c1f0212d4c94010d8e376f649b", "type": "FRAMEWORK"}, {"color": "#97c2fc", "description": "OneFitsAll model is a model that adapts a pre-trained LLM for forecasting", "id": "ONEFITSALL MODEL", "label": "ONEFITSALL MODEL", "shape": "dot", "size": 10, "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "type": "MODEL"}, {"color": "#97c2fc", "description": "Beijing-pm2.5 is a dataset used in the experiments", "id": "BEIJING-PM2.5", "label": "BEIJING-PM2.5", "shape": "dot", "size": 10, "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"DECODER-ONLY TRANSFORMER ARCHITECTURE\" is a concept mentioned in the text, as indicated by the use of the phrase \"decoder-only transformer architecture.\" This architecture is utilized by Lag-Llama, which employs a decoder-only transformer architecture that incorporates lags as covariates. The use of this architecture suggests a focus on time series forecasting, as it is often used in conjunction with techniques such as frequency analysis and long-term series forecasting. The presence of mathematical equations and formulas in the text further supports this interpretation, as they are commonly used in English-language academic papers to describe and analyze time series data. Overall, the \"DECODER-ONLY TRANSFORMER ARCHITECTURE\" appears to be a key component of Lag-Llama\u0027s approach to time series forecasting, leveraging the strengths of transformer architectures to improve forecasting accuracy.", "id": "DECODER-ONLY TRANSFORMER ARCHITECTURE", "label": "DECODER-ONLY TRANSFORMER ARCHITECTURE", "shape": "dot", "size": 10, "source_id": "1727ee77a376bbef1c7625a001e4ac3d,c7167cf92abe7513ccb936ca79871932", "type": "MODEL"}, {"color": "#97c2fc", "description": "Arjun is a person mentioned in the text as the one who organized, planned, and led the project", "id": "ARJUN", "label": "ARJUN", "shape": "dot", "size": 10, "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nKashif is a researcher who contributed to the development of the Lag-Llama architecture and wrote the code for its architecture and training strategies. As the primary developer of Lag-Llama, Kashif played a crucial role in shaping the model\u0027s design and implementation. The Lag-Llama architecture is a significant contribution to the field of machine learning, and Kashif\u0027s work on it has the potential to impact various applications, including time series forecasting and long-term series forecasting.\n\nThe Lag-Llama architecture is likely to involve advanced techniques such as frequency analysis and multi-head cross-attention, which are commonly used in modern machine learning models. Kashif\u0027s expertise in these areas is evident from his work on Lag-Llama, and his contributions to the field of machine learning are likely to be recognized in academic conferences and journals, such as ICLR, AAAI, and PMLR.\n\nOverall, Kashif is a talented researcher who has made significant contributions to the development of Lag-Llama, and his work has the potential to shape the future of machine learning and time series forecasting.", "id": "KASHIF", "label": "KASHIF", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,1727ee77a376bbef1c7625a001e4ac3d", "type": "PERSON"}, {"color": "#97c2fc", "description": "Hena is a researcher who contributed to the development of Lag-Llama and added support for time features and other features", "id": "HENA", "label": "HENA", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "PERSON"}, {"color": "#97c2fc", "description": "Andrew is a researcher who contributed to the development of Lag-Llama and expanded the empirical design of the paper", "id": "ANDREW", "label": "ANDREW", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "PERSON"}, {"color": "#97c2fc", "description": "Rishika is a researcher who contributed to the development of Lag-Llama and ran experiments and contributed to the writing of the first version of the paper", "id": "RISHIKA", "label": "RISHIKA", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "PERSON"}, {"color": "#97c2fc", "description": "Arian is a researcher who contributed to the development of Lag-Llama and ran experiments and contributed to the writing of the first version of the paper", "id": "ARIAN", "label": "ARIAN", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "PERSON"}, {"color": "#97c2fc", "description": "Mohammad is a researcher who contributed to the development of Lag-Llama and worked with all AutoGluon models and experiments", "id": "MOHAMMAD", "label": "MOHAMMAD", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "PERSON"}, {"color": "#97c2fc", "description": "George is a researcher who contributed to the development of Lag-Llama and ran experiments and contributed to the writing of the first version of the paper", "id": "GEORGE", "label": "GEORGE", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"ROLAND\" is a researcher who has made significant contributions to the development of the One-FitsAll model and Lag-Llama. Specifically, ROLAND worked closely with the code and experiments of the One-FitsAll model, demonstrating expertise in this area. Additionally, ROLAND\u0027s involvement in the development of Lag-Llama highlights their role as a researcher in the field, further emphasizing their technical capabilities and contributions to the project.\n\nThis summary is based on the information provided in the description list, which presents ROLAND as a contributor to the project and a researcher who has worked on the One-FitsAll model and Lag-Llama. The summary aims to provide a clear and concise overview of ROLAND\u0027s role and contributions, while also highlighting their expertise and involvement in the project.", "id": "ROLAND", "label": "ROLAND", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,88d96b5f76042688e9dd745eb822d919", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nNadhir is a researcher who has made significant contributions to the development of various projects, including the integration of the N-BEATS model. Specifically, Nadhir was involved in the development of Lag-Llama, a project where they integrated the N-BEATS model, showcasing their expertise in time series forecasting and model development. This highlights Nadhir\u0027s role as a contributor to the project, leveraging their skills in machine learning and time series analysis to enhance the capabilities of Lag-Llama.\n\nThe mention of the N-BEATS model and Lag-Llama suggests that Nadhir\u0027s work is focused on time series forecasting and model development, which is further supported by the context of the project. The integration of the N-BEATS model into Lag-Llama implies that Nadhir has expertise in both long-term series forecasting and frequency analysis, as well as the ability to apply complex models like multi-head cross-attention.\n\nOverall, Nadhir\u0027s contributions to the project demonstrate their proficiency in machine learning, time series analysis, and model development, making them a valuable asset to the research community.", "id": "NADHIR", "label": "NADHIR", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,88d96b5f76042688e9dd745eb822d919", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMARIN is a researcher who contributed to the development of Lag-Llama, a project that involves the use of time series analysis and long-term series forecasting. Specifically, MARIN wrote the initial code for sampling windows for the pretraining set, which is a crucial component of the project. This suggests that MARIN has expertise in time series analysis and has made significant contributions to the development of Lag-Llama.\n\nThe use of technical terms such as \"sampling windows\" and \"pretraining set\" indicates that MARIN is familiar with the technical aspects of machine learning and time series forecasting. Additionally, the fact that MARIN contributed to the development of Lag-Llama suggests that they have a strong background in research and development.\n\nOverall, MARIN is a skilled researcher with expertise in time series analysis and machine learning, who has made significant contributions to the development of Lag-Llama.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which is consistent with the formal and academic tone of the descriptions provided.\n* The text also mentions the use of technical terms and mathematical equations, which suggests that the descriptions provided are from a technical or academic context.\n* The fact that MARIN is a researcher who contributed to the development of Lag-Llama suggests that the project is related to machine learning and time series forecasting, which is consistent with the technical terms used in the descriptions.", "id": "MARIN", "label": "MARIN", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,88d96b5f76042688e9dd745eb822d919", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nVALIDATION LOSS is a concept used to evaluate the performance of a model. It is a metric that is used to assess how well a model is performing during the training process. Specifically, validation loss is a measure of the model\u0027s performance on a validation set, which is a subset of the data used to evaluate the model\u0027s generalization ability. This metric is crucial in machine learning as it helps in determining the optimal model parameters and preventing overfitting.\n\nThe validation loss is typically used in conjunction with other metrics such as training loss and test loss to get a comprehensive understanding of the model\u0027s performance. It is an essential component in the model development process, as it allows data scientists to identify areas of improvement and make necessary adjustments to the model.\n\nIn the context of machine learning, validation loss is often used to evaluate the performance of deep learning models, such as neural networks. It is a key metric in the training process, as it helps in determining the optimal number of epochs, learning rate, and other hyperparameters.\n\nOverall, validation loss is a critical metric in machine learning that provides valuable insights into the performance of a model. It is an essential tool for data scientists and machine learning engineers to develop and fine-tune models that generalize well to new, unseen data.", "id": "VALIDATION LOSS", "label": "VALIDATION LOSS", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,c60a8238db3d56eff9cc9692e7ac5b1c,e995e5477f470244a4a6afb9417f6d96,fa01b75dccc556af8aca0d6c47e1970f", "type": "METRIC"}, {"color": "#97c2fc", "description": "Train split is a concept used to divide the dataset into training and validation sets", "id": "TRAIN SPLIT", "label": "TRAIN SPLIT", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"NUMBER OF LAYERS\" is a hyperparameter that plays a crucial role in determining the depth of a model. This hyperparameter is a key component in the architecture of various machine learning models, and its value can significantly impact the performance and complexity of the model.\n\nIn the context of deep learning models, the number of layers refers to the number of neural network layers that are stacked together to process input data. The depth of the model, which is determined by the number of layers, can affect the model\u0027s ability to learn complex patterns and relationships in the data.\n\nThe choice of the number of layers is often a trade-off between model complexity and performance. A deeper model with more layers can learn more complex patterns, but it may also lead to overfitting and increased computational requirements. On the other hand, a shallower model with fewer layers may be less prone to overfitting, but it may not be able to learn as complex patterns.\n\nOverall, the number of layers is an important hyperparameter that requires careful tuning to achieve optimal performance in machine learning models.\n\nRelevant information from the nearby text is not provided, but based on general knowledge in the field of machine learning, the above summary provides a comprehensive understanding of the entity \"NUMBER OF LAYERS\".", "id": "NUMBER OF LAYERS", "label": "NUMBER OF LAYERS", "shape": "dot", "size": 10, "source_id": "9e88afa28686ff93769bfc5eb0f1095e,c5dc13d7191b625e7373e79907b5782a", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"NUMBER OF HEADS\" can be described as follows:\n\nThe \"NUMBER OF HEADS\" is a hyperparameter of the TimesFM model, which determines the number of attention heads in the model. This hyperparameter plays a crucial role in the TimesFM model, as it influences the model\u0027s ability to process and analyze time series data.\n\nIn the context of the TimesFM model, the \"NUMBER OF HEADS\" is a key component that enables the model to perform frequency analysis and long-term series forecasting. The model\u0027s architecture, which incorporates multi-head cross-attention, relies on the \"NUMBER OF HEADS\" to effectively process and integrate information from different attention heads.\n\nOverall, the \"NUMBER OF HEADS\" is a critical hyperparameter that affects the performance and capabilities of the TimesFM model, particularly in its ability to analyze and forecast time series data.\n\nNote: The description is based on the provided information and does not include any external knowledge or assumptions. The output is written in third person and includes the entity name for context.", "id": "NUMBER OF HEADS", "label": "NUMBER OF HEADS", "shape": "dot", "size": 10, "source_id": "9e88afa28686ff93769bfc5eb0f1095e,c5dc13d7191b625e7373e79907b5782a,ee8135f4497807724f665a5cdaa775fb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "HYPERPARAMETER", "label": "HYPERPARAMETER", "shape": "dot", "size": 10, "source_id": "c5dc13d7191b625e7373e79907b5782a", "type": ""}, {"color": "#97c2fc", "description": "Supervised models are models that are trained on labeled data, as indicated by the text", "id": "SUPERVISED MODEL", "label": "SUPERVISED MODEL", "shape": "dot", "size": 10, "source_id": "e995e5477f470244a4a6afb9417f6d96", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"NEURAL SCALING LAWS\" refers to a set of mathematical laws that describe the relationship between model parameters and performance. These laws are used to understand how the performance of a model scales with respect to different parameters. Specifically, neural scaling laws are mathematical laws that describe the relationship between model parameters and performance, as indicated by the text. They are used to analyze and predict the behavior of neural networks, particularly in the context of long-term series forecasting, frequency analysis, and multi-head cross-attention.\n\nThe language used to describe neural scaling laws is formal and academic, suggesting that the text is from a research paper or a technical document. The language is primarily English, as indicated by the use of English words and phrases, mathematical equations, and references to English-language academic papers and authors.\n\nOverall, neural scaling laws are a crucial concept in the field of machine learning, particularly in the context of neural networks and time series forecasting. They provide a mathematical framework for understanding how model parameters affect performance and can be used to improve the accuracy and efficiency of neural network models.", "id": "NEURAL SCALING LAWS", "label": "NEURAL SCALING LAWS", "shape": "dot", "size": 10, "source_id": "6c11bd339c9630f1d61f2024e90bce5e,e995e5477f470244a4a6afb9417f6d96", "type": "MODEL"}, {"color": "#97c2fc", "description": "Forecast visualizations are visualizations that are used to display the results of a forecast, as indicated by the text", "id": "FORECAST VISUALIZATIONS", "label": "FORECAST VISUALIZATIONS", "shape": "dot", "size": 10, "source_id": "e995e5477f470244a4a6afb9417f6d96", "type": "VISUALIZATION"}, {"color": "#97c2fc", "description": "Electricity hourly is a dataset that is used to train and evaluate models, as indicated by the text", "id": "ELECTRICITY HOURLY", "label": "ELECTRICITY HOURLY", "shape": "dot", "size": 10, "source_id": "e995e5477f470244a4a6afb9417f6d96", "type": "DATASET"}, {"color": "#97c2fc", "description": "ETT-H2 is a dataset that is used to train and evaluate models, as indicated by the text", "id": "ETT-H2", "label": "ETT-H2", "shape": "dot", "size": 10, "source_id": "e995e5477f470244a4a6afb9417f6d96", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe ETT-M2 dataset is a collection of data used to evaluate the performance of Lag-Llama, a model that requires training and evaluation. This dataset is utilized for both training and testing purposes, as indicated by the text. The ETT-M2 dataset is a valuable resource for assessing the capabilities of models like Lag-Llama in time series forecasting and analysis.\n\nIn the context of time series forecasting, the ETT-M2 dataset is particularly relevant, as it provides a comprehensive set of data points that can be used to train and evaluate models. The dataset\u0027s structure and content are well-suited for tasks such as long-term series forecasting, frequency analysis, and multi-head cross-attention, which are all essential components of advanced time series forecasting techniques.\n\nOverall, the ETT-M2 dataset is a crucial tool for researchers and developers working on time series forecasting and analysis, and its use is likely to be a key factor in the development of more accurate and effective models like Lag-Llama.", "id": "ETT-M2", "label": "ETT-M2", "shape": "dot", "size": 10, "source_id": "bcf830b85e12e1ab9498e3ac3593a88e,e995e5477f470244a4a6afb9417f6d96", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"PEDESTRIAN COUNTS\" refers to a dataset used for training, evaluating, pretraining, and fine-tuning machine learning models. It is a specific measure or indicator of traffic or movement, representing the number of people walking in a given area. This dataset is utilized in various applications, including long-term series forecasting, frequency analysis, and multi-head cross-attention models, as indicated by the text.\n\nThe use of pedestrian counts as a dataset is supported by the presence of mathematical equations and formulas, as well as references to academic papers and authors, which are commonly used in English-language academic conferences and journals. The dataset is likely used in research papers or technical documents, as suggested by the formal and academic language used.\n\nOverall, pedestrian counts are a crucial aspect of traffic analysis, providing valuable insights into the movement and behavior of pedestrians in a given area. The dataset is a valuable resource for researchers and practitioners working in the field of transportation and urban planning.", "id": "PEDESTRIAN COUNTS", "label": "PEDESTRIAN COUNTS", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,6820cea275275e87630a6876e890c525,e995e5477f470244a4a6afb9417f6d96,ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"REQUESTS MINUTE\" is a concept related to cloud computing. It is also a dataset used for various machine learning tasks, including training, evaluation, pretraining, and fine-tuning models. The dataset is likely used in the context of cloud computing to analyze and predict the number of requests per minute, which is a critical metric for understanding the performance and scalability of cloud-based systems.\n\nThe use of \"Requests Minute\" as a dataset suggests that it contains a large amount of data related to cloud computing requests, which can be used to train and evaluate machine learning models. The fact that it is used for pretraining and fine-tuning indicates that it is a valuable resource for developing and improving models that can handle complex tasks in cloud computing.\n\nOverall, \"REQUESTS MINUTE\" is a dataset and a concept that plays a crucial role in the field of cloud computing, particularly in the development and evaluation of machine learning models.\n\nRelevant information from the nearby text:\n\n* The text mentions that the language of the text is English, which is consistent with the formal and academic tone of the descriptions provided.\n* The use of technical terms and mathematical equations in the descriptions suggests that the field of cloud computing is a complex and technical domain that requires specialized knowledge and expertise.\n* The fact that \"Requests Minute\" is used as a dataset for pretraining and fine-tuning models suggests that it is a valuable resource for researchers and developers working in the field of cloud computing.", "id": "REQUESTS MINUTE", "label": "REQUESTS MINUTE", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e,e995e5477f470244a4a6afb9417f6d96", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Summary of Chronos Models**\n\nChronos models are a type of model that can be trained once across multiple datasets, streamlining production forecasting systems. They are specifically designed for time series forecasting, utilizing their capabilities to analyze and predict future values in a series of data points. Chronos models are also used in experimental settings, where they can be employed to test hypotheses and evaluate the performance of different forecasting strategies. Overall, Chronos models are a versatile and powerful tool for time series forecasting, offering a range of benefits for production forecasting systems and experimental research.\n\n**Key Characteristics of Chronos Models**\n\n* Can be trained once across multiple datasets\n* Streamline production forecasting systems\n* Designed for time series forecasting\n* Utilize frequency analysis and multi-head cross-attention mechanisms\n* Used in experimental settings for testing hypotheses and evaluating forecasting strategies\n\n**Relevant Information**\n\n* Chronos models are mentioned in the context of time series forecasting, suggesting a strong focus on analyzing and predicting future values in a series of data points.\n* The use of mathematical equations and formulas in the text suggests a high level of technical complexity, indicating that Chronos models are a sophisticated tool for time series forecasting.\n* The citation of academic papers and authors in the text suggests that Chronos models are a topic of ongoing research and development in the field of time series forecasting.\n\nOverall, Chronos models are a powerful and versatile tool for time series forecasting, offering a range of benefits for production forecasting systems and experimental research.", "id": "CHRONOS MODELS", "label": "CHRONOS MODELS", "shape": "dot", "size": 10, "source_id": "2eea45c6111e468189580a21686cb14a,53dbeea6d05c3695460c71f89f468def,5b1135d9e53e53428a042e8bbc89eaa7,63e284ec7e76fa859cc46b72d3746654,6a222f9ed7fcf5fc945dfc22e16a3502,c7f04fa1168df64cce110e20a1b72b51,e5e4a8f03f502fada5b17cba5dc942ba", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"LAG-LAGMA\" refers to a pre-trained model used for time series forecasting. Specifically, it is a specific model or algorithm that utilizes a local forecasting method, assuming a non-parametric sampling distribution. This method is distinct from other approaches, as it focuses on local forecasting rather than relying on global assumptions.\n\nThe name \"LAG-LAGMA\" is likely derived from the term \"Lag-Llama,\" which is a local forecasting method. However, it is essential to note that \"LAG-LAGMA\" is a distinct entity, with its own characteristics and applications in time series forecasting.\n\nIn the context of time series forecasting, \"LAG-LAGMA\" is a valuable tool for predicting future values based on historical data. Its pre-trained nature suggests that it has been trained on a large dataset, allowing it to learn patterns and relationships within the data.\n\nOverall, \"LAG-LAGMA\" is a sophisticated model that leverages local forecasting techniques to provide accurate predictions for time series data. Its unique approach and pre-trained capabilities make it a valuable asset for researchers and practitioners working in the field of time series forecasting.", "id": "LAG-LAGMA", "label": "LAG-LAGMA", "shape": "dot", "size": 10, "source_id": "1ddbab2dca370c9ef7b5a724075518cc,ec7705b83cf4fe3aa18662c917b18c1a,f0c52387a7b3a5c3850fe6991f0a7c83", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nGPT-4 is a foundation language model that is capable of performing well on a diverse range of Natural Language Processing (NLP) tasks. Specifically, it can excel in a few-shot or even zero-shot setting, indicating its ability to adapt and generalize across various tasks with minimal training data. As a language model, GPT-4 is designed to process and understand human language, making it a valuable tool for a wide range of applications in the field of NLP.\n\nThis summary is based on the provided descriptions, which are consistent and provide a clear understanding of GPT-4\u0027s capabilities and characteristics. There are no contradictions in the descriptions, and the information is coherent and easy to understand.", "id": "GPT-4", "label": "GPT-4", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075,e6a6bc6fbd362394320961ac10cdd230", "type": "MODEL"}, {"color": "#97c2fc", "description": "Audio models (AMS) are pre-trained models that can be fine-tuned for specific tasks", "id": "AMS", "label": "AMS", "shape": "dot", "size": 10, "source_id": "de8e097ce66fbc954bd529888cbc15ea", "type": "MODEL"}, {"color": "#97c2fc", "description": "Medical text reports refer to the written summaries of medical information, often used in healthcare and other fields", "id": "MEDICAL TEXT REPORTS", "label": "MEDICAL TEXT REPORTS", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "KMH+20 is a paper that established a power law like relationship between the number of parameters in a language model and its downstream performance", "id": "KMH+20", "label": "KMH+20", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "PAPER"}, {"color": "#97c2fc", "description": "Chung et al. is a paper that discusses the performance of LLMs on various natural language processing tasks", "id": "CHUNG ET AL.", "label": "CHUNG ET AL.", "shape": "dot", "size": 10, "source_id": "7e69d9444a2084b6452e291735bf5a49", "type": "PAPER"}, {"color": "#97c2fc", "description": "Touvron et al. is a paper that discusses the performance of LLMs on various natural language processing tasks", "id": "TOUVRON ET AL.", "label": "TOUVRON ET AL.", "shape": "dot", "size": 10, "source_id": "7e69d9444a2084b6452e291735bf5a49", "type": "PAPER"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"TASK INSTRUCTIONS\" refers to clear guidance provided to the Large Language Model (LLM) on the task to be performed. This concept is mentioned in the text, as indicated by the use of the phrase \"task instructions.\" The task instructions are a crucial aspect of the LLM\u0027s functionality, as they enable the model to understand the specific task it needs to accomplish and provide accurate results.\n\nIn the context of the text, task instructions are likely used to guide the LLM in various tasks, such as language translation, text summarization, or question-answering. The clear guidance provided by task instructions helps the LLM to focus on the specific task at hand and avoid potential errors or misinterpretations.\n\nOverall, the entity \"TASK INSTRUCTIONS\" plays a vital role in the functioning of the LLM, and its accurate interpretation is essential for achieving optimal results in various tasks.", "id": "TASK INSTRUCTIONS", "label": "TASK INSTRUCTIONS", "shape": "dot", "size": 10, "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8,b27c89cb0db6646b1203b2701e017aeb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Task-specific learning is a concept mentioned in the text, as indicated by the use of the phrase \"task-specific learning\"", "id": "TASK-SPECIFIC LEARNING", "label": "TASK-SPECIFIC LEARNING", "shape": "dot", "size": 10, "source_id": "b27c89cb0db6646b1203b2701e017aeb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "TIME-LLM reprogrammed retains few-shot learning capabilities in forecasting tasks", "id": "TIME-LLM (REPROGRAMMED)", "label": "TIME-LLM (REPROGRAMMED)", "shape": "dot", "size": 10, "source_id": "6d66c2ea37e25b646a13d751c05a8e4d", "type": "MODEL"}, {"color": "#97c2fc", "description": "arXiv is a database of electronic preprints in fields such as physics, mathematics, computer science, and related disciplines", "id": "ARXIV", "label": "ARXIV", "shape": "dot", "size": 10, "source_id": "a73df99fe49b288e1c8751be2008b191", "type": "DATABASE"}, {"color": "#97c2fc", "description": "Spatio-temporal graphs refer to graphs that represent data that changes over time and space", "id": "SPATIO-TEMPORAL GRAPHS", "label": "SPATIO-TEMPORAL GRAPHS", "shape": "dot", "size": 10, "source_id": "ad8efcfda80253036294f2eeb48926e8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Spatio-temporal rasters refer to rasters that represent data that changes over time and space", "id": "SPATIO-TEMPORAL RASTERS", "label": "SPATIO-TEMPORAL RASTERS", "shape": "dot", "size": 10, "source_id": "ad8efcfda80253036294f2eeb48926e8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "SPATIO-TEMPORAL GRAPH FORECASTING", "label": "SPATIO-TEMPORAL GRAPH FORECASTING", "shape": "dot", "size": 10, "source_id": "859c14b7112010530eddafc204b4b305", "type": ""}, {"color": "#97c2fc", "description": "FengWu is a model mentioned in the text, as indicated by the use of the phrase \"FengWu [13]\"", "id": "FENGWU", "label": "FENGWU", "shape": "dot", "size": 10, "source_id": "859c14b7112010530eddafc204b4b305", "type": "MODEL"}, {"color": "#97c2fc", "description": "W-MAE is a model mentioned in the text, as indicated by the use of the phrase \"W-MAE [65]\"", "id": "W-MAE", "label": "W-MAE", "shape": "dot", "size": 10, "source_id": "859c14b7112010530eddafc204b4b305", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nAuxMobLCast is a model that has been mentioned in the text. It is described as a model that fine-tunes pre-trained Large Language Models (LLMs) through mobility prompting and auxiliary POI Category classification. This suggests that AuxMobLCast is a type of model that leverages the capabilities of pre-trained LLMs and adapts them to a specific task or domain through additional training and fine-tuning.\n\nThe use of mobility prompting and auxiliary POI Category classification indicates that AuxMobLCast is designed to handle tasks related to mobility and location-based information. This could be relevant in applications such as route planning, location-based recommendations, or other tasks that require understanding of spatial relationships and mobility patterns.\n\nOverall, AuxMobLCast appears to be a model that has been developed to address specific challenges in the field of natural language processing and machine learning, particularly in the context of mobility and location-based information.\n\nNote: The reference number [103] mentioned in the description is likely a citation or a reference to a specific academic paper or research study that discusses the AuxMobLCast model in more detail. However, without further information, it is not possible to provide a more specific summary of the paper or its findings.", "id": "AUXMOBLCAST", "label": "AUXMOBLCAST", "shape": "dot", "size": 10, "source_id": "859c14b7112010530eddafc204b4b305,ef8fd6170b08af3bd99c9df44ffa8b57", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe Transformer Architecture is a type of neural network model that utilizes self-attention mechanisms to process sequential data. This architecture is a type of deep learning architecture that is widely used in many popular language models. Specifically, the decoder-only Transformer Architecture is a type of model used in Lag-Llama. The Transformer Architecture is designed to handle sequential data, making it a versatile tool for various applications in natural language processing and other fields.\n\nThe summary is based on the following information collected from the descriptions:\n\n* The Transformer Architecture uses self-attention mechanisms to process sequential data.\n* The decoder-only Transformer Architecture is used in Lag-Llama.\n* The Transformer Architecture is a type of deep learning architecture used in many popular language models.\n* The Transformer Architecture is designed to handle sequential data.\n\nThere are no contradictions in the provided descriptions, and the summary is a coherent and concise representation of the information.", "id": "TRANSFORMER ARCHITECTURE", "label": "TRANSFORMER ARCHITECTURE", "shape": "dot", "size": 10, "source_id": "7dbc961fe1959f844ebac283498e7fb0,7e69d9444a2084b6452e291735bf5a49,83b49bf68dab6c36bdc09f02a63803fe,c7167cf92abe7513ccb936ca79871932", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "ATTENTION FUNCTION", "label": "ATTENTION FUNCTION", "shape": "dot", "size": 10, "source_id": "7dbc961fe1959f844ebac283498e7fb0", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"DATA\" can be generated as follows:\n\nThe entity \"DATA\" refers to the information and values presented in the text, which serves as the input or output of a machine learning model. Specifically, in the context of a transformer model, data refers to the input used to compute the anomaly score. This suggests that data plays a crucial role in machine learning model development, particularly in transformer models, where it is used to generate predictions or scores.\n\nIn the context of time series analysis and forecasting, data is likely to refer to the information and values presented in the text, which can be used to train and evaluate machine learning models. The use of technical terms such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis\" in the text suggests that data is being analyzed and modeled to make predictions about future trends and patterns.\n\nOverall, the entity \"DATA\" is a critical component of machine learning model development, particularly in transformer models, and is used to generate predictions, scores, and insights about future trends and patterns.\n\nRelevant information from the nearby text that has been incorporated into this summary includes:\n\n* The use of technical terms such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis\" in the text.\n* The presence of mathematical equations and formulas, which are written in a standard mathematical notation used in English-language academic papers.\n* The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals.\n* The citation of English-language academic papers and authors, which suggests that the text is written in English.\n\nThis summary provides a comprehensive and coherent description of the entity \"DATA\" based on the provided information, and incorporates relevant information from the nearby text to enrich the description.", "id": "DATA", "label": "DATA", "shape": "dot", "size": 10, "source_id": "1b51ec337efd822ca3a0b3eb819c1b91,7dbc961fe1959f844ebac283498e7fb0,a875a1c0bede47a1c4e3823be81c42c6,deb67d5386710136cf24bcbf135a66c4", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Queries are the input elements that are used to compute the attention weights in the attention function.", "id": "QUERIES", "label": "QUERIES", "shape": "dot", "size": 10, "source_id": "7dbc961fe1959f844ebac283498e7fb0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Sensors are used to collect data from various sources, including traffic and pedestrian counts", "id": "SENSORS", "label": "SENSORS", "shape": "dot", "size": 10, "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "type": "DEVICE"}, {"color": "#97c2fc", "description": "Pedestrians are counted between 2009 and 2020", "id": "PEDESTRIANS", "label": "PEDESTRIANS", "shape": "dot", "size": 10, "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "type": "ENTITY"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"VALUES\" can be generated as follows:\n\nThe entity \"VALUES\" refers to the input elements used to compute attention weights in the attention function, which are typically numerical or quantitative data points or observations in a time series. These values represent specific numbers and measurements presented in the text, serving as the data points or observations in a time series. In essence, values are the numerical or quantitative data being represented, which are used to compute attention weights in the attention function.\n\nThis summary is written in third person and includes information collected from all the descriptions, resolving any potential contradictions. The relevant information from the nearby text has been enriched to provide a comprehensive understanding of the entity \"VALUES\".", "id": "VALUES", "label": "VALUES", "shape": "dot", "size": 10, "source_id": "6fa5f635ad5f7e6b67d5e467f130345c,7dbc961fe1959f844ebac283498e7fb0,deb67d5386710136cf24bcbf135a66c4,f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Reconstruction refers to the process of generating a predicted value based on the input data", "id": "RECONSTRUCTION", "label": "RECONSTRUCTION", "shape": "dot", "size": 10, "source_id": "1b51ec337efd822ca3a0b3eb819c1b91", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"KEYS\" can be described as follows:\n\nThe entity \"KEYS\" refers to the input elements used in the attention function to compute attention weights. Specifically, KEYS are the input used by the window encoder for attention operations. In the context of attention mechanisms, KEYS play a crucial role in determining the importance of different input elements, allowing the model to focus on the most relevant information.\n\nThis description is based on the information provided in the description list, which suggests that KEYS are used in the attention function and are related to the window encoder. The use of KEYS in attention mechanisms is a common technique in natural language processing and machine learning, where the goal is to selectively focus on the most relevant information in the input data.\n\nIn the context of the provided text, it is likely that KEYS are used in a time series forecasting or long-term series forecasting model, where the attention mechanism is used to selectively focus on the most relevant historical data points. The use of KEYS in this context would allow the model to learn the relationships between different time series data points and make more accurate predictions.\n\nOverall, the entity \"KEYS\" is a critical component of the attention mechanism, and its role is to provide the input elements used to compute attention weights.", "id": "KEYS", "label": "KEYS", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e,7dbc961fe1959f844ebac283498e7fb0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"VALUE\" has two distinct descriptions related to it. \n\nThe first description states that \"Value is the input feature used to compute the attention weights.\" This suggests that in the context of a specific model or algorithm, the \"VALUE\" entity serves as an input feature that is used to calculate attention weights. \n\nThe second description indicates that \"Value refers to the output of the window encoder.\" This implies that the \"VALUE\" entity is also associated with the output of a window encoder, which is a component of a larger model or system.\n\nConsidering both descriptions, it appears that the \"VALUE\" entity has a dual role: it is both an input feature used for attention weight computation and the output of a window encoder. This suggests that the \"VALUE\" entity is a critical component in a larger model or system, possibly related to attention-based mechanisms or encoder-decoder architectures.\n\nGiven the technical nature of the descriptions, it is likely that the \"VALUE\" entity is related to a machine learning or deep learning model, possibly in the context of natural language processing or sequence modeling. However, without further context or information, it is difficult to provide a more specific interpretation.\n\nIn summary, the \"VALUE\" entity has two distinct descriptions, indicating its role as both an input feature and the output of a window encoder. This suggests a complex and multifaceted relationship between the \"VALUE\" entity and the larger model or system in which it is embedded.", "id": "VALUE", "label": "VALUE", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e,a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Query matrix refers to the encoded input window used by the window encoder for attention operations", "id": "QUERY MATRIX", "label": "QUERY MATRIX", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"SOFTMAX\" can be generated as follows:\n\nThe entity \"SOFTMAX\" is a concept mentioned in the text, which is related to the normalization of attention weights and the output of the attention mechanism. Specifically, it is a function used to normalize the attention weights, as well as the output of the attention mechanism. This function is crucial in various applications, including long-term series forecasting, frequency analysis, and multi-head cross-attention.\n\nIn the context of the text, SOFTMAX is used in the mathematical equation \"SOFTMAX(Q(i)Kkk(i)\u22a4)\", which suggests its application in a specific model or algorithm. The use of SOFTMAX in this equation implies its role in normalizing the attention weights, which is essential for accurate predictions and analysis in time series forecasting and other related tasks.\n\nOverall, the entity \"SOFTMAX\" is a fundamental concept in the text, and its application is critical in various aspects of time series analysis and forecasting.\n\nRelevant information from the nearby text includes:\n\n* The text is written in English, which is a formal and academic language.\n* The text contains technical terms, mathematical equations, and references to academic papers, which are all written in English.\n* The language used is consistent with English-language academic papers and conferences, such as ICLR, AAAI, and PMLR.\n\nThis information provides context to the entity \"SOFTMAX\" and highlights its importance in the field of time series analysis and forecasting.", "id": "SOFTMAX", "label": "SOFTMAX", "shape": "dot", "size": 10, "source_id": "509b431231e669be373f593b31412eed,7dbc961fe1959f844ebac283498e7fb0,a7604286fb84b26c53950861f9aca4b1,f2e78b25a535b82e2743a0ea4052eb9a", "type": "FUNCTION"}, {"color": "#97c2fc", "description": "", "id": "ERROR BARS", "label": "ERROR BARS", "shape": "dot", "size": 10, "source_id": "6fa5f635ad5f7e6b67d5e467f130345c", "type": ""}, {"color": "#97c2fc", "description": "Measurements refer to the quantitative data presented in the text", "id": "MEASUREMENTS", "label": "MEASUREMENTS", "shape": "dot", "size": 10, "source_id": "deb67d5386710136cf24bcbf135a66c4", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The scaling factor is a parameter used to moderate the magnitude of the dot products in the attention function.", "id": "SCALING FACTOR", "label": "SCALING FACTOR", "shape": "dot", "size": 10, "source_id": "7dbc961fe1959f844ebac283498e7fb0", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Attention head is a component of the attention mechanism", "id": "ATTENTION HEAD", "label": "ATTENTION HEAD", "shape": "dot", "size": 10, "source_id": "a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Attention is a concept mentioned in the text, as indicated by the use of the phrase \"ATTENTION(Qk \u2208 RP \u00d7d, Kk(i), Vk(i))\"", "id": "ATTENTION", "label": "ATTENTION", "shape": "dot", "size": 10, "source_id": "509b431231e669be373f593b31412eed", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"VIDEO\" can be described as follows:\n\nThe entity \"VIDEO\" refers to a field of study that deals with the interaction between computers and visual data. Specifically, it involves the use of computer vision and machine learning algorithms to interpret and understand visual data. This field of study encompasses various aspects, including the analysis and processing of visual information, the development of algorithms to recognize and classify visual patterns, and the application of machine learning techniques to improve the accuracy and efficiency of visual data interpretation.\n\nIn the context of computer science and artificial intelligence, video is a multidisciplinary field that combines concepts from computer vision, machine learning, and data analysis to enable computers to understand and interact with visual data. This field has numerous applications in areas such as image and video processing, object recognition, facial recognition, and surveillance systems.\n\nOverall, the entity \"VIDEO\" represents a dynamic and rapidly evolving field that continues to advance our understanding of visual data and its potential applications in various domains.", "id": "VIDEO", "label": "VIDEO", "shape": "dot", "size": 10, "source_id": "7dbc961fe1959f844ebac283498e7fb0,b9d22fe9f2eecb1665f4bea365c7d612", "type": "FIELD"}, {"color": "#97c2fc", "description": "", "id": "LANGUAGE MODEL WEIGHTS", "label": "LANGUAGE MODEL WEIGHTS", "shape": "dot", "size": 10, "source_id": "c7f04fa1168df64cce110e20a1b72b51", "type": ""}, {"color": "#97c2fc", "description": "Prior deep learning approaches are outperformed by Lag-Llama when fine-tuned on relatively small fractions of previously unseen datasets", "id": "DEEP LEARNING APPROACHES", "label": "DEEP LEARNING APPROACHES", "shape": "dot", "size": 10, "source_id": "c7167cf92abe7513ccb936ca79871932", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"STATISTICAL MODELS\" refers to a type of model used in time series forecasting. These models utilize statistical techniques to make predictions, as indicated by the use of phrases such as \"ARIMA\" and \"ETS.\" Statistical models are a crucial component in the field of time series forecasting, enabling researchers and practitioners to analyze and predict future trends and patterns in data.\n\nIn the context of time series forecasting, statistical models are employed to identify relationships between variables, account for seasonality and trends, and make accurate predictions. The use of phrases such as \"ARIMA\" and \"ETS\" suggests that these models are specifically designed for time series analysis, which involves the examination of data that is collected over time.\n\nOverall, the entity \"STATISTICAL MODELS\" plays a vital role in time series forecasting, and its applications are widespread in various fields, including finance, economics, and engineering.", "id": "STATISTICAL MODELS", "label": "STATISTICAL MODELS", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0,51ee17c1f0212d4c94010d8e376f649b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "General-purpose forecasting algorithm is a concept mentioned in the text, as indicated by the use of the phrase \"general-purpose forecasting algorithm\"", "id": "GENERAL-PURPOSE FORECASTING ALGORITHM", "label": "GENERAL-PURPOSE FORECASTING ALGORITHM", "shape": "dot", "size": 10, "source_id": "d08a82328191907abfcdb72efab9a6cf", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Jin et al. (2023a) is a paper that discusses time series forecasting", "id": "JIN ET AL. (2023A)", "label": "JIN ET AL. (2023A)", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PAPER"}, {"color": "#97c2fc", "description": "Leonard (2001) is a paper that discusses demand planning", "id": "LEONARD (2001)", "label": "LEONARD (2001)", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PAPER"}, {"color": "#97c2fc", "description": "Li et al. (2022) is a paper that discusses inventory optimization", "id": "LI ET AL. (2022)", "label": "LI ET AL. (2022)", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PAPER"}, {"color": "#97c2fc", "description": "Liu et al. (2023a) is a paper that discusses energy load forecasting", "id": "LIU ET AL. (2023A)", "label": "LIU ET AL. (2023A)", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PAPER"}, {"color": "#97c2fc", "description": "Schneider \u0026 Dickinson (1974) is a paper that discusses climate modeling", "id": "SCHNEIDER \u0026 DICKINSON (1974)", "label": "SCHNEIDER \u0026 DICKINSON (1974)", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PAPER"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"MULTIMODAL KNOWLEDGE\" refers to a diverse knowledge base that encompasses various modalities, including vision, speech, and text. This type of knowledge is gained by Large Language Models (LLMs), which are capable of processing and integrating information from multiple sources and modalities. The multimodal knowledge is characterized by its ability to capture and represent complex relationships between different types of data, such as visual, auditory, and textual information.\n\nThe descriptions provided suggest that multimodal knowledge is a key aspect of LLMs, enabling them to gain a more comprehensive understanding of the world and make more informed decisions. The entity \"MULTIMODAL KNOWLEDGE\" is therefore closely related to the capabilities and limitations of LLMs, and its development and application have significant implications for various fields, including natural language processing, computer vision, and human-computer interaction.\n\nOverall, the summary highlights the importance of multimodal knowledge in the context of LLMs and its potential to revolutionize the way we interact with and understand complex data.", "id": "MULTIMODAL KNOWLEDGE", "label": "MULTIMODAL KNOWLEDGE", "shape": "dot", "size": 10, "source_id": "89b79391630ac478085efea89fad5736,b67d18d306fde251ee94b0a831d1e075", "type": "PROPERTY"}, {"color": "#97c2fc", "description": "Time series forecasts refer to the output of the language model used in the TIME-LLM framework", "id": "TIME SERIES FORECASTS", "label": "TIME SERIES FORECASTS", "shape": "dot", "size": 10, "source_id": "89b79391630ac478085efea89fad5736", "type": "OUTPUT"}, {"color": "#97c2fc", "description": "A sequence of historical observations is a set of data points that are used to train a model", "id": "SEQUENCE OF HISTORICAL OBSERVATIONS", "label": "SEQUENCE OF HISTORICAL OBSERVATIONS", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a", "type": "DATA"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"PROMPT-AS-PREFIX\" can be generated as follows:\n\nPROMPT-AS-PREFIX, also referred to as Prompt-as-Prefix (PaP), is a novel technique introduced in the context of computer vision. This approach enriches the input time series with additional context and provides task instructions in natural language, enabling the transformation of reprogrammed input patches. Specifically, PROMPT-AS-PREFIX is used to generate high-precision numerals with precision and efficiency in computer vision applications. Furthermore, it is applied in the context of Large Language Models (LLMs) for effective time series forecasting. Overall, PROMPT-AS-PREFIX is a technique that combines the benefits of natural language processing and computer vision to achieve accurate and efficient results.\n\nNote that the contradictions in the descriptions have been resolved by focusing on the common themes and applications of PROMPT-AS-PREFIX, which include computer vision, time series forecasting, and the use of natural language processing. The summary is written in third person and includes the entity name for context. Relevant information from the nearby text has been incorporated to provide a comprehensive understanding of the entity.", "id": "PROMPT-AS-PREFIX", "label": "PROMPT-AS-PREFIX", "shape": "dot", "size": 10, "source_id": "072b166b9a1b6afecf5874f45af61699,2bb4fc2b46b9c8bdd052b2755d986aa8,3e937ba8de0e7eca993c50506ceb8f1f,89b79391630ac478085efea89fad5736,8f4724ff6541b8924f0cebe9872ed040,e6a6bc6fbd362394320961ac10cdd230", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe TIME-LLM FRAMEWORK is a framework specifically designed for adapting frozen large language models (LLMs) for time series forecasting. This framework is a particular approach or architecture that utilizes LLMs for time series forecasting, indicating a tailored method for leveraging the capabilities of LLMs in this domain.\n\nThe TIME-LLM FRAMEWORK is characterized by its ability to adapt frozen LLMs for time series forecasting, suggesting that it involves modifying or fine-tuning existing LLMs to better suit the requirements of time series forecasting tasks. This adaptation is likely aimed at improving the performance and accuracy of LLMs in predicting future values in time series data.\n\nOverall, the TIME-LLM FRAMEWORK represents a specialized approach to time series forecasting that leverages the strengths of LLMs, making it a valuable tool for researchers and practitioners working in this area.", "id": "TIME-LLM FRAMEWORK", "label": "TIME-LLM FRAMEWORK", "shape": "dot", "size": 10, "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8,ec3fbfb800fd9bf1d913584fda4ae925", "type": "MODEL"}, {"color": "#97c2fc", "description": "Language task refers to a specific problem or task that can be solved using language models, such as text classification or machine translation", "id": "LANGUAGE TASK", "label": "LANGUAGE TASK", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Short-term time series forecasting is a concept mentioned in the text, as indicated by the use of the phrase \"short-term time series forecasting results\"", "id": "SHORT-TERM TIME SERIES FORECASTING", "label": "SHORT-TERM TIME SERIES FORECASTING", "shape": "dot", "size": 10, "source_id": "f49330b6fd81d86d14e7a9d4b8e45576", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "MASE values are data points used to evaluate the performance of time series forecasting models", "id": "MASE VALUES", "label": "MASE VALUES", "shape": "dot", "size": 10, "source_id": "d540ee970ce7debedc6b1b95e9c88be8", "type": "DATA"}, {"color": "#97c2fc", "description": "OWA values are data points used to evaluate the performance of time series forecasting models", "id": "OWA VALUES", "label": "OWA VALUES", "shape": "dot", "size": 10, "source_id": "d540ee970ce7debedc6b1b95e9c88be8", "type": "DATA"}, {"color": "#97c2fc", "description": "SMAPE values are data points used to evaluate the performance of time series forecasting models", "id": "SMAPE VALUES", "label": "SMAPE VALUES", "shape": "dot", "size": 10, "source_id": "d540ee970ce7debedc6b1b95e9c88be8", "type": "DATA"}, {"color": "#97c2fc", "description": "Reprogramming framework is a model mentioned in the text, as indicated by the use of the phrase \"our reprogramming framework\"", "id": "REPROGRAMMING FRAMEWORK", "label": "REPROGRAMMING FRAMEWORK", "shape": "dot", "size": 10, "source_id": "7e97089185883c456c798ddc5ec86373", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe primary entity of interest is \"TIME SERIES FORECASTING MODELS.\" These models are a type of neural network model specifically designed for time series forecasting. They are a crucial aspect of time series analysis, which involves the study of patterns and trends in data that is collected over time.\n\nThe language used to describe these models is formal and academic, suggesting that the text is from a research paper or a technical document. The language is primarily English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers.\n\nThe descriptions provided do not contain any contradictory information, and the summary is therefore a straightforward representation of the entity and its characteristics.", "id": "TIME SERIES FORECASTING MODELS", "label": "TIME SERIES FORECASTING MODELS", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831,7dbc961fe1959f844ebac283498e7fb0", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"PATCHES\" can be generated as follows:\n\nThe entity \"PATCHES\" refers to a technique used in time series analysis to segment time series data into smaller, manageable chunks. In the context of time series forecasting models, patches are a type of segment used to encapsulate local dynamics within input tokens, allowing for more accurate and effective forecasting. Specifically, patches are instances or examples of a patch, which can be used to represent specific segments of time series data. This technique is commonly employed in time series analysis and forecasting models to improve their performance and accuracy.\n\nThe use of patches in time series analysis and forecasting is supported by various mathematical equations and formulas, which are used to segment and analyze time series data. Additionally, the concept of patches is often discussed in academic papers and conferences, such as ICLR, AAAI, and PMLR, which suggests that it is a widely recognized and researched topic in the field of time series analysis and forecasting.\n\nOverall, the entity \"PATCHES\" is a key concept in time series analysis and forecasting, and its use has been extensively researched and applied in various models and techniques.", "id": "PATCHES", "label": "PATCHES", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831,7dbc961fe1959f844ebac283498e7fb0,8f4724ff6541b8924f0cebe9872ed040", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"NORMALIZATION LAYER\" is a crucial component in neural networks and time series forecasting models. It is designed to normalize the input data, ensuring that all values are standardized and on the same scale. This is achieved through reversible instance normalization techniques, which are critical in time series forecasting models. The normalization layer plays a vital role in preparing the data for further processing and analysis, enabling accurate predictions and forecasts.\n\nIn the context of time series forecasting, the normalization layer is essential for standardizing data, which is a critical step in preparing the data for modeling. By normalizing the data, the model can better capture patterns and trends, leading to more accurate predictions. The use of reversible instance normalization techniques in the normalization layer further enhances the model\u0027s ability to handle complex time series data.\n\nOverall, the \"NORMALIZATION LAYER\" is a critical component in neural networks and time series forecasting models, and its role in normalizing input data is essential for accurate predictions and forecasts.", "id": "NORMALIZATION LAYER", "label": "NORMALIZATION LAYER", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831,7dbc961fe1959f844ebac283498e7fb0", "type": "LAYER"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe INPUT TIME SERIES is a sequence of data points that are used to make predictions. Specifically, it refers to the sequence of data points measured at regular time intervals. This sequence of data points is used as input for various applications, including long-term series forecasting, frequency analysis, and other predictive models. The INPUT TIME SERIES can be analyzed using techniques such as multi-head cross-attention, and its predictions can be evaluated using various metrics and benchmarks.\n\nThe INPUT TIME SERIES is a fundamental concept in time series analysis and forecasting, and its accurate representation and analysis are crucial for making informed predictions and decisions. The use of INPUT TIME SERIES has been extensively studied in academic papers and conferences, including those published in top-tier journals such as PMLR and conferences like ICLR and AAAI.\n\nOverall, the INPUT TIME SERIES is a critical component of time series analysis and forecasting, and its accurate representation and analysis are essential for making informed predictions and decisions in various fields.", "id": "INPUT TIME SERIES", "label": "INPUT TIME SERIES", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a,8f4724ff6541b8924f0cebe9872ed040", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nINSTANCE NORMALIZATION is a technique used in neural networks to normalize the input data. It is a method employed in the normalization layer to standardize data through instance-specific mean and variance. This technique is particularly useful in computer vision and natural language processing applications, where it is used to normalize the activations of a neural network layer. INSTANCE NORMALIZATION is a crucial component in various deep learning models, enabling them to learn more robust and generalizable representations of the input data.\n\nThe use of INSTANCE NORMALIZATION has been explored in various research papers and academic conferences, such as ICLR, AAAI, and PMLR. It has been shown to improve the performance of neural networks in tasks such as image classification, object detection, and language modeling. INSTANCE NORMALIZATION is often used in conjunction with other normalization techniques, such as batch normalization, to achieve better results.\n\nOverall, INSTANCE NORMALIZATION is a powerful technique in the field of deep learning, enabling neural networks to learn more effective representations of the input data and improve their performance on a wide range of tasks.", "id": "INSTANCE NORMALIZATION", "label": "INSTANCE NORMALIZATION", "shape": "dot", "size": 10, "source_id": "5805d8a1afe3d6bc6300ac71f7163831,7dbc961fe1959f844ebac283498e7fb0,83f62beddf5cf53ed2e2d4517569deb8", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"PATCHING\" can be described as follows:\n\nPATCHING refers to a technique used to divide a large dataset into smaller, more manageable pieces, often employed in various fields such as computer vision and natural language processing. Additionally, PATCHING can also be applied to time series analysis, where it involves segmenting a time series into smaller parts to facilitate more efficient and effective analysis. This technique is utilized to simplify complex data and improve the accuracy of predictions and insights.\n\nThe descriptions provided are not contradictory, but rather complementary, highlighting the versatility and applicability of PATCHING in different domains. By combining the information from both descriptions, a comprehensive understanding of PATCHING can be obtained, showcasing its utility in both computer vision and natural language processing, as well as time series analysis.", "id": "PATCHING", "label": "PATCHING", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8,83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data is as follows:\n\nConvolutional Neural Networks (CNNs) are a type of neural network architecture that has been widely used in various applications, including time series forecasting and image/video processing tasks. They are specifically designed to handle image and video data, utilizing their unique architecture to extract relevant features and make predictions. Initially used as the backbone for pre-training in time series forecasting, CNNs have proven to be a versatile tool in the field of machine learning, with their applications extending beyond image and video processing to other areas such as time series analysis.\n\nThis summary combines the information from all the descriptions, resolving any potential contradictions and providing a coherent overview of the entity \"Convolutional Neural Networks\" (CNNs). The summary highlights their primary use in image and video processing tasks, as well as their application in time series forecasting, showcasing their versatility and effectiveness in various machine learning tasks.", "id": "CONVOLUTIONAL NEURAL NETWORKS", "label": "CONVOLUTIONAL NEURAL NETWORKS", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6,5805d8a1afe3d6bc6300ac71f7163831,b9d22fe9f2eecb1665f4bea365c7d612", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"Deep learning forecasting models\" are a type of machine learning approach used for time series forecasting. These models are designed to learn across multiple time series in a given dataset, enabling them to capture complex patterns and relationships within the data. The primary application of deep learning forecasting models is in time series forecasting, where they can be used to predict future values based on historical data.\n\nThe use of deep learning forecasting models is supported by their ability to learn from large datasets and adapt to changing patterns over time. This is particularly useful in applications where traditional forecasting methods may struggle to capture the complexity of the data. The models\u0027 ability to learn across multiple time series also allows them to identify relationships between different variables, which can be used to improve forecasting accuracy.\n\nOverall, deep learning forecasting models are a powerful tool for time series forecasting, offering a flexible and adaptive approach to predicting future values. Their ability to learn from large datasets and capture complex patterns makes them a valuable asset in a wide range of applications, from finance and economics to healthcare and climate modeling.\n\nRelevant information from the nearby text:\n\n* The text mentions that the language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n* The text also mentions the use of technical terms, mathematical equations, and references to academic papers, which are all written in English.\n* The presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports the conclusion that the text is written in English.\n\nNote: The summary is based on the provided information and does not include any external knowledge or assumptions.", "id": "DEEP LEARNING FORECASTING MODELS", "label": "DEEP LEARNING FORECASTING MODELS", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6,7e69d9444a2084b6452e291735bf5a49", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ResNet is a type of neural network architecture used for image classification tasks", "id": "RESNET", "label": "RESNET", "shape": "dot", "size": 10, "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "type": "MODEL"}, {"color": "#97c2fc", "description": "Dilated convolution layers are a type of neural network layer used for image and video processing tasks", "id": "DILATED CONVOLUTION LAYERS", "label": "DILATED CONVOLUTION LAYERS", "shape": "dot", "size": 10, "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "type": "MODEL"}, {"color": "#97c2fc", "description": "Computer Vision is a field of study that deals with the use of computer vision and machine learning algorithms to interpret and understand visual data", "id": "CV", "label": "CV", "shape": "dot", "size": 10, "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "type": "FIELD"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TRAFFIC FORECASTING\" is a field of study that deals with the use of machine learning algorithms to predict traffic patterns and optimize traffic flow. This process, also referred to as traffic forecasting, involves predicting traffic patterns and volumes in a given area. The primary language of the text related to this entity is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nThe language used in the text is formal and academic, suggesting that the text is from a research paper or a technical document. The presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports the conclusion that the primary language of the text is English.\n\nIn the context of traffic forecasting, machine learning algorithms play a crucial role in predicting traffic patterns and optimizing traffic flow. The process involves analyzing various factors, including traffic volume, speed, and other relevant data to make accurate predictions. By leveraging machine learning techniques, traffic forecasting can help optimize traffic flow, reduce congestion, and improve overall traffic management.\n\nOverall, the entity \"TRAFFIC FORECASTING\" is a critical area of study that involves the use of machine learning algorithms to predict traffic patterns and optimize traffic flow, with the primary language of the related text being English.", "id": "TRAFFIC FORECASTING", "label": "TRAFFIC FORECASTING", "shape": "dot", "size": 10, "source_id": "9125a7d3794226f109debe90e5db041c,b9d22fe9f2eecb1665f4bea365c7d612", "type": "FIELD"}, {"color": "#97c2fc", "description": "Temporal dynamics refer to the patterns and relationships between data points in a time series over time", "id": "TEMPORAL DYNAMICS", "label": "TEMPORAL DYNAMICS", "shape": "dot", "size": 10, "source_id": "9125a7d3794226f109debe90e5db041c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Self-supervision signals are used in contrastive learning to generate informative positive pairs and filter out unsuitable negative pairs", "id": "SELF-SUPERVISION SIGNALS", "label": "SELF-SUPERVISION SIGNALS", "shape": "dot", "size": 10, "source_id": "de8e097ce66fbc954bd529888cbc15ea", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Lei Zhang is an author mentioned in the text, as indicated by the citation of their paper", "id": "LEI ZHANG", "label": "LEI ZHANG", "shape": "dot", "size": 10, "source_id": "98c6b5003112ab7110e45414a2fa468b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Spatial-temporal encoders are used in time series forecasting to form the latent representation space", "id": "SPATIAL-TEMPORAL ENCODERS", "label": "SPATIAL-TEMPORAL ENCODERS", "shape": "dot", "size": 10, "source_id": "de8e097ce66fbc954bd529888cbc15ea", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"PROBABILISTIC MODELING\" is a method used in time series forecasting, where the primary goal is to model the uncertainty and variability in a time series. This is achieved by optimizing the latent representation space toward an estimated density. In essence, probabilistic modeling is a process that aims to capture the underlying patterns and trends in a time series, while also accounting for the inherent uncertainty and randomness present in the data.\n\nThis summary is derived from the two descriptions provided, which are not contradictory but rather complementary. The first description highlights the specific application of probabilistic modeling in time series forecasting, while the second description provides a more general definition of the process. By combining these two descriptions, a more comprehensive understanding of probabilistic modeling can be obtained.\n\nIt is worth noting that the context of probabilistic modeling is time series forecasting, which is a critical aspect of many fields, including finance, economics, and climate science. The ability to accurately forecast time series data is essential for making informed decisions and predicting future outcomes.\n\nIn terms of relevant information from the nearby text, it is mentioned that the primary language of the text is English, which is consistent with the formal and academic tone of the descriptions provided. The use of technical terms, mathematical equations, and references to academic papers also suggests that the text is from a research paper or technical document, further supporting the notion that probabilistic modeling is a complex and technical topic.", "id": "PROBABILISTIC MODELING", "label": "PROBABILISTIC MODELING", "shape": "dot", "size": 10, "source_id": "9125a7d3794226f109debe90e5db041c,de8e097ce66fbc954bd529888cbc15ea", "type": "PHASE"}, {"color": "#97c2fc", "description": "Log-likelihood is a measure of the probability of a model given the data", "id": "LOG-LIKELIHOOD", "label": "LOG-LIKELIHOOD", "shape": "dot", "size": 10, "source_id": "9125a7d3794226f109debe90e5db041c", "type": "PHASE"}, {"color": "#97c2fc", "description": "Temporal encoders are used in time series forecasting to form the latent representation space", "id": "TEMPORAL ENCODERS", "label": "TEMPORAL ENCODERS", "shape": "dot", "size": 10, "source_id": "de8e097ce66fbc954bd529888cbc15ea", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time series forecasting models (TSFM) are pre-trained models that can be fine-tuned for specific time series tasks", "id": "TSFM", "label": "TSFM", "shape": "dot", "size": 10, "source_id": "de8e097ce66fbc954bd529888cbc15ea", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"TIME SERIES TOKENIZATION\" refers to a method of adapting Time Series Forecasting Models (TSFM) to specific tasks or datasets. This process involves representing time series data as embeddings, which is a crucial step in various time series analysis and forecasting applications. By tokenizing time series data, researchers and practitioners can effectively adapt TSFM to suit the needs of specific datasets, leading to improved forecasting accuracy and performance. This technique has significant implications for various fields, including finance, economics, and climate science, where accurate time series forecasting is critical for decision-making and prediction.", "id": "TIME SERIES TOKENIZATION", "label": "TIME SERIES TOKENIZATION", "shape": "dot", "size": 10, "source_id": "53f80422fbf85856ecf940d5c2450665,de8e097ce66fbc954bd529888cbc15ea", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Climate forecasting refers to the prediction of future weather patterns", "id": "CLIMATE FORECASTING", "label": "CLIMATE FORECASTING", "shape": "dot", "size": 10, "source_id": "53f80422fbf85856ecf940d5c2450665", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"NN5\" is a dataset used to evaluate the performance of the Chronos model. Specifically, it contains cash withdrawal data from ATMs, which is a key aspect of the dataset. This information suggests that NN5 is a financial dataset, likely related to banking or transactional data.\n\nGiven the context of the Chronos model, it is likely that NN5 is used for time series forecasting or analysis, as Chronos is a model designed for long-term series forecasting. The presence of cash withdrawal data from ATMs implies that the dataset may be used for frequency analysis or other types of time series analysis.\n\nOverall, NN5 appears to be a dataset used for evaluating the performance of the Chronos model in the context of time series forecasting and analysis, specifically with regards to cash withdrawal data from ATMs.", "id": "NN5", "label": "NN5", "shape": "dot", "size": 10, "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7,d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"UBER TLC\" entity refers to a dataset, a function, and a company/service. Specifically, \"UBER TLC\" is a dataset that contains the number of Uber pick-ups from various locations. Additionally, it is also used as a function delay for fine-tuning forecasting tasks. Furthermore, \"UBER TLC\" can be interpreted as referring to a specific company or service, likely related to the ride-hailing industry, given the context of Uber pick-ups.\n\nIt is worth noting that the term \"TLC\" is often used as an abbreviation for \"Taxi and Limousine Commission,\" which is a regulatory agency in New York City that oversees the taxi and for-hire vehicle industries. However, without further context, it is unclear whether this is the specific meaning intended by the \"UBER TLC\" entity.\n\nOverall, the \"UBER TLC\" entity appears to be a multifaceted concept that encompasses a dataset, a function, and a company/service, with potential connections to the ride-hailing industry and regulatory agencies.", "id": "UBER TLC", "label": "UBER TLC", "shape": "dot", "size": 10, "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26,a875a1c0bede47a1c4e3823be81c42c6,ec7705b83cf4fe3aa18662c917b18c1a", "type": "ENTITY"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"UBER TLC HOURLY\" dataset is a collection of data used for pretraining and fine-tuning purposes. This dataset is specifically designed for applications related to time series analysis and forecasting, particularly in the context of long-term series forecasting. The data is likely to be useful for researchers and practitioners working on frequency analysis, multi-head cross-attention, and other advanced techniques in time series forecasting.\n\nGiven the context of the dataset, it is reasonable to infer that the data is related to the transportation industry, specifically the ride-hailing service provided by Uber. The \"TLC\" abbreviation likely refers to the Taxi and Limousine Commission, a regulatory body in New York City that oversees the taxi industry. Therefore, the \"UBER TLC HOURLY\" dataset may contain hourly data related to Uber\u0027s operations in New York City, including metrics such as demand, supply, and revenue.\n\nOverall, the \"UBER TLC HOURLY\" dataset appears to be a valuable resource for researchers and practitioners working on time series forecasting and analysis, particularly in the context of the transportation industry.", "id": "UBER TLC HOURLY", "label": "UBER TLC HOURLY", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"FUNCTION DELAY\" refers to a delay or slowdown in a specific function or process. This concept is also related to the time taken by a model to perform a function, indicating that function delay is a measure of the efficiency or speed of a model\u0027s performance. \n\nIn the context of machine learning and time series forecasting, function delay may be an important factor to consider when evaluating the performance of a model. It could be used to identify areas where the model is taking too long to process data or make predictions, and to optimize the model\u0027s performance accordingly.\n\nOverall, function delay is a critical aspect of understanding the behavior and performance of a model, and it has significant implications for the development and deployment of machine learning models in various applications.", "id": "FUNCTION DELAY", "label": "FUNCTION DELAY", "shape": "dot", "size": 10, "source_id": "990578022879395d00b7a5b229863c2f,ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Electricity weather CPU usage refers to the measure of the impact of weather on electricity usage and CPU performance", "id": "ELECTRICITY WEATHER CPU USAGE", "label": "ELECTRICITY WEATHER CPU USAGE", "shape": "dot", "size": 10, "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"ECL\" is a model used for forecasting, as indicated by the presence of numerical values and performance metrics in the text. It is also mentioned as a concept in the text, as denoted by the use of the abbreviation \"ECL\". The model is likely used for long-term series forecasting, given the context of time series analysis and forecasting in the text. The use of technical terms such as \"frequency analysis\" and \"multi-head cross-attention\" suggests that the model may be a complex, advanced forecasting model. Overall, \"ECL\" appears to be a sophisticated forecasting model used for predicting future values in a time series.\n\nThis summary is based on the information provided in the description list, which includes the following key points:\n\n* \"ECL is a concept mentioned in the text, as indicated by the use of the abbreviation \\\"ECL\\\"\"\n* \"ECL is a model used for forecasting\"\n* \"ECL is a model, as indicated by the presence of numerical values and performance metrics\"\n\nThese points are consistent with each other and provide a clear understanding of the entity \"ECL\". The summary also takes into account the context of time series analysis and forecasting in the text, as well as the use of technical terms such as \"frequency analysis\" and \"multi-head cross-attention\".", "id": "ECL", "label": "ECL", "shape": "dot", "size": 10, "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,91e161ba596a0cbbcae541ddb2106310,bb03c20a6fc1d9af2f3cbfa55b50cfb8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"1STCOUNT\" is a concept mentioned in the text, as indicated by the use of the name \"1stCount\". It refers to the count of a particular value, which is a fundamental aspect of time series analysis and forecasting. The count is likely used to identify patterns, trends, and anomalies in the data, which can be crucial for long-term series forecasting and frequency analysis.\n\nGiven the context of time series analysis and forecasting, it is likely that the count of a particular value is used to inform decisions in various fields, such as finance, economics, and engineering. The use of mathematical equations and formulas, as well as references to academic papers and conferences (e.g., ICLR, AAAI, PMLR), suggests that the concept of 1stCount is rooted in academic research and is likely to be used in a technical or research setting.\n\nOverall, the entity \"1STCOUNT\" is a key concept in time series analysis and forecasting, and its use is likely to be found in technical or research contexts where data analysis and pattern recognition are critical.", "id": "1STCOUNT", "label": "1STCOUNT", "shape": "dot", "size": 10, "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,91e161ba596a0cbbcae541ddb2106310", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Traffic flow refers to the rate at which vehicles or pedestrians move through a given area", "id": "TRAFFIC FLOW", "label": "TRAFFIC FLOW", "shape": "dot", "size": 10, "source_id": "2dbfdb45630a023a5a6979b9573a868f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"Reversible instance normalization\" is a technique used for time series tokenization and normalization. It is employed to normalize input data and mitigate the time series distribution shift, which is a common issue in time series forecasting. This technique is particularly useful in the context of long-term series forecasting, where the distribution of the data can change over time. By applying reversible instance normalization, the model can better adapt to these changes and improve its forecasting accuracy.\n\nThe technique is also relevant in the context of frequency analysis, where it can help to identify patterns and trends in the data. Additionally, reversible instance normalization can be used in conjunction with other techniques, such as multi-head cross-attention, to further improve the model\u0027s performance.\n\nOverall, reversible instance normalization is a valuable tool in the field of time series forecasting, and its application can lead to significant improvements in model accuracy and robustness.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which suggests that the information provided is from an academic or technical document.\n* The text also mentions that the language used is formal and academic, which further supports the idea that the information is from a research paper or technical document.\n* The text includes technical terms and mathematical equations, which are commonly used in English-language academic papers.\n* The text also includes references to academic papers and authors, which suggests that the information is from a credible and reliable source.", "id": "REVERSIBLE INSTANCE NORMALIZATION", "label": "REVERSIBLE INSTANCE NORMALIZATION", "shape": "dot", "size": 10, "source_id": "53f80422fbf85856ecf940d5c2450665,fececbac281c1e2b13921f378df30919", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe INPUT TRANSFORMATION is a crucial component of the model that plays a vital role in preparing the input data for use by the model. It refers to the process of transforming input data into a format that can be utilized by the model, enabling it to effectively process and analyze the information.\n\nThis process is essential for ensuring that the input data is in a suitable format for the model to learn from and make accurate predictions or forecasts. The INPUT TRANSFORMATION component is a critical step in the overall model development process, and its effectiveness can significantly impact the model\u0027s performance and accuracy.\n\nIn the context of time series forecasting, long-term series forecasting, and frequency analysis, the INPUT TRANSFORMATION component is particularly important. It enables the model to handle complex data structures and relationships, allowing it to make more accurate predictions and forecasts.\n\nOverall, the INPUT TRANSFORMATION is a vital component of the model that enables it to effectively process and analyze input data, making it a critical step in the model development process.", "id": "INPUT TRANSFORMATION", "label": "INPUT TRANSFORMATION", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a,fececbac281c1e2b13921f378df30919", "type": "COMPONENT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"TIME SERIES DECOMPOSITION\" is a technique used for time series tokenization and refers to the process of breaking down a time series into its component parts. This process is often utilized in various fields, including finance and healthcare, to analyze and understand the underlying patterns and trends within the data. The technique is employed to decompose a time series into its additive components, such as trend, seasonality, and residuals, allowing for a more detailed examination of the data and its behavior over time.\n\nThis summary is based on the provided descriptions, which are consistent and provide a clear understanding of the concept of \"TIME SERIES DECOMPOSITION\". The information collected from the descriptions has been used to create a concise and coherent summary that includes the entity name and relevant details.", "id": "TIME SERIES DECOMPOSITION", "label": "TIME SERIES DECOMPOSITION", "shape": "dot", "size": 10, "source_id": "53f80422fbf85856ecf940d5c2450665,83f62beddf5cf53ed2e2d4517569deb8", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Explainable components refer to the process of identifying and interpreting the underlying factors that contribute to a time series, often used in finance, healthcare, and other fields", "id": "EXPLAINABLE COMPONENTS", "label": "EXPLAINABLE COMPONENTS", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Channel independence strategy refers to a technique used to extract features from a dataset, often used in computer vision and natural language processing", "id": "CHANNEL INDEPENDENCE STRATEGY", "label": "CHANNEL INDEPENDENCE STRATEGY", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Fine-tuning strategies refer to the process of adapting a pre-trained model to a specific task or dataset, often used in natural language processing and computer vision", "id": "FINE-TUNING STRATEGIES", "label": "FINE-TUNING STRATEGIES", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Privacy-preserved manner refers to the process of protecting sensitive information while still allowing for the use of machine learning models, often used in finance, healthcare, and other fields", "id": "PRIVACY-PRESERVED MANNER", "label": "PRIVACY-PRESERVED MANNER", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Multi-modality refers to the use of multiple types of data or inputs to train a machine learning model, often used in finance, healthcare, and other fields", "id": "MULTI-MODALITY", "label": "MULTI-MODALITY", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Task-oriented FMs refer to a type of machine learning model that is designed to perform a specific task or set of tasks, often used in finance, healthcare, and other fields", "id": "TASK-ORIENTED FMS", "label": "TASK-ORIENTED FMS", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "MODEL"}, {"color": "#97c2fc", "description": "External ChatGPT refers to a type of machine learning model that is designed to generate human-like text, often used in natural language processing and computer vision", "id": "EXTERNAL CHATGPT", "label": "EXTERNAL CHATGPT", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Evolving graph structure refers to a type of graph that changes over time, often used in finance, healthcare, and other fields", "id": "EVOLVING GRAPH STRUCTURE", "label": "EVOLVING GRAPH STRUCTURE", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "News headlines refer to the titles or summaries of news articles, often used in finance, healthcare, and other fields", "id": "NEWS HEADLINES", "label": "NEWS HEADLINES", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Stock prices refer to the prices at which stocks are traded on a stock exchange, often used in finance and other fields", "id": "STOCK PRICES", "label": "STOCK PRICES", "shape": "dot", "size": 10, "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Optimal reprogramming representations refer to the best way to represent or encode the knowledge or data needed for reprogramming", "id": "OPTIMAL REPROGRAMMING REPRESENTATIONS", "label": "OPTIMAL REPROGRAMMING REPRESENTATIONS", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TIME-SERIES FORECASTING\" is a process that involves predicting future values in a time-series dataset based on past trends and patterns. This process is a crucial aspect of data analysis and is widely used in various fields, including finance, economics, and engineering.\n\nTime-series forecasting is a complex task that requires the use of advanced statistical and machine learning techniques to identify patterns and trends in the data. The goal of time-series forecasting is to make accurate predictions about future values, which can be used to inform business decisions, optimize resource allocation, and mitigate risks.\n\nThe process of time-series forecasting typically involves several steps, including data preprocessing, feature engineering, model selection, and model evaluation. Data preprocessing involves cleaning and transforming the data to prepare it for analysis, while feature engineering involves selecting and creating relevant features that can be used to train the model. Model selection involves choosing the most suitable algorithm or technique for the task at hand, and model evaluation involves assessing the performance of the model using metrics such as mean absolute error (MAE) and mean squared error (MSE).\n\nIn recent years, there has been a significant increase in the use of deep learning techniques, such as recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, for time-series forecasting. These techniques have been shown to be highly effective in capturing complex patterns and trends in time-series data.\n\nOverall, time-series forecasting is a critical component of data analysis and is widely used in various fields to make informed decisions and optimize resource allocation.\n\nRelevant information from the nearby text:\n\n* The text mentions that the language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n* The text also mentions the use of technical terms, mathematical equations, and references to academic papers, which are all written in English.\n* The text cites English-language academic papers and authors, which suggests that the text is written in English.\n\nNote: The description list was empty, so I used the provided information to create a comprehensive summary of the entity \"TIME-SERIES FORECASTING\".", "id": "TIME-SERIES FORECASTING", "label": "TIME-SERIES FORECASTING", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491,dd9bcf836b1de9b8c99d5ba8188a5ed4", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe TIME-SERIES CORPUS is a collection of time-series data used for training and testing models. Specifically, it is a dataset of time-series data utilized for training and fine-tuning the model. This corpus serves as a valuable resource for developing and evaluating time-series forecasting models, enabling researchers and practitioners to leverage its contents for long-term series forecasting, frequency analysis, and other related applications.\n\nThe TIME-SERIES CORPUS is likely to be employed in various academic and research settings, including conferences such as ICLR, AAAI, and PMLR, where it can be used to advance the field of time-series analysis and forecasting. The corpus may also be referenced in academic papers and publications, further contributing to the growth of knowledge in this area.\n\nOverall, the TIME-SERIES CORPUS is a critical component in the development and evaluation of time-series forecasting models, and its use is expected to have a significant impact on the field of time-series analysis and forecasting.", "id": "TIME-SERIES CORPUS", "label": "TIME-SERIES CORPUS", "shape": "dot", "size": 10, "source_id": "ad421ec9ba83531336aaf3bec414b3d5,dd9bcf836b1de9b8c99d5ba8188a5ed4", "type": "DATA"}, {"color": "#97c2fc", "description": "Logits refer to the output of the neural network before applying the softmax function", "id": "LOGITS", "label": "LOGITS", "shape": "dot", "size": 10, "source_id": "4ab34d32601d452f14b4a1c31415292d", "type": "OUTPUT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nGoogle Trends is a data source utilized in an experiment, capturing search interest over time for millions of queries. Specifically, it is used for pretraining a model, providing a rich source of information for understanding temporal patterns and trends in online search behavior.\n\nThis summary incorporates information from all the descriptions, resolving any potential contradictions and providing a coherent overview of Google Trends as a data source. The relevant details from the nearby text, such as its use in pretraining a model and capturing search interest over time, have been included to enrich the summary.", "id": "GOOGLE TRENDS", "label": "GOOGLE TRENDS", "shape": "dot", "size": 10, "source_id": "4ab34d32601d452f14b4a1c31415292d,dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "DATA SOURCE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"WIKI PAGEVIEWS\" refers to a data source used for pretraining a model, capturing hourly views of all Wikimedia pages. This data source is utilized to capture the hourly views of all Wikimedia pages, providing a comprehensive understanding of the online engagement and activity on these pages. The data is collected on an hourly basis, offering a detailed and granular view of the pageviews over time. This information can be leveraged to inform various applications, such as long-term series forecasting, frequency analysis, and other analytical tasks.", "id": "WIKI PAGEVIEWS", "label": "WIKI PAGEVIEWS", "shape": "dot", "size": 10, "source_id": "4ab34d32601d452f14b4a1c31415292d,ad7c537267a3aaae402b03f7150950cf", "type": "DATA SOURCE"}, {"color": "#97c2fc", "description": "Synthetic time-series refers to a data source used for pretraining the model, generating artificial time-series data", "id": "SYNTHETIC TIME-SERIES", "label": "SYNTHETIC TIME-SERIES", "shape": "dot", "size": 10, "source_id": "4ab34d32601d452f14b4a1c31415292d", "type": "DATA SOURCE"}, {"color": "#97c2fc", "description": "Encoder-decoder style training is a type of training method used for machine learning models", "id": "ENCODER-DECODER STYLE TRAINING", "label": "ENCODER-DECODER STYLE TRAINING", "shape": "dot", "size": 10, "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "type": "TRAINING METHOD"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe \"CONTEXT WINDOW\" is a concept mentioned in the text, referring to the length of time considered when making predictions. This concept is likely related to time series analysis and forecasting, as indicated by the use of technical terms such as \"time series\" and \"long-term series forecasting\" in the surrounding text. The context window is a crucial aspect of modeling and prediction, as it determines the scope of data considered when making predictions.\n\nGiven the formal and academic tone of the surrounding text, it is likely that the concept of the context window is being discussed in the context of a research paper or technical document, possibly in the field of machine learning or artificial intelligence. The use of technical terms and mathematical equations in the surrounding text suggests a high level of technical expertise and a focus on developing and evaluating machine learning models.\n\nOverall, the context window is a key concept in time series analysis and forecasting, and its length has a significant impact on the accuracy and reliability of predictions made using machine learning models.", "id": "CONTEXT WINDOW", "label": "CONTEXT WINDOW", "shape": "dot", "size": 10, "source_id": "500710c8a95f1310d383fa71fd806c1c,f98d2bec31738be3f7be750b5fe6180e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ARMA processes are a type of time-series model that combines autoregressive and moving average components", "id": "ARMA PROCESSES", "label": "ARMA PROCESSES", "shape": "dot", "size": 10, "source_id": "ad7c537267a3aaae402b03f7150950cf", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"SEASONAL PATTERNS\" can be described as follows:\n\nSEASONAL PATTERNS refer to recurring patterns or cycles in time series data, often related to seasonal or periodic events, such as periodic fluctuations in data that occur at regular intervals, typically corresponding to seasonal or annual cycles.\n\nThis description is a consolidation of the two provided descriptions, which are essentially identical in meaning. The use of the phrase \"periodic fluctuations\" in the first description is equivalent to the phrase \"recurring patterns or cycles\" in the second description, and both descriptions convey the idea that seasonal patterns are related to seasonal or periodic events.\n\nThe context of time series data is also consistent across both descriptions, suggesting that seasonal patterns are a characteristic of time series data that can be analyzed and modeled using various techniques, such as frequency analysis and long-term series forecasting.", "id": "SEASONAL PATTERNS", "label": "SEASONAL PATTERNS", "shape": "dot", "size": 10, "source_id": "ad7c537267a3aaae402b03f7150950cf,cc1063ba5913ea38c84ac0250c01fe84", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Trends refer to long-term patterns or directions in time-series data", "id": "TRENDS", "label": "TRENDS", "shape": "dot", "size": 10, "source_id": "ad7c537267a3aaae402b03f7150950cf", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Step functions are a type of time-series model that represents sudden changes or discontinuities", "id": "STEP FUNCTIONS", "label": "STEP FUNCTIONS", "shape": "dot", "size": 10, "source_id": "ad7c537267a3aaae402b03f7150950cf", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Entity:** DEEP LEARNING MODELS\n\n**Summary:** DEEP LEARNING MODELS are a type of machine learning model that utilizes neural networks to learn from data. They are a type of model mentioned in the text, specifically referring to models commonly used for time series applications. The use of DEEP LEARNING MODELS is indicated by the phrase \"Deep learning models\" in the text, suggesting their relevance in the context of machine learning and time series forecasting.\n\n**Additional Context:** The text suggests that DEEP LEARNING MODELS are used in academic and technical contexts, as indicated by the presence of mathematical equations, formulas, and references to academic papers and conferences (e.g., ICLR, AAAI, PMLR). This implies that DEEP LEARNING MODELS are a topic of interest in the research community, particularly in the areas of machine learning and time series analysis.\n\n**Relevance to Time Series Forecasting:** DEEP LEARNING MODELS are mentioned as being commonly used for time series applications, suggesting their potential for long-term series forecasting and frequency analysis. This implies that DEEP LEARNING MODELS can be used to analyze and predict time series data, making them a valuable tool in the field of time series forecasting.", "id": "DEEP LEARNING MODELS", "label": "DEEP LEARNING MODELS", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491,6a976b57f1171abc9ea2ba6bb5b638f8,ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "Salinas et al. is an academic paper mentioned in the text, referring to a study on deep learning models for time series applications", "id": "SALINAS ET AL.", "label": "SALINAS ET AL.", "shape": "dot", "size": 10, "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "GARCH is a type of model mentioned in the text, as indicated by the use of the phrase \"ARIMA or GARCH\"", "id": "GARCH", "label": "GARCH", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491", "type": "MODEL"}, {"color": "#97c2fc", "description": "Exponential smoothing is a statistical model used for time-series forecasting", "id": "EXPOENTIAL SMOOTHING", "label": "EXPOENTIAL SMOOTHING", "shape": "dot", "size": 10, "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"DARTS\" can be generated as follows:\n\nDARTS is a dataset and a model used for time-series forecasting. Specifically, it is a collection of 8 univariate datasets that are utilized for evaluating and training time-series forecasting models. As a model, DARTS is designed to be user-friendly and modern, catering to the needs of machine learning practitioners working with time-series data. Furthermore, DARTS has demonstrated its effectiveness in performing well for time-series forecasting tasks, making it a valuable resource for researchers and practitioners in the field.\n\nThis summary incorporates information from all the provided descriptions, resolving any potential contradictions and presenting a coherent overview of the entity \"DARTS\".", "id": "DARTS", "label": "DARTS", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7,892b821ed44c13c4d09e0ceedac3051d,a73df99fe49b288e1c8751be2008b191,d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "DATASET"}, {"color": "#97c2fc", "description": "GP is a model that performs well for time-series forecasting tasks", "id": "GP", "label": "GP", "shape": "dot", "size": 10, "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TCN\" refers to a model mentioned in the text, specifically LVRH16. This model is known for its performance in time-series forecasting tasks, where it has shown to be effective. The model\u0027s capabilities are particularly notable in the context of long-term series forecasting, where it can provide accurate predictions. The use of TCN in time-series forecasting is attributed to its ability to leverage frequency analysis and multi-head cross-attention mechanisms, which enable it to capture complex patterns and relationships within the data.\n\nOverall, TCN (LVRH16) is a model that has demonstrated strong performance in time-series forecasting tasks, making it a valuable tool for researchers and practitioners in the field.", "id": "TCN", "label": "TCN", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7,d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"NAIVE\" can be generated as follows:\n\nThe entity \"NAIVE\" refers to a statistical model used for time series forecasting. It is a type of model that employs a basic approach to forecasting, utilizing the last values in the context repeatedly. NAIVE is considered a local statistical baseline method for benchmarking Chronos and performs competitively, making it a relevant model for comparison in time series forecasting tasks. As a simple model, NAIVE is often used as a baseline model for evaluating the performance of more complex forecasting models. It is also mentioned alongside other models such as Seasonal Naive, AutoETS, and AutoARIMA, indicating its relevance in the context of time series forecasting. Overall, NAIVE is a fundamental model in the field of time series forecasting, providing a basic yet effective approach to predicting future values in a time series.", "id": "NAIVE", "label": "NAIVE", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,12395cf4e8efa64a847ede9775ecdf3f,2565ae205d4c98342168bb67a4f7a309,5b1135d9e53e53428a042e8bbc89eaa7,a875a1c0bede47a1c4e3823be81c42c6,af36d1634490149b96980fb1dff57cd1,d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"ETS\" can be described as follows:\n\nETS, which stands for Error, Trend, Seasonality, is a statistical model used for time series forecasting. It is a classical forecasting method that fits a separate model to each time series independently. ETS is a model used for evaluation in the context of time series forecasting, and it is a type of statistical model that is specifically designed to handle the complexities of time series data, including error, trend, and seasonality.\n\nThe model is used to forecast future values in a time series by analyzing the past behavior of the series and identifying patterns and relationships that can be used to make predictions. ETS is a widely used and well-established method in the field of time series forecasting, and it is often used in conjunction with other forecasting techniques to improve accuracy and reliability.\n\nOverall, ETS is a powerful and flexible statistical model that is well-suited to a wide range of time series forecasting applications, from short-term forecasting to long-term series forecasting. Its ability to handle error, trend, and seasonality makes it a valuable tool for analysts and researchers working with time series data.\n\nRelevant information from the nearby text:\n\n* The text mentions that ETS is a statistical model used for time series forecasting, which is consistent with the description provided above.\n* The text also mentions that ETS is a classical forecasting method that fits a separate model to each time series independently, which suggests that ETS is a model that is designed to be flexible and adaptable to different types of time series data.\n* The text does not mention any specific limitations or drawbacks of ETS, but it does suggest that ETS is a widely used and well-established method in the field of time series forecasting.\n\nOverall, the description of ETS provided above is consistent with the information provided in the text, and it provides a clear and concise summary of the key characteristics and features of the ETS model.", "id": "ETS", "label": "ETS", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0,41b0bd14ce7ff7419b7ee1f78b4701ae,6771b6279846e780d6807b15184ae008,7e69d9444a2084b6452e291735bf5a49,ca3dfe42c66d68dd6dbad037936b2360", "type": "MODEL"}, {"color": "#97c2fc", "description": "PR is a model used for evaluation", "id": "PR", "label": "PR", "shape": "dot", "size": 10, "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nPROPHET is a non-autoregressive model primarily used for time-series forecasting. However, it was excluded from the analysis due to its prohibitive computational requirements and extensive training times, which hindered its feasibility for practical application.", "id": "PROPHET", "label": "PROPHET", "shape": "dot", "size": 10, "source_id": "fb67fcff21ac521a5ed8b202412ec1fc,ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "M5 competition is an event mentioned in the text, as indicated by the use of the phrase \"M5 competition\"", "id": "M5 COMPETITION", "label": "M5 COMPETITION", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491", "type": "EVENT"}, {"color": "#97c2fc", "description": "IARAI Traffic4cast contest is an event mentioned in the text, as indicated by the use of the phrase \"IARAI Traffic4cast contest\"", "id": "IARAI TRAFFIC4CAST CONTEST", "label": "IARAI TRAFFIC4CAST CONTEST", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491", "type": "EVENT"}, {"color": "#97c2fc", "description": "Abhimanyu Das is an author mentioned in the text, as indicated by the use of the phrase \"Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou\"", "id": "ABHIMANYU DAS", "label": "ABHIMANYU DAS", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Weihao Kong is an author mentioned in the text, as indicated by the use of the phrase \"Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou\"", "id": "WEIHAO KONG", "label": "WEIHAO KONG", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Rajat Sen is an author mentioned in the text, as indicated by the use of the phrase \"Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou\"", "id": "RAJAT SEN", "label": "RAJAT SEN", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Yichen Zhou is an author mentioned in the text, as indicated by the use of the phrase \"Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou\"", "id": "YICHEN ZHOU", "label": "YICHEN ZHOU", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "A time-series foundation model is a type of pretrained model designed specifically for time-series forecasting tasks", "id": "TIME-SERIES FOUNDATION MODEL", "label": "TIME-SERIES FOUNDATION MODEL", "shape": "dot", "size": 10, "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nBenchmark I is a dataset used to evaluate the performance of time series forecasting models. It comprises 15 in-domain datasets for time series forecasting, making it a benchmark that is clearly less challenging than Benchmark II. This dataset is utilized to assess the capabilities of various time series forecasting models, providing a standardized platform for comparison and evaluation.\n\nThe summary is written in a formal and academic tone, consistent with the language used in the provided text. The information is derived from the descriptions provided, and any contradictions have been resolved to create a coherent summary.", "id": "BENCHMARK I", "label": "BENCHMARK I", "shape": "dot", "size": 10, "source_id": "2eea45c6111e468189580a21686cb14a,532a6d434dab611a80aef1e94dd2fb45,56de1d5a5b467101344afa4248d829dc,63e284ec7e76fa859cc46b72d3746654,e37f4063d074e1697e1f539131b3d963", "type": ""}, {"color": "#97c2fc", "description": "Test datasets refer to the collections of data used to evaluate the performance of the model", "id": "TEST DATASETS", "label": "TEST DATASETS", "shape": "dot", "size": 10, "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Task-specific deep learning baselines are compared to Chronos models in terms of their in-domain performance, with Chronos models performing competitively", "id": "TASK-SPECIFIC DEEP LEARNING BASELINES", "label": "TASK-SPECIFIC DEEP LEARNING BASELINES", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "MODEL"}, {"color": "#97c2fc", "description": "Lag-Llama is a model mentioned in the text, as indicated by the use of the phrase \"pretrained Lag-Llama\"", "id": "LAG-LAGGAGE", "label": "LAG-LAGGAGE", "shape": "dot", "size": 10, "source_id": "d08a82328191907abfcdb72efab9a6cf", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "FINE-TUNED MODEL", "label": "FINE-TUNED MODEL", "shape": "dot", "size": 10, "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"SYNTHETIC DATA\" refers to artificially generated data used for various purposes. It is a type of data that is created artificially to supplement real-world data for training and testing models. Synthetic data is generated according to predefined trends, seasonalities, and other characteristics, making it a valuable tool for researchers and developers. It is used to supplement real-world data, allowing for more comprehensive and robust model training and testing. Synthetic data can be used for both training and testing purposes, providing a reliable and controlled environment for evaluating model performance.\n\nThis summary is based on the information collected from all the descriptions provided, and it resolves any potential contradictions by presenting a unified and coherent understanding of the entity \"SYNTHETIC DATA\". The summary is written in third person and includes the entity name for context. Relevant information from the nearby text has been incorporated to enrich the summary and provide a more comprehensive understanding of the entity.", "id": "SYNTHETIC DATA", "label": "SYNTHETIC DATA", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,a90c6ca26542ea5935f9d8d5bf3688a9,ad7c537267a3aaae402b03f7150950cf,bc54a718d1886698232d578fd88c3ac7,c7f04fa1168df64cce110e20a1b72b51,dc2c05938eb6dbe0217f4c9e6b111e2a,dd9bcf836b1de9b8c99d5ba8188a5ed4", "type": "DATA"}, {"color": "#97c2fc", "description": "Dataset ablation is an experiment that showcases the need for synthetic data", "id": "DATASET ABLATION", "label": "DATASET ABLATION", "shape": "dot", "size": 10, "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "EXPERIMENT"}, {"color": "#97c2fc", "description": "KernelSynth data is a type of synthetic data used for training and testing", "id": "KERNELSYNTH DATA", "label": "KERNELSYNTH DATA", "shape": "dot", "size": 10, "source_id": "c7f04fa1168df64cce110e20a1b72b51", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Figure 10b is a visual representation of the performance of models trained with different proportions of synthetic data", "id": "FIGURE 10B", "label": "FIGURE 10B", "shape": "dot", "size": 10, "source_id": "c7f04fa1168df64cce110e20a1b72b51", "type": "FIGURE"}, {"color": "#97c2fc", "description": "Attention architecture is a type of neural network architecture that focuses on specific parts of the input data", "id": "ATTENTION ARCHITECTURE", "label": "ATTENTION ARCHITECTURE", "shape": "dot", "size": 10, "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Parameter size refers to the number of parameters in a model, which can affect its complexity and performance", "id": "PARAMETER SIZE", "label": "PARAMETER SIZE", "shape": "dot", "size": 10, "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Pretraining data size refers to the amount of data used to train a model before fine-tuning it for a specific task", "id": "PRETRAINING DATA SIZE", "label": "PRETRAINING DATA SIZE", "shape": "dot", "size": 10, "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Local univariate models are a type of model that is trained individually for each time-series in a dataset", "id": "LOCAL UNIVARIATE MODELS", "label": "LOCAL UNIVARIATE MODELS", "shape": "dot", "size": 10, "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "Global univariate models are a type of model that is trained globally on many time-series", "id": "GLOBAL UNIVARIATE MODELS", "label": "GLOBAL UNIVARIATE MODELS", "shape": "dot", "size": 10, "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "Global multivariate models are a type of model that takes in the past of all time-series in the dataset to predict the future of all the time-series", "id": "GLOBAL MULTIVARIATE MODELS", "label": "GLOBAL MULTIVARIATE MODELS", "shape": "dot", "size": 10, "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "VAR model is a classical model used for time-series forecasting", "id": "VAR MODEL", "label": "VAR MODEL", "shape": "dot", "size": 10, "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "Domain adaptation is a technique used in machine learning where a model trained on one dataset is adapted to work on another dataset", "id": "DOMAIN ADAPTATION", "label": "DOMAIN ADAPTATION", "shape": "dot", "size": 10, "source_id": "0bef137d159ccc86cdee0a8be788bd26", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Hassan Ismail Fawaz is an author who has published a paper on transfer learning for time series classification", "id": "HASSAN ISMAIL FAWAZ", "label": "HASSAN ISMAIL FAWAZ", "shape": "dot", "size": 10, "source_id": "a73df99fe49b288e1c8751be2008b191", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Horizon refers to the number of future time points that are predicted", "id": "HORIZON", "label": "HORIZON", "shape": "dot", "size": 10, "source_id": "6f6e27166a1506482bfcaf5585322595", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "For evaluation metrics, we utilize the mean square error (MSE)", "id": "MEAN SQUARE ERROR", "label": "MEAN SQUARE ERROR", "shape": "dot", "size": 10, "source_id": "50eeacd99c68b2581be90310bedcbc2c", "type": "METRIC"}, {"color": "#97c2fc", "description": "For short-term forecasting on M4 benchmark, we adopt the mean absolute scaled error (MASE)", "id": "MEAN ABSOLUTE SCALDED ERROR", "label": "MEAN ABSOLUTE SCALDED ERROR", "shape": "dot", "size": 10, "source_id": "50eeacd99c68b2581be90310bedcbc2c", "type": "METRIC"}, {"color": "#97c2fc", "description": "Patch length refers to the number of time points in a patch", "id": "PATCH LENGTH", "label": "PATCH LENGTH", "shape": "dot", "size": 10, "source_id": "6f6e27166a1506482bfcaf5585322595", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "LONG HORIZON FORECASTING", "label": "LONG HORIZON FORECASTING", "shape": "dot", "size": 10, "source_id": "f98d2bec31738be3f7be750b5fe6180e", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"OUTPUT PATCH LENGTH\" can be generated as follows:\n\nThe \"OUTPUT PATCH LENGTH\" is a hyperparameter of the TimesFM model, as described in the text. It refers to the size of each patch in the output time-series, which is also equivalent to the length of the output patch in the model. Furthermore, the output patch length is directly related to the number of time-points predicted by the model. In essence, the output patch length is a critical parameter that determines the granularity of the predictions made by the TimesFM model.\n\nThis summary is derived from the provided descriptions, which are all related to the same entity or group of entities. The contradictions have been resolved, and a single, coherent summary has been generated. The summary is written in the third person and includes the entity name for context. Relevant information from the nearby text has been incorporated to enrich the summary.", "id": "OUTPUT PATCH LENGTH", "label": "OUTPUT PATCH LENGTH", "shape": "dot", "size": 10, "source_id": "3cf566c27c75f886658002bf9b3b0b15,4ab34d32601d452f14b4a1c31415292d,ee8135f4497807724f665a5cdaa775fb,f7ce89346ea6715560163ef3a630a5df", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nPATCH 5 is a specific instance or example of a patch. The description of PATCH 5 is limited to this single statement, suggesting that it is a well-defined and distinct entity within its context. However, without further information, it is unclear what PATCH 5 refers to or its specific characteristics.\n\nGiven the lack of additional details, it is not possible to provide a more detailed or enriched summary. The description provided is concise and to the point, indicating that PATCH 5 is a singular example of a patch, but the nature and context of this patch are not specified.", "id": "PATCH 5", "label": "PATCH 5", "shape": "dot", "size": 10, "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe entity \"REPROGRAM\" refers to the process of modifying or updating a model or system. This process involves making changes to the existing model or system to improve its performance, accuracy, or functionality. The term \"Reprogramming\" is commonly used in the context of machine learning and artificial intelligence, where models need to be updated or modified to adapt to new data, changing environments, or evolving requirements.\n\nIn the context of machine learning, reprogramming can involve techniques such as model fine-tuning, model updating, or model retraining. This process can be used to improve the performance of a model on a specific task or to adapt the model to a new task or dataset.\n\nOverall, the concept of \"REPROGRAM\" is an essential aspect of machine learning and artificial intelligence, enabling models to learn from new data, adapt to changing environments, and improve their performance over time.\n\nNote: The description list was empty, so the summary is based on the provided entity name and the general context of reprogramming in machine learning and artificial intelligence.", "id": "REPROGRAM", "label": "REPROGRAM", "shape": "dot", "size": 10, "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a014d14e003d0998b877eacffa8ebfc4,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Local semantic information refers to the meaning and context of individual patches or tokens", "id": "LOCAL SEMANTIC INFORMATION", "label": "LOCAL SEMANTIC INFORMATION", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Horizontal sliding stride refers to the process of sliding a window over the input data to extract patches", "id": "HORIZONTAL SLIDING STRIDE", "label": "HORIZONTAL SLIDING STRIDE", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Representations refer to the output of the language model after processing the input", "id": "REPRESENTATIONS", "label": "REPRESENTATIONS", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"QLORA\" can be generated as follows:\n\nQLORA is a model that refers to a parameter-efficient fine-tuning method. It is a model mentioned in the text, as indicated by the use of the phrase \"QLoRA Dettmers et al. (2023)\". QLORA is also a model used for comparison with TIME-LLM, and it is designed for efficient fine-tuning of quantized LLMs (Large Language Models). This suggests that QLORA is a model that enables efficient and effective fine-tuning of quantized LLMs, making it a valuable tool for researchers and practitioners in the field of natural language processing.\n\nThe use of QLORA as a parameter-efficient fine-tuning method implies that it can be used to adapt pre-trained LLMs to specific tasks or domains with minimal loss of performance, while also reducing the computational resources required for fine-tuning. This is particularly important for large-scale language models, which can be computationally expensive to fine-tune.\n\nOverall, QLORA appears to be a model that offers a promising solution for efficient fine-tuning of quantized LLMs, making it a valuable contribution to the field of natural language processing.\n\nRelevant information from the nearby text suggests that QLORA is a model that is being compared to TIME-LLM, which implies that it is being evaluated for its performance and efficiency in fine-tuning large language models. The mention of Dettmers et al. (2023) in the text also suggests that QLORA is a model that has been studied and evaluated in a research context, which adds to its credibility and validity as a parameter-efficient fine-tuning method.", "id": "QLORA", "label": "QLORA", "shape": "dot", "size": 10, "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8,7e97089185883c456c798ddc5ec86373,84bc2afcbbd278961c3c7a637c6a189e,a73df99fe49b288e1c8751be2008b191,e6a6bc6fbd362394320961ac10cdd230", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"PATCH MASKING\" is a concept mentioned in the text, which refers to the process of randomly masking parts of the patches during training. This process is a technique used in machine learning, particularly in the context of image classification and object detection tasks. The use of patch masking is indicated by the phrase \"patch masking\" in the text, suggesting that it is a relevant and important concept in the field of machine learning.\n\nThe description of patch masking as a process of randomly masking parts of the patches during training implies that it is a technique used to improve the robustness and generalizability of machine learning models. By randomly masking parts of the patches, the model is forced to learn features that are invariant to the presence or absence of certain parts of the image, which can lead to improved performance on a variety of tasks.\n\nOverall, patch masking is a technique used in machine learning to improve the robustness and generalizability of models, particularly in the context of image classification and object detection tasks.\n\nRelevant information from the nearby text suggests that patch masking is a technique that is commonly used in conjunction with other techniques, such as data augmentation and transfer learning, to improve the performance of machine learning models. The use of patch masking is also mentioned in the context of long-term series forecasting, suggesting that it may be used in time series analysis and forecasting tasks as well.\n\nIn terms of the language used, the text is written in English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers. The language is formal and academic, suggesting that the text is from a research paper or a technical document.", "id": "PATCH MASKING", "label": "PATCH MASKING", "shape": "dot", "size": 10, "source_id": "3cf566c27c75f886658002bf9b3b0b15,f98d2bec31738be3f7be750b5fe6180e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Reprogrammed LLM is a model used in the text", "id": "REPROGRAMMED LLM", "label": "REPROGRAMMED LLM", "shape": "dot", "size": 10, "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "type": "MODEL"}, {"color": "#97c2fc", "description": "Masking refers to the process of randomly masking parts of the patches or entire patches during training", "id": "MASKING", "label": "MASKING", "shape": "dot", "size": 10, "source_id": "3cf566c27c75f886658002bf9b3b0b15", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe \"RESIDUAL BLOCK\" is a component of the neural network architecture used for time-series forecasting. It refers to a type of neural network layer that adds the input to the output of a hidden layer, which is a crucial aspect of its functionality in the context of time-series forecasting.\n\nThis summary is derived from the two descriptions provided, which are not contradictory but rather complementary. The first description highlights the role of the residual block in time-series forecasting, while the second description provides a more detailed explanation of its internal workings. By combining these two pieces of information, we can gain a deeper understanding of the residual block\u0027s purpose and functionality within the context of neural networks and time-series forecasting.\n\nIt is worth noting that the residual block is a key component of various deep learning architectures, including those used for time-series forecasting. Its ability to add the input to the output of a hidden layer allows it to learn long-term dependencies and patterns in the data, making it a powerful tool for tasks such as forecasting and prediction.", "id": "RESIDUAL BLOCK", "label": "RESIDUAL BLOCK", "shape": "dot", "size": 10, "source_id": "3cf566c27c75f886658002bf9b3b0b15,4ab34d32601d452f14b4a1c31415292d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Auto-regressive generation refers to the process of generating output based on the previous output", "id": "AUTO-REGRESSIVE GENERATION", "label": "AUTO-REGRESSIVE GENERATION", "shape": "dot", "size": 10, "source_id": "3cf566c27c75f886658002bf9b3b0b15", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The fully connected layer is a type of layer used in the TimesFM model, as described in the text", "id": "FULLY CONNECTED LAYER", "label": "FULLY CONNECTED LAYER", "shape": "dot", "size": 10, "source_id": "ee8135f4497807724f665a5cdaa775fb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Model dimension is a hyperparameter of the TimesFM model, as described in the text", "id": "MODEL DIMENSION", "label": "MODEL DIMENSION", "shape": "dot", "size": 10, "source_id": "ee8135f4497807724f665a5cdaa775fb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Model sizes refer to the number of parameters in a model", "id": "MODEL SIZES", "label": "MODEL SIZES", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Multi-layer perceptron (MLP) refers to a type of neural network layer with multiple hidden layers", "id": "MULTI-LAYER PERCEPTRON", "label": "MULTI-LAYER PERCEPTRON", "shape": "dot", "size": 10, "source_id": "3cf566c27c75f886658002bf9b3b0b15", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Output residual block refers to a component of the neural network architecture used for time-series forecasting", "id": "OUTPUT RESIDUAL BLOCK", "label": "OUTPUT RESIDUAL BLOCK", "shape": "dot", "size": 10, "source_id": "4ab34d32601d452f14b4a1c31415292d", "type": "MODEL COMPONENT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"POSITIONAL ENCODING\" is a concept mentioned in the text, which refers to a technique used to incorporate information about the position of a data point in a sequence. This technique involves adding a position-dependent encoding to the input, allowing the model to capture the sequential nature of the data. Positional encoding is a crucial aspect of various machine learning models, particularly in natural language processing and time series forecasting.\n\nThe use of positional encoding enables models to understand the relationships between data points in a sequence, which is essential for tasks such as long-term series forecasting and frequency analysis. By incorporating positional information, models can better capture the underlying patterns and trends in the data, leading to more accurate predictions and improved performance.\n\nThe entity \"POSITIONAL ENCODING\" is mentioned in the context of technical terms and mathematical equations, suggesting that it is a concept relevant to academic research and technical documents. The use of English-language abbreviations and citations to academic papers further supports the idea that positional encoding is a concept discussed in the English-language academic community.\n\nOverall, positional encoding is a technique used to incorporate position-dependent information into machine learning models, enabling them to better capture sequential relationships and make more accurate predictions.", "id": "POSITIONAL ENCODING", "label": "POSITIONAL ENCODING", "shape": "dot", "size": 10, "source_id": "3cf566c27c75f886658002bf9b3b0b15,89e5f6a93205d5e87ad7e7641c0bbb91,fb67fcff21ac521a5ed8b202412ec1fc", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Input embedding is a concept mentioned in the text, as indicated by the use of the phrase \"input embedding\"", "id": "INPUT EMBEDDING", "label": "INPUT EMBEDDING", "shape": "dot", "size": 10, "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Input refers to the data points that are used as inputs to the model", "id": "INPUT", "label": "INPUT", "shape": "dot", "size": 10, "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "type": "DATA"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"TRANSFORMER LAYERS\" refers to a component of the model used for time-series forecasting. Specifically, they are a type of neural network layer that utilizes self-attention and feed-forward networks. Additionally, the term \"Transformer layers\" can also refer to the number of layers in the transformer-based language model. However, in the context of time-series forecasting, it is clear that the primary function of Transformer layers is to facilitate the processing and analysis of time-series data, leveraging their self-attention mechanism to capture complex relationships and patterns within the data.\n\nThis summary is based on the information provided in the description list, which collectively paint a picture of the multifaceted nature of Transformer layers. By combining the different descriptions, we can gain a deeper understanding of the role and functionality of Transformer layers in the context of time-series forecasting and language modeling.", "id": "TRANSFORMER LAYERS", "label": "TRANSFORMER LAYERS", "shape": "dot", "size": 10, "source_id": "3cf566c27c75f886658002bf9b3b0b15,673b0feee6eb5843954defc670e4ba29,e57d44d23f5a3a82ce9a9b9532d31cbe", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"LAYER NORM\" can be generated as follows:\n\n\"LAYER NORM\" is a technique used in various models, including the Transformer encoder and decoder, to normalize the input data or features. It is a concept used to standardize the input, ensuring that all features are on the same scale, which is essential for accurate model performance. Specifically, LayerNorm is a module used in the Transformer encoder to normalize the input features, while in the decoder, it is used to normalize the input. This normalization technique is crucial for achieving optimal results in models that rely on LAYER NORM, such as those used in natural language processing and time series forecasting applications.\n\nThe use of LAYER NORM is also evident in the context of long-term series forecasting, frequency analysis, and multi-head cross-attention, which are all relevant to the field of time series analysis and machine learning. The presence of LAYER NORM in these contexts suggests its importance in ensuring accurate and reliable model performance.\n\nOverall, LAYER NORM is a critical component in various machine learning models, particularly those used in natural language processing and time series forecasting. Its role in normalizing input data or features is essential for achieving optimal results, making it a vital technique in the field of machine learning and data analysis.", "id": "LAYER NORM", "label": "LAYER NORM", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e,673b0feee6eb5843954defc670e4ba29,a5d60c1a355b77187003182f6df57646", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Residual blocks are a component of the model used for time-series forecasting", "id": "RESIDUAL BLOCKS", "label": "RESIDUAL BLOCKS", "shape": "dot", "size": 10, "source_id": "673b0feee6eb5843954defc670e4ba29", "type": "MODEL COMPONENT"}, {"color": "#97c2fc", "description": "Forecasting accuracy refers to the accuracy of the model\u0027s predictions", "id": "FORECASTING ACCURACY", "label": "FORECASTING ACCURACY", "shape": "dot", "size": 10, "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"STACKED TRANSFORMER\" is a type of neural network architecture that stacks multiple transformer layers. This architecture is specifically used in the TimesFM model, as described in the text. The stacked transformer is a key component of the TimesFM model, which is likely used for time series forecasting or long-term series forecasting tasks. The use of the stacked transformer in the TimesFM model suggests that it is well-suited for frequency analysis and multi-head cross-attention tasks, which are common in time series forecasting applications.\n\nThe language used to describe the stacked transformer is formal and academic, suggesting that it is from a research paper or technical document. The text contains technical terms and mathematical equations, which are written in a standard mathematical notation used in English-language academic papers. The presence of English-language abbreviations and citations of English-language academic papers further supports the conclusion that the primary language of the text is English.\n\nOverall, the stacked transformer is a powerful neural network architecture that is used in the TimesFM model for time series forecasting tasks. Its use of multiple transformer layers and multi-head cross-attention makes it well-suited for frequency analysis and other tasks common in time series forecasting applications.", "id": "STACKED TRANSFORMER", "label": "STACKED TRANSFORMER", "shape": "dot", "size": 10, "source_id": "3cf566c27c75f886658002bf9b3b0b15,ee8135f4497807724f665a5cdaa775fb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Causal attention is a type of attention mechanism used in the TimesFM model, as described in the text", "id": "CAUSAL ATTENTION", "label": "CAUSAL ATTENTION", "shape": "dot", "size": 10, "source_id": "ee8135f4497807724f665a5cdaa775fb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"RELATIVE POSITION BIAS\" is a module used in the Transformer encoder, specifically in the context of natural language processing. It is designed to consider the relative positions between features in a window, which is a crucial aspect of time series forecasting and long-term series forecasting. This module is likely used to incorporate temporal information into the data, which is a key component of frequency analysis and multi-head cross-attention techniques.\n\nIn the context of natural language processing, relative position bias is also a task that involves inserting appropriate temporal information into data. This suggests that the module is used to address the challenge of incorporating temporal relationships between features in a window, which is essential for accurate time series forecasting and long-term series forecasting.\n\nOverall, the \"RELATIVE POSITION BIAS\" module plays a critical role in the Transformer encoder, enabling it to effectively consider the relative positions between features in a window and incorporate temporal information into the data. This is particularly relevant in the context of time series forecasting, long-term series forecasting, and frequency analysis.\n\nRelevant information from the nearby text suggests that the \"RELATIVE POSITION BIAS\" module is used in conjunction with other techniques such as multi-head cross-attention, which is a key component of the Transformer encoder. This highlights the importance of relative position bias in enabling the Transformer encoder to effectively capture temporal relationships between features in a window.\n\nIn terms of academic references, the \"RELATIVE POSITION BIAS\" module is likely related to research papers and conferences such as ICLR, AAAI, and PMLR, which are all prominent venues for natural language processing and machine learning research.", "id": "RELATIVE POSITION BIAS", "label": "RELATIVE POSITION BIAS", "shape": "dot", "size": 10, "source_id": "83b49bf68dab6c36bdc09f02a63803fe,a5d60c1a355b77187003182f6df57646,a7604286fb84b26c53950861f9aca4b1", "type": "TASK"}, {"color": "#97c2fc", "description": "Relative position bias table is a learnable bias table used to store relative position biases", "id": "RELATIVE POSITION BIAS TABLE", "label": "RELATIVE POSITION BIAS TABLE", "shape": "dot", "size": 10, "source_id": "a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe WINDOW ENCODER is a crucial component of the TranAD model, which plays a significant role in processing the input window. Specifically, the WINDOW ENCODER performs operations on the input window, effectively encoding it as part of the TranAD model\u0027s architecture. This encoding process is essential for the model\u0027s ability to analyze and understand the input data, enabling it to make accurate predictions and forecasts.\n\nIn the context of time series analysis and long-term series forecasting, the WINDOW ENCODER\u0027s role is particularly important. By encoding the input window, the model can capture complex patterns and relationships within the data, allowing for more accurate frequency analysis and multi-head cross-attention mechanisms. The WINDOW ENCODER\u0027s functionality is likely to be influenced by the TranAD model\u0027s architecture and the specific requirements of the time series forecasting task at hand.\n\nOverall, the WINDOW ENCODER is a vital component of the TranAD model, and its encoding operations are critical for the model\u0027s ability to analyze and forecast time series data.", "id": "WINDOW ENCODER", "label": "WINDOW ENCODER", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e,f2e78b25a535b82e2743a0ea4052eb9a", "type": "MODEL COMPONENT"}, {"color": "#97c2fc", "description": "Embedding dimensions per head is a hyperparameter that determines the size of the embeddings", "id": "EMBEDDING DIMENSIONS PER HEAD", "label": "EMBEDDING DIMENSIONS PER HEAD", "shape": "dot", "size": 10, "source_id": "c5dc13d7191b625e7373e79907b5782a", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Mean squared error refers to a loss function used for point forecasting in time-series forecasting", "id": "MEAN SQUARED ERROR", "label": "MEAN SQUARED ERROR", "shape": "dot", "size": 10, "source_id": "4ab34d32601d452f14b4a1c31415292d", "type": "LOSS FUNCTION"}, {"color": "#97c2fc", "description": "Point forecasting refers to the task of predicting a single value for a future time-point", "id": "POINT FORECASTING", "label": "POINT FORECASTING", "shape": "dot", "size": 10, "source_id": "4ab34d32601d452f14b4a1c31415292d", "type": "TASK"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"QUANTILE LOSS\" is a concept used to evaluate the performance of time series forecasting models. Specifically, it refers to a loss function used for probabilistic forecasting in time-series forecasting. This loss function is utilized to assess the accuracy of predictions made by time series forecasting models, providing a measure of how well the model performs in terms of capturing the uncertainty and variability of the data.\n\nIn the context of time series forecasting, quantile loss is an essential metric for evaluating the performance of models, particularly those that provide probabilistic forecasts. By using quantile loss, researchers and practitioners can gain insights into the model\u0027s ability to predict different quantiles of the data distribution, which is crucial for making informed decisions in various applications, such as finance, weather forecasting, and demand planning.\n\nOverall, the entity \"QUANTILE LOSS\" plays a significant role in the evaluation and development of time series forecasting models, enabling the creation of more accurate and reliable predictions.", "id": "QUANTILE LOSS", "label": "QUANTILE LOSS", "shape": "dot", "size": 10, "source_id": "4ab34d32601d452f14b4a1c31415292d,e37f4063d074e1697e1f539131b3d963", "type": "LOSS FUNCTION"}, {"color": "#97c2fc", "description": "Chain-of-thought refers to a technique used in machine learning to improve the performance of a model by providing it with a series of intermediate steps or reasoning", "id": "CHAIN-OF-THOUGHT", "label": "CHAIN-OF-THOUGHT", "shape": "dot", "size": 10, "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Entity:** CONFORMAL PREDICTION\n\n**Summary:** CONFORMAL PREDICTION is a non-parametric framework used to generate prediction intervals with a pre-specified level of coverage accuracy. This framework is utilized to calibrate or adjust the model for a specific task, allowing for the generation of accurate prediction intervals. The techniques used in CONFORMAL PREDICTION are designed to provide a high level of coverage accuracy, making it a valuable tool in various applications.\n\n**Key Features:**\n\n* Non-parametric framework\n* Generates prediction intervals with pre-specified level of coverage accuracy\n* Used to calibrate or adjust the model for a specific task\n* Provides high level of coverage accuracy\n\n**Relevance:** CONFORMAL PREDICTION is a significant concept in machine learning and statistical modeling, particularly in the context of prediction and uncertainty estimation. Its applications can be found in various fields, including but not limited to, time series forecasting, regression analysis, and classification problems.\n\n**Language:** The primary language of the text related to CONFORMAL PREDICTION is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.", "id": "CONFORMAL PREDICTION", "label": "CONFORMAL PREDICTION", "shape": "dot", "size": 10, "source_id": "42e1bb44edbe787e104f589e74b95d6c,f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time series domain refers to the field or application where time series data is used", "id": "TIME SERIES DOMAIN", "label": "TIME SERIES DOMAIN", "shape": "dot", "size": 10, "source_id": "42e1bb44edbe787e104f589e74b95d6c", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Weighted average is a concept that is used to aggregate errors in WQL", "id": "WEIGHTED AVERAGE", "label": "WEIGHTED AVERAGE", "shape": "dot", "size": 10, "source_id": "e37f4063d074e1697e1f539131b3d963", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe Continuous Ranked Probability Score (CRPS) is a metric used to evaluate the performance of a model in making probabilistic predictions. It is a concept that assesses the accuracy of a model\u0027s predictions by comparing them to the actual outcomes. The CRPS is a widely used metric in the field of machine learning and time series forecasting, particularly in evaluating the performance of models that provide probabilistic forecasts.\n\nIn the context of time series forecasting, the CRPS is used to evaluate the accuracy of a model\u0027s predictions over a long-term series. It takes into account the frequency analysis of the data and provides a comprehensive measure of the model\u0027s performance. The CRPS is often used in conjunction with other metrics, such as multi-head cross-attention, to evaluate the performance of models in complex forecasting tasks.\n\nThe CRPS has been extensively studied and applied in various fields, including research papers and technical documents published in conferences and journals such as ICLR, AAAI, and PMLR. The metric has been used to evaluate the performance of models in a wide range of applications, from weather forecasting to financial modeling.\n\nOverall, the CRPS is a powerful tool for evaluating the performance of models in making probabilistic predictions, and its use is widespread in the field of machine learning and time series forecasting.", "id": "CONTINUOUS RANKED PROBABILITY SCORE", "label": "CONTINUOUS RANKED PROBABILITY SCORE", "shape": "dot", "size": 10, "source_id": "e37f4063d074e1697e1f539131b3d963,f0808ad97bc4d26391284145427770e5,fa01b75dccc556af8aca0d6c47e1970f", "type": "METRIC"}, {"color": "#97c2fc", "description": "Data privacy is a concern related to the use of the model", "id": "DATA PRIVACY", "label": "DATA PRIVACY", "shape": "dot", "size": 10, "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "CONCERN"}, {"color": "#97c2fc", "description": "Differentially private data refers to data that is protected from individual user activity", "id": "DIFFERENTIALLY PRIVATE", "label": "DIFFERENTIALLY PRIVATE", "shape": "dot", "size": 10, "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "DATA"}, {"color": "#97c2fc", "description": "ARMA (AutoRegressive Integrated Moving Average) refers to a statistical model used to forecast time series data", "id": "ARMA", "label": "ARMA", "shape": "dot", "size": 10, "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Sine waves refer to a type of periodic wave pattern, often used in time series data to represent seasonal or periodic events", "id": "SINE WAVES", "label": "SINE WAVES", "shape": "dot", "size": 10, "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Cosine waves refer to a type of periodic wave pattern, often used in time series data to represent seasonal or periodic events", "id": "COSINE WAVES", "label": "COSINE WAVES", "shape": "dot", "size": 10, "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe M4 DATASET is a publicly available dataset containing time-series data from various sources. It is mentioned in the text as indicated by the use of the name \"M4\". The dataset is likely used for long-term series forecasting, frequency analysis, and other time series-related tasks, given the context of the text. The M4 DATASET is likely used in academic and research settings, as indicated by the presence of technical terms, mathematical equations, and references to academic papers. The dataset may be used in conferences and journals such as ICLR, AAAI, and PMLR, given the presence of their abbreviations in the text. Overall, the M4 DATASET appears to be a valuable resource for researchers and practitioners working with time series data.", "id": "M4 DATASET", "label": "M4 DATASET", "shape": "dot", "size": 10, "source_id": "74527a4337ed6919731be520311ae774,ad7c537267a3aaae402b03f7150950cf", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the \"ELECTRICITY DATASET\" in third person:\n\nThe \"ELECTRICITY DATASET\" is a publicly available dataset containing time-series data related to electricity usage. It is mentioned in the text as indicated by the use of the name \"Electricity.\" This dataset is likely to be of interest to researchers and analysts working in the field of energy consumption and time-series forecasting, as it provides valuable insights into electricity usage patterns.\n\nGiven the context of the text, which discusses technical terms, mathematical equations, and references to academic papers, it is likely that the \"ELECTRICITY DATASET\" is used for research purposes, such as long-term series forecasting, frequency analysis, and multi-head cross-attention. The dataset may have been used in studies published in conferences and journals such as ICLR, AAAI, and PMLR, and may have been cited in academic papers by researchers in the field.\n\nOverall, the \"ELECTRICITY DATASET\" appears to be a valuable resource for researchers and analysts working in the field of energy consumption and time-series forecasting, providing a rich source of data for analysis and modeling.", "id": "ELECTRICITY DATASET", "label": "ELECTRICITY DATASET", "shape": "dot", "size": 10, "source_id": "74527a4337ed6919731be520311ae774,ad7c537267a3aaae402b03f7150950cf", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the \"TRAFFIC DATASET\" entity:\n\nThe \"TRAFFIC DATASET\" is a publicly available dataset containing time-series data related to traffic patterns. It encompasses 862 hourly time series depicting road occupancy rates on the freeways in the San Francisco Bay area from 2015 to 2016. This dataset is a collection of data used to train and evaluate machine learning models, providing valuable insights into traffic patterns and road occupancy rates.\n\nThe dataset is specifically focused on the San Francisco Bay area, offering a unique opportunity to analyze and understand traffic dynamics in a specific region. The time-series data spans two years, from 2015 to 2016, allowing for the examination of long-term trends and patterns in traffic occupancy rates.\n\nOverall, the \"TRAFFIC DATASET\" is a valuable resource for researchers and practitioners interested in traffic analysis, machine learning, and time-series forecasting, offering a rich source of data for training and evaluating models.", "id": "TRAFFIC DATASET", "label": "TRAFFIC DATASET", "shape": "dot", "size": 10, "source_id": "74527a4337ed6919731be520311ae774,76cc338e586223647fd3dbe4e7a7c131,ad7c537267a3aaae402b03f7150950cf,c60a8238db3d56eff9cc9692e7ac5b1c", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the \"WEATHER DATASET\" entity:\n\nThe \"WEATHER DATASET\" is a publicly available dataset containing time-series data related to weather patterns. Specifically, it includes hourly climate data near Monash University, Clayton, Victoria, Australia, spanning from January 2010 to May 2021. This dataset is mentioned in the text and is available for use, providing valuable insights into weather patterns over a long period.\n\nThe dataset\u0027s primary characteristics include:\n\n* Time series data: The dataset contains hourly climate data, which is a type of time series data.\n* Location: The data is collected near Monash University, Clayton, Victoria, Australia.\n* Timeframe: The dataset spans from January 2010 to May 2021, covering a period of over 11 years.\n* Publicly available: The dataset is publicly available, making it accessible for research and analysis purposes.\n\nOverall, the \"WEATHER DATASET\" is a valuable resource for researchers and analysts interested in understanding weather patterns and climate trends over an extended period.", "id": "WEATHER DATASET", "label": "WEATHER DATASET", "shape": "dot", "size": 10, "source_id": "74527a4337ed6919731be520311ae774,76cc338e586223647fd3dbe4e7a7c131,ad7c537267a3aaae402b03f7150950cf", "type": "DATASET"}, {"color": "#97c2fc", "description": "M3 dataset is a dataset mentioned in the text, as indicated by the use of the name \"M3\"", "id": "M3 DATASET", "label": "M3 DATASET", "shape": "dot", "size": 10, "source_id": "74527a4337ed6919731be520311ae774", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe ETT datasets are a collection of time-series data used for testing and evaluation, specifically for training and testing Lag-Llama models. They are a set of datasets mentioned in the text, as indicated by the use of the name \"ETT\", and are utilized for evaluating the performance of models in the context of time-series forecasting.\n\nThis summary is derived from the following information collected from the descriptions:\n\n* The ETT datasets are a collection of datasets used for training and testing Lag-Llama.\n* The ETT datasets are a collection of time-series data used for testing and evaluation.\n* The ETT datasets are a set of datasets mentioned in the text, as indicated by the use of the name \"ETT\".\n* The ETT datasets are a type of dataset used for time-series forecasting.\n* The ETT datasets are used for evaluating the performance of models.\n\nThe contradictions in the descriptions have been resolved by combining the relevant information, resulting in a single, coherent summary that accurately reflects the characteristics of the ETT datasets.", "id": "ETT DATASETS", "label": "ETT DATASETS", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,4ade7764ebe998b5f35a7fd2b58d4796,500710c8a95f1310d383fa71fd806c1c,74527a4337ed6919731be520311ae774,ab91381dd032db318e5aec1ad5b914a6,cd4ff69415c7b37cec643f8289d1c98d,f7ce89346ea6715560163ef3a630a5df", "type": "DATASET"}, {"color": "#97c2fc", "description": "ILI dataset is a dataset mentioned in the text, as indicated by the use of the name \"ILI\"", "id": "ILI DATASET", "label": "ILI DATASET", "shape": "dot", "size": 10, "source_id": "74527a4337ed6919731be520311ae774", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Road occupancy rates refer to the percentage of time a road is occupied by vehicles", "id": "ROAD OCCUPANCY RATES", "label": "ROAD OCCUPANCY RATES", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ETT-H2 dataset is a collection of data used to train and evaluate machine learning models", "id": "ETT-H2 DATASET", "label": "ETT-H2 DATASET", "shape": "dot", "size": 10, "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the ETT-M2 DATASET in third person:\n\nThe ETT-M2 DATASET is a collection of data used to train and evaluate machine learning models, specifically for fine-tuning forecasting tasks. This dataset is designed to support the development and evaluation of machine learning models for long-term series forecasting, which involves analyzing and predicting time series data over extended periods. The ETT-M2 DATASET is likely used in academic and research settings, given the presence of technical terms, mathematical equations, and references to academic papers, all of which are written in English. The dataset is likely used in conferences and journals such as ICLR, AAAI, and PMLR, and may be cited in research papers and technical documents. Overall, the ETT-M2 DATASET is a valuable resource for researchers and practitioners working on time series forecasting and machine learning applications.", "id": "ETT-M2 DATASET", "label": "ETT-M2 DATASET", "shape": "dot", "size": 10, "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26,c60a8238db3d56eff9cc9692e7ac5b1c", "type": "DATASET"}, {"color": "#97c2fc", "description": "Climate data refers to information about the weather and climate in a given area", "id": "CLIMATE DATA", "label": "CLIMATE DATA", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The synthetic dataset is a subset of the TimesFM pretraining dataset", "id": "SYNTHETIC", "label": "SYNTHETIC", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The weather dataset is a subset of the TimesFM pretraining dataset", "id": "WEATHER ZZP+21", "label": "WEATHER ZZP+21", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The Favorita sales dataset is a subset of the TimesFM pretraining dataset", "id": "FAVORITA SALES", "label": "FAVORITA SALES", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The LibCity dataset is a subset of the TimesFM pretraining dataset", "id": "LIBCITY WJJ", "label": "LIBCITY WJJ", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The M4 hourly dataset is a subset of the TimesFM pretraining dataset", "id": "M4 HOURLY", "label": "M4 HOURLY", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The M4 daily dataset is a subset of the TimesFM pretraining dataset", "id": "M4 DAILY", "label": "M4 DAILY", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe Australian Electricity dataset is a collection of electricity demand data from 5 states in Australia. It is primarily used for empirical evaluation purposes, likely in the context of time series forecasting and analysis. The dataset is likely to be of interest to researchers and practitioners in the field of energy demand forecasting, who may use it to develop and test various models and techniques for predicting electricity demand.\n\nGiven the context of the dataset, it is likely that the data includes information on electricity consumption patterns, demand trends, and possibly other relevant factors such as weather, seasonality, and economic indicators. The dataset may also include metadata such as data collection dates, locations, and measurement units.\n\nOverall, the Australian Electricity dataset appears to be a valuable resource for researchers and practitioners seeking to understand and predict electricity demand in Australia, and to develop more accurate and effective forecasting models.\n\nRelevant information from the nearby text is not provided, but based on the context, it is likely that the dataset is used in the field of machine learning and time series analysis, and may be used in conjunction with techniques such as long-term series forecasting, frequency analysis, and multi-head cross-attention.", "id": "AUSTRALIAN ELECTRICITY", "label": "AUSTRALIAN ELECTRICITY", "shape": "dot", "size": 10, "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502,e5e4a8f03f502fada5b17cba5dc942ba", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"ERCOT LOAD\":\n\nThe ERCOT LOAD dataset is a collection of hourly energy load data from 8 US regions spanning the period from 2004 to 2021. This dataset contains energy usage data, providing valuable insights into the temporal patterns and trends of energy consumption in these regions. The ERCOT LOAD dataset is a valuable resource for researchers and analysts interested in time series analysis, long-term series forecasting, and frequency analysis, as it offers a comprehensive view of energy load dynamics over an extended period.\n\nThe dataset\u0027s scope and duration make it an ideal candidate for studying seasonal patterns, trends, and anomalies in energy consumption. The use of hourly data allows for a detailed examination of daily and weekly cycles, while the 18-year time span enables the identification of long-term trends and changes in energy usage patterns.\n\nThe ERCOT LOAD dataset can be leveraged for various applications, including:\n\n1. Time series forecasting: By analyzing the historical energy load data, researchers can develop accurate models to predict future energy consumption patterns, helping utilities and policymakers make informed decisions.\n2. Frequency analysis: The dataset can be used to study the frequency components of energy load, providing insights into the underlying drivers of energy consumption.\n3. Multi-head cross-attention: This technique can be applied to the ERCOT LOAD dataset to identify complex relationships between different regions and time periods, enabling a more nuanced understanding of energy load dynamics.\n\nOverall, the ERCOT LOAD dataset is a rich resource for researchers and analysts interested in energy load analysis, time series forecasting, and frequency analysis. Its comprehensive scope and duration make it an ideal candidate for a wide range of applications, from short-term forecasting to long-term trend analysis.", "id": "ERCOT LOAD", "label": "ERCOT LOAD", "shape": "dot", "size": 10, "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502,e067234b24b0625a3f95ecc18035f915", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"IN-DOMAIN EVALUATION\" refers to a type of evaluation metric used in machine learning. Specifically, it involves evaluating a model on a dataset that is similar to the one it was trained on. This process is crucial in assessing the model\u0027s performance and generalizability on data that is representative of the real-world scenarios it is intended to address.\n\nIn the context of machine learning, in-domain evaluation is a critical step in model development and deployment. It helps to identify the model\u0027s strengths and weaknesses, and to refine its performance on data that is similar to the one it was trained on. This, in turn, enables the model to make more accurate predictions and decisions in real-world applications.\n\nOverall, in-domain evaluation is an essential aspect of machine learning model development, and it plays a vital role in ensuring that models are effective and reliable in their intended domains.", "id": "IN-DOMAIN EVALUATION", "label": "IN-DOMAIN EVALUATION", "shape": "dot", "size": 10, "source_id": "63e284ec7e76fa859cc46b72d3746654,e067234b24b0625a3f95ecc18035f915", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"WIKI DAILY\" dataset is a subset of the TimesFM pretraining dataset, specifically designed to capture daily page views on the top-100k English Wikipedia articles. This dataset is a valuable resource for researchers and practitioners interested in time series analysis and forecasting, particularly in the context of web traffic and online engagement.\n\nThe \"WIKI DAILY\" dataset is likely to be useful for tasks such as long-term series forecasting, frequency analysis, and multi-head cross-attention, which are commonly employed in machine learning and time series forecasting applications. The dataset\u0027s focus on daily page views on top Wikipedia articles also suggests that it may be relevant for studies on information diffusion, online behavior, and user engagement.\n\nOverall, the \"WIKI DAILY\" dataset appears to be a rich and informative resource for researchers and practitioners interested in understanding online behavior and web traffic patterns, particularly in the context of Wikipedia and other online platforms.", "id": "WIKI DAILY", "label": "WIKI DAILY", "shape": "dot", "size": 10, "source_id": "2565ae205d4c98342168bb67a4f7a309,269a6f95aee3c9e28d0290fd10ed476d,a875a1c0bede47a1c4e3823be81c42c6", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe English Wikipedia is a dataset containing information on the top-100k English Wikipedia articles. It serves as the source of the Wiki Daily dataset, providing a wealth of information for analysis and research. This dataset is likely to be of interest to researchers and data scientists working in the field of natural language processing, machine learning, and time series analysis, given its potential applications in understanding online behavior, user engagement, and content trends.\n\nGiven the context of the English Wikipedia as a dataset, it is likely that the information contained within it is related to the structure and dynamics of online communities, user interactions, and content creation. The Wiki Daily dataset, derived from the English Wikipedia, may provide insights into daily patterns and trends in online behavior, such as user engagement, article views, and editing activity.\n\nOverall, the English Wikipedia dataset and the Wiki Daily dataset offer a valuable resource for researchers and data scientists seeking to understand the complexities of online communities and the dynamics of online behavior.", "id": "ENGLISH WIKIPEDIA", "label": "ENGLISH WIKIPEDIA", "shape": "dot", "size": 10, "source_id": "2565ae205d4c98342168bb67a4f7a309,a875a1c0bede47a1c4e3823be81c42c6", "type": "ENTITY"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"GEOMETRIC MEAN\" is a metric mentioned in the text, specifically in Figure 2a. \n\nThis summary is derived from the description list provided, which directly mentions the entity \"GEOMETRIC MEAN\" and its relation to the text, specifically Figure 2a.", "id": "GEOMETRIC MEAN", "label": "GEOMETRIC MEAN", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7,892b821ed44c13c4d09e0ceedac3051d", "type": ""}, {"color": "#97c2fc", "description": "Teraflops refer to a unit of measurement for computing power", "id": "TERAFLOPS", "label": "TERAFLOPS", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "GM is a metric used to evaluate the performance of a model, representing the mean scaled MAE", "id": "GM", "label": "GM", "shape": "dot", "size": 10, "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "type": "METRIC"}, {"color": "#97c2fc", "description": "Metrics are numerical values used to evaluate the performance of a machine learning model", "id": "METRICS", "label": "METRICS", "shape": "dot", "size": 10, "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Zero-shot forecasts refer to predictions made without any prior knowledge or training data", "id": "ZERO-SHOT FORECASTS", "label": "ZERO-SHOT FORECASTS", "shape": "dot", "size": 10, "source_id": "500710c8a95f1310d383fa71fd806c1c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"MSE\" can be described as follows:\n\nMSE, or Mean Squared Error, is a metric used to evaluate the performance of a model by measuring the difference between predicted and actual values. It is a widely used metric in various fields, including machine learning and time series forecasting, to assess the accuracy of a model\u0027s predictions. MSE is calculated by taking the average of the squared differences between predicted and actual values, providing a quantitative measure of a model\u0027s performance.\n\nThis description is based on the information collected from all the descriptions provided, and it resolves any potential contradictions by presenting a unified and coherent summary of the entity \"MSE\". The description is written in third person and includes relevant information from the nearby text, as requested.", "id": "MSE", "label": "MSE", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458,1d6fb60c5060c25ae4791b03a0513a7f,41fe893a178ebc8790ef4da83da5ab6e,919ff66615400ea06113a2a59ff34ef0,9c155897f3e35902f57696e0abaeb161,e900311a447985c0967f87fff49c58b8,fd1092903d83bf6e90a6caa371d7c514", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data is as follows:\n\nThe entity \"SMAPE\" refers to the Symmetric Mean Absolute Percentage Error, a metric used to evaluate the performance of time series forecasting models. Specifically, SMAPE is utilized to assess the accuracy of models for short-term time series forecasting, as indicated by its application in tables and other evaluation contexts. This metric is a measure of the average absolute percentage error between predicted and actual values, providing a symmetric and percentage-based evaluation of model performance.\n\nThe use of SMAPE is widespread in the field of time series forecasting, particularly for evaluating the performance of models in short-term forecasting scenarios. Its application is evident in various contexts, including tables and academic papers, where it is used to compare the performance of different models and identify areas for improvement.\n\nOverall, SMAPE is a crucial metric in the field of time series forecasting, providing a standardized and widely accepted measure of model performance. Its use is essential for researchers and practitioners seeking to evaluate and improve the accuracy of their time series forecasting models.", "id": "SMAPE", "label": "SMAPE", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458,d4551c2839eaa68a7cb7324089956581,d540ee970ce7debedc6b1b95e9c88be8", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"MONASH\" refers to a dataset used for various purposes, including testing and evaluation, testing models, time-series forecasting, and evaluating specific models such as PatchTST and TimesFM. Additionally, \"MONASH\" can also refer to an institution or university, suggesting that the dataset may be associated with or derived from this educational institution.\n\nThe dataset, known as \"MONASH,\" is likely used in academic or research settings, given the context of the descriptions provided. The fact that it is used for testing and evaluating models, as well as for time-series forecasting, implies that it is a valuable resource for researchers and developers in the field of machine learning and time series analysis.\n\nThe mention of specific models, such as PatchTST and TimesFM, suggests that the dataset is used to evaluate the performance of these models, which are likely designed for time-series forecasting tasks. This further reinforces the idea that the dataset is used in the context of time-series analysis and forecasting.\n\nOverall, the entity \"MONASH\" appears to be a multifaceted concept, encompassing both a dataset and an institution or university. The dataset is used for various purposes, including testing and evaluation, time-series forecasting, and model evaluation, while the institution or university is likely associated with the creation or use of the dataset.", "id": "MONASH", "label": "MONASH", "shape": "dot", "size": 10, "source_id": "28b097d339554431fa14e113c98ed49e,39365aee753fb73130c208fdf4046bb7,41b0bd14ce7ff7419b7ee1f78b4701ae,892b821ed44c13c4d09e0ceedac3051d,cd4ff69415c7b37cec643f8289d1c98d,dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "DATASET"}, {"color": "#97c2fc", "description": "Julien Herzen is an author who has published a paper on user-friendly modern machine learning for time series", "id": "JULIEN HERZEN", "label": "JULIEN HERZEN", "shape": "dot", "size": 10, "source_id": "a73df99fe49b288e1c8751be2008b191", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"ETT\" refers to a dataset containing oil temperatures and other covariates of electrical transformers from two stations in China. It is used for various purposes, including testing models, time-series forecasting, and evaluating the performance of machine learning models for time-series forecasting. Specifically, ETT is used to evaluate the PatchTST and TimesFM models. The dataset is also utilized in an experiment to assess the effectiveness of these models. Additionally, ETT is described as a dataset of energy usage data, which further supports its relevance to time-series forecasting and energy-related applications. Overall, ETT is a dataset used for evaluating and testing machine learning models, particularly in the context of time-series forecasting and energy usage data.", "id": "ETT", "label": "ETT", "shape": "dot", "size": 10, "source_id": "28b097d339554431fa14e113c98ed49e,39365aee753fb73130c208fdf4046bb7,50eeacd99c68b2581be90310bedcbc2c,6a222f9ed7fcf5fc945dfc22e16a3502,7cb1ca3f60421fbd20d6ae39bbf2b296,892b821ed44c13c4d09e0ceedac3051d,dc2c05938eb6dbe0217f4c9e6b111e2a,e067234b24b0625a3f95ecc18035f915", "type": "DATASET"}, {"color": "#97c2fc", "description": "Size Monash refers to the size or scope of the Monash institution", "id": "SIZE MONASH", "label": "SIZE MONASH", "shape": "dot", "size": 10, "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The ETT benchmark is an indicator reflective of long-term electric power deployment", "id": "ELECTRICITY TRANSFORMER TEMPERATURE", "label": "ELECTRICITY TRANSFORMER TEMPERATURE", "shape": "dot", "size": 10, "source_id": "50eeacd99c68b2581be90310bedcbc2c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"CHINA\" is a country that is included in the Exchange Rate compilation. Specifically, the ETT benchmark is sourced from two counties in China, indicating that the country plays a significant role in the compilation of exchange rates. This information suggests that China\u0027s economic data and exchange rates are being used as a benchmark for comparison and analysis.\n\nIt is worth noting that the context of the Exchange Rate compilation and the ETT benchmark suggests that the data is related to economic and financial analysis, possibly in the context of international trade and currency exchange. However, without further information, it is difficult to provide a more specific or detailed summary.\n\nOverall, the summary provides a clear and concise description of the entity \"CHINA\" and its relationship to the Exchange Rate compilation and the ETT benchmark.", "id": "CHINA", "label": "CHINA", "shape": "dot", "size": 10, "source_id": "50eeacd99c68b2581be90310bedcbc2c,85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"ZERO-SHOT EVALUATION\" refers to a method used to evaluate a model\u0027s performance on a task without any prior training. This type of evaluation metric is commonly used in machine learning to assess a model\u0027s ability to generalize to new, unseen data. Specifically, \"ZERO-SHOT EVALUATION\" involves evaluating a model on a dataset that it has not seen before, which is a key aspect of its definition. This process allows researchers to gauge a model\u0027s ability to adapt to new situations and tasks without requiring additional training data. Overall, \"ZERO-SHOT EVALUATION\" is a crucial concept in machine learning that enables the evaluation of a model\u0027s performance in real-world scenarios.", "id": "ZERO-SHOT EVALUATION", "label": "ZERO-SHOT EVALUATION", "shape": "dot", "size": 10, "source_id": "63e284ec7e76fa859cc46b72d3746654,892b821ed44c13c4d09e0ceedac3051d,e067234b24b0625a3f95ecc18035f915", "type": "EVALUATION"}, {"color": "#97c2fc", "description": "Pretrained model is a model that has been trained on a large dataset and can be fine-tuned for a specific task", "id": "PRETRAINED MODEL", "label": "PRETRAINED MODEL", "shape": "dot", "size": 10, "source_id": "892b821ed44c13c4d09e0ceedac3051d", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"CATBOOST\" is a machine learning model that serves two primary purposes. Firstly, it is utilized for time-series forecasting, indicating its capability to analyze and predict future values in a sequence of data points. Secondly, it is employed for evaluation purposes, suggesting its role in assessing the performance of other models or algorithms.\n\nThis summary is based on the information provided in the description list, which collectively paints a picture of CATBOOST as a versatile machine learning model with applications in both time-series forecasting and evaluation.", "id": "CATBOOST", "label": "CATBOOST", "shape": "dot", "size": 10, "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae,892b821ed44c13c4d09e0ceedac3051d", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "LOCAL FORECASTING METHOD", "label": "LOCAL FORECASTING METHOD", "shape": "dot", "size": 10, "source_id": "f0c52387a7b3a5c3850fe6991f0a7c83", "type": ""}, {"color": "#97c2fc", "description": "Arithmetic mean is a metric mentioned in the text, specifically Figure 4", "id": "ARITHMETIC MEAN", "label": "ARITHMETIC MEAN", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7", "type": "METRIC"}, {"color": "#97c2fc", "description": "ODZ+16 is a model mentioned in the text, specifically Net", "id": "ODZ+16", "label": "ODZ+16", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "NET", "label": "NET", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7", "type": ""}, {"color": "#97c2fc", "description": "Monash Huggingface repository is a repository mentioned in the text, specifically 5", "id": "MONASH HUGGINGFACE REPOSITORY", "label": "MONASH HUGGINGFACE REPOSITORY", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7", "type": "REPOSITORY"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMachine Learning for IoT is a field of research that focuses on applying machine learning techniques to IoT data. It is a workshop at ICLR 2023, which suggests that it is a topic of interest in the academic community. The field of research involves the application of machine learning techniques to Internet of Things (IoT) devices, enabling the analysis and processing of vast amounts of data generated by these devices.\n\nThe primary language of the text related to Machine Learning for IoT is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers. The language is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nOverall, Machine Learning for IoT is a rapidly growing field that aims to leverage machine learning techniques to extract insights and value from IoT data, with applications in various domains such as smart cities, industrial automation, and healthcare.", "id": "MACHINE LEARNING FOR IOT", "label": "MACHINE LEARNING FOR IOT", "shape": "dot", "size": 10, "source_id": "3e0985c172a09bb6c99e305b9f40a513,71f873c12230e6ede47583d81eb85e23,9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1", "type": "WORKSHOP"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"PERCEPTION\" is a complex process that involves interpreting sensory information. It is a multifaceted concept that encompasses the ability to understand and make sense of data. In essence, perception refers to the process of interpreting and understanding sensory information, which is a fundamental aspect of human cognition.\n\nThis summary is derived from the provided descriptions, which are largely consistent in their definition of perception. The repetition of the phrase \"perception refers to the process of interpreting sensory information\" suggests a high degree of agreement among the descriptions. The additional description \"perception refers to the process of interpreting and understanding data\" provides further insight into the concept, highlighting its relevance to data interpretation and understanding.\n\nOverall, the summary provides a clear and concise definition of the entity \"PERCEPTION,\" highlighting its key aspects and characteristics.", "id": "PERCEPTION", "label": "PERCEPTION", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23,9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The Monash Time Series Forecasting Repository is a source of datasets used in the collection", "id": "MONASH TIME SERIES FORECASTING REPOSITORY", "label": "MONASH TIME SERIES FORECASTING REPOSITORY", "shape": "dot", "size": 10, "source_id": "63e284ec7e76fa859cc46b72d3746654", "type": "SOURCE"}, {"color": "#97c2fc", "description": "The M-competitions are a source of datasets used in the collection", "id": "M-COMPETITIONS", "label": "M-COMPETITIONS", "shape": "dot", "size": 10, "source_id": "63e284ec7e76fa859cc46b72d3746654", "type": "SOURCE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nKAGGLE is a platform that hosts various competitions, including the Tourism Forecasting competition. It serves as a source of public domain datasets used in the collection. The platform is utilized for hosting competitions and providing access to datasets, which are essential for data-driven research and analysis.\n\nThe summary is based on the provided descriptions, which are not contradictory. The information is written in a formal and academic tone, suggesting that it is from a research paper or a technical document. The language used is English, as indicated by the presence of technical terms, mathematical equations, and references to academic papers.", "id": "KAGGLE", "label": "KAGGLE", "shape": "dot", "size": 10, "source_id": "63e284ec7e76fa859cc46b72d3746654,a875a1c0bede47a1c4e3823be81c42c6", "type": "SOURCE"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"TIMESTAMP\" can be generated as follows:\n\nThe entity \"TIMESTAMP\" refers to a concept used to prevent information leakage in datasets, which is a measure of time or sequence. It represents a specific point in time used to label data as anomalous or not. In essence, a timestamp is a temporal identifier that provides context to the data, enabling the distinction between normal and abnormal patterns. This concept is crucial in various applications, including data analysis, machine learning, and time series forecasting, where accurate labeling of data is essential for model development and evaluation.\n\nThe use of timestamps allows for the identification of anomalies, which can be critical in domains such as finance, healthcare, and cybersecurity. By labeling data as anomalous or not, timestamps facilitate the development of models that can detect and respond to unusual patterns, thereby enhancing the overall performance and reliability of the system.\n\nIn the context of machine learning and time series forecasting, timestamps play a vital role in enabling the analysis of temporal relationships and patterns within the data. By incorporating timestamps into the data, analysts and researchers can gain insights into the underlying dynamics of the system, making it possible to develop more accurate and effective models.\n\nOverall, the concept of timestamp is a fundamental aspect of data analysis and machine learning, enabling the accurate labeling and analysis of data, which is critical for various applications and domains.", "id": "TIMESTAMP", "label": "TIMESTAMP", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,1b51ec337efd822ca3a0b3eb819c1b91,4bdf596e75e1cb06d11b25e95491037e,532a6d434dab611a80aef1e94dd2fb45", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"TABLE 2\" is a document that provides a list of datasets used for empirical evaluation. Specifically, it is a table presenting the results of an experiment, which suggests that it contains quantitative data and statistical analysis. The table likely includes information about the performance of various machine learning models or time series forecasting techniques on different datasets, which is consistent with the context of empirical evaluation in the field of machine learning and time series analysis. Overall, \"TABLE 2\" appears to be a crucial component of the research paper or technical document, providing valuable insights into the effectiveness of different approaches in the field.", "id": "TABLE 2", "label": "TABLE 2", "shape": "dot", "size": 10, "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502,ad421ec9ba83531336aaf3bec414b3d5,e5e4a8f03f502fada5b17cba5dc942ba", "type": "TABLE"}, {"color": "#97c2fc", "description": "Evaluation settings refer to the criteria used to evaluate the performance of a model or algorithm", "id": "EVALUATION SETTINGS", "label": "EVALUATION SETTINGS", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "DOMAIN", "label": "DOMAIN", "shape": "dot", "size": 10, "source_id": "51ee17c1f0212d4c94010d8e376f649b,53dbeea6d05c3695460c71f89f468def", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"LAG-LAMMA\" can be generated as follows:\n\nLAG-LAMMA is a type of machine learning model that is used for fine-tuning and pretraining. It is specifically designed for time series forecasting tasks and has been used for fine-tuning forecasting tasks. The model performs better than local statistical models, but it is worth noting that it can perform worse than Chronos models on some datasets. LAG-LAMMA is a pretrained model that has been applied to the 7/20 datasets, as indicated by the phrase \"Lag-Llama on the 7/20 datasets.\" Overall, LAG-LAMMA is a versatile model that can be used for various time series forecasting tasks, but its performance may vary depending on the specific dataset and comparison model used.\n\nThis summary is based on the information collected from all the descriptions provided, and it resolves any potential contradictions by presenting a balanced view of the model\u0027s capabilities and limitations.", "id": "LAG-LAMMA", "label": "LAG-LAMMA", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,3ba0bc9230706fb8f4a61d16ecf8fd26,4bd5a8e9285aae7ca7363c8e61ba361c,56de1d5a5b467101344afa4248d829dc,6c11bd339c9630f1d61f2024e90bce5e,990578022879395d00b7a5b229863c2f,c5dc13d7191b625e7373e79907b5782a,c60a8238db3d56eff9cc9692e7ac5b1c", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"Appendix A.5.2 is a document mentioned in the text, specifically in the context of datasets. It is a section within this document that reports the results on the Monash dataset, providing detailed information and analysis related to this specific dataset.\"\n\nThis summary includes all the provided descriptions and resolves any potential contradictions, providing a clear and coherent understanding of the entity \"APPENDIX A.5.2\" and its relation to the Monash dataset.", "id": "APPENDIX A.5.2", "label": "APPENDIX A.5.2", "shape": "dot", "size": 10, "source_id": "28b097d339554431fa14e113c98ed49e,2bc70082c142ee2ef5d6a941611505b7", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Appendix A.5.3 is a section in the document that reports the results on the ETT dataset", "id": "APPENDIX A.5.3", "label": "APPENDIX A.5.3", "shape": "dot", "size": 10, "source_id": "28b097d339554431fa14e113c98ed49e", "type": "SECTION"}, {"color": "#97c2fc", "description": "Mean MAE is a metric mentioned in the text, specifically prior work", "id": "MEAN MAE", "label": "MEAN MAE", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe \"NAIVE BASELINE\" is a simple machine learning model that serves as a baseline for comparison in various contexts. It is characterized by making a constant prediction, specifically by predicting the last value in the context repeatedly. This model is mentioned in the text as a reference point for evaluating the performance of other models, particularly in the context of time series forecasting and long-term series forecasting. The \"NAIVE BASELINE\" model is a straightforward approach that can be used as a starting point for more complex models, and its simplicity makes it a useful benchmark for assessing the effectiveness of other models.\n\nThis summary is based on the information provided in the description list, which collectively paint a picture of the \"NAIVE BASELINE\" model as a simple, yet useful, machine learning model. The summary is written in the third person and includes the entity name \"NAIVE BASELINE\" to provide context.", "id": "NAIVE BASELINE", "label": "NAIVE BASELINE", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7,41b0bd14ce7ff7419b7ee1f78b4701ae,7cb1ca3f60421fbd20d6ae39bbf2b296", "type": "MODEL"}, {"color": "#97c2fc", "description": "Multivariate datasets are datasets that contain multiple time-series", "id": "MULTIVARIATE DATASETS", "label": "MULTIVARIATE DATASETS", "shape": "dot", "size": 10, "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "type": "DATASET"}, {"color": "#97c2fc", "description": "PatchTST is a model that places 1st in terms of point forecasting performance", "id": "PATCH-TST", "label": "PATCH-TST", "shape": "dot", "size": 10, "source_id": "532a6d434dab611a80aef1e94dd2fb45", "type": "MODEL"}, {"color": "#97c2fc", "description": "AAAI Conference on Artificial Intelligence is a conference", "id": "AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE", "label": "AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "CONFERENCE"}, {"color": "#97c2fc", "description": "Tab. 19 is a table mentioned in the text, which is likely a reference to a previous table or figure", "id": "TAB. 19", "label": "TAB. 19", "shape": "dot", "size": 10, "source_id": "fd1092903d83bf6e90a6caa371d7c514", "type": "TABLE"}, {"color": "#97c2fc", "description": "Tab. 20 is a table mentioned in the text, which is likely a reference to a previous table or figure", "id": "TAB. 20", "label": "TAB. 20", "shape": "dot", "size": 10, "source_id": "fd1092903d83bf6e90a6caa371d7c514", "type": "TABLE"}, {"color": "#97c2fc", "description": "", "id": "LAGLLAMA", "label": "LAGLLAMA", "shape": "dot", "size": 10, "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "type": ""}, {"color": "#97c2fc", "description": "", "id": "TIAN ZHANG", "label": "TIAN ZHANG", "shape": "dot", "size": 10, "source_id": "98c6b5003112ab7110e45414a2fa468b", "type": ""}, {"color": "#97c2fc", "description": "", "id": "FORMER", "label": "FORMER", "shape": "dot", "size": 10, "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c", "type": ""}, {"color": "#97c2fc", "description": "Informer baselines are a set of models used for comparison with the TimesFM model", "id": "INFORMER BASELINES", "label": "INFORMER BASELINES", "shape": "dot", "size": 10, "source_id": "673b0feee6eb5843954defc670e4ba29", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nZiqing Ma is a researcher and author who has published several papers on time series forecasting. Specifically, Ziqing Ma is an author of the papers \"Sadi: A self-adaptive decomposed interpretable framework for electric load forecasting under extreme events\" and \"Transformers in time series: A survey.\" Additionally, Ziqing Ma is associated with a paper on \"Sadi: A self-adaptive decomposed interpretable framework for time series forecasting.\" These publications suggest that Ziqing Ma\u0027s research focuses on time series forecasting, with a particular emphasis on developing interpretable and adaptive frameworks for electric load forecasting and other applications.\n\nThe papers authored by Ziqing Ma likely involve the use of advanced machine learning techniques, such as transformers, and may incorporate frequency analysis and multi-head cross-attention mechanisms. The research may also draw on the work of other authors and conferences, as indicated by the presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR.\" Overall, Ziqing Ma\u0027s research contributions demonstrate expertise in time series forecasting and machine learning, with a focus on developing innovative and interpretable models for real-world applications.", "id": "ZIQING MA", "label": "ZIQING MA", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,97c1f318683bb63131ece0a51f096c5b,98c6b5003112ab7110e45414a2fa468b,a4bb4cfc468e2f87ad5d8a4b451bcbbf,a69a914fb6c895c7202532b69ad3e094", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"TIAN ZHOU\":\n\nTIAN ZHOU is a researcher and author who has made significant contributions to the field of time series forecasting. Specifically, he has authored several research papers on this topic, including \"Sadi: A self-adaptive decomposed interpretable framework for electric load forecasting under extreme events\" and \"Transformers in time series: A survey\". Additionally, he has published a paper on \"Sadi: A self-adaptive decomposed interpretable framework for time series forecasting\", which further highlights his expertise in this area.\n\nTIAN ZHOU\u0027s research focuses on developing innovative frameworks and techniques for time series forecasting, with a particular emphasis on interpretability and adaptability. His work has been recognized through the publication of his research papers in reputable academic venues.\n\nOverall, TIAN ZHOU is a prominent figure in the field of time series forecasting, and his research has the potential to make a significant impact on the development of more accurate and reliable forecasting models.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which is consistent with the formal and academic tone of TIAN ZHOU\u0027s research papers.\n* The text also mentions the use of technical terms, mathematical equations, and references to academic papers, which are all written in English and suggest that TIAN ZHOU\u0027s research is grounded in established academic traditions.\n\nNote: The information provided is based solely on the descriptions related to TIAN ZHOU and does not include any additional information from the nearby text.", "id": "TIAN ZHOU", "label": "TIAN ZHOU", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,97c1f318683bb63131ece0a51f096c5b,a4bb4cfc468e2f87ad5d8a4b451bcbbf,a69a914fb6c895c7202532b69ad3e094", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "International Conference on Machine Learning is a conference where research papers are presented", "id": "INTERNATIONAL CONFERENCE ON MACHINE LEARNING", "label": "INTERNATIONAL CONFERENCE ON MACHINE LEARNING", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "EVENT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe Monash datasets are a collection of datasets specifically designed for benchmarking and evaluating the performance of various supervised long-horizon forecasting methods. These datasets are primarily used for time-series forecasting, allowing researchers and practitioners to assess the efficacy of different models and techniques in accurately predicting future values. The Monash datasets are a valuable resource for the time-series forecasting community, providing a standardized and comprehensive set of data for model evaluation and comparison.\n\nThis summary incorporates information from all the provided descriptions, resolving any potential contradictions and presenting a coherent overview of the Monash datasets. The summary highlights the primary purpose of the datasets, which is to facilitate the evaluation of time-series forecasting models, and emphasizes their importance in the field of time-series forecasting.", "id": "MONASH DATASETS", "label": "MONASH DATASETS", "shape": "dot", "size": 10, "source_id": "e6ec99a117b9abd42452ff51cd6e8f0b,efb7975581b22620b8277417edeee3e9,f7ce89346ea6715560163ef3a630a5df", "type": "DATASET"}, {"color": "#97c2fc", "description": "GPU memory is a resource mentioned in the text, as indicated by the use of the phrase \"GPU memory (in mebibyte)\"", "id": "GPU MEMORY", "label": "GPU MEMORY", "shape": "dot", "size": 10, "source_id": "7e97089185883c456c798ddc5ec86373", "type": "RESOURCE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"FLOPS\" is a unit of measurement for the computational complexity of models. Specifically, it stands for floating-point operations per second, which is a measure of computing power. This unit is used to evaluate the performance and efficiency of various models, particularly in the context of machine learning and high-performance computing.\n\nIn this context, FLOPS is an essential metric for understanding the computational requirements of complex models, such as those used in time series forecasting, frequency analysis, and multi-head cross-attention. The use of FLOPS as a unit of measurement allows researchers and developers to compare the performance of different models and optimize their computational complexity for improved efficiency and accuracy.\n\nOverall, FLOPS is a critical concept in the field of computational complexity and high-performance computing, and its understanding is essential for developing and evaluating efficient models for various applications, including time series forecasting and long-term series forecasting.", "id": "FLOPS", "label": "FLOPS", "shape": "dot", "size": 10, "source_id": "39365aee753fb73130c208fdf4046bb7,cd4ff69415c7b37cec643f8289d1c98d,f7ce89346ea6715560163ef3a630a5df", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Darts datasets are a set of datasets used to evaluate the performance of time-series forecasting models", "id": "DARTS DATASETS", "label": "DARTS DATASETS", "shape": "dot", "size": 10, "source_id": "e6ec99a117b9abd42452ff51cd6e8f0b", "type": "DATASET"}, {"color": "#97c2fc", "description": "Mask sampling strategy is a type of training method used for machine learning models", "id": "MASK SAMPLING STRATEGY", "label": "MASK SAMPLING STRATEGY", "shape": "dot", "size": 10, "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "type": "TRAINING METHOD"}, {"color": "#97c2fc", "description": "Training steps refer to the number of iterations or epochs used to train a model", "id": "TRAINING STEPS", "label": "TRAINING STEPS", "shape": "dot", "size": 10, "source_id": "56613213ed14f292e8ff44f4f0a8bab1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"VOCABULARY SIZE\" can be described as follows:\n\nThe vocabulary size refers to the number of unique words or tokens in a model\u0027s vocabulary, which is a fundamental concept in natural language processing (NLP) and machine learning. It represents the total number of distinct words or tokens used in a language or model, encompassing both the words and their variations, such as inflected forms, synonyms, and hyponyms. Vocabulary size is a critical parameter in various NLP applications, including language modeling, text classification, and language translation, as it affects the model\u0027s ability to capture the nuances of language and generate coherent text.\n\nIn the context of language modeling, vocabulary size is often used to determine the complexity of a model, with larger vocabulary sizes indicating more comprehensive language coverage. However, it also increases the computational requirements and memory usage of the model. Therefore, finding the optimal vocabulary size is a crucial task in NLP, balancing the trade-off between model complexity and performance.\n\nThe provided descriptions are consistent, and no contradictions were found. The information collected from all the descriptions has been incorporated into the summary, providing a comprehensive understanding of the entity \"VOCABULARY SIZE\".", "id": "VOCABULARY SIZE", "label": "VOCABULARY SIZE", "shape": "dot", "size": 10, "source_id": "56613213ed14f292e8ff44f4f0a8bab1,973ea1d37e8f6bb0ab004f360c04ddc6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"PREDICTION LENGTH\" can be described as follows:\n\nThe \"PREDICTION LENGTH\" is a crucial parameter in machine learning models, particularly in time series forecasting, that determines the number of time steps ahead that the model is asked to predict. It controls the length of the predictions made by a model, essentially specifying how far into the future the model should forecast. In essence, the \"PREDICTION LENGTH\" is the number of time steps to predict, which is a fundamental aspect of long-term series forecasting and frequency analysis.\n\nThis parameter is essential in various applications, including academic research papers and technical documents, where it is often used in conjunction with multi-head cross-attention mechanisms and other advanced techniques. The \"PREDICTION LENGTH\" is typically denoted by a specific value, which can be adjusted based on the requirements of the problem being solved.\n\nIn the context of academic conferences and journals, such as ICLR, AAAI, and PMLR, the \"PREDICTION LENGTH\" is often discussed in relation to the performance of machine learning models, particularly in terms of their ability to accurately forecast time series data over a specified prediction length.\n\nOverall, the \"PREDICTION LENGTH\" is a critical component of time series forecasting, and its proper selection is essential for achieving accurate and reliable predictions.", "id": "PREDICTION LENGTH", "label": "PREDICTION LENGTH", "shape": "dot", "size": 10, "source_id": "1ddbab2dca370c9ef7b5a724075518cc,51ee17c1f0212d4c94010d8e376f649b,fa01b75dccc556af8aca0d6c47e1970f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Multiplier is a value used to set the context length", "id": "MULTIPLIER", "label": "MULTIPLIER", "shape": "dot", "size": 10, "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Autoregressive sampling is a process used to sample from a model", "id": "AUTOREGRESSIVE SAMPLING", "label": "AUTOREGRESSIVE SAMPLING", "shape": "dot", "size": 10, "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Downstream performance refers to the performance of a model on a specific task or dataset", "id": "DOWNSTREAM PERFORMANCE", "label": "DOWNSTREAM PERFORMANCE", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "T5 family refers to a set of transformer-based language models developed by Google", "id": "T5 FAMILY", "label": "T5 FAMILY", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "MODEL"}, {"color": "#97c2fc", "description": "Backbone language model refers to the pre-trained language model that is used as the backbone of TIME-LLM", "id": "BACKBONE LANGUAGE MODEL", "label": "BACKBONE LANGUAGE MODEL", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTPUV5E6 is a type of Tensor Processing Unit (TPU), a specialized hardware device designed for machine learning computations. It is also referred to as a type of setup used for training models, suggesting that it is a configuration or architecture for training machine learning models. This setup is likely optimized for efficient computation and training of complex models, leveraging the capabilities of the TPU hardware.\n\nGiven the context of machine learning and the mention of TPUs, it is likely that TPUV5E6 is a specific implementation or variant of a TPU, designed for high-performance computing and training of large-scale machine learning models. The fact that it is used for training models implies that it is a critical component in the development and deployment of machine learning systems.\n\nOverall, TPUV5E6 appears to be a specialized hardware device and setup for machine learning computations, designed to facilitate efficient and effective training of complex models.", "id": "TPUV5E6", "label": "TPUV5E6", "shape": "dot", "size": 10, "source_id": "cd4ff69415c7b37cec643f8289d1c98d,f7ce89346ea6715560163ef3a630a5df", "type": "SETUP"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"TOKENS\" can be generated as follows:\n\nThe entity \"TOKENS\" refers to the individual units of text or data used in a language model, sequence, or time series. These tokens are discrete values or representations of the data, which can be words, numbers, or any other type of text or data unit. In the context of a language model, tokens are the basic building blocks of text, while in time series analysis, tokens refer to the individual units of data that make up the series. Overall, tokens are the fundamental units of text or data that are used to represent and analyze information in various models and applications.\n\nThis summary is based on the information collected from all the descriptions provided, and it resolves any potential contradictions by highlighting the common thread of tokens being individual units of text or data. The summary is written in the third person and includes the entity name \"TOKENS\" for context.", "id": "TOKENS", "label": "TOKENS", "shape": "dot", "size": 10, "source_id": "7e69d9444a2084b6452e291735bf5a49,85e4fbea9a08a0a663f3c5dc3f3a95a7,f622f27b5c3f52c6b04ada48bd63b03d,f7ce89346ea6715560163ef3a630a5df", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Rolling validation task is a type of evaluation task used to assess the performance of a model", "id": "ROLLING VALIDATION TASK", "label": "ROLLING VALIDATION TASK", "shape": "dot", "size": 10, "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "type": "TASK"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"PREDICTION HORIZONS\" refers to the different time scales at which a model is evaluated, specifically the length of time into the future for which predictions are made, and the time steps used for prediction. In other words, prediction horizons represent the various time intervals at which a model\u0027s performance is assessed, encompassing both the duration of the forecast and the frequency of prediction.\n\nThis summary is derived from the descriptions provided, which collectively convey the concept of prediction horizons as a multifaceted aspect of model evaluation. The descriptions highlight the importance of time scales, prediction duration, and time steps in understanding prediction horizons.\n\nThe information collected from all the descriptions is as follows:\n\n* Prediction horizons refer to the different time scales at which the model is evaluated.\n* Prediction horizons refer to the length of time into the future for which predictions are made.\n* Prediction horizons refer to the time steps used for prediction.\n\nThese descriptions are not contradictory, and the summary provided is a coherent and concise representation of the entity \"PREDICTION HORIZONS.\"", "id": "PREDICTION HORIZONS", "label": "PREDICTION HORIZONS", "shape": "dot", "size": 10, "source_id": "4ade7764ebe998b5f35a7fd2b58d4796,500710c8a95f1310d383fa71fd806c1c,51ee17c1f0212d4c94010d8e376f649b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Bins refer to the number of discrete intervals or categories in a time series", "id": "BINS", "label": "BINS", "shape": "dot", "size": 10, "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"PRECISION\" can be generated as follows:\n\nPrecision is a metric used to evaluate the performance of an anomaly detection model, as well as a measure of the accuracy or detail of data representation in a time series. It refers to the degree of accuracy or detail in a measurement or representation, and is often used to assess the accuracy of a model\u0027s predictions. The abbreviation \"P\" is also used to indicate Precision in tables, further emphasizing its importance in evaluating model performance.\n\nIn the context of time series analysis, Precision is crucial in ensuring that the data representation is accurate and detailed, which is essential for making informed decisions. The use of Precision as a metric in anomaly detection models highlights its significance in identifying and mitigating potential issues.\n\nOverall, Precision is a multifaceted concept that encompasses the accuracy and detail of data representation, model predictions, and measurement or representation, making it a vital component in various fields, including machine learning and time series analysis.", "id": "PRECISION", "label": "PRECISION", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,85e4fbea9a08a0a663f3c5dc3f3a95a7,a90c6ca26542ea5935f9d8d5bf3688a9,f622f27b5c3f52c6b04ada48bd63b03d,f6e57fa18831bcc732a631240536777a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe \"TRAINING DATASET\" is a dataset used to train a model, specifically referred to as TimeGPT. This dataset serves as the primary source of information for the model\u0027s learning and development process. In essence, the \"TRAINING DATASET\" is a crucial component in the model\u0027s ability to learn and make predictions or forecasts.\n\nThis summary is derived from the provided descriptions, which are consistent and non-contradictory. The information is written in a formal and academic tone, suggesting that it is from a research paper or technical document. The language used is English, as indicated by the presence of technical terms, mathematical equations, and references to academic papers.", "id": "TRAINING DATASET", "label": "TRAINING DATASET", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2,f7ce89346ea6715560163ef3a630a5df", "type": "DATASET"}, {"color": "#97c2fc", "description": "Global batch-size refers to the number of samples used to train a model", "id": "GLOBAL BATCH-SIZE", "label": "GLOBAL BATCH-SIZE", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Checkpoints refer to the saved models or states of a model during training", "id": "CHECKPOINTS", "label": "CHECKPOINTS", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Recent work refers to recent studies or papers on a specific topic", "id": "RECENT WORK", "label": "RECENT WORK", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "GD23 is a paper that discusses scaling studies in LLMs", "id": "GD23", "label": "GD23", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "PAPER"}, {"color": "#97c2fc", "description": "Autoregressive decoding is a technique used in time-series forecasting", "id": "AUTOREGRESSIVE DECODING", "label": "AUTOREGRESSIVE DECODING", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "ZCZX23 is a paper that discusses autoregressive decoding in time-series forecasting", "id": "ZCZX23", "label": "ZCZX23", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "PAPER"}, {"color": "#97c2fc", "description": "Future refers to a concept or time period that is yet to come", "id": "FUTURE", "label": "FUTURE", "shape": "dot", "size": 10, "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Output patch length is a parameter of the model, representing the length of the output patch", "id": "OUTPUT_PATCH_LEN", "label": "OUTPUT_PATCH_LEN", "shape": "dot", "size": 10, "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Input patch length is a parameter of the model, representing the length of the input patch", "id": "INPUT_PATCH_LEN", "label": "INPUT_PATCH_LEN", "shape": "dot", "size": 10, "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"MODEL CARD\" is a document that provides crucial information about a machine learning model. It serves as a comprehensive report that outlines the model\u0027s purpose, limitations, and potential biases. Specifically, a model card includes details about the model\u0027s architecture, the training data used to develop it, and its performance metrics. This document is essential for understanding the capabilities and constraints of a machine learning model, enabling users to make informed decisions about its deployment and potential applications.\n\nIn the context of machine learning, a model card is a critical component of model development and deployment. It facilitates transparency, accountability, and reproducibility by providing a clear and concise overview of the model\u0027s characteristics and performance. By examining a model card, users can identify potential biases, limitations, and areas for improvement, ultimately leading to more effective and responsible model development and deployment.\n\nThe information collected from the descriptions is as follows:\n\n* The purpose of a model card is to provide information about a machine learning model.\n* A model card includes details about the model\u0027s architecture, training data, and performance.\n* The document serves as a comprehensive report that outlines the model\u0027s purpose, limitations, and potential biases.\n\nThe contradictions in the descriptions are resolved by combining the information into a single, coherent summary. The empty descriptions are ignored, and only the relevant information is included in the summary.", "id": "MODEL CARD", "label": "MODEL CARD", "shape": "dot", "size": 10, "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296,9b150491b487d5b2da482652e2fe509d,a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Larger dataset regimes refer to the use of larger datasets to train and evaluate the model", "id": "LARGER DATASET REGIMES", "label": "LARGER DATASET REGIMES", "shape": "dot", "size": 10, "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Masked multi-head attention is a technique used to run the training process in parallel across several batches", "id": "MASKED MULTI-HEAD ATTENTION", "label": "MASKED MULTI-HEAD ATTENTION", "shape": "dot", "size": 10, "source_id": "6ea15432a841705c2e74cbc01f6004e9", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"F1 SCORE\" is a metric used to evaluate the performance of a model or software. It is a measure of the accuracy of a model, specifically designed to assess its ability to detect anomalies. The F1 score is a widely used metric in machine learning and data science, often employed to evaluate the performance of models on various tasks, including anomaly detection. The abbreviation \"F1\" is commonly used in tables and academic papers to refer to this metric.\n\nIn the context of machine learning and data science, the F1 score is a crucial evaluation metric that provides insights into the model\u0027s ability to balance precision and recall. It is often used in conjunction with other metrics, such as precision and recall, to provide a comprehensive understanding of the model\u0027s performance.\n\nOverall, the F1 score is a fundamental concept in machine learning and data science, and its use is widespread in various applications, including anomaly detection, classification, and regression tasks.", "id": "F1 SCORE", "label": "F1 SCORE", "shape": "dot", "size": 10, "source_id": "0e3a4048d3d693cb8fd969fd606f1e8a,5277ea101e78f34ea2c62fa10007b2ac,78ee4a3d7a2bffd4405a03d94a4f6cb1,9b35a2c607e0f4b8cbc9179f424f180a,a39d9d28126733c33db624ccc25f5782,f6e57fa18831bcc732a631240536777a", "type": ""}, {"color": "#97c2fc", "description": "Transformer-based encoder-decoder is a type of model architecture used in natural language processing and computer vision", "id": "TRANSFORMER-BASED ENCODER-DECODER", "label": "TRANSFORMER-BASED ENCODER-DECODER", "shape": "dot", "size": 10, "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "type": "MODEL"}, {"color": "#97c2fc", "description": "Feed-forward network is a type of neural network architecture used in machine learning", "id": "FEED-FORWARD NETWORK", "label": "FEED-FORWARD NETWORK", "shape": "dot", "size": 10, "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"SELF-CONDITIONING\" can be generated as follows:\n\nSELF-CONDITIONING is a concept and technique used in machine learning, particularly in the TranAD model, to improve the performance and robustness of a model. It involves using the model\u0027s own predictions as input, which is a type of training method that modifies the attention weights in the second phase. This technique is used to amplify errors and gain training stability, ultimately leading to improved generalization and robustness of the model.\n\nThe use of self-conditioning in machine learning is a method to condition the model on its own output, which enables the model to learn from its own predictions and improve its performance. This approach is particularly useful in the TranAD model, where it is used to modify the attention weights and improve the overall performance of the model.\n\nOverall, self-conditioning is a powerful technique in machine learning that enables models to learn from their own predictions and improve their performance, robustness, and generalization.\n\nRelevant information from the nearby text:\n\n* The text mentions that self-conditioning is used in the TranAD model, which suggests that it is a specific technique used in this model.\n* The text also mentions that self-conditioning is used to improve the performance of a model by conditioning it on its own output, which suggests that it is a general technique used in machine learning.\n* The text does not provide any information that contradicts the descriptions of self-conditioning, so the summary is based on the information provided in the descriptions.\n\nNote: The summary is written in third person and includes the entity name \"SELF-CONDITIONING\" to provide full context.", "id": "SELF-CONDITIONING", "label": "SELF-CONDITIONING", "shape": "dot", "size": 10, "source_id": "5277ea101e78f34ea2c62fa10007b2ac,78ee4a3d7a2bffd4405a03d94a4f6cb1,a39d9d28126733c33db624ccc25f5782,a65c0f1eae6e779357739df141f75d36,bd73ee0439609823e12a877840c6ebae", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Adversarial loss is a type of loss function used in machine learning to improve the robustness of a model to adversarial attacks", "id": "ADVERSARIAL LOSS", "label": "ADVERSARIAL LOSS", "shape": "dot", "size": 10, "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Reconstruction loss is a type of loss function used in machine learning to evaluate the performance of a model in terms of its ability to reconstruct the input data", "id": "RECONSTRUCTION LOSS", "label": "RECONSTRUCTION LOSS", "shape": "dot", "size": 10, "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time-step refers to a unit of time or a specific point in time", "id": "TIME-STEP", "label": "TIME-STEP", "shape": "dot", "size": 10, "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"AUTOTHETA\" can be generated as follows:\n\nAUTOTHETA is a statistical model primarily used for time series forecasting. It is a local statistical baseline method for benchmarking Chronos and is mentioned alongside other models such as Naive, Seasonal Naive, AutoETS, and AutoARIMA. AUTOTHETA is utilized for comparison purposes in the text and is considered a model for time series forecasting. The model\u0027s primary function is to provide a statistical approach to forecasting, making it a valuable tool in the field of time series analysis.\n\nThe information collected from all the descriptions has been combined to provide a single, coherent summary. The contradictions have been resolved, and the summary is written in the third person to provide a clear and concise description of the entity \"AUTOTHETA\". Relevant information from the nearby text has been incorporated to enrich the summary and provide a more comprehensive understanding of the entity.", "id": "AUTOTHETA", "label": "AUTOTHETA", "shape": "dot", "size": 10, "source_id": "1ddbab2dca370c9ef7b5a724075518cc,2565ae205d4c98342168bb67a4f7a309,5fa25d3abec59ccd2718e00c0e0eb440,a875a1c0bede47a1c4e3823be81c42c6,af36d1634490149b96980fb1dff57cd1,e900311a447985c0967f87fff49c58b8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Metric refers to a measure or evaluation criterion", "id": "METRIC", "label": "METRIC", "shape": "dot", "size": 10, "source_id": "e900311a447985c0967f87fff49c58b8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"HOURLY\" refers to a time series with a specific frequency and granularity. Specifically, it is characterized by a frequency of 8760, which is a common frequency for hourly time series data. This frequency is a result of the 24 hours in a day multiplied by 365 days in a year, resulting in 8760 hours per year. The \"HOURLY\" entity is also described as a granularity of time-series data, indicating that it represents a specific level of detail or resolution in the data. Furthermore, it is mentioned that \"HOURLY\" refers to a frequency of data or a time period, which reinforces the idea that it is a specific unit of time used to measure or analyze data.\n\nOverall, the \"HOURLY\" entity is a well-defined concept in the context of time series data, with a clear frequency and granularity that can be used for analysis and modeling purposes.", "id": "HOURLY", "label": "HOURLY", "shape": "dot", "size": 10, "source_id": "ac37bdb991d1513e44e4c5fc7a28b187,dc2c05938eb6dbe0217f4c9e6b111e2a,f8afc5a341360ab82dfbe9ebbb7f8e84", "type": "GRANULARITY"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"DAILY\" refers to a time series with a specific frequency and granularity. It is characterized by a frequency of 365, indicating a daily frequency of data collection or analysis. This frequency is typically associated with a time period of 24 hours, spanning from midnight to midnight. In the context of time-series data, \"DAILY\" represents a granularity level, signifying the frequency at which data is collected or observed. This entity is commonly used in various applications, including long-term series forecasting, frequency analysis, and multi-head cross-attention models.\n\nThe summary is written in third person and includes the entity name \"DAILY\" for context. It also incorporates relevant information from the provided descriptions, resolving any potential contradictions to provide a coherent and concise summary.", "id": "DAILY", "label": "DAILY", "shape": "dot", "size": 10, "source_id": "ac37bdb991d1513e44e4c5fc7a28b187,dc2c05938eb6dbe0217f4c9e6b111e2a,edab6fd342bc0a563fd98a50eb8b3462,f8afc5a341360ab82dfbe9ebbb7f8e84", "type": "GRANULARITY"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"LAG INDICES\" are a set of positive integers that play a crucial role in time series analysis. Specifically, they are used to specify the time intervals between the current and past values of a time series. This allows for the construction of lagged features, which are essential components in the tokenization scheme of Lag-Llama. In essence, lag indices enable the creation of lagged features that capture the temporal relationships within a time series, thereby facilitating more accurate and informative analysis.\n\nThe use of lag indices is particularly relevant in the context of time series forecasting, where understanding the relationships between past and present values is critical for making informed predictions. By incorporating lag indices into the tokenization scheme of Lag-Llama, researchers and analysts can leverage the power of lagged features to improve the accuracy and reliability of their models.\n\nIn summary, the \"LAG INDICES\" are a fundamental concept in time series analysis, enabling the construction of lagged features that capture the temporal relationships within a time series. Their application in Lag-Llama\u0027s tokenization scheme highlights their importance in facilitating more accurate and informative analysis, particularly in the context of time series forecasting.", "id": "LAG INDICES", "label": "LAG INDICES", "shape": "dot", "size": 10, "source_id": "55099ca8e21d1db3bdaf7bd04e590b72,ac37bdb991d1513e44e4c5fc7a28b187", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"QUARTERLY\" refers to a time series with a frequency of 4, indicating that it is a type of data that is collected or measured at regular intervals of 3 months. This frequency is typically associated with periods such as January to March, April to June, July to September, or October to December. As a granularity of time-series data, quarterly frequency is often used in short-term forecasting, particularly in datasets that require a medium-term perspective. Overall, the \"QUARTERLY\" entity encompasses a specific type of time series data that is characterized by its 3-month frequency and is commonly used in forecasting applications.", "id": "QUARTERLY", "label": "QUARTERLY", "shape": "dot", "size": 10, "source_id": "27efab80b405b365d8e9dd9834dd1ca8,ac37bdb991d1513e44e4c5fc7a28b187,dc2c05938eb6dbe0217f4c9e6b111e2a,edab6fd342bc0a563fd98a50eb8b3462", "type": "GRANULARITY"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"TOURISM\" can be generated as follows:\n\nThe entity \"TOURISM\" refers to the practice of traveling for pleasure or recreation, often involving the visitation of attractions, landmarks, and cultural events. This entity is also associated with a dataset used for the Kaggle Tourism Forecasting competition, suggesting that it is a topic of interest in the field of data science and machine learning. The dataset likely contains information related to tourism trends, patterns, and forecasting, which can be used to develop predictive models and inform decision-making in the tourism industry.\n\nIn the context of machine learning and time series forecasting, the entity \"TOURISM\" is likely to be associated with various technical terms and concepts, such as time series analysis, long-term series forecasting, and frequency analysis. The use of multi-head cross-attention mechanisms may also be relevant in modeling tourism-related data, given its ability to capture complex relationships and patterns in time series data.\n\nOverall, the entity \"TOURISM\" encompasses both the practice of traveling for pleasure or recreation and the associated dataset used for forecasting and predictive modeling.", "id": "TOURISM", "label": "TOURISM", "shape": "dot", "size": 10, "source_id": "a875a1c0bede47a1c4e3823be81c42c6,edab6fd342bc0a563fd98a50eb8b3462", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"WEEKLY\" refers to a time series with a specific frequency and time period. Specifically, it is characterized by a frequency of 52, indicating that the data is collected or observed on a weekly basis. This frequency is equivalent to a period of 7 days, typically spanning from Sunday to Saturday. In essence, the term \"WEEKLY\" encompasses both the frequency and duration of the time series, highlighting its periodic nature with a consistent interval of 7 days.\n\nThis summary integrates the provided descriptions, resolving any potential contradictions and providing a clear, coherent understanding of the entity \"WEEKLY\". The information is written in the third person, and the entity name is included for context.", "id": "WEEKLY", "label": "WEEKLY", "shape": "dot", "size": 10, "source_id": "ac37bdb991d1513e44e4c5fc7a28b187,edab6fd342bc0a563fd98a50eb8b3462,f8afc5a341360ab82dfbe9ebbb7f8e84", "type": "TIME"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"YEARLY\" refers to a specific granularity of time-series data, which is characterized by a period of 12 months, typically spanning from January to December. This granularity is commonly used in time-series analysis and forecasting, particularly in the context of short-term forecasting. In this context, yearly datasets are utilized to predict future values or trends over a relatively short period, often within a year or less.\n\nThis summary incorporates information from all the provided descriptions, resolving any potential contradictions and providing a coherent explanation of the entity \"YEARLY\". The summary highlights the key characteristics of yearly time-series data, including its duration and application in short-term forecasting.", "id": "YEARLY", "label": "YEARLY", "shape": "dot", "size": 10, "source_id": "27efab80b405b365d8e9dd9834dd1ca8,dc2c05938eb6dbe0217f4c9e6b111e2a,edab6fd342bc0a563fd98a50eb8b3462", "type": "GRANULARITY"}, {"color": "#97c2fc", "description": "10 minutes is a granularity of time-series data", "id": "10 MINUTES", "label": "10 MINUTES", "shape": "dot", "size": 10, "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "GRANULARITY"}, {"color": "#97c2fc", "description": "15 minutes is a granularity of time-series data", "id": "15 MINUTES", "label": "15 MINUTES", "shape": "dot", "size": 10, "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "GRANULARITY"}, {"color": "#97c2fc", "description": "200M is the size of a model used in the experiment", "id": "200M", "label": "200M", "shape": "dot", "size": 10, "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "MODEL SIZE"}, {"color": "#97c2fc", "description": "Appendix A.3 is a section of the paper that provides a finetuning study", "id": "APPENDIX A.3", "label": "APPENDIX A.3", "shape": "dot", "size": 10, "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "SECTION"}, {"color": "#97c2fc", "description": "ZNW+23 is a paper cited in the experiment", "id": "ZNW+23", "label": "ZNW+23", "shape": "dot", "size": 10, "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "PAPER"}, {"color": "#97c2fc", "description": "Downstream tasks refer to tasks that use the model as a foundation", "id": "DOWNSTREAM TASKS", "label": "DOWNSTREAM TASKS", "shape": "dot", "size": 10, "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "TASK"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"BIAS\" has two distinct descriptions. To create a comprehensive summary, we will analyze and reconcile these descriptions.\n\nThe first description states that \"Bias is a concern related to the use of the model.\" This suggests that bias is a potential issue that arises when using a model, possibly indicating a problem with the model\u0027s performance or fairness.\n\nThe second description describes bias as \"a learnable parameter used to adjust the attention matrix.\" This implies that bias is a parameter that can be learned and adjusted during the training process, specifically in the context of attention mechanisms.\n\nTo reconcile these descriptions, we can infer that the bias in question is related to the attention mechanism, and its primary concern is adjusting the attention matrix. This adjustment is likely aimed at improving the model\u0027s performance or fairness.\n\nConsidering the context of machine learning and time series forecasting, it is possible that the bias is used to mitigate issues such as:\n\n* Overfitting or underfitting in the attention mechanism\n* Imbalanced data distribution in the attention matrix\n* Inaccurate or biased attention weights\n\nHowever, without further information, it is difficult to provide a more specific explanation.\n\nIn summary, the entity \"BIAS\" is a learnable parameter used to adjust the attention matrix, which is a concern related to the use of the model. This bias is likely used to improve the model\u0027s performance or fairness, particularly in the context of attention mechanisms.\n\nRelevant information from the nearby text is not provided, so no additional information can be included in the summary.", "id": "BIAS", "label": "BIAS", "shape": "dot", "size": 10, "source_id": "a7604286fb84b26c53950861f9aca4b1,dc2c05938eb6dbe0217f4c9e6b111e2a", "type": "CONCERN"}, {"color": "#97c2fc", "description": "Bias table is a table used to store learnable biases", "id": "BIAS TABLE", "label": "BIAS TABLE", "shape": "dot", "size": 10, "source_id": "a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Attention matrix is a matrix used to compute the attention weights", "id": "ATTENTION MATRIX", "label": "ATTENTION MATRIX", "shape": "dot", "size": 10, "source_id": "a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\n\"Covariate handling\" refers to the practice of accounting for or adjusting for the influence of covariates on a machine learning model\u0027s predictions, which involves incorporating additional variables or features into a model to improve its accuracy and robustness. This process is crucial in machine learning, as covariates can significantly impact the model\u0027s performance and generalizability. By handling covariates effectively, data scientists and researchers can develop more reliable and accurate models that can better capture the underlying relationships between variables and make more informed predictions.\n\nIn the context of machine learning, covariate handling is essential for ensuring that the model\u0027s predictions are not biased by irrelevant or confounding variables. By accounting for these variables, researchers can develop models that are more robust and generalizable, and that can better capture the underlying patterns and relationships in the data. This is particularly important in applications where the model\u0027s predictions have significant consequences, such as in healthcare, finance, or social sciences.\n\nOverall, covariate handling is a critical aspect of machine learning model development, and its effective implementation can significantly improve the accuracy and reliability of machine learning models.", "id": "COVARIATE HANDLING", "label": "COVARIATE HANDLING", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d,a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Date features refer to variables or features that represent time or date information", "id": "DATE FEATURES", "label": "DATE FEATURES", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Model color is an example of a time-independent covariate", "id": "MODEL COLOR", "label": "MODEL COLOR", "shape": "dot", "size": 10, "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Sale days are an example of a time-varying covariate", "id": "SALE DAYS", "label": "SALE DAYS", "shape": "dot", "size": 10, "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Crime rates refer to the frequency or rate of criminal activity in a given area or population", "id": "CRIME RATES", "label": "CRIME RATES", "shape": "dot", "size": 10, "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "BIASED FORECASTS", "label": "BIASED FORECASTS", "shape": "dot", "size": 10, "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "type": ""}, {"color": "#97c2fc", "description": "Disproportionate impact refers to the unequal or unfair effects of a policy or action on a particular group or community", "id": "DISPROPORTIONATE IMPACT", "label": "DISPROPORTIONATE IMPACT", "shape": "dot", "size": 10, "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Data sources refer to the origin or source of the data used to train or test a machine learning model", "id": "DATA SOURCES", "label": "DATA SOURCES", "shape": "dot", "size": 10, "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Open weights release refers to the practice of releasing a machine learning model\u0027s weights or parameters in an open and transparent manner", "id": "OPEN WEIGHTS RELEASE", "label": "OPEN WEIGHTS RELEASE", "shape": "dot", "size": 10, "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "SOTA LLMs refer to the state-of-the-art large language models, which are the most advanced and effective models in a given field or task", "id": "SOTA LLMS", "label": "SOTA LLMS", "shape": "dot", "size": 10, "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "MODEL"}, {"color": "#97c2fc", "description": "TPUV5E refers to a specific type of computing device, a TPUv5e, which is used for machine learning computations", "id": "TPUV5E", "label": "TPUV5E", "shape": "dot", "size": 10, "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "DEVICE"}, {"color": "#97c2fc", "description": "Human-in-the-loop refers to the practice of involving human judgment or oversight in the decision-making process of a machine learning model", "id": "HUMAN-IN-THE-LOOP", "label": "HUMAN-IN-THE-LOOP", "shape": "dot", "size": 10, "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Limitations refer to the constraints or weaknesses of a machine learning model or approach", "id": "LIMITATIONS", "label": "LIMITATIONS", "shape": "dot", "size": 10, "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"FUTURE WORK\" can be described as follows:\n\nFuture work refers to the potential research or development directions that can be explored or pursued in a given field or task, with the aim of improving the model or approach. This encompasses tasks or projects that need to be done in the future to enhance the existing framework, which may involve exploring new research directions or developing new methodologies to address existing limitations.\n\nThe primary focus of future work is to identify and address potential gaps or areas for improvement in the current model or approach, with the ultimate goal of advancing the field or task at hand. This may involve conducting further research, developing new tools or techniques, or refining existing methods to achieve better results.\n\nIn the context of machine learning and time series forecasting, future work may involve exploring new architectures, developing more accurate forecasting models, or improving the interpretability of existing models. The specific directions for future work will depend on the goals and objectives of the project, as well as the current state of the field.\n\nOverall, future work is an essential component of any research or development project, as it allows researchers and developers to identify areas for improvement and pursue new directions that can lead to breakthroughs and advancements in the field.", "id": "FUTURE WORK", "label": "FUTURE WORK", "shape": "dot", "size": 10, "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1,f622f27b5c3f52c6b04ada48bd63b03d,f8afc5a341360ab82dfbe9ebbb7f8e84", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Testing set refers to the set of data used to test or evaluate a model or system", "id": "TESTING SET", "label": "TESTING SET", "shape": "dot", "size": 10, "source_id": "f8afc5a341360ab82dfbe9ebbb7f8e84", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data:\n\nThe entity \"LOSS FUNCTION\" is a mathematical formula used to evaluate the performance of a machine learning model. It is a crucial component in the development and training of machine learning models, as it provides a quantitative measure of the model\u0027s accuracy and effectiveness.\n\nIn the context of machine learning, the loss function plays a vital role in determining the model\u0027s performance, and it is often used in conjunction with other techniques such as time series analysis and frequency analysis to improve the model\u0027s forecasting capabilities.\n\nThe use of loss functions is a common practice in various machine learning applications, including long-term series forecasting, where the goal is to predict future values in a time series based on historical data. In such cases, the loss function is used to evaluate the model\u0027s performance and to adjust the model\u0027s parameters to minimize the error between the predicted and actual values.\n\nOverall, the loss function is a fundamental concept in machine learning, and its proper selection and implementation are critical to the development of accurate and effective machine learning models.\n\nRelevant information from the nearby text:\n\n* The text mentions the use of technical terms, mathematical equations, and references to academic papers, which suggests that the entity \"LOSS FUNCTION\" is a complex and technical concept.\n* The text also mentions the use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals, further indicating that the entity \"LOSS FUNCTION\" is a topic of interest in the academic community.\n* The text does not provide any contradictory information, and the summary is based on the provided description of the entity \"LOSS FUNCTION\".", "id": "LOSS FUNCTION", "label": "LOSS FUNCTION", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d,a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Framework refers to a set of tools, methods, or principles used to build or implement a system", "id": "FRAMEWORK", "label": "FRAMEWORK", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Joint universal representation refers to a single representation that combines multiple variables or features", "id": "JOINT UNIVERSAL REPRESENTATION", "label": "JOINT UNIVERSAL REPRESENTATION", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Zero-shot setting refers to a scenario where a model is trained on a specific task or dataset without any prior knowledge or experience", "id": "ZERO-SHOT SETTING", "label": "ZERO-SHOT SETTING", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "In-context refers to a scenario where a model is trained on a specific task or dataset within a specific context or environment", "id": "IN-CONTEXT", "label": "IN-CONTEXT", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables", "id": "LINEAR REGRESSION", "label": "LINEAR REGRESSION", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Residual model refers to a model that is used to predict the residual or difference between the actual and predicted values", "id": "RESIDUAL MODEL", "label": "RESIDUAL MODEL", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "MODEL"}, {"color": "#97c2fc", "description": "msMAPE (Mean Squared Percentage Error) is a metric used to evaluate the performance of a machine learning model", "id": "MSMAPE", "label": "MSMAPE", "shape": "dot", "size": 10, "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "type": "METRIC"}, {"color": "#97c2fc", "description": "Monash benchmarks are a set of datasets used to evaluate the performance of machine learning models for time-series forecasting", "id": "MONASH BENCHMARKS", "label": "MONASH BENCHMARKS", "shape": "dot", "size": 10, "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nGPT2 is a machine learning model primarily used for natural language processing. It is also utilized as a pre-trained language model for comparison purposes in experiments. The model\u0027s capabilities and performance are likely being evaluated and analyzed in the context of its application in natural language processing tasks.\n\nThis summary is based on the information provided in the description list, which collectively describe GPT2 as a machine learning model with a specific use case in natural language processing. The entity name \"GPT2\" is mentioned, and the descriptions are consistent in their portrayal of the model\u0027s purpose and application.", "id": "GPT2", "label": "GPT2", "shape": "dot", "size": 10, "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296,ad421ec9ba83531336aaf3bec414b3d5", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "CHRONOS FORECASTING", "label": "CHRONOS FORECASTING", "shape": "dot", "size": 10, "source_id": "56de1d5a5b467101344afa4248d829dc", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nMASE (Mean Absolute Scaled Error) is a metric used to evaluate the performance of time series forecasting models. It is a point forecasting metric that measures the performance of a model on a given dataset, specifically in the context of short-term time series forecasting. MASE is used to compare the performance of different models and is often used in conjunction with other metrics to provide a comprehensive evaluation of a model\u0027s performance. In particular, MASE is used to evaluate the performance of models on Benchmark II and is a type of metric used to assess the accuracy of time series forecasting models.\n\nThe use of MASE is indicated in the table, suggesting that it is a widely used and accepted metric in the field of time series forecasting. The fact that MASE is used to evaluate the performance of models in forecasting tasks, as well as its use in short-term time series forecasting, further emphasizes its importance in the field.\n\nOverall, MASE is a key metric in the evaluation of time series forecasting models, providing a standardized way to compare the performance of different models and identify areas for improvement.", "id": "MASE", "label": "MASE", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458,2eea45c6111e468189580a21686cb14a,6d66c2ea37e25b646a13d751c05a8e4d,a90c6ca26542ea5935f9d8d5bf3688a9,baa887ae552c6b3dac10f74896d8c4a2,c522ea3766ae5129c11b833252340695,d4551c2839eaa68a7cb7324089956581,d540ee970ce7debedc6b1b95e9c88be8,e37f4063d074e1697e1f539131b3d963", "type": "METRIC"}, {"color": "#97c2fc", "description": "The finetuned model weights are the weights of the model that have been updated during the finetuning process", "id": "FINE-TUNED MODEL WEIGHTS", "label": "FINE-TUNED MODEL WEIGHTS", "shape": "dot", "size": 10, "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "type": "MODEL WEIGHTS"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"SECTION 5\" is a document that provides a brief description of each dataset used for empirical evaluation. It is a part of a larger document that presents additional analysis. The document is written in English, as indicated by the use of technical terms, mathematical equations, and references to academic papers in English. The language is formal and academic, suggesting that the document is from a research paper or a technical document. The document likely contains information on time series analysis, long-term series forecasting, frequency analysis, and multi-head cross-attention, as these terms are mentioned in the context of the document. Additionally, the document may include citations to English-language academic papers and authors, further supporting the conclusion that the primary language of the document is English.", "id": "SECTION 5", "label": "SECTION 5", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9,e5e4a8f03f502fada5b17cba5dc942ba", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Table 13 is a table presenting the baseline numbers used in the experiment", "id": "TABLE 13", "label": "TABLE 13", "shape": "dot", "size": 10, "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "type": "TABLE"}, {"color": "#97c2fc", "description": "Table 14 is a table presenting the results of another experiment used as a comparison", "id": "TABLE 14", "label": "TABLE 14", "shape": "dot", "size": 10, "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "type": "TABLE"}, {"color": "#97c2fc", "description": "SES is a model used for evaluation", "id": "SES", "label": "SES", "shape": "dot", "size": 10, "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "type": "MODEL"}, {"color": "#97c2fc", "description": "Heart rate refers to the rate at which the heart beats", "id": "HEART RATE", "label": "HEART RATE", "shape": "dot", "size": 10, "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "HEART RATE DATASET", "label": "HEART RATE DATASET", "shape": "dot", "size": 10, "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "type": ""}, {"color": "#97c2fc", "description": "MAE (Arithmetic Mean) is a metric used to evaluate the performance of a model", "id": "MAE (ARITHMETIC MEAN)", "label": "MAE (ARITHMETIC MEAN)", "shape": "dot", "size": 10, "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "type": "METRIC"}, {"color": "#97c2fc", "description": "MAE (Geometric Mean) is a metric used to evaluate the performance of a model", "id": "MAE (GEOMETRIC MEAN)", "label": "MAE (GEOMETRIC MEAN)", "shape": "dot", "size": 10, "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\n**Entity: THETA**\n\nTHETA is a statistical model used for time series forecasting. It is a classical forecasting method that fits a separate model to each time series independently. Additionally, THETA is also used as a model for evaluation purposes. The model is specifically designed for time series forecasting, indicating its primary application in this domain.\n\nThis summary is derived from the provided descriptions, which are consistent in their portrayal of THETA as a statistical model for time series forecasting. The contradictions are resolved by acknowledging the multiple roles of THETA, including its use as a forecasting method and an evaluation model.", "id": "THETA", "label": "THETA", "shape": "dot", "size": 10, "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae,6771b6279846e780d6807b15184ae008,7e69d9444a2084b6452e291735bf5a49", "type": "MODEL"}, {"color": "#97c2fc", "description": "TBATS is a model used for evaluation", "id": "TBATS", "label": "TBATS", "shape": "dot", "size": 10, "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "type": "MODEL"}, {"color": "#97c2fc", "description": "MSTL is a statistical model used for time series forecasting", "id": "MSTL", "label": "MSTL", "shape": "dot", "size": 10, "source_id": "6771b6279846e780d6807b15184ae008", "type": "MODEL"}, {"color": "#97c2fc", "description": "CES is a statistical model used for time series forecasting", "id": "CES", "label": "CES", "shape": "dot", "size": 10, "source_id": "6771b6279846e780d6807b15184ae008", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe CIF 2016 is a machine learning competition that took place in 2016. It contains banking data that was used in the CIF 2016 forecasting competition, indicating that the primary focus of the event was on time series forecasting and long-term series forecasting. The competition likely involved frequency analysis and the application of advanced techniques such as multi-head cross-attention, given the technical nature of the event.\n\nThe CIF 2016 competition was likely a formal academic event, given the presence of technical terms and mathematical equations in the descriptions. The use of English-language abbreviations such as \"CIF\" and the citation of banking data suggests that the event was conducted in English. Overall, the CIF 2016 competition was a significant event in the field of machine learning and time series forecasting, bringing together experts to compete and showcase their skills in forecasting banking data.", "id": "CIF 2016", "label": "CIF 2016", "shape": "dot", "size": 10, "source_id": "57a3473e2c7c112cf81d520ea1ba95fa,d4bc1397526f236029876d4fbc22a721,e067234b24b0625a3f95ecc18035f915,edab6fd342bc0a563fd98a50eb8b3462", "type": "EVENT"}, {"color": "#97c2fc", "description": "NN5 weekly is a model or algorithm used for a specific task or application", "id": "NN5 WEEKLY", "label": "NN5 WEEKLY", "shape": "dot", "size": 10, "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"COVID DEATHS\" can be generated as follows:\n\nThe entity \"COVID DEATHS\" refers to a dataset of healthcare data, specifically containing daily count data of COVID-19 deaths in a set of countries and states. This dataset encompasses the period from January to August 2020, providing a comprehensive view of the number of deaths caused by COVID-19 during this time frame. The data is related to the number of deaths caused by COVID-19, making it a valuable resource for understanding the impact of the pandemic on global health.\n\nThis summary is based on the information provided in the description list, which includes the following key points:\n\n* The dataset is related to healthcare data.\n* The dataset contains daily count data of COVID-19 deaths.\n* The dataset covers a set of countries and states.\n* The dataset spans the period from January to August 2020.\n* The data is related to the number of deaths caused by COVID-19.\n\nOverall, the summary provides a clear and concise description of the entity \"COVID DEATHS\" and its associated dataset, highlighting its relevance to healthcare data and the COVID-19 pandemic.", "id": "COVID DEATHS", "label": "COVID DEATHS", "shape": "dot", "size": 10, "source_id": "57a3473e2c7c112cf81d520ea1ba95fa,d4bc1397526f236029876d4fbc22a721,e067234b24b0625a3f95ecc18035f915", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "FRED MD refers to a dataset or metric related to the Federal Reserve Economic Data", "id": "FRED MD", "label": "FRED MD", "shape": "dot", "size": 10, "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Traffic hourly refers to a dataset or metric related to traffic patterns", "id": "TRAFFIC HOURLY", "label": "TRAFFIC HOURLY", "shape": "dot", "size": 10, "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Traffic weekly refers to a dataset or metric related to traffic patterns", "id": "TRAFFIC WEEKLY", "label": "TRAFFIC WEEKLY", "shape": "dot", "size": 10, "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Saugen day refers to a dataset or metric related to a specific day or event", "id": "SAUGEN DAY", "label": "SAUGEN DAY", "shape": "dot", "size": 10, "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "US births refer to the number of births in the United States", "id": "US BIRTHS", "label": "US BIRTHS", "shape": "dot", "size": 10, "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"CAR PARTS\" refers to a dataset of retail data, which consists of various components of a vehicle. This dataset is likely used for analysis and forecasting purposes, such as long-term series forecasting, frequency analysis, and multi-head cross-attention. The data may include information on the sales, prices, and other relevant metrics of different car parts, which can be used to gain insights into the retail market and make informed decisions.\n\nIt is worth noting that the term \"Car Parts\" can be interpreted in two ways: as a dataset of retail data and as the components of a vehicle. However, in the context of the provided information, it appears that the primary focus is on the dataset of retail data, which is used for analysis and forecasting purposes.\n\nRelevant information from the nearby text suggests that the language used is formal and academic, indicating that the text is likely from a research paper or a technical document. The use of technical terms, mathematical equations, and references to academic papers further supports this interpretation.", "id": "CAR PARTS", "label": "CAR PARTS", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525,e067234b24b0625a3f95ecc18035f915", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Banking data is the source of the CIF 2016 dataset", "id": "BANKING DATA", "label": "BANKING DATA", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "NN5 daily is a model or algorithm used for a specific task or application", "id": "NN5 DAILY", "label": "NN5 DAILY", "shape": "dot", "size": 10, "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "type": "MODEL"}, {"color": "#97c2fc", "description": "Platform-delay dataset is one of the datasets used to evaluate Lag-Llama\u0027s performance", "id": "PLATFORM-DELAY", "label": "PLATFORM-DELAY", "shape": "dot", "size": 10, "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"NATURE\" can be generated as follows:\n\nThe entity \"NATURE\" refers to the natural world or the environment. It is a domain that consists of time series datasets related to nature, which are used for fine-tuning forecasting tasks. Additionally, \"NATURE\" is used as a domain to label datasets, indicating its relevance in categorizing and organizing data related to the natural world. This domain is likely used in various applications, such as environmental monitoring, climate modeling, and ecological research, where time series forecasting and analysis are crucial.\n\nThe use of \"NATURE\" as a domain for fine-tuning forecasting tasks suggests that it is a specific area of interest within the broader field of time series analysis. The fact that it is used to label datasets implies that it is a well-defined and recognizable category within the context of data science and machine learning.\n\nOverall, the entity \"NATURE\" encompasses a broad range of concepts and applications related to the natural world, and its use as a domain in time series analysis and forecasting tasks highlights its importance in understanding and predicting environmental phenomena.", "id": "NATURE", "label": "NATURE", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,3ba0bc9230706fb8f4a61d16ecf8fd26,4c09f35749179fe18c7d7290eaa57955,ec7705b83cf4fe3aa18662c917b18c1a", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe KDD Cup 2018 is a machine learning competition that took place in 2018. It is a data science competition related to nature, specifically focusing on air quality indicators. The competition contains various air quality indicators, including PM2.5, PM10, NO2, CO, O3, and SO2, measured in a specific unit (although the unit is not specified in the provided descriptions). This competition is a type of event or competition that refers to a specific event or competition, and it is related to the field of data science and machine learning.\n\nThe KDD Cup 2018 is likely a competition that involved predicting or analyzing air quality indicators using machine learning algorithms, given its focus on data science and machine learning. The competition may have involved various tasks, such as time series forecasting, frequency analysis, or multi-head cross-attention, given the technical terms mentioned in the descriptions. However, the specific details of the competition are not provided in the descriptions.\n\nOverall, the KDD Cup 2018 is a machine learning competition that focused on air quality indicators and took place in 2018.", "id": "KDD CUP 2018", "label": "KDD CUP 2018", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,6820cea275275e87630a6876e890c525,9e88afa28686ff93769bfc5eb0f1095e,d4bc1397526f236029876d4fbc22a721,e067234b24b0625a3f95ecc18035f915,ec7705b83cf4fe3aa18662c917b18c1a", "type": "EVENT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data:\n\nThe entity \"SUNSPOT\" is a concept related to nature. \n\nThis summary is derived from the description list provided, which directly states that \"Sunspot is a concept related to nature.\" There is no additional information provided in the text that contradicts or adds to this description.", "id": "SUNSPOT", "label": "SUNSPOT", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided descriptions, it can be concluded that the entity \"ET T M2\" refers to a specific type of model or algorithm used for making predictions or forecasts. The model is mentioned in the text, as indicated by the use of the name \"ET T M2\" and the presence of numerical values and performance metrics. Additionally, the notation \"ET T m1 \u2192 ET T m2\" suggests that \"ET T M2\" is a model that is related to or built upon another model, \"ET T M1\".\n\nThe descriptions also suggest that \"ET T M2\" is a concept that refers to a specific type of data or model, and that it is a metric mentioned in the text. However, these descriptions are not contradictory to the primary conclusion that \"ET T M2\" is a model or algorithm used for making predictions or forecasts.\n\nIn terms of the language and content of the text, it appears to be written in English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers. The text is formal and academic in tone, suggesting that it is from a research paper or technical document.\n\nOverall, the entity \"ET T M2\" can be described as a specific type of model or algorithm used for making predictions or forecasts, which is mentioned in the text and is related to another model, \"ET T M1\".", "id": "ET T M2", "label": "ET T M2", "shape": "dot", "size": 10, "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,919ff66615400ea06113a2a59ff34ef0,b90805909f7b1f31ddc03014f161d3ba,bb03c20a6fc1d9af2f3cbfa55b50cfb8,f05d25f1f55ce768a5d240373d5283a6", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nET T M1 is a concept that refers to a specific type of data or model. It is a metric mentioned in the text, as indicated by the use of the name \"ET T M1\". Furthermore, ET T M1 is a model or algorithm used to make predictions or forecasts. This suggests that ET T M1 is a type of predictive model, possibly used for time series forecasting or long-term series forecasting, given the context of the text.\n\nThe use of the name \"ET T M1\" in the text, along with the descriptions provided, implies that ET T M1 is a technical term or a specific model used in the field of time series analysis or forecasting. The fact that it is mentioned as a metric and a model suggests that it is a quantifiable measure or a specific algorithm used to make predictions.\n\nOverall, ET T M1 appears to be a technical term or a specific model used in the field of time series analysis or forecasting, possibly used for making predictions or forecasts.", "id": "ET T M1", "label": "ET T M1", "shape": "dot", "size": 10, "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,919ff66615400ea06113a2a59ff34ef0,b90805909f7b1f31ddc03014f161d3ba,bb03c20a6fc1d9af2f3cbfa55b50cfb8,f05d25f1f55ce768a5d240373d5283a6", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nET T H1 is a metric mentioned in the text, as indicated by the use of the name \"ET T h1\". It is also described as a model or algorithm used to make predictions or forecasts. The text does not provide further information about the specific characteristics or capabilities of ET T H1, but it is clear that it is a technical term related to time series analysis or forecasting, given the context of the descriptions.\n\nGiven the formal and academic language used in the text, it is likely that ET T H1 is a concept or technique discussed in a research paper or technical document, possibly related to long-term series forecasting, frequency analysis, or multi-head cross-attention. However, without further information, it is difficult to provide a more detailed description of ET T H1.\n\nIt is worth noting that the text does not provide any information that contradicts the descriptions of ET T H1 as both a metric and a model or algorithm. Therefore, it is possible that ET T H1 is a term that encompasses both meanings, or that the text is using the term in a way that is not immediately clear. Further analysis or context would be necessary to fully understand the meaning and application of ET T H1.", "id": "ET T H1", "label": "ET T H1", "shape": "dot", "size": 10, "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,919ff66615400ea06113a2a59ff34ef0", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"ET T H2\" can be described as follows:\n\n\"ET T H2 is a model or algorithm used for making predictions or forecasts, specifically mentioned in the text as indicated by the notation \u0027ET T m2 \u2192 ET T h2\u0027. This model is likely a variant of the \u0027ET T m2\u0027 model, and its primary function is to generate forecasts or predictions. The notation suggests a transformation or evolution from the \u0027ET T m2\u0027 model to the \u0027ET T h2\u0027 model, indicating a potential improvement or refinement in the forecasting capabilities of the model.\"\n\nThis description is based on the information provided in the description list, which includes the following points:\n\n* \"ET T h2 is a metric mentioned in the text, as indicated by the use of the name \u0027ET T h2\u0027\"\n* \"ET T h2 is a model or algorithm used to make predictions or forecasts\"\n* \"ET T h2 is a model, as indicated by the use of the notation \u0027ET T m2 \u2192 ET T h2\u0027\"\n\nThese points are consistent with each other, and the description provided above is a coherent summary of the information.", "id": "ET T H2", "label": "ET T H2", "shape": "dot", "size": 10, "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,919ff66615400ea06113a2a59ff34ef0,f05d25f1f55ce768a5d240373d5283a6", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nDOMINICK is a retailer that refers to a dataset of retail data. This dataset contains various information related to retail sales, customer behavior, and market trends, which can be useful for analyzing and forecasting long-term series forecasting, frequency analysis, and other aspects of retail business.\n\nThe dataset is likely to be used for time series analysis, multi-head cross-attention, and other machine learning techniques to gain insights into customer behavior, sales patterns, and market trends. The data may also be used to evaluate the performance of different algorithms and models in predicting retail sales and customer behavior.\n\nOverall, DOMINICK is a retailer that has a dataset of retail data, which can be used for various analytical and forecasting purposes in the retail industry.", "id": "DOMINICK", "label": "DOMINICK", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525,e067234b24b0625a3f95ecc18035f915", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "COVID-19 deaths data is the source of the Covid Deaths dataset", "id": "COVID-19 DEATHS DATA", "label": "COVID-19 DEATHS DATA", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "Medical product data is the source of the Hospital dataset", "id": "MEDICAL PRODUCT DATA", "label": "MEDICAL PRODUCT DATA", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "Figure 2a is a visual representation of the MAE numbers presented in Table 4", "id": "FIGURE 2A", "label": "FIGURE 2A", "shape": "dot", "size": 10, "source_id": "500710c8a95f1310d383fa71fd806c1c", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "", "id": "TABLE 4", "label": "TABLE 4", "shape": "dot", "size": 10, "source_id": "500710c8a95f1310d383fa71fd806c1c", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nFIGURE 9 is a visual representation that showcases the performance comparison between two types of models: those initialized with language model weights and those initialized randomly. This figure presents a visual representation of the performance metrics of these models, providing a clear understanding of their strengths and weaknesses. Furthermore, FIGURE 9 also includes some examples of zero-shot forecasts, which are predictions made without any prior training data. These examples demonstrate the ability of the models to generate accurate forecasts in the absence of explicit training data. Overall, FIGURE 9 serves as a valuable tool for understanding the performance of language-based models in the context of time series forecasting.", "id": "FIGURE 9", "label": "FIGURE 9", "shape": "dot", "size": 10, "source_id": "500710c8a95f1310d383fa71fd806c1c,c7f04fa1168df64cce110e20a1b72b51", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the entity \"STATIONARY\" can be generated as follows:\n\nThe entity \"STATIONARY\" refers to a specific type of time series model used for forecasting, particularly for short-term time series forecasting. It is a model mentioned in the text, as indicated by the use of the phrase \"Stationary,\" and is likely a type of stationary model. Stationary is also a property of time series data, which is essential for effective time series forecasting. The model is used for forecasting and is mentioned in a table, indicating its application in time series analysis.\n\nIn the context of time series forecasting, Stationary is a model that is used to forecast future values based on past observations. It is a type of model that is suitable for short-term forecasting, as indicated by its use in the table. The model is likely to be a type of statistical model, given its application in time series analysis.\n\nOverall, the entity \"STATIONARY\" refers to a specific type of time series model used for forecasting, which is a property of time series data and is essential for effective time series forecasting.\n\nRelevant information from the nearby text:\n\n* The text mentions the use of technical terms, mathematical equations, and references to academic papers, which are all written in English.\n* The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n* The text mentions the use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals.\n* The citation of English-language academic papers and authors, which suggests that the text is written in English.\n\nThis information provides context to the entity \"STATIONARY\" and highlights its application in time series forecasting, which is a key aspect of the text.", "id": "STATIONARY", "label": "STATIONARY", "shape": "dot", "size": 10, "source_id": "7e97089185883c456c798ddc5ec86373,91e161ba596a0cbbcae541ddb2106310,9ba0189af2ef0720a721c16eef0f0788,c84edbea28fbbed451e8d0b7df4ffb7c,d4551c2839eaa68a7cb7324089956581,f49330b6fd81d86d14e7a9d4b8e45576,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "type": "MODEL"}, {"color": "#97c2fc", "description": "The AdamW optimizer is used with a learning rate of 1 \u00d7 10\u22124", "id": "ADAMW OPTIMIZER", "label": "ADAMW OPTIMIZER", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022", "type": "OPTIMIZER"}, {"color": "#97c2fc", "description": "Gradient clipping is applied at a norm of 1.0", "id": "GRADIENT CLIPPING", "label": "GRADIENT CLIPPING", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Batch size is a parameter used to control the number of samples processed by a model in a single iteration", "id": "BATCH SIZE", "label": "BATCH SIZE", "shape": "dot", "size": 10, "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe Transformer Body is a crucial component of the model, responsible for taking embedded features and yielding latent features. Specifically, it consists of six layers with an embedding dimension of 512 and eight attention heads. This architecture enables the Transformer Body to effectively process and transform the input features, laying the groundwork for subsequent model operations.\n\nThis summary incorporates information from all the descriptions, resolving any potential contradictions and providing a coherent overview of the Transformer Body\u0027s structure and function.", "id": "TRANSFORMER BODY", "label": "TRANSFORMER BODY", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022,a5d60c1a355b77187003182f6df57646", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"DECODER\" can be described as follows:\n\nThe DECODER is a crucial component of the transformer model, specifically designed to decode the encoded representation of the input sequence. It is also a key component of the TranAD model, where it performs operations on the encoded input. The DECODER\u0027s primary function is to take the encoded representation of the input sequence and generate the output sequence, making it a vital part of the transformer architecture.\n\nIn the context of the transformer model, the DECODER is responsible for generating the output sequence by performing a series of operations on the encoded input. This process involves multiple steps, including attention mechanisms and feed-forward neural networks, which enable the DECODER to generate coherent and contextually relevant output.\n\nThe DECODER\u0027s architecture is typically composed of multiple layers, each of which performs a specific function, such as self-attention, encoder-decoder attention, and feed-forward neural networks. These layers work together to generate the output sequence, taking into account the encoded representation of the input sequence and the context in which it is being generated.\n\nOverall, the DECODER is a critical component of the transformer model, playing a central role in the process of generating output sequences from encoded input sequences. Its ability to perform complex operations on the encoded input enables it to generate coherent and contextually relevant output, making it a vital part of many natural language processing and machine learning applications.", "id": "DECODER", "label": "DECODER", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e,f2e78b25a535b82e2743a0ea4052eb9a", "type": "MODEL COMPONENT"}, {"color": "#97c2fc", "description": "FFN stands for feed-forward network, a component of the model used for time-series forecasting", "id": "FFN", "label": "FFN", "shape": "dot", "size": 10, "source_id": "673b0feee6eb5843954defc670e4ba29", "type": "MODEL COMPONENT"}, {"color": "#97c2fc", "description": "Model dimensions are the number of dimensions used in the model to represent the input data", "id": "MODEL DIMENSIONS", "label": "MODEL DIMENSIONS", "shape": "dot", "size": 10, "source_id": "673b0feee6eb5843954defc670e4ba29", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Output patch length is a hyper-parameter that controls the length of the output patch", "id": "OUTPUT PATCH LEN", "label": "OUTPUT PATCH LEN", "shape": "dot", "size": 10, "source_id": "673b0feee6eb5843954defc670e4ba29", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Input patch length is a hyper-parameter that controls the length of the input patch", "id": "INPUT PATCH LEN", "label": "INPUT PATCH LEN", "shape": "dot", "size": 10, "source_id": "673b0feee6eb5843954defc670e4ba29", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Darts baselines are a set of models used for comparison with the TimesFM model", "id": "DARTS BASELINES", "label": "DARTS BASELINES", "shape": "dot", "size": 10, "source_id": "673b0feee6eb5843954defc670e4ba29", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"GROUND TRUTH\" can be generated as follows:\n\nGROUND TRUTH refers to the actual or true values of data, which serve as a benchmark for evaluating the performance of time-series forecasting models. Specifically, it represents the actual value or outcome that a model is trying to predict, and is used to assess the accuracy of the model\u0027s predictions. In the context of time-series forecasting, ground truth is the actual data used to evaluate the performance of these models, providing a true representation of the variable or outcome being predicted.\n\nThis summary incorporates information from all the descriptions, resolving any potential contradictions and providing a coherent understanding of the entity \"GROUND TRUTH\". The summary highlights the importance of ground truth in evaluating the performance of time-series forecasting models and its role as a benchmark for assessing model accuracy.", "id": "GROUND TRUTH", "label": "GROUND TRUTH", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,6ae1ee8f0c4bcf7ec4d04b1048451e96,85e4fbea9a08a0a663f3c5dc3f3a95a7,e6ec99a117b9abd42452ff51cd6e8f0b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nFORECASTS are predicted values or outcomes that are generated by time-series forecasting models, based on historical data. These predicted values refer to the forecasted data or outcomes of the target variable, which are generated by analyzing and processing historical data using time-series forecasting models.\n\nThe FORECASTS are generated by considering the patterns, trends, and seasonality present in the historical data, and are used to make predictions about future values or outcomes of the target variable. The FORECASTS can be used in various applications, such as predicting sales, stock prices, or weather patterns, among others.\n\nOverall, FORECASTS are an essential component of time-series forecasting, and are used to make informed decisions based on predicted values or outcomes.\n\nRelevant information from the nearby text:\n\n* The text mentions that the language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n* The text also mentions that the language used is English, with specific indicators such as technical terms, mathematical equations, and references to academic papers.\n* The text does not provide any information that contradicts the provided descriptions, and therefore, the summary is based on the provided information.\n\nNote: The summary is written in third person, and includes the entity name \"FORECASTS\" to provide context.", "id": "FORECASTS", "label": "FORECASTS", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,e6ec99a117b9abd42452ff51cd6e8f0b,fb67fcff21ac521a5ed8b202412ec1fc", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Token ID refers to a unique identifier for a token in a sequence", "id": "TOKEN ID", "label": "TOKEN ID", "shape": "dot", "size": 10, "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Median forecast refers to the middle value of a set of predicted values", "id": "MEDIAN FORECAST", "label": "MEDIAN FORECAST", "shape": "dot", "size": 10, "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7,a90c6ca26542ea5935f9d8d5bf3688a9", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "App is a document that contains the visualization of the forecasts produced by Lag-Llama", "id": "APP", "label": "APP", "shape": "dot", "size": 10, "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Dimensions refer to the features or attributes of the data", "id": "DIMENSIONS", "label": "DIMENSIONS", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "HitRate is a metric used to evaluate the performance of an anomaly detection model", "id": "HITRATE", "label": "HITRATE", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"OUTPUT\" refers to the data points that are produced by the model, specifically the final output of the TranAD model. This output is a result of the model\u0027s processing and is likely to be a key component in the analysis and interpretation of the data.\n\nIn the context of the TranAD model, the output is a critical aspect that requires careful consideration and evaluation. The model\u0027s output is likely to be influenced by various factors, including the model\u0027s architecture, the quality of the input data, and the specific application or task at hand.\n\nGiven the technical nature of the TranAD model and its output, it is likely that the output will be a complex and multifaceted representation of the data, potentially involving various mathematical equations and formulas. The output may also be subject to various forms of analysis and evaluation, including frequency analysis and multi-head cross-attention, in order to gain a deeper understanding of the underlying patterns and relationships in the data.\n\nOverall, the output of the TranAD model is a critical component of the model\u0027s functionality and is likely to be a key area of focus in any analysis or evaluation of the model\u0027s performance.", "id": "OUTPUT", "label": "OUTPUT", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e,fb67fcff21ac521a5ed8b202412ec1fc", "type": "DATA"}, {"color": "#97c2fc", "description": "", "id": "EXISTING METHOD", "label": "EXISTING METHOD", "shape": "dot", "size": 10, "source_id": "6917a14af275e30abe2d30a8f8a9a4e6", "type": ""}, {"color": "#97c2fc", "description": "Numerical Computing \u0026 Image Analysis Lab is a research lab at Seoul National University", "id": "NUMERICAL COMPUTING \u0026 IMAGE ANALYSIS LAB", "label": "NUMERICAL COMPUTING \u0026 IMAGE ANALYSIS LAB", "shape": "dot", "size": 10, "source_id": "3e0985c172a09bb6c99e305b9f40a513", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nSeoul National University is a university located in Seoul, Republic of Korea, which is also situated in South Korea. The university is situated in the capital city of South Korea, indicating its prominent location in the country. \n\nThis summary combines the two descriptions provided, resolving any potential contradictions by acknowledging that Seoul National University is indeed located in both Seoul, Republic of Korea, and South Korea.", "id": "SEOUL NATIONAL UNIVERSITY", "label": "SEOUL NATIONAL UNIVERSITY", "shape": "dot", "size": 10, "source_id": "3e0985c172a09bb6c99e305b9f40a513,71f873c12230e6ede47583d81eb85e23", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe \"ICLR 2023 WORKSHOP\" is a conference event where a paper was presented. It is an event where the paper was presented, indicating that it is a platform for academic and research-based discussions. The workshop is likely related to the field of artificial intelligence and machine learning, given the mention of technical terms such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention.\" The use of English-language abbreviations such as \"ICLR\" and the citation of English-language academic papers and authors further suggest that the workshop is an English-language event.\n\nThe fact that the paper was presented at the workshop implies that it is a research-based paper, possibly related to the field of machine learning and time series forecasting. The presence of mathematical equations and formulas in the text also supports this conclusion.\n\nOverall, the \"ICLR 2023 WORKSHOP\" appears to be a conference event focused on machine learning and related research, where a paper was presented to an academic audience.", "id": "ICLR 2023 WORKSHOP", "label": "ICLR 2023 WORKSHOP", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1", "type": "EVENT"}, {"color": "#97c2fc", "description": "Transformer-based model refers to a type of neural network architecture that uses self-attention mechanisms", "id": "TRANSFORMER-BASED MODEL", "label": "TRANSFORMER-BASED MODEL", "shape": "dot", "size": 10, "source_id": "587ca8c6cc86a93299e9540153babbc6", "type": "MODEL"}, {"color": "#97c2fc", "description": "Autoencoder is a type of neural network that is designed to learn a compact representation of the input data", "id": "AUTOENCODER", "label": "AUTOENCODER", "shape": "dot", "size": 10, "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "type": "MODEL"}, {"color": "#97c2fc", "description": "Adversarial network is a type of neural network that is designed to learn a representation of the input data that is robust to adversarial attacks", "id": "ADVERSARIAL NETWORK", "label": "ADVERSARIAL NETWORK", "shape": "dot", "size": 10, "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "type": "MODEL"}, {"color": "#97c2fc", "description": "Masked language modeling is a task in natural language processing that involves predicting the missing words in a sentence", "id": "MASKED LANGUAGE MODELING", "label": "MASKED LANGUAGE MODELING", "shape": "dot", "size": 10, "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "type": "TASK"}, {"color": "#97c2fc", "description": "Density-based outlier detection is a method for identifying outliers based on the density of the data", "id": "DENSITY-BASED OUTLIER DETECTION", "label": "DENSITY-BASED OUTLIER DETECTION", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Statistical methods are used for anomaly detection, including methods such as LOF and others", "id": "STATISTICAL METHODS", "label": "STATISTICAL METHODS", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ML-based methods are used for anomaly detection, including methods such as LOF and others", "id": "MACHINE LEARNING-BASED (ML-BASED) METHODS", "label": "MACHINE LEARNING-BASED (ML-BASED) METHODS", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "GMM is a statistical algorithm used for anomaly detection", "id": "GAUSSIAN MIXTURE MODEL (GMM)", "label": "GAUSSIAN MIXTURE MODEL (GMM)", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Deep neural networks are used for anomaly detection, including models such as DAGMM", "id": "DEEP NEURAL NETWORK", "label": "DEEP NEURAL NETWORK", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca", "type": "MODEL"}, {"color": "#97c2fc", "description": "RNNs are used for anomaly detection, including models such as LSTM-VAE", "id": "RECURRENT NEURAL NETWORKS (RNNS)", "label": "RECURRENT NEURAL NETWORKS (RNNS)", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca", "type": "MODEL"}, {"color": "#97c2fc", "description": "LSTM is a type of RNN used for anomaly detection", "id": "LONG SHORT-TERM MEMORY (LSTM)", "label": "LONG SHORT-TERM MEMORY (LSTM)", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca", "type": "MODEL"}, {"color": "#97c2fc", "description": "Span masking is a concept used in the SpanBERT model", "id": "SPAN MASKING", "label": "SPAN MASKING", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "NLP refers to Natural Language Processing, a field of study in computer science", "id": "NLP", "label": "NLP", "shape": "dot", "size": 10, "source_id": "6917a14af275e30abe2d30a8f8a9a4e6", "type": "FIELD"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe M4 COMPETITION is a competition that challenges teams to develop and evaluate time series forecasting models. This competition is focused on long-term series forecasting, which involves predicting future values in a time series based on historical data. The models developed for this competition utilize various techniques, including frequency analysis and multi-head cross-attention, to improve forecasting accuracy.\n\nThe competition is likely to involve the use of mathematical equations and formulas, as well as English-language abbreviations such as ICLR, AAAI, and PMLR, which are commonly used in English-language academic conferences and journals. The citation of English-language academic papers and authors also suggests that the text is written in English.\n\nOverall, the M4 COMPETITION is a challenging event that requires teams to develop and evaluate advanced time series forecasting models, leveraging various techniques and tools to improve forecasting accuracy and precision.", "id": "M4 COMPETITION", "label": "M4 COMPETITION", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6,7d5d82d600620153153772a9bc498ac0", "type": "EVENT"}, {"color": "#97c2fc", "description": "RNNs are a type of neural network model that were outperformed by CNNs in multiple tasks on sequential data", "id": "RNN", "label": "RNN", "shape": "dot", "size": 10, "source_id": "7d5d82d600620153153772a9bc498ac0", "type": "MODEL"}, {"color": "#97c2fc", "description": "Sequential data refers to data that is ordered in time or space", "id": "SEQUENTIAL DATA", "label": "SEQUENTIAL DATA", "shape": "dot", "size": 10, "source_id": "7d5d82d600620153153772a9bc498ac0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Bidirectional LSTMs is a type of neural network layer used for capturing long-term temporal trends", "id": "BIDIRECTIONAL LSTMS", "label": "BIDIRECTIONAL LSTMS", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420", "type": "LAYER"}, {"color": "#97c2fc", "description": "Self-supervised training is a training strategy where the model is trained on a task without labeled data", "id": "SELF-SUPERVISED TRAINING", "label": "SELF-SUPERVISED TRAINING", "shape": "dot", "size": 10, "source_id": "a5d60c1a355b77187003182f6df57646", "type": "TRAINING STRATEGY"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"WINDOW\" can be generated as follows:\n\nThe entity \"WINDOW\" is a concept used in time series analysis and machine learning models to represent a sequence of data points, input features, or a subset of data points used for forecasting. It refers to a specific period or range of time, encompassing a set of historical values used to produce a forecast. In essence, a window is a sequence of data points or input features that are used to train a machine learning model or to analyze time series data.\n\nThis summary is derived from the descriptions provided, which are not contradictory but rather complementary. The descriptions highlight the various aspects of a window in the context of time series analysis and machine learning models. The summary is written in third person and includes the entity name \"WINDOW\" for clarity and context.\n\nRelevant information from the nearby text is not provided, but based on the descriptions, it can be inferred that windows are used in time series analysis and machine learning models to analyze and forecast data. The concept of a window is essential in these fields, and its various aspects are highlighted in the provided descriptions.", "id": "WINDOW", "label": "WINDOW", "shape": "dot", "size": 10, "source_id": "1902e651467179a9a1d4c4df0035e980,518bfcd6711530089fe3914ca16459c2,6917a14af275e30abe2d30a8f8a9a4e6,a5d60c1a355b77187003182f6df57646,a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Local positional encoding is a technique used to enrich the input", "id": "LOCAL POSITIONAL ENCODING", "label": "LOCAL POSITIONAL ENCODING", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Recurrent models are a type of neural network that uses feedback connections to process sequential data", "id": "RECURRENT MODELS", "label": "RECURRENT MODELS", "shape": "dot", "size": 10, "source_id": "1902e651467179a9a1d4c4df0035e980", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"PREDICTION BLOCK\" is a crucial component of the model, responsible for outputting anomaly scores of data points. Specifically, it contains one hidden layer with 2,048 neurons, utilizing the GELU (Gaussian Error Linear Unit) activation function in between. This configuration enables the prediction block to effectively process and analyze the data, ultimately producing accurate anomaly scores.", "id": "PREDICTION BLOCK", "label": "PREDICTION BLOCK", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022,a5d60c1a355b77187003182f6df57646", "type": "MODEL"}, {"color": "#97c2fc", "description": "GELU is an activation function used in the Transformer encoder", "id": "GELU", "label": "GELU", "shape": "dot", "size": 10, "source_id": "a5d60c1a355b77187003182f6df57646", "type": "ACTIVATION"}, {"color": "#97c2fc", "description": "2-layer MLPs are used as the prediction block", "id": "MLPS", "label": "MLPS", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022", "type": "MODEL"}, {"color": "#97c2fc", "description": "The embedding dimension is 512", "id": "EMBEDDING DIMENSION", "label": "EMBEDDING DIMENSION", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Input features are the features used as input to the attention mechanism", "id": "INPUT FEATURES", "label": "INPUT FEATURES", "shape": "dot", "size": 10, "source_id": "a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Query is the input feature used to compute the attention weights", "id": "QUERY", "label": "QUERY", "shape": "dot", "size": 10, "source_id": "a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Key is the input feature used to compute the attention weights", "id": "KEY", "label": "KEY", "shape": "dot", "size": 10, "source_id": "a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"FEATURES\" can be described as follows:\n\nIn the context of machine learning and data analysis, FEATURES refer to the specific attributes or characteristics of a dataset or model that are used to train and evaluate models, such as Lag-Llama. These features are the input features used in the attention mechanism, which is a key component of various machine learning models. They are essential in understanding the characteristics of the data and making informed decisions about model development and evaluation.\n\nThe features used in the attention mechanism are typically derived from the data used to train and evaluate Lag-Llama, and they play a crucial role in determining the performance of the model. By analyzing the features, researchers and practitioners can gain insights into the underlying structure of the data and make improvements to the model to enhance its accuracy and reliability.\n\nOverall, FEATURES are a critical component of machine learning and data analysis, and their careful selection and analysis are essential for developing effective models that can accurately capture the underlying patterns and relationships in the data.", "id": "FEATURES", "label": "FEATURES", "shape": "dot", "size": 10, "source_id": "6c11bd339c9630f1d61f2024e90bce5e,a7604286fb84b26c53950861f9aca4b1,ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"PCA\" can be described as follows:\n\nThe entity \"PCA\" refers to the Principal Component Analysis algorithm, which is a specific concept or measure related to data analysis and dimensionality reduction. It is used to assess diversity across datasets, providing a comprehensive understanding of the underlying structure and relationships within the data.\n\nThis description is derived from the two provided descriptions, which are not contradictory but rather complementary. The first description highlights the general concept of PCA, while the second description provides a more specific application of PCA in assessing diversity across datasets. By combining these two descriptions, we can gain a deeper understanding of the entity \"PCA\" and its role in data analysis.\n\nIt is worth noting that PCA is a widely used algorithm in machine learning and data analysis, and its applications extend beyond assessing diversity across datasets. However, based on the provided information, this description provides a concise and accurate summary of the entity \"PCA\".", "id": "PCA", "label": "PCA", "shape": "dot", "size": 10, "source_id": "6c11bd339c9630f1d61f2024e90bce5e,ec7705b83cf4fe3aa18662c917b18c1a", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "HCTSA library refers to the Highly Comparable Time Series Analysis library used to select features for classification ability", "id": "HCTSA LIBRARY", "label": "HCTSA LIBRARY", "shape": "dot", "size": 10, "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "type": "LIBRARY"}, {"color": "#97c2fc", "description": "Catch22 refers to a specific concept or measure related to data or features", "id": "CATCH22", "label": "CATCH22", "shape": "dot", "size": 10, "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"SYNTHETIC OUTLIERS\" refers to artificial outliers added to the input data. These synthetic outliers are specifically used in experiments and studies as artificial data points to analyze and understand their impact on the data. They are intentionally created to mimic real outliers, allowing researchers to test and evaluate the performance of various machine learning models and time series forecasting techniques. The use of synthetic outliers enables the exploration of different scenarios and edge cases, providing valuable insights into the robustness and reliability of these models.", "id": "SYNTHETIC OUTLIERS", "label": "SYNTHETIC OUTLIERS", "shape": "dot", "size": 10, "source_id": "15c3350fad556f666d93817c5036109c,a7604286fb84b26c53950861f9aca4b1,deb67d5386710136cf24bcbf135a66c4", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"SOFT REPLACEMENT\" refers to a technique used to synthesize outliers, specifically a type of synthetic outlier. This technique is designed to cover all typical types of outliers, making it a comprehensive approach to outlier synthesis. Soft replacement is a method used to generate outliers, which can be useful in various applications, such as data analysis and machine learning.\n\nIn the context of data analysis and machine learning, soft replacement can be used to create synthetic outliers that mimic real-world outliers, allowing for more robust testing and evaluation of models. This technique can be particularly useful in scenarios where real-world outliers are scarce or difficult to obtain.\n\nOverall, soft replacement is a valuable tool for data scientists and researchers looking to generate high-quality synthetic outliers for testing and evaluation purposes.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which is consistent with the formal and academic tone of the description.\n* The text also mentions technical terms, mathematical equations, and references to academic papers, which are all written in English, further supporting the conclusion that the primary language of the text is English.\n\nNote: The description is not empty, and the information provided is consistent and coherent, allowing for a comprehensive summary to be generated.", "id": "SOFT REPLACEMENT", "label": "SOFT REPLACEMENT", "shape": "dot", "size": 10, "source_id": "19a1a12db412295d0ddd19ffffb78332,9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"UNIFORM REPLACEMENT\" is a synthetic outlier that partially covers typical types of outliers. It is a type of synthetic outlier that refers to a technique used to synthesize outliers. This technique is utilized to create outliers that mimic the characteristics of real-world outliers, allowing for more accurate testing and evaluation of machine learning models.\n\nThe use of uniform replacement as a synthetic outlier is particularly useful in time series analysis and forecasting, where outliers can significantly impact the accuracy of predictions. By generating synthetic outliers using this technique, researchers and practitioners can better understand how their models perform in the presence of outliers and make necessary adjustments to improve their robustness.\n\nOverall, uniform replacement is a valuable tool in the field of machine learning and time series analysis, enabling the creation of realistic outliers for testing and evaluation purposes.", "id": "UNIFORM REPLACEMENT", "label": "UNIFORM REPLACEMENT", "shape": "dot", "size": 10, "source_id": "19a1a12db412295d0ddd19ffffb78332,9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nPEAK NOISE is a synthetic outlier that partially covers typical types of outliers. It is a type of synthetic outlier that refers to a technique used to synthesize outliers. This technique is likely used in data analysis and machine learning to create artificial outliers for testing and evaluation purposes.\n\nIn the context of data analysis, synthetic outliers like PEAK NOISE are often used to simulate real-world data anomalies and evaluate the robustness of machine learning models. By generating artificial outliers, researchers and practitioners can test the performance of their models under various conditions and improve their accuracy and reliability.\n\nThe use of PEAK NOISE and other synthetic outliers is a common practice in the field of machine learning and data analysis, particularly in time series forecasting and frequency analysis. By understanding the characteristics and behavior of synthetic outliers, researchers can develop more effective models and techniques for handling real-world data anomalies.\n\nOverall, PEAK NOISE is a synthetic outlier technique used to synthesize outliers for testing and evaluation purposes in data analysis and machine learning.", "id": "PEAK NOISE", "label": "PEAK NOISE", "shape": "dot", "size": 10, "source_id": "19a1a12db412295d0ddd19ffffb78332,9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"LENGTH ADJUSTMENT\" refers to a technique used to synthesize outliers, specifically a type of synthetic outlier that only covers the seasonal outlier. This technique is utilized to create artificial data points that mimic the characteristics of outliers in a dataset, allowing for the evaluation and analysis of outlier detection methods.\n\nIn the context of time series analysis, length adjustment is a synthetic outlier that can be used to simulate seasonal outliers, enabling researchers to develop and test more effective methods for detecting and handling such anomalies. The technique is particularly useful in long-term series forecasting, where seasonal outliers can significantly impact the accuracy of predictions.\n\nOverall, length adjustment is a valuable tool in the field of time series analysis, enabling researchers to create realistic synthetic outliers and improve the robustness of their models.", "id": "LENGTH ADJUSTMENT", "label": "LENGTH ADJUSTMENT", "shape": "dot", "size": 10, "source_id": "19a1a12db412295d0ddd19ffffb78332,9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Data degradation is the process of adding synthetic outliers to the input data", "id": "DATA DEGRADATION", "label": "DATA DEGRADATION", "shape": "dot", "size": 10, "source_id": "a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"ABALATION STUDIES\" can be described as follows:\n\nABALATION STUDIES refer to the experiments conducted to analyze the impact of synthetic outliers, as well as the process of removing or modifying components to analyze their impact. This suggests that ablation studies are a methodological approach used to understand the effects of individual components or features on a system or model, whether it be through the introduction of synthetic outliers or the removal/modification of components.\n\nIn the context of machine learning and time series forecasting, ablation studies can be used to identify the most important features or components that contribute to the performance of a model, and to understand how changes to these components affect the overall performance of the model. This can be particularly useful in identifying potential areas for improvement or optimization in a model.\n\nThe use of ablation studies can also be seen as a form of sensitivity analysis, where the impact of individual components or features on the model\u0027s performance is evaluated. This can provide valuable insights into the robustness and reliability of a model, and can help to identify potential vulnerabilities or areas for improvement.\n\nOverall, ablation studies are a valuable tool for understanding the behavior and performance of complex systems, and can be used to inform the development and optimization of machine learning models and time series forecasting systems.", "id": "ABALATION STUDIES", "label": "ABALATION STUDIES", "shape": "dot", "size": 10, "source_id": "15c3350fad556f666d93817c5036109c,deb67d5386710136cf24bcbf135a66c4", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"F1-SCORES\" refers to a measure of accuracy used in a study to evaluate the performance of models. Specifically, F1-scores are a metric used to assess the accuracy of models, providing a comprehensive evaluation of their performance.\n\nThis summary is derived from the two descriptions provided, which are consistent in their reference to F1-scores as a measure of accuracy and a metric for evaluating model performance. The information is written in a formal and academic tone, suggesting that it is from a research paper or technical document.\n\nThe use of technical terms such as \"F1-scores\" and \"models\" further supports the conclusion that the entity \"F1-SCORES\" is related to the field of machine learning or artificial intelligence. The descriptions also imply that the F1-scores are used to evaluate the performance of models in a specific study or context.\n\nOverall, the summary provides a clear and concise understanding of the entity \"F1-SCORES\" and its role in evaluating model performance.", "id": "F1-SCORES", "label": "F1-SCORES", "shape": "dot", "size": 10, "source_id": "15c3350fad556f666d93817c5036109c,deb67d5386710136cf24bcbf135a66c4", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "SYNTHESIS TECHNIQUE", "label": "SYNTHESIS TECHNIQUE", "shape": "dot", "size": 10, "source_id": "19a1a12db412295d0ddd19ffffb78332", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"UNDERSTANDING\" can be described as follows:\n\n\"UNDERSTANDING\" refers to the process of interpreting and making sense of information, which involves gaining insight and knowledge from data. This process enables individuals to comprehend and analyze information, ultimately leading to a deeper understanding of the subject matter.\n\nThe descriptions provided are consistent and reinforce each other, providing a clear and coherent understanding of the entity \"UNDERSTANDING\". The use of similar language and concepts across the descriptions further supports this interpretation.\n\nIn the context of machine learning and time series forecasting, understanding is a crucial aspect of data analysis, as it enables researchers and practitioners to identify patterns, relationships, and trends in data, ultimately informing decision-making and model development.\n\nOverall, the entity \"UNDERSTANDING\" is a fundamental concept in data analysis and interpretation, and its description provides a clear and concise understanding of its meaning and significance.", "id": "UNDERSTANDING", "label": "UNDERSTANDING", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23,9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Binary cross entropy loss is a type of loss function used in machine learning", "id": "BINARY CROSS ENTROPY LOSS", "label": "BINARY CROSS ENTROPY LOSS", "shape": "dot", "size": 10, "source_id": "6917a14af275e30abe2d30a8f8a9a4e6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Artificial labels refer to labels or annotations added to data points for training purposes", "id": "ARTIFICIAL LABELS", "label": "ARTIFICIAL LABELS", "shape": "dot", "size": 10, "source_id": "6917a14af275e30abe2d30a8f8a9a4e6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Goh et al. (2017) is a research paper that describes the SWaT dataset", "id": "GOH ET AL. (2017)", "label": "GOH ET AL. (2017)", "shape": "dot", "size": 10, "source_id": "fbc83a616e9fa96c245138bca69c177f", "type": "PAPER"}, {"color": "#97c2fc", "description": "Ahmed et al. (2017) is a research paper that describes the WADI dataset", "id": "AHMED ET AL. (2017)", "label": "AHMED ET AL. (2017)", "shape": "dot", "size": 10, "source_id": "fbc83a616e9fa96c245138bca69c177f", "type": "PAPER"}, {"color": "#97c2fc", "description": "", "id": "DATA POINTS", "label": "DATA POINTS", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": ""}, {"color": "#97c2fc", "description": "Score prediction refers to the process of predicting the anomaly score of a data point", "id": "SCORE PREDICTION", "label": "SCORE PREDICTION", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Replication padding is a concept mentioned in the text, as indicated by the use of the phrase \"replication padding\"", "id": "REPPLICATION PADDING", "label": "REPPLICATION PADDING", "shape": "dot", "size": 10, "source_id": "477c5b7f23f4e00030ab389788a4c88d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"THRESHOLD VALUE\" is a concept mentioned in the text, which is primarily written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document. The entity \"THRESHOLD VALUE\" is used to determine whether an input window is anomalous or not, indicating its significance in the context of time series analysis or long-term series forecasting.\n\nThe use of the phrase \"threshold value\" in the text, along with the presence of mathematical equations and formulas, suggests that the entity is related to quantitative analysis. The entity is also mentioned in the context of determining whether an input window is anomalous, which implies its application in anomaly detection or frequency analysis.\n\nOverall, the entity \"THRESHOLD VALUE\" appears to be a critical component in the analysis of time series data, particularly in the context of determining anomalies or unusual patterns.", "id": "THRESHOLD VALUE", "label": "THRESHOLD VALUE", "shape": "dot", "size": 10, "source_id": "477c5b7f23f4e00030ab389788a4c88d,f2e78b25a535b82e2743a0ea4052eb9a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Model weights are the parameters of the model that are updated during the training process", "id": "MODEL WEIGHTS", "label": "MODEL WEIGHTS", "shape": "dot", "size": 10, "source_id": "6ea15432a841705c2e74cbc01f6004e9", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "False positives refer to data points that are incorrectly identified as anomalies", "id": "FALSE POSITIVES", "label": "FALSE POSITIVES", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "False negatives refer to data points that are not identified as anomalies when they should be", "id": "FALSE NEGATIVES", "label": "FALSE NEGATIVES", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Global outliers refer to anomalies that are present throughout a time series", "id": "GLOBAL OUTLIERS", "label": "GLOBAL OUTLIERS", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Contextual outliers refer to anomalies that are present in a specific context or location", "id": "CONTEXTUAL OUTLIERS", "label": "CONTEXTUAL OUTLIERS", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Shapelet outliers refer to anomalies that are present in a specific shape or pattern", "id": "SHAPELET OUTLIERS", "label": "SHAPELET OUTLIERS", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Seasonal outliers refer to anomalies that are present in a specific season or time period", "id": "SEASONAL OUTLIERS", "label": "SEASONAL OUTLIERS", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Trend outliers refer to anomalies that are present in a specific trend or direction", "id": "TREND OUTLIERS", "label": "TREND OUTLIERS", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Point adjustment refers to a technique used to process prediction results", "id": "POINT ADJUSTMENT", "label": "POINT ADJUSTMENT", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Xu et al. is a paper that discusses the point adjustment technique", "id": "XU ET AL.", "label": "XU ET AL.", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": "PAPER"}, {"color": "#97c2fc", "description": "Kim et al. is a paper that discusses the point adjustment technique", "id": "KIM ET AL.", "label": "KIM ET AL.", "shape": "dot", "size": 10, "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "type": "PAPER"}, {"color": "#97c2fc", "description": "Shapelet outlier is a type of outlier that requires selecting a shape to be added", "id": "SHAPELET OUTLIER", "label": "SHAPELET OUTLIER", "shape": "dot", "size": 10, "source_id": "19a1a12db412295d0ddd19ffffb78332", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Trend outlier is a type of outlier that requires analyzing the data trend", "id": "TREND OUTLIER", "label": "TREND OUTLIER", "shape": "dot", "size": 10, "source_id": "19a1a12db412295d0ddd19ffffb78332", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"ENGLISH\" is a language spoken by millions of people around the world. It is the primary language of the text, as indicated by the presence of English words and phrases, such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention.\" The language used in the text is formal and academic, suggesting that it is from a research paper or a technical document.\n\nThe use of mathematical equations and formulas, written in a standard mathematical notation, further supports the fact that the text is written in English. Additionally, the presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals, confirms the language of the text.\n\nThe citation of English-language academic papers and authors also suggests that the text is written in English. Overall, based on the language and content of the text, it is clear that the primary language of the text is English, spoken by millions of people around the world.", "id": "ENGLISH", "label": "ENGLISH", "shape": "dot", "size": 10, "source_id": "0654926be53a9cf18e4def1c94371576,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,9f70fa3e2d0ba714627b9ce1a442fd21,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c80929fa1aa2a6f9652319844c4ca742,c9efd571b05c136c0bf9d7e89194ec88,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "type": "LANGUAGE"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe primary entity of interest is \"LANGUAGE.\" A description of this entity is that it refers to a system of communication used by humans. \n\nThis summary is derived from the provided description list, which directly states that \"Language refers to a system of communication used by humans.\" There is no additional information provided in the text that contradicts or adds to this description.\n\nTherefore, the final summary is:\n\n\"LANGUAGE refers to a system of communication used by humans.\"", "id": "LANGUAGE", "label": "LANGUAGE", "shape": "dot", "size": 10, "source_id": "0654926be53a9cf18e4def1c94371576,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,69457f873272a693c1f813c75ecf030a,9f70fa3e2d0ba714627b9ce1a442fd21,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c80929fa1aa2a6f9652319844c4ca742,c9efd571b05c136c0bf9d7e89194ec88,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe primary entity of interest is \"ACADEMIC PAPERS.\" The descriptions provided suggest that academic papers are written documents that present research or scholarly work. Furthermore, the text contains references to academic papers and authors, indicating that it is from a research paper or a technical document.\n\nGiven the context, it can be inferred that the text is likely a research paper or a technical document that discusses academic papers. The language used is formal and academic, suggesting a high level of technical expertise and a focus on presenting research findings.\n\nSome specific indicators of the language being English include the use of technical terms such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention.\" Additionally, the presence of mathematical equations and formulas, as well as English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" further support the conclusion that the primary language of the text is English.\n\nOverall, the summary of the data suggests that the entity \"ACADEMIC PAPERS\" is a written document that presents research or scholarly work, and the text is likely a research paper or technical document that discusses academic papers in the English language.", "id": "ACADEMIC PAPERS", "label": "ACADEMIC PAPERS", "shape": "dot", "size": 10, "source_id": "0654926be53a9cf18e4def1c94371576,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,9f70fa3e2d0ba714627b9ce1a442fd21,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c80929fa1aa2a6f9652319844c4ca742,c9efd571b05c136c0bf9d7e89194ec88,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"ICLR\" can be described as follows:\n\nICLR is an academic conference focused on machine learning and artificial intelligence, where researchers present their work. It is mentioned in the text as an academic conference, and the use of the abbreviation \"ICLR\" further supports this indication. The conference is likely associated with the fields of machine learning and artificial intelligence, given the context of the text, which discusses technical terms and mathematical equations related to these areas.\n\nThe description is consistent across all provided descriptions, and no contradictions were found. The information is written in third person, and the entity name \"ICLR\" is included for context. Relevant information from the nearby text, such as the mention of machine learning and artificial intelligence, has been incorporated into the description to provide a more comprehensive understanding of the entity.", "id": "ICLR", "label": "ICLR", "shape": "dot", "size": 10, "source_id": "0654926be53a9cf18e4def1c94371576,15c3350fad556f666d93817c5036109c,171fb6df1bf9905b151dfb846d75d0f8,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,9f70fa3e2d0ba714627b9ce1a442fd21,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c80929fa1aa2a6f9652319844c4ca742,c9efd571b05c136c0bf9d7e89194ec88,cf0d73cbe44e03ef7300f5c53b72090a,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "type": "ACADEMIC CONFERENCE"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"AAAI\" can be described as follows:\n\n\"AAAI is an academic conference focused on artificial intelligence and machine learning, where researchers present their work. It is mentioned in the text as an academic conference, and the use of the abbreviation \u0027AAAI\u0027 further indicates its relevance to the field of artificial intelligence and machine learning. AAAI is a platform where experts in the field come together to share their research and findings, making it a significant event in the academic calendar.\"\n\nThis description is a comprehensive summary of the provided information, taking into account the various descriptions and resolving any potential contradictions. The use of the abbreviation \u0027AAAI\u0027 and its mention in the text as an academic conference further solidify its relevance to the field of artificial intelligence and machine learning.", "id": "AAAI", "label": "AAAI", "shape": "dot", "size": 10, "source_id": "0654926be53a9cf18e4def1c94371576,15c3350fad556f666d93817c5036109c,171fb6df1bf9905b151dfb846d75d0f8,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,9f70fa3e2d0ba714627b9ce1a442fd21,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c80929fa1aa2a6f9652319844c4ca742,c9efd571b05c136c0bf9d7e89194ec88,cf0d73cbe44e03ef7300f5c53b72090a,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "type": "ACADEMIC CONFERENCE"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"PMLR\" can be described as follows:\n\nPMLR is a publisher of research papers and an academic conference focused on machine learning and artificial intelligence. It is mentioned in the text as an academic conference where researchers present their work, and the use of the abbreviation \"PMLR\" further indicates its association with academic conferences. PMLR is a platform where researchers share their findings and advancements in the field of machine learning and artificial intelligence.\n\nThe description is enriched with relevant information from the nearby text, which confirms that PMLR is indeed an academic conference and a publisher of research papers. The use of the abbreviation \"PMLR\" in the text further supports this conclusion.\n\nIt is worth noting that the text also mentions that PMLR is mentioned in the text, which suggests that it is a relevant and important entity in the context of the text. However, this information is not necessary to include in the final description, as it is already implied by the other descriptions.", "id": "PMLR", "label": "PMLR", "shape": "dot", "size": 10, "source_id": "0654926be53a9cf18e4def1c94371576,15c3350fad556f666d93817c5036109c,171fb6df1bf9905b151dfb846d75d0f8,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,9f70fa3e2d0ba714627b9ce1a442fd21,a69a914fb6c895c7202532b69ad3e094,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c80929fa1aa2a6f9652319844c4ca742,c9efd571b05c136c0bf9d7e89194ec88,cf0d73cbe44e03ef7300f5c53b72090a,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "type": "ACADEMIC CONFERENCE"}, {"color": "#97c2fc", "description": "The text is likely from a research paper or a technical document, as indicated by the formal and academic language used", "id": "RESEARCH PAPER", "label": "RESEARCH PAPER", "shape": "dot", "size": 10, "source_id": "4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "", "id": "TEXT DOCUMENT", "label": "TEXT DOCUMENT", "shape": "dot", "size": 10, "source_id": "0654926be53a9cf18e4def1c94371576,9f70fa3e2d0ba714627b9ce1a442fd21,c80929fa1aa2a6f9652319844c4ca742", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"MULTI-HEAD CROSS-ATTENTION\" is a concept mentioned in the text, referring to a type of attention mechanism used in neural networks. It is a technique used in some models, including the TIME-LLM model, to analyze the data and attend to different parts of the input data simultaneously. Specifically, multi-head cross-attention allows the model to attend to multiple different positions in the input sequence simultaneously, enabling it to capture complex relationships and patterns in the data.\n\nThis summary is based on the information collected from all the descriptions, and it resolves any potential contradictions by providing a clear and concise overview of the entity \"MULTI-HEAD CROSS-ATTENTION\". The summary is written in third person and includes the entity name for context.\n\nRelevant information from the nearby text that has been incorporated into the summary includes:\n\n* The use of the phrase \"multi-head cross-attention\" in the text, indicating its importance and relevance to the topic.\n* The mention of the TIME-LLM model, which uses multi-head cross-attention as a technique to analyze the data.\n* The description of multi-head cross-attention as a type of attention mechanism used in neural networks, which allows the model to attend to different parts of the input data simultaneously.\n\nOverall, the summary provides a clear and comprehensive understanding of the entity \"MULTI-HEAD CROSS-ATTENTION\" and its role in neural networks and data analysis.", "id": "MULTI-HEAD CROSS-ATTENTION", "label": "MULTI-HEAD CROSS-ATTENTION", "shape": "dot", "size": 10, "source_id": "15c3350fad556f666d93817c5036109c,171fb6df1bf9905b151dfb846d75d0f8,1b48e9ca066ac5ba037066bb762d3458,4742f536818b2fce762157bdb2cb1a3c,509b431231e669be373f593b31412eed,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,cf0d73cbe44e03ef7300f5c53b72090a,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"PATCH AS PREFIX\" is a concept mentioned in the text, as indicated by the use of the phrase \"Patch-as-Prefix\". This concept refers to a specific technique or approach in computer vision. The text does not provide further details about the technique, but it is mentioned in the context of computer vision, suggesting that it is related to image processing or object recognition.\n\nIt is worth noting that the text does not provide any additional information about the entity \"PATCH AS PREFIX\", and the description list is empty except for the two provided descriptions. However, based on the context and the use of the phrase \"Patch-as-Prefix\", it can be inferred that the entity is related to computer vision and is a specific technique or approach used in this field.\n\nOverall, the summary provides a concise description of the entity \"PATCH AS PREFIX\" and its relation to computer vision, based on the provided information.", "id": "PATCH AS PREFIX", "label": "PATCH AS PREFIX", "shape": "dot", "size": 10, "source_id": "4742f536818b2fce762157bdb2cb1a3c,509b431231e669be373f593b31412eed,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a014d14e003d0998b877eacffa8ebfc4,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"PROMPT AS PREFIX\" is a concept mentioned in the text, as indicated by the use of the phrase \"Prompt-as-Prefix\". This concept is specifically related to computer vision and refers to a technique or approach used in the field. It is described as a specific method or strategy used in computer vision, but no further details are provided in the given descriptions.\n\nTo enrich this summary, we can consider the context in which \"Prompt as prefix\" is mentioned. The text appears to be discussing technical terms and concepts related to computer vision, including time series analysis and multi-head cross-attention. The use of phrases such as \"Prompt-as-Prefix\" suggests that the text is discussing a specific technique or approach that is used in computer vision.\n\nGiven the formal and academic tone of the text, it is likely that \"Prompt as prefix\" is a concept that has been discussed in academic papers or research studies. The use of phrases such as \"Prompt-as-Prefix\" and the mention of technical terms such as \"time series\" and \"multi-head cross-attention\" suggest that the text is discussing a specific area of research in computer vision.\n\nOverall, the summary of \"PROMPT AS PREFIX\" is as follows:\n\n\"PROMPT AS PREFIX\" is a concept mentioned in the text, referring to a specific technique or approach used in computer vision. It is likely that this concept has been discussed in academic papers or research studies, and is related to the broader field of computer vision.", "id": "PROMPT AS PREFIX", "label": "PROMPT AS PREFIX", "shape": "dot", "size": 10, "source_id": "4742f536818b2fce762157bdb2cb1a3c,509b431231e669be373f593b31412eed,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,7e03744dea80e2138baff03611104fa8,a014d14e003d0998b877eacffa8ebfc4,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Text embeddings are a type of embedding used in natural language processing models", "id": "TEXT EMBEDDINGS", "label": "TEXT EMBEDDINGS", "shape": "dot", "size": 10, "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Query matrices are a type of matrix used in multi-head cross-attention, as indicated by the use of the phrase \"query matrices QkQ\"", "id": "QUERY MATRICES", "label": "QUERY MATRICES", "shape": "dot", "size": 10, "source_id": "509b431231e669be373f593b31412eed", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Key matrices are a type of matrix used in multi-head cross-attention, as indicated by the use of the phrase \"key matrices Kk(i)\"", "id": "KEY MATRICES", "label": "KEY MATRICES", "shape": "dot", "size": 10, "source_id": "509b431231e669be373f593b31412eed", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Value matrices are a type of matrix used in multi-head cross-attention, as indicated by the use of the phrase \"value matrices Vk(i)\"", "id": "VALUE MATRICES", "label": "VALUE MATRICES", "shape": "dot", "size": 10, "source_id": "509b431231e669be373f593b31412eed", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Heads are a parameter used in the TIME-LLM model", "id": "HEADS", "label": "HEADS", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"POT\" refers to a technique used in anomaly detection models. Specifically, POT is an adjusted version of the Peak Over Threshold (POT) method, which is used for automated anomaly threshold selection. This technique is employed to set threshold values in anomaly detection models, allowing for the identification of anomalies in a dataset.\n\nIn the context of anomaly detection, POT is a valuable tool for selecting optimal threshold values, enabling the model to accurately identify anomalies and distinguish them from normal data points. The adjusted POT method is designed to automate this process, making it a useful technique for practitioners working with anomaly detection models.\n\nOverall, POT is a technique that plays a crucial role in anomaly detection, and its adjusted version, the Peak Over Threshold method, is a key component of automated anomaly threshold selection.", "id": "POT", "label": "POT", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,ffbbbf29ffb8d038e241f023079cb0a2", "type": "METHOD"}, {"color": "#97c2fc", "description": "The POT technique is used in TranAD and other models to set more accurate threshold values", "id": "POT TECHNIQUE", "label": "POT TECHNIQUE", "shape": "dot", "size": 10, "source_id": "8e075f1de7293e0cd1724bd167c263be", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "LSTMs are a type of recurrent neural network known for their ability to learn long-term dependencies in data", "id": "LSTMS", "label": "LSTMS", "shape": "dot", "size": 10, "source_id": "bd73ee0439609823e12a877840c6ebae", "type": "MODEL"}, {"color": "#97c2fc", "description": "SWAT dataset refers to a dataset of water treatment plant data", "id": "SWAT DATASET", "label": "SWAT DATASET", "shape": "dot", "size": 10, "source_id": "587ca8c6cc86a93299e9540153babbc6", "type": "DATASET"}, {"color": "#97c2fc", "description": "Tab 8 is a table mentioned in the text, as indicated by the use of the label \"Tab. 8\"", "id": "TAB 8", "label": "TAB 8", "shape": "dot", "size": 10, "source_id": "74527a4337ed6919731be520311ae774", "type": "TABLE"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe INPUT WINDOW is a concept mentioned in the text, referring to a sequence of data points used as input to the model. Specifically, it refers to the input data used by the window encoder, which is a crucial component in the model\u0027s architecture. The INPUT WINDOW is a fundamental concept in the context of the model, and its definition is closely tied to the model\u0027s ability to process and analyze sequential data.\n\nIn the context of time series forecasting, the INPUT WINDOW is likely used to capture a specific period of time, such as a day, week, or month, which is then used as input to the model. The use of the INPUT WINDOW allows the model to focus on a specific sequence of data points, enabling it to capture patterns and trends that may not be apparent when analyzing the entire dataset.\n\nOverall, the INPUT WINDOW is a key concept in the model\u0027s architecture, and its use is essential for the model\u0027s ability to accurately forecast future values in a time series.\n\nRelevant information from the nearby text suggests that the INPUT WINDOW is used in conjunction with other techniques, such as frequency analysis and multi-head cross-attention, to improve the model\u0027s performance. Additionally, the INPUT WINDOW is likely used in the context of long-term series forecasting, where the model is tasked with predicting future values in a time series over an extended period.\n\nThe use of technical terms, mathematical equations, and references to academic papers in the text further supports the idea that the INPUT WINDOW is a complex concept that requires a deep understanding of machine learning and time series analysis.", "id": "INPUT WINDOW", "label": "INPUT WINDOW", "shape": "dot", "size": 10, "source_id": "477c5b7f23f4e00030ab389788a4c88d,4bdf596e75e1cb06d11b25e95491037e,f2e78b25a535b82e2743a0ea4052eb9a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Machines are the sources of the sub-datasets", "id": "MACHINES", "label": "MACHINES", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022", "type": "ENTITY"}, {"color": "#97c2fc", "description": "The Internet company is the provider of the sub-datasets", "id": "INTERNET COMPANY", "label": "INTERNET COMPANY", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022", "type": "ENTITY"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"LINEAR LAYER\" is a type of layer used in the TimeGPT model. It is utilized as the embedding layer, which implies its role in transforming input data into a suitable format for further processing. Additionally, the linear layer is used to map the decoder\u0027s output to the forecasting window dimension, indicating its involvement in the time series forecasting process. This suggests that the linear layer plays a crucial role in the TimeGPT model\u0027s ability to perform long-term series forecasting and frequency analysis.", "id": "LINEAR LAYER", "label": "LINEAR LAYER", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2,5809618fe3a2014c2140601fcf103022", "type": "MODEL"}, {"color": "#97c2fc", "description": "Forecasting window dimension refers to the dimension of the output", "id": "FORECASTING WINDOW DIMENSION", "label": "FORECASTING WINDOW DIMENSION", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "There are eight attention heads", "id": "ATTENTION HEADS", "label": "ATTENTION HEADS", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the data related to the entity \"WINDOW SIZE\" can be generated as follows:\n\nThe window size is a critical parameter in various applications, including machine learning, time series analysis, and computer vision. It refers to the number of data points used in a single inference or prediction, as well as the number of data points used to calculate a statistic. The window size can vary depending on the dataset being used, and it is often specified in terms of a fixed number of data points, such as 7, 168, 4,096, 1,024, or 2,048.\n\nIn the context of computer vision models, the window size is used to define the size of a window, which is a fundamental concept in image processing and analysis. The window size is typically used to extract features from images or videos, and it plays a crucial role in determining the accuracy and efficiency of the model.\n\nOverall, the window size is a versatile parameter that is used in a wide range of applications, and its value can have a significant impact on the performance of the model. By understanding the concept of window size and its various applications, researchers and practitioners can develop more effective and efficient models for time series analysis, machine learning, and computer vision.\n\nNote: The contradictions in the descriptions have been resolved by considering the context and the various applications of the window size. The summary provides a comprehensive overview of the concept of window size, its various definitions, and its applications in different fields.", "id": "WINDOW SIZE", "label": "WINDOW SIZE", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022,78ee4a3d7a2bffd4405a03d94a4f6cb1,8ff636d53a50d7a3bad17443bf104ba2,9b35a2c607e0f4b8cbc9179f424f180a", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"PATCH SIZE\" is a variable parameter that can vary depending on the specific dataset being used. This is evident from the description \"The patch size varies with datasets,\" which suggests that the patch size is not a fixed value, but rather a value that changes based on the characteristics of the dataset.\n\nIn the context of machine learning and time series forecasting, the patch size is likely a critical parameter that affects the performance of models and algorithms. However, without further information, it is difficult to provide a more detailed summary of the patch size\u0027s role in these applications.\n\nOverall, the summary of the entity \"PATCH SIZE\" is that it is a variable parameter that can vary depending on the dataset, and its specific role and characteristics are not well-defined without further information.", "id": "PATCH SIZE", "label": "PATCH SIZE", "shape": "dot", "size": 10, "source_id": "5809618fe3a2014c2140601fcf103022,8ff636d53a50d7a3bad17443bf104ba2", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Max length % of outlier refers to the maximum length of an outlier in a dataset, as indicated by the use of percentages 20% and 15%", "id": "MAX LENGTH % OF OUTLIER", "label": "MAX LENGTH % OF OUTLIER", "shape": "dot", "size": 10, "source_id": "8ff636d53a50d7a3bad17443bf104ba2", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Dataset size refers to the number of data points in a dataset", "id": "DATASET SIZE", "label": "DATASET SIZE", "shape": "dot", "size": 10, "source_id": "9b35a2c607e0f4b8cbc9179f424f180a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"TRAINING TIME\" refers to a metric that measures the amount of time it takes to train a model on a dataset. This metric is denoted by the abbreviation \"Time\" in the table, indicating its significance in the context of model training. In essence, training time is the time taken to train a model or software, which is a crucial aspect of the machine learning process.\n\nThis summary is derived from the descriptions provided, which collectively convey the meaning and significance of the entity \"TRAINING TIME\". The information is written in a formal and academic tone, suggesting that it is from a research paper or technical document. The use of technical terms such as \"metric\" and \"dataset\" further reinforces this notion.\n\nOverall, the summary provides a clear and concise understanding of the entity \"TRAINING TIME\" and its relevance in the context of machine learning and model training.", "id": "TRAINING TIME", "label": "TRAINING TIME", "shape": "dot", "size": 10, "source_id": "0e3a4048d3d693cb8fd969fd606f1e8a,78ee4a3d7a2bffd4405a03d94a4f6cb1,f6e57fa18831bcc732a631240536777a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Use of length adjustment refers to the use of length adjustment in a computer vision model, as indicated by the use of symbols \u00d7 and", "id": "USE OF LENGTH ADJUSTMENT", "label": "USE OF LENGTH ADJUSTMENT", "shape": "dot", "size": 10, "source_id": "8ff636d53a50d7a3bad17443bf104ba2", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "t-SNE is a technique used for dimensionality reduction and visualization of high-dimensional data", "id": "TSNE", "label": "TSNE", "shape": "dot", "size": 10, "source_id": "ee651b9bedb0cbcb6272a67deda44bb0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "2D planes refer to the visual representation of high-dimensional data in two dimensions", "id": "2D PLANES", "label": "2D PLANES", "shape": "dot", "size": 10, "source_id": "ee651b9bedb0cbcb6272a67deda44bb0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Abnormal windows refer to the time intervals where anomalies occur", "id": "ABNORMAL WINDOWS", "label": "ABNORMAL WINDOWS", "shape": "dot", "size": 10, "source_id": "ee651b9bedb0cbcb6272a67deda44bb0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Lagged features are constructed from the prior values of the time series", "id": "LAGGED FEATURES", "label": "LAGGED FEATURES", "shape": "dot", "size": 10, "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Lagged features are combined with date-time features to create a larger window of historical pointsLagged features are used in the tokenization scheme of Lag-LlamaLagged features are used in the Lag-Llama architecture", "entity_type": "LLAMA", "id": "LAG-LAGGED FEATURES", "label": "LAG-LAGGED FEATURES", "shape": "dot", "size": 10, "source_id": "55099ca8e21d1db3bdaf7bd04e590b72", "type": "TOKENIZATION"}, {"color": "#97c2fc", "description": "Date-time features are a set of features that describe the date and time of a time seriesDate-time features are used in the Lag-Llama architectureDate-time features are used in the tokenization scheme of Lag-Llama", "entity_type": "LLAMA", "id": "DATE-TIME FEATURES", "label": "DATE-TIME FEATURES", "shape": "dot", "size": 10, "source_id": "55099ca8e21d1db3bdaf7bd04e590b72", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Computational burdens refer to the computational resources required to process input data", "id": "COMPUTATIONAL BURDENS", "label": "COMPUTATIONAL BURDENS", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Historical time series refers to a sequence of data points measured at regular time intervals in the past", "id": "HISTORICAL TIME SERIES", "label": "HISTORICAL TIME SERIES", "shape": "dot", "size": 10, "source_id": "f7e3207706b170296cb036538a709689", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Bin centers are a concept mentioned in the text, referring to the points used to define bins in quantization", "id": "BIN CENTERS", "label": "BIN CENTERS", "shape": "dot", "size": 10, "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Bin edges are a concept mentioned in the text, referring to the points used to separate bins in quantization", "id": "BIN EDGES", "label": "BIN EDGES", "shape": "dot", "size": 10, "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Rabanser et al. is an academic paper mentioned in the text, referring to a study on quantization schemes for time series", "id": "RABANSER ET AL.", "label": "RABANSER ET AL.", "shape": "dot", "size": 10, "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Binning refers to the process of dividing a continuous range of values into discrete bins or intervals", "id": "BINNING", "label": "BINNING", "shape": "dot", "size": 10, "source_id": "c5c841baa0bc205103ac433d446da3b6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Rabanser et al. (2020) is an academic paper that discusses quantization schemes for time series", "id": "RABANSER ET AL. (2020)", "label": "RABANSER ET AL. (2020)", "shape": "dot", "size": 10, "source_id": "c5c841baa0bc205103ac433d446da3b6", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Memory footprints refer to the amount of memory required to store the model and its parameters", "id": "MEMORY FOOTPRINTS", "label": "MEMORY FOOTPRINTS", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Transformer-based language model architectures are models that use self-attention mechanisms to process sequential data", "id": "TRANSFORMER-BASED LANGUAGE MODEL ARCHITECTURES", "label": "TRANSFORMER-BASED LANGUAGE MODEL ARCHITECTURES", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe entity \"PROTOTYPES\" refers to a set of examples or instances that are used to represent a concept or class. Prototypes are essentially examples or instances that embody the characteristics of a particular concept or class, serving as a representation or model for other similar entities. They are often used in various fields, including machine learning, to facilitate understanding and classification of complex concepts.\n\nIn the context of machine learning, prototypes are used to train models and enable them to make predictions or classifications based on the characteristics of the prototypes. This approach is particularly useful when dealing with complex or abstract concepts that are difficult to define or quantify.\n\nOverall, the concept of prototypes is a fundamental idea in various fields, including computer science, artificial intelligence, and cognitive psychology, and is used to facilitate understanding, classification, and prediction of complex concepts.\n\nRelevant information from the nearby text is not provided, but based on the given descriptions, the summary is enriched with relevant information from the descriptions themselves.", "id": "PROTOTYPES", "label": "PROTOTYPES", "shape": "dot", "size": 10, "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8,4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,8f4724ff6541b8924f0cebe9872ed040,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Cross-entropy loss is a loss function used to train machine learning models", "id": "CROSS-ENTROPY LOSS", "label": "CROSS-ENTROPY LOSS", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "LOSS FUNCTION"}, {"color": "#97c2fc", "description": "AWS AI Labs is a research organization that focuses on artificial intelligence and machine learning", "id": "AWS AI LABS", "label": "AWS AI LABS", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nAbdul Fatir Ansari is an author of the paper \"Deep Explicit Duration Switching Models for Time Series.\" This paper is likely a research paper or a technical document, given the formal and academic language used. The content of the paper appears to be related to time series analysis, specifically long-term series forecasting, and may involve techniques such as frequency analysis and multi-head cross-attention. The paper may have been presented at a conference or published in a journal, given the presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR.\"", "id": "ABDUL FATIR ANSARI", "label": "ABDUL FATIR ANSARI", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1,fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Amazon Supply Chain Optimization Technologies is a research organization that focuses on supply chain optimization", "id": "AMAZON SUPPLY CHAIN OPTIMIZATION TECHNOLOGIES", "label": "AMAZON SUPPLY CHAIN OPTIMIZATION TECHNOLOGIES", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Kari Torkkola is an author of the paper", "id": "KARI TORKKOLA", "label": "KARI TORKKOLA", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "UC San Diego is a university that focuses on research and education", "id": "UC SAN DIEGO", "label": "UC SAN DIEGO", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Caner Turkmen is an author of the paper", "id": "CANER TURKMEN", "label": "CANER TURKMEN", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "UC Berkeley is a university that focuses on research and education", "id": "UC BERKELEY", "label": "UC BERKELEY", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Xiyuan Zhang is an author of the paper", "id": "XIYUAN ZHANG", "label": "XIYUAN ZHANG", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "New York University is a university that focuses on research and education", "id": "NEW YORK UNIVERSITY", "label": "NEW YORK UNIVERSITY", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Andrew Gordon Wilson is an author of the paper", "id": "ANDREW GORDON WILSON", "label": "ANDREW GORDON WILSON", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Lorenzo Stella is an author of the paper", "id": "LORENZO STELLA", "label": "LORENZO STELLA", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Michael W. Ma-Wang is an author of the paper", "id": "MICHAEL W. MA-WANG", "label": "MICHAEL W. MA-WANG", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Deep Explicit Duration Switching Models is a type of time series model", "id": "DEEP EXPLICIT DURATION SWITCHING MODELS", "label": "DEEP EXPLICIT DURATION SWITCHING MODELS", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Pedro Mercado is an author of the paper", "id": "PEDRO MERCADO", "label": "PEDRO MERCADO", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Huibin Shen is an author of the paper", "id": "HUIBIN SHEN", "label": "HUIBIN SHEN", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Oleksandr Shchur is an author of the paper", "id": "OLEKSANDR SHCHUR", "label": "OLEKSANDR SHCHUR", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nSyama Sundar Rangapuram is an author of the paper \"Deep learning for time series forecasting: Tutorial and literature survey\". This paper is likely a technical document or research paper, given the formal and academic language used. The content of the paper is related to time series forecasting, specifically focusing on deep learning techniques. \n\nThe language used in the paper is English, as indicated by the use of English words and phrases, mathematical equations, and references to English-language academic papers and authors. The presence of technical terms such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis\" further supports the conclusion that the paper is written in English.\n\nOverall, Syama Sundar Rangapuram is an author of a paper that explores the application of deep learning techniques in time series forecasting, and the paper is written in English.", "id": "SYAMA SUNDAR RANGAPURAM", "label": "SYAMA SUNDAR RANGAPURAM", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1,fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Sebastian Pineda Arango is an author of the paper", "id": "SEBASTIAN PINEDA ARANGO", "label": "SEBASTIAN PINEDA ARANGO", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Shubham Kapoor is an author of the paper", "id": "SHUBHAM KAPOOR", "label": "SHUBHAM KAPOOR", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jasper Zschiegner is an author of the paper", "id": "JASPER ZSCHIEGNER", "label": "JASPER ZSCHIEGNER", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Danielle C. Maddix is an author of the paper", "id": "DANIELLE C. MADDIX", "label": "DANIELLE C. MADDIX", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Hao Wang is an author of the paper", "id": "HAO WANG", "label": "HAO WANG", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Honey is an author of the paper", "id": "HONEY", "label": "HONEY", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Michael Bohlke-Schneider is an author of the paper", "id": "MICHAEL BOHLKE-SCHNEIDER", "label": "MICHAEL BOHLKE-SCHNEIDER", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Yuyang is an author of the paper", "id": "YUYANG", "label": "YUYANG", "shape": "dot", "size": 10, "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Cross-domain adaptation is a concept that refers to the ability of a model to adapt to a new task or domain without any prior training", "id": "CROSS-DOMAIN ADAPTATION", "label": "CROSS-DOMAIN ADAPTATION", "shape": "dot", "size": 10, "source_id": "7e03744dea80e2138baff03611104fa8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Context tokens refer to the tokens used to represent the context of a time series", "id": "CONTEXT TOKENS", "label": "CONTEXT TOKENS", "shape": "dot", "size": 10, "source_id": "f7e3207706b170296cb036538a709689", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Entropy refers to a measure of the uncertainty or randomness of a time series", "id": "ENTROPY", "label": "ENTROPY", "shape": "dot", "size": 10, "source_id": "f7e3207706b170296cb036538a709689", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"MIXUP\" is a concept mentioned in the text, which refers to a data augmentation scheme proposed in the context of image classification. This scheme is likely related to the field of machine learning, as indicated by the use of technical terms such as \"data augmentation\" and \"image classification.\" The mention of \"mixup\" in the text suggests that it is a specific technique or method used to improve the performance of machine learning models, particularly in the context of image classification tasks.\n\nGiven the formal and academic language used in the text, it is likely that the concept of \"MIXUP\" is discussed in a research paper or technical document, possibly in the context of a conference or journal such as ICLR, AAAI, or PMLR. The use of mathematical equations and formulas in the text further supports this conclusion, as it is common in academic papers to present mathematical derivations and proofs to support the proposed techniques.\n\nOverall, the summary of the entity \"MIXUP\" is that it is a data augmentation scheme proposed for image classification tasks, likely discussed in a research paper or technical document in the field of machine learning.", "id": "MIXUP", "label": "MIXUP", "shape": "dot", "size": 10, "source_id": "0e3c8905bd533021b4f9bf9875ea66e0,6212b2146d9ad27124114c334a946854", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nGaussian Processes are a type of probabilistic model used for generating synthetic time series data. They are distributions over functions defined by the mean function and the positive definite kernel. Specifically, Gaussian processes are used in the Chronos framework to generate synthetic time series, as indicated by the discussion of its training protocols. This technique is employed in this work to generate synthetic time series data, highlighting its utility in data generation.\n\nThe summary is written in third person and includes the entity name \"Gaussian Processes\" for context. It also incorporates relevant information from the nearby text, resolving any potential contradictions and providing a coherent description of the entity.", "id": "GAUSSIAN PROCESSES", "label": "GAUSSIAN PROCESSES", "shape": "dot", "size": 10, "source_id": "0e3c8905bd533021b4f9bf9875ea66e0,116332ac4538a1430c83a34fcbec22d1,bc54a718d1886698232d578fd88c3ac7,c522ea3766ae5129c11b833252340695,f72ba51a17dd00cff43bcdda22c273e8,f7e3207706b170296cb036538a709689", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "TSMix is a model for time series data augmentation", "id": "TSMIX", "label": "TSMIX", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe KERNEL BANK is a collection of basis kernels that define fundamental time series patterns. This collection of kernels serves as a crucial component in identifying and understanding the underlying structures of time series data. The KERNEL BANK is essentially a repository of essential time series patterns, which can be used as a foundation for various time series analysis and forecasting tasks.\n\nThe KERNEL BANK is particularly relevant in the context of long-term series forecasting, where accurate identification of fundamental time series patterns is essential for making reliable predictions. By leveraging the KERNEL BANK, researchers and practitioners can gain valuable insights into the underlying dynamics of time series data, enabling them to develop more effective forecasting models.\n\nThe KERNEL BANK is also closely related to frequency analysis, which is a critical aspect of time series analysis. By analyzing the frequency components of time series data, researchers can identify patterns and trends that are not immediately apparent from the raw data. The KERNEL BANK can be used in conjunction with frequency analysis techniques to gain a deeper understanding of the underlying structures of time series data.\n\nOverall, the KERNEL BANK is a powerful tool for time series analysis and forecasting, and its applications extend beyond the realm of traditional time series analysis to include areas such as machine learning and artificial intelligence.", "id": "KERNEL BANK", "label": "KERNEL BANK", "shape": "dot", "size": 10, "source_id": "0e3c8905bd533021b4f9bf9875ea66e0,53dbeea6d05c3695460c71f89f468def", "type": "DATA"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nCLASSICAL FORECASTING METHODS are a type of method that fits a separate model to each time series independently. This approach is characterized by its ability to handle individual time series data, allowing for unique modeling and forecasting for each series. The use of classical forecasting methods is a common practice in time series analysis, particularly when dealing with multiple time series data. \n\nNote: The description list was empty, so the summary is based solely on the provided entity and the description list, which contained a single description.", "id": "CLASSICAL FORECASTING METHODS", "label": "CLASSICAL FORECASTING METHODS", "shape": "dot", "size": 10, "source_id": "42c90ca40b234c098c55632ec70038a1,7e69d9444a2084b6452e291735bf5a49", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"DEEP LEARNING METHODS\" refers to a type of machine learning approach that utilizes neural networks to analyze and learn from data. Specifically, these methods are designed to learn across time series in a given dataset, enabling them to capture complex patterns and relationships within the data. This approach is particularly useful for tasks such as long-term series forecasting, frequency analysis, and other applications where temporal relationships are crucial.\n\nThe use of deep learning methods in time series analysis allows for the extraction of meaningful insights and predictions from large datasets. By leveraging the power of neural networks, these methods can identify patterns and trends that may not be apparent through traditional machine learning techniques. The multi-head cross-attention mechanism, for instance, is a key component of deep learning methods that enables the model to focus on different aspects of the data simultaneously, leading to improved performance and accuracy.\n\nThe entity \"DEEP LEARNING METHODS\" is closely related to the field of artificial intelligence and machine learning, and its applications can be found in various domains, including but not limited to, natural language processing, computer vision, and predictive analytics. The use of deep learning methods has been extensively explored in academic research, with numerous papers and conferences, such as ICLR, AAAI, and PMLR, dedicating significant attention to this topic.\n\nOverall, the entity \"DEEP LEARNING METHODS\" represents a powerful and versatile approach to machine learning that has the potential to revolutionize the way we analyze and understand complex data.", "id": "DEEP LEARNING METHODS", "label": "DEEP LEARNING METHODS", "shape": "dot", "size": 10, "source_id": "42c90ca40b234c098c55632ec70038a1,4de223d7df157faf857c3e17d9e6e5b6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Suboptimal models refer to machine learning models that do not perform well due to limitations in the data or the model itself", "id": "SUBOPTIMAL MODELS", "label": "SUBOPTIMAL MODELS", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "DeepState is a deep learning forecasting model that uses RNNs", "id": "DEEPSTATE", "label": "DEEPSTATE", "shape": "dot", "size": 10, "source_id": "42c90ca40b234c098c55632ec70038a1,7e69d9444a2084b6452e291735bf5a49", "type": "MODEL"}, {"color": "#97c2fc", "description": "DeepFactor is a deep learning forecasting model that uses RNNs", "id": "DEEPFACTOR", "label": "DEEPFACTOR", "shape": "dot", "size": 10, "source_id": "7e69d9444a2084b6452e291735bf5a49", "type": "MODEL"}, {"color": "#97c2fc", "description": "Transformer-based models are a type of neural network model that are gaining popularity in recent years", "id": "TRANSFORMER-BASED MODELS", "label": "TRANSFORMER-BASED MODELS", "shape": "dot", "size": 10, "source_id": "7d5d82d600620153153772a9bc498ac0", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nLAG-LAGGAMA is a foundation model for univariate probabilistic forecasting. It is a type of model that is specifically designed to handle time series data, which involves analyzing and forecasting the future values of a single variable over time. The model is trained on a corpus of time series data, allowing it to learn patterns and relationships within the data that can be used to make accurate predictions.\n\nThe use of the term \"univariate probabilistic forecasting\" suggests that LAG-LAGGAMA is capable of producing probability distributions for future values, rather than simply point estimates. This is a key aspect of probabilistic forecasting, as it allows for the quantification of uncertainty and the generation of confidence intervals.\n\nOverall, LAG-LAGGAMA appears to be a sophisticated model for time series forecasting, with a strong focus on probabilistic predictions and the analysis of complex time series data.\n\nNote: The information provided is based solely on the descriptions provided and does not include any external information.", "id": "LAG-LAGGAMA", "label": "LAG-LAGGAMA", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,ac37bdb991d1513e44e4c5fc7a28b187", "type": "MODEL"}, {"color": "#97c2fc", "description": "Stratified sampling is a process that involves sampling a dataset in a way that ensures that each subgroup is represented proportionally", "id": "STRATIFIED SAMPLING", "label": "STRATIFIED SAMPLING", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Reasoning abilities refer to the ability to draw conclusions or make inferences based on data", "id": "REASONING ABILITIES", "label": "REASONING ABILITIES", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"OUTPUT PROJECTION\" is a component of the model that projects the output of the Large Language Model (LLM) into a specific format. This process involves transforming the output of the LLM into a format that is suitable for a particular application or use case. The output projection is a critical component of the model, as it enables the LLM to produce output that is relevant and useful to the user.\n\nIn the context of time series forecasting, the output projection may involve projecting the output of the LLM into a specific format that is suitable for forecasting long-term series. This may involve frequency analysis and multi-head cross-attention mechanisms to capture complex patterns and relationships in the data.\n\nOverall, the \"OUTPUT PROJECTION\" plays a crucial role in enabling the LLM to produce output that is relevant and useful to the user, and its design and implementation are critical to the success of the model.\n\nRelevant information from the nearby text suggests that the output projection is a technical term used in the context of machine learning and natural language processing, and is likely to be used in academic papers and technical documents. The use of English words and phrases, mathematical equations, and references to academic papers and authors also suggests that the text is written in English.", "id": "OUTPUT PROJECTION", "label": "OUTPUT PROJECTION", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a,fececbac281c1e2b13921f378df30919", "type": "COMPONENT"}, {"color": "#97c2fc", "description": "Patch embedder is a simple linear layer used to create dimensions for patch embeddings", "id": "PATCH EMBEDDER", "label": "PATCH EMBEDDER", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "MODEL"}, {"color": "#97c2fc", "description": "Tokenized time series refers to the process of breaking down time series data into individual tokens or patches", "entity_type": "CONCEPT", "id": "TOKENIZED TIME SERIES", "label": "TOKENIZED TIME SERIES", "shape": "dot", "size": 10, "source_id": "bc54a718d1886698232d578fd88c3ac7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Few-shot learners are a type of machine learning model", "id": "FEW-SHOT LEARNERS", "label": "FEW-SHOT LEARNERS", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "NUMERALS", "label": "NUMERALS", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TIME SERIES MACHINES\" refers to models or systems that can process and analyze time series data. These machines are proficient in time series tasks, indicating their ability to handle and interpret temporal data effectively. The primary language of the text related to TIME SERIES MACHINES is English, as evident from the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nThe language used is formal and academic, suggesting that the text is from a research paper or a technical document. The presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports the conclusion that the primary language of the text is English. The citation of English-language academic papers and authors also confirms this finding.\n\nOverall, TIME SERIES MACHINES are sophisticated models or systems designed to handle and analyze time series data, with a strong emphasis on proficiency in time series tasks. The language and content of the text related to these machines are consistent with English-language academic papers and technical documents.", "id": "TIME SERIES MACHINES", "label": "TIME SERIES MACHINES", "shape": "dot", "size": 10, "source_id": "6d66c2ea37e25b646a13d751c05a8e4d,ec3fbfb800fd9bf1d913584fda4ae925", "type": "MODEL"}, {"color": "#97c2fc", "description": "Observations refer to the individual data points in a time series", "id": "OBSERVATIONS", "label": "OBSERVATIONS", "shape": "dot", "size": 10, "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"STANDARD SCALING\" is a normalization technique that involves applying an affine transformation to the time series. This concept is mentioned in the text and is often used in deep learning models. Standard scaling is a method used to normalize the data, making it suitable for use in various machine learning models, including those that utilize time series data.\n\nThe description of standard scaling as a normalization technique that involves applying an affine transformation to the time series is consistent with the concept of standard scaling being used in deep learning models. This suggests that standard scaling is a widely applicable technique that can be used in a variety of contexts, including time series analysis and deep learning.\n\nOverall, the summary of the entity \"STANDARD SCALING\" is that it is a normalization technique used to transform time series data, often applied in deep learning models to normalize the data and make it suitable for use in machine learning algorithms.", "id": "STANDARD SCALING", "label": "STANDARD SCALING", "shape": "dot", "size": 10, "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8,6eb4c16edf2eedfd03721efb199478d8", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMIN-MAX SCALING is a normalization technique that involves applying an affine transformation to time series data, often used in deep learning models. This technique is mentioned in the text as a concept that is commonly used in various applications, including time series analysis and machine learning.\n\nThe description of MIN-MAX SCALING as a normalization technique that applies an affine transformation to time series data is consistent with the information provided in the text. The text does not mention any contradictions or conflicting information regarding MIN-MAX SCALING, and therefore, the summary provided above is a coherent and accurate representation of the entity.\n\nRelevant information from the nearby text suggests that MIN-MAX SCALING is a widely used technique in deep learning models, which is consistent with the description provided in the description list. The text does not provide any additional information about MIN-MAX SCALING, but the summary provided above captures the essence of the entity and its application in time series analysis and machine learning.", "id": "MIN-MAX SCALING", "label": "MIN-MAX SCALING", "shape": "dot", "size": 10, "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8,6eb4c16edf2eedfd03721efb199478d8", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"CATEGORICAL DISTRIBUTION\" refers to a type of probability distribution used in time series analysis, particularly in the context of deep learning models. Specifically, it is used to model the probability distribution over the elements of the vocabulary, which in this case are the time series tokens. This distribution is a crucial component in various machine learning applications, including long-term series forecasting and frequency analysis.\n\nIn the context of time series analysis, the categorical distribution is used to capture the underlying patterns and relationships in the data, enabling more accurate predictions and forecasts. The use of deep learning models, such as those employing multi-head cross-attention mechanisms, can further enhance the performance of categorical distribution-based models.\n\nOverall, the categorical distribution plays a vital role in time series analysis, particularly in the development of deep learning models that can accurately capture the underlying patterns and relationships in the data.\n\nRelevant information from the nearby text:\n\n* The text mentions the use of technical terms, mathematical equations, and references to academic papers, which are all written in English.\n* The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n* The text cites English-language academic papers and authors, which further supports the conclusion that the primary language of the text is English.\n\nNote: The provided descriptions are not contradictory, and the summary is a coherent representation of the information provided.", "id": "CATEGORICAL DISTRIBUTION", "label": "CATEGORICAL DISTRIBUTION", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8,c5c841baa0bc205103ac433d446da3b6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"CROSS ENTROPY\" is a measure of the difference between two probability distributions. It is often used as a loss function in machine learning, indicating its significance in the field of artificial intelligence and data analysis. The concept of cross entropy is rooted in the idea of comparing the predicted probabilities of a model with the actual probabilities of a given dataset, allowing for the evaluation of the model\u0027s performance and the identification of areas for improvement.\n\nIn the context of machine learning, cross entropy is a crucial component in the development and training of models, particularly in tasks such as classification and regression. Its widespread use is a testament to its effectiveness in measuring the discrepancy between predicted and actual outcomes, making it an essential tool for data scientists and researchers.\n\nThe use of cross entropy as a loss function is also supported by its mathematical formulation, which involves the calculation of the difference between the predicted and actual probabilities. This calculation is often represented by the formula H(p, q) = -\u2211(p(x)log(q(x))), where p(x) and q(x) are the predicted and actual probabilities, respectively.\n\nOverall, the entity \"CROSS ENTROPY\" plays a vital role in the field of machine learning, serving as a key metric for evaluating model performance and guiding the development of more accurate and effective models.", "id": "CROSS ENTROPY", "label": "CROSS ENTROPY", "shape": "dot", "size": 10, "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7,c5c841baa0bc205103ac433d446da3b6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"CLASSIFICATION\" is a time series analysis task that involves predicting the class or category of a time series. Classification is a process of assigning a label or category to a time series, which is often used in time series analysis. This task is a crucial aspect of various applications, including but not limited to, forecasting, anomaly detection, and decision-making.\n\nThe summary is written in third person and includes the entity name \"CLASSIFICATION\" for context. It also incorporates relevant information from the provided descriptions, resolving any potential contradictions to provide a coherent summary.", "id": "CLASSIFICATION", "label": "CLASSIFICATION", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8,b4d5306b46bbfa4564727fe5ac6630e0", "type": "TASK"}, {"color": "#97c2fc", "description": "Clustering is a time series analysis task that involves grouping similar time series together", "id": "CLUSTERING", "label": "CLUSTERING", "shape": "dot", "size": 10, "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Quantile binning is a concept mentioned in the text, referring to a type of data-dependent binning", "id": "QUANTILE BINNING", "label": "QUANTILE BINNING", "shape": "dot", "size": 10, "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Uniform binning is a concept mentioned in the text, referring to a type of binning", "id": "UNIFORM BINNING", "label": "UNIFORM BINNING", "shape": "dot", "size": 10, "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time series tokens refer to the individual data points in a time series, often represented as a sequence of numbers", "id": "TIME SERIES TOKENS", "label": "TIME SERIES TOKENS", "shape": "dot", "size": 10, "source_id": "c5c841baa0bc205103ac433d446da3b6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The PAD token is a special token used to pad time series of different lengths to a fixed length for batch construction and to replace missing values", "id": "PAD TOKEN", "label": "PAD TOKEN", "shape": "dot", "size": 10, "source_id": "c5c841baa0bc205103ac433d446da3b6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The EOS token is a special token appended to the quantized and padded time series to denote the end of the sequence", "id": "EOS TOKEN", "label": "EOS TOKEN", "shape": "dot", "size": 10, "source_id": "c5c841baa0bc205103ac433d446da3b6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Bin refers to a specific category or group in the text, as indicated by the use of the term \"bin\"", "id": "BIN", "label": "BIN", "shape": "dot", "size": 10, "source_id": "6212b2146d9ad27124114c334a946854", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "CATEGORICAL CROSS ENTROPY LOSS", "label": "CATEGORICAL CROSS ENTROPY LOSS", "shape": "dot", "size": 10, "source_id": "6212b2146d9ad27124114c334a946854", "type": ""}, {"color": "#97c2fc", "description": "Gaussian is a concept mentioned in the text, as indicated by the use of the term \"Gaussian\"", "id": "GAUSSIAN", "label": "GAUSSIAN", "shape": "dot", "size": 10, "source_id": "6212b2146d9ad27124114c334a946854", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Student\u0027s-t is a concept mentioned in the text, as indicated by the use of the term \"Student\u0027s-t\"", "id": "STUDENT\u0027S-T", "label": "STUDENT\u0027S-T", "shape": "dot", "size": 10, "source_id": "6212b2146d9ad27124114c334a946854", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "WikiText-103 is a dataset mentioned in the text, as indicated by the use of the name \"WikiText-103\"", "id": "WIKITEXT-103", "label": "WIKITEXT-103", "shape": "dot", "size": 10, "source_id": "6212b2146d9ad27124114c334a946854", "type": "DATASET"}, {"color": "#97c2fc", "description": "C4 is a dataset mentioned in the text, as indicated by the use of the name \"C4\"", "id": "C4", "label": "C4", "shape": "dot", "size": 10, "source_id": "6212b2146d9ad27124114c334a946854", "type": "DATASET"}, {"color": "#97c2fc", "description": "The Pile is a dataset mentioned in the text, as indicated by the use of the name \"The Pile\"", "id": "THE PILE", "label": "THE PILE", "shape": "dot", "size": 10, "source_id": "6212b2146d9ad27124114c334a946854", "type": "DATASET"}, {"color": "#97c2fc", "description": "", "id": "DIVERSITY", "label": "DIVERSITY", "shape": "dot", "size": 10, "source_id": "0e3c8905bd533021b4f9bf9875ea66e0", "type": ""}, {"color": "#97c2fc", "description": "Tab. 1 is a table mentioned in the text, which is likely a reference to a previous table or figure", "id": "TAB. 1", "label": "TAB. 1", "shape": "dot", "size": 10, "source_id": "fd1092903d83bf6e90a6caa371d7c514", "type": "TABLE"}, {"color": "#97c2fc", "description": "Linear kernel refers to a type of kernel used to define trend in time series", "id": "LINEAR KERNEL", "label": "LINEAR KERNEL", "shape": "dot", "size": 10, "source_id": "0e3c8905bd533021b4f9bf9875ea66e0", "type": "KERNEL"}, {"color": "#97c2fc", "description": "RBF kernel refers to a type of kernel used to define smooth local variation in time series", "id": "RBF KERNEL", "label": "RBF KERNEL", "shape": "dot", "size": 10, "source_id": "0e3c8905bd533021b4f9bf9875ea66e0", "type": "KERNEL"}, {"color": "#97c2fc", "description": "Periodic kernel refers to a type of kernel used to define seasonalities in time series", "id": "PERIODIC KERNEL", "label": "PERIODIC KERNEL", "shape": "dot", "size": 10, "source_id": "0e3c8905bd533021b4f9bf9875ea66e0", "type": "KERNEL"}, {"color": "#97c2fc", "description": "The kernel specifies a covariance function which defines the joint variability of the function values at an arbitrary pair of points", "id": "KERNEL", "label": "KERNEL", "shape": "dot", "size": 10, "source_id": "53dbeea6d05c3695460c71f89f468def", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Basis kernels are fundamental time series patterns used to construct the kernel bank", "id": "BASIS KERNELS", "label": "BASIS KERNELS", "shape": "dot", "size": 10, "source_id": "53dbeea6d05c3695460c71f89f468def", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Synthetic time series is generated by drawing a sample of length lsyn from the GP prior", "id": "SYNTHETIC TIME SERIES", "label": "SYNTHETIC TIME SERIES", "shape": "dot", "size": 10, "source_id": "53dbeea6d05c3695460c71f89f468def", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Linear kernels are used to represent trend in time series", "id": "LINEAR KERNELS", "label": "LINEAR KERNELS", "shape": "dot", "size": 10, "source_id": "53dbeea6d05c3695460c71f89f468def", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "RBF kernels are used to represent smooth local variation in time series", "id": "RBF KERNELS", "label": "RBF KERNELS", "shape": "dot", "size": 10, "source_id": "53dbeea6d05c3695460c71f89f468def", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Periodic kernels are used to represent seasonalities found in typical time series frequencies", "id": "PERIODIC KERNELS", "label": "PERIODIC KERNELS", "shape": "dot", "size": 10, "source_id": "53dbeea6d05c3695460c71f89f468def", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "GP prior is a probability distribution used to generate synthetic time series", "id": "GP PRIOR", "label": "GP PRIOR", "shape": "dot", "size": 10, "source_id": "53dbeea6d05c3695460c71f89f468def", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nWQL is a metric used to evaluate the performance of time series forecasting models. It is a probabilistic forecasting metric that is scale-dependent, meaning its performance evaluation is influenced by the scale of the data being analyzed. Specifically, WQL is used to measure the performance of models on Benchmark II, indicating its relevance in evaluating model performance in a specific context. Overall, WQL is a type of metric used to assess the effectiveness of time series forecasting models, providing valuable insights into their performance and accuracy.\n\nThis summary is written in third person and includes the entity name \"WQL\" to provide context. It also incorporates information from all the descriptions, resolving any potential contradictions and providing a coherent summary.", "id": "WQL", "label": "WQL", "shape": "dot", "size": 10, "source_id": "2eea45c6111e468189580a21686cb14a,a90c6ca26542ea5935f9d8d5bf3688a9,baa887ae552c6b3dac10f74896d8c4a2,c522ea3766ae5129c11b833252340695,e37f4063d074e1697e1f539131b3d963", "type": "METRIC"}, {"color": "#97c2fc", "description": "Lag-Lagga is a type of model used for time series forecasting", "id": "LAG-LAGGA", "label": "LAG-LAGGA", "shape": "dot", "size": 10, "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "type": "MODEL"}, {"color": "#97c2fc", "description": "LLM Time is a type of model used for time series forecasting", "id": "LLM TIME", "label": "LLM TIME", "shape": "dot", "size": 10, "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "type": "MODEL"}, {"color": "#97c2fc", "description": "Benchmark II baseline is a type of model used for time series forecasting", "id": "BENCHMARK II BASELINE", "label": "BENCHMARK II BASELINE", "shape": "dot", "size": 10, "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "FINE TUNING", "label": "FINE TUNING", "shape": "dot", "size": 10, "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "type": ""}, {"color": "#97c2fc", "description": "TSMixup augmentations refer to a type of data augmentation used in time series forecasting", "id": "TSMIXUP AUGMENTATIONS", "label": "TSMIXUP AUGMENTATIONS", "shape": "dot", "size": 10, "source_id": "c7f04fa1168df64cce110e20a1b72b51", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "T5 model is a type of pre-trained language model used for initialization", "id": "T5 MODEL", "label": "T5 MODEL", "shape": "dot", "size": 10, "source_id": "c7f04fa1168df64cce110e20a1b72b51", "type": "MODEL"}, {"color": "#97c2fc", "description": "Pretraining-only data is data used only for training Chronos models", "id": "PRETRAINING-ONLY DATA", "label": "PRETRAINING-ONLY DATA", "shape": "dot", "size": 10, "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "type": "DATA"}, {"color": "#97c2fc", "description": "In-domain evaluation data is data used for training and testing Chronos models", "id": "IN-DOMAIN EVALUATION DATA", "label": "IN-DOMAIN EVALUATION DATA", "shape": "dot", "size": 10, "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "type": "DATA"}, {"color": "#97c2fc", "description": "Zero-shot evaluation data is data used only for evaluation of Chronos models", "id": "ZERO-SHOT EVALUATION DATA", "label": "ZERO-SHOT EVALUATION DATA", "shape": "dot", "size": 10, "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "type": "DATA"}, {"color": "#97c2fc", "description": "Validation set refers to the portion of data used to evaluate a model\u0027s performance", "id": "VALIDATION SET", "label": "VALIDATION SET", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nWeatherBench is a comprehensive benchmark dataset specifically designed for weather prediction research. It contains a dataset of weather and climate data, which is a collection of various weather-related information. This dataset is intended to serve as a valuable resource for researchers and scientists working in the field of weather prediction, providing a standardized and reliable source of data for testing and evaluating different models and techniques.\n\nThe WeatherBench dataset is a crucial tool for advancing our understanding of weather patterns and improving weather forecasting capabilities. By leveraging this dataset, researchers can develop and refine their models, ultimately leading to more accurate and reliable weather predictions.\n\nThe dataset\u0027s comprehensive nature, including its coverage of various weather and climate-related aspects, makes it an ideal choice for researchers seeking to explore different aspects of weather prediction. The WeatherBench dataset is a significant contribution to the field of weather prediction research, and its impact is expected to be substantial in the years to come.\n\nOverall, WeatherBench is a vital resource for the weather prediction community, providing a standardized and reliable dataset for researchers to work with. Its comprehensive nature and focus on weather prediction research make it an essential tool for advancing our understanding of weather patterns and improving weather forecasting capabilities.", "id": "WEATHERBENCH", "label": "WEATHERBENCH", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525,6a222f9ed7fcf5fc945dfc22e16a3502,e067234b24b0625a3f95ecc18035f915", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"CHRONOS-T5 (LARGE)\" is a model that has been fine-tuned on datasets from Benchmark II. This model has demonstrated superior performance compared to local models, outperforming them significantly. Additionally, it has been observed that Chronos-T5 (Large) performs better than local statistical models, showcasing its effectiveness in time series forecasting.\n\nIn terms of point forecasting performance, Chronos-T5 (Large) has placed third, indicating its strong capabilities in this area. Furthermore, the model has significantly outperformed baseline models on Benchmark I, solidifying its position as a top-performing model in time series forecasting.\n\nOverall, Chronos-T5 (Large) is a robust and accurate model that has been validated through its performance on various benchmarks, making it a reliable choice for time series forecasting tasks.", "id": "CHRONOS-T5 (LARGE)", "label": "CHRONOS-T5 (LARGE)", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,532a6d434dab611a80aef1e94dd2fb45,56de1d5a5b467101344afa4248d829dc,baa887ae552c6b3dac10f74896d8c4a2", "type": "MODEL"}, {"color": "#97c2fc", "description": "Transformers library is a software used to train the Chronos models, as indicated by the discussion of its features and configuration", "id": "TRANSFORMERS LIBRARY", "label": "TRANSFORMERS LIBRARY", "shape": "dot", "size": 10, "source_id": "f72ba51a17dd00cff43bcdda22c273e8", "type": "SOFTWARE"}, {"color": "#97c2fc", "description": "GPU inference speed refers to the speed at which a model can perform inference on a GPU", "id": "GPU INFERENCE SPEED", "label": "GPU INFERENCE SPEED", "shape": "dot", "size": 10, "source_id": "55eb54ef455d14cd1a11760924f99eb8", "type": "MEASURE"}, {"color": "#97c2fc", "description": "STATSFORCAST is a platform used for statistical baselines", "id": "STATSFORCAST", "label": "STATSFORCAST", "shape": "dot", "size": 10, "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "type": "PLATFORM"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nGLUONTS is a platform used for time series forecasting, specifically designed as a framework for developing and training time series models. It is a probabilistic and neural time series modeling framework in Python, which is an open-source library used in various projects. GLUONTS provides a comprehensive set of tools and techniques for building and evaluating time series models, making it a valuable resource for researchers and practitioners in the field of time series forecasting.\n\nThe framework is particularly well-suited for long-term series forecasting, frequency analysis, and other advanced time series modeling tasks. Its probabilistic nature allows for the incorporation of uncertainty and variability in the forecasting process, making it a robust and reliable tool for real-world applications. Additionally, GLUONTS is designed to be highly flexible and customizable, enabling users to easily adapt the framework to their specific needs and requirements.\n\nOverall, GLUONTS is a powerful and versatile platform for time series forecasting, offering a wide range of features and capabilities that make it an essential tool for anyone working with time series data.", "id": "GLUONTS", "label": "GLUONTS", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,1ddbab2dca370c9ef7b5a724075518cc,35e5d369f2d5fe8c06e11244cf7c9ba1,88d96b5f76042688e9dd745eb822d919", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data related to the entity \"UBER\":\n\nUBER is a ride-hailing company that operates as a rideshare service, providing various hourly statistics. The entity is also associated with a dataset, referred to as the \"Uber dataset,\" which is mentioned in the text as part of the pretraining corpus. Specifically, the text mentions the \"CRPS of Lag-Llama on the 7/20 datasets in the pretraining corpus, compared to supervised baselines trained solely on the respective datasets,\" indicating that the Uber dataset is used for evaluating the performance of machine learning models, such as Lag-Llama, in long-term series forecasting tasks.\n\nThe use of the Uber dataset in the context of time series forecasting and model evaluation suggests that the company\u0027s hourly statistics, which are likely related to ride-hailing activity, are being analyzed and modeled using advanced machine learning techniques. This analysis is likely aimed at improving the accuracy of long-term series forecasting, which is a critical aspect of ride-hailing services, where demand and supply need to be balanced to ensure efficient service delivery.\n\nOverall, the summary provides a comprehensive understanding of the entity \"UBER\" as a ride-hailing company with a dataset used for time series forecasting and model evaluation, highlighting the importance of advanced machine learning techniques in analyzing and improving the efficiency of ride-hailing services.", "id": "UBER", "label": "UBER", "shape": "dot", "size": 10, "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c,6820cea275275e87630a6876e890c525,a875a1c0bede47a1c4e3823be81c42c6", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Windfarms is a dataset mentioned in the text, as indicated by the use of the phrase \"CRPS of Lag-Llama on the 7/20 datasets in the pretraining corpus, compared to supervised baselines trained solely on the respective datasets\"", "id": "WINDFARMS", "label": "WINDFARMS", "shape": "dot", "size": 10, "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nETT H1 is a concept related to energy and is also a dataset used in the context of machine learning and time series forecasting. Specifically, ETT H1 is a dataset that is mentioned in the text as being used for pretraining and fine-tuning, as indicated by the use of the phrase \"CRPS of Lag-Llama on the 7/20 datasets in the pretraining corpus, compared to supervised baselines trained solely on the respective datasets.\" This suggests that ETT H1 is a dataset that is used to evaluate the performance of machine learning models, particularly in the context of long-term series forecasting and frequency analysis. The use of ETT H1 in this context implies that it is a dataset that is relevant to the field of energy and time series forecasting, and is likely used to train and evaluate models that can predict energy-related time series data.\n\nIt is worth noting that the language used to describe ETT H1 is formal and academic, suggesting that it is a dataset that is used in research and academic settings. The use of technical terms and mathematical equations in the text also suggests that ETT H1 is a dataset that is used in the context of machine learning and time series forecasting, and is likely used to evaluate the performance of models that can predict energy-related time series data.", "id": "ETT H1", "label": "ETT H1", "shape": "dot", "size": 10, "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c,4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the entity \"ETT H2\" in third person:\n\nETT H2 is a concept related to energy and is also a dataset used in various machine learning tasks. Specifically, it is a dataset mentioned in the text that is used for pretraining and fine-tuning, as well as a traffic metric used for fine-tuning forecasting tasks. The dataset is likely used in the context of long-term series forecasting, frequency analysis, and multi-head cross-attention, given the technical terms and mathematical equations mentioned in the text. ETT H2 is a specific concept or measure that is used in the field of energy and is likely used in conjunction with other datasets, such as the 7/20 datasets, for training and evaluation purposes.\n\nThe use of ETT H2 as a dataset for pretraining and fine-tuning suggests that it is a valuable resource for training machine learning models, particularly those used for forecasting tasks. The fact that it is used in conjunction with other datasets, such as the 7/20 datasets, also suggests that it is a widely used and respected dataset in the field of energy and machine learning.\n\nOverall, ETT H2 is a concept and dataset that plays a significant role in the field of energy and machine learning, particularly in the context of forecasting tasks and long-term series forecasting.", "id": "ETT H2", "label": "ETT H2", "shape": "dot", "size": 10, "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26,4bd5a8e9285aae7ca7363c8e61ba361c,4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e,ec7705b83cf4fe3aa18662c917b18c1a", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nETT M1 is a concept related to energy and is also a dataset used in the context of machine learning and time series forecasting. Specifically, ETT M1 is a dataset that is mentioned in the text as being used for pretraining and fine-tuning, as indicated by the use of the phrase \"CRPS of Lag-Llama on the 7/20 datasets in the pretraining corpus, compared to supervised baselines trained solely on the respective datasets.\" This suggests that ETT M1 is a dataset that is used to evaluate the performance of machine learning models, particularly in the context of time series forecasting.\n\nThe use of ETT M1 as a dataset for pretraining and fine-tuning implies that it is a valuable resource for training and evaluating machine learning models, particularly those that are designed to handle time series data. The fact that it is mentioned in the context of comparing the performance of Lag-Llama to supervised baselines trained solely on the respective datasets suggests that ETT M1 is a challenging dataset that requires sophisticated machine learning models to achieve good performance.\n\nOverall, ETT M1 appears to be a dataset that is used in the context of machine learning and time series forecasting, and is particularly relevant to the development and evaluation of models that are designed to handle energy-related data.", "id": "ETT M1", "label": "ETT M1", "shape": "dot", "size": 10, "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c,4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "type": "DATASET"}, {"color": "#97c2fc", "description": "Air Quality UCI is a dataset mentioned in the text, as indicated by the use of the phrase \"CRPS of Lag-Llama on the 7/20 datasets in the pretraining corpus, compared to supervised baselines trained solely on the respective datasets\"", "id": "AIR QUALITY UCI", "label": "AIR QUALITY UCI", "shape": "dot", "size": 10, "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"BEIJING MULTISITE\" is a concept related to air quality. It is also a dataset mentioned in the text, which is used for pretraining and fine-tuning. Specifically, the dataset is referred to as the \"7/20 datasets\" in the pretraining corpus, and it is compared to supervised baselines trained solely on the respective datasets. The CRPS (Continuous Ranked Probability Score) of Lag-Llama on the Beijing Multisite dataset is evaluated and compared to the supervised baselines. This suggests that the Beijing Multisite dataset is used for evaluating the performance of machine learning models, particularly in the context of long-term series forecasting and frequency analysis.", "id": "BEIJING MULTISITE", "label": "BEIJING MULTISITE", "shape": "dot", "size": 10, "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c,4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"CHRONOS-T5 (BASE)\" is a model that has been developed for time series forecasting. It is a specific implementation of the Chronos model, which is designed to generate plausible forecasts across various AR processes. The model has been fine-tuned on datasets from Benchmark II and has demonstrated significant performance improvements over local statistical models. In fact, it has outperformed baseline models on Benchmark I and placed 2nd in terms of point forecasting performance. This suggests that CHRONOS-T5 (BASE) is a robust and effective model for time series forecasting, particularly when compared to local statistical models.\n\nThe model\u0027s performance can be attributed to its ability to handle complex time series data and generate accurate forecasts. Its fine-tuning on Benchmark II datasets has enabled it to learn from a diverse range of time series patterns and anomalies, making it a valuable tool for practitioners and researchers in the field of time series forecasting.\n\nOverall, CHRONOS-T5 (BASE) is a state-of-the-art model that has shown significant promise in the field of time series forecasting. Its performance on Benchmark I and II datasets, as well as its ability to outperform local statistical models, make it a valuable addition to the toolkit of any practitioner or researcher working with time series data.", "id": "CHRONOS-T5 (BASE)", "label": "CHRONOS-T5 (BASE)", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,532a6d434dab611a80aef1e94dd2fb45,56de1d5a5b467101344afa4248d829dc,9c155897f3e35902f57696e0abaeb161,baa887ae552c6b3dac10f74896d8c4a2,bca10b04933dc3a7f98a3f1b610e419b", "type": "MODEL"}, {"color": "#97c2fc", "description": "AR (with correct order) refers to an autoregressive model with the correct order", "id": "AR (WITH CORRECT ORDER)", "label": "AR (WITH CORRECT ORDER)", "shape": "dot", "size": 10, "source_id": "bca10b04933dc3a7f98a3f1b610e419b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "StatsForecast library is a software library used for statistical forecasting", "id": "STATSFORECAST LIBRARY", "label": "STATSFORECAST LIBRARY", "shape": "dot", "size": 10, "source_id": "2565ae205d4c98342168bb67a4f7a309", "type": "SOFTWARE"}, {"color": "#97c2fc", "description": "N-BEATSN-HITS models are a type of model that uses a neural network approach to forecasting", "id": "N-BEATSN-HITS", "label": "N-BEATSN-HITS", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1", "type": "MODEL TYPE"}, {"color": "#97c2fc", "description": "Probabilistic forecasts refer to a predicted distribution of possible values for a given time series", "id": "PROBABILISTIC FORECASTS", "label": "PROBABILISTIC FORECASTS", "shape": "dot", "size": 10, "source_id": "f0808ad97bc4d26391284145427770e5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Quantiles refer to the predicted values at specific probability levels for a given time series", "id": "QUANTILES", "label": "QUANTILES", "shape": "dot", "size": 10, "source_id": "f0808ad97bc4d26391284145427770e5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"CHRONOS-T5 (MINI)\" is a model that has been fine-tuned on datasets from Benchmark II. This fine-tuning has resulted in significant performance improvements compared to local models. Specifically, Chronos-T5 (Mini) outperforms local statistical models, demonstrating its effectiveness in time series forecasting tasks. The model\u0027s performance is likely due to its ability to leverage the large-scale datasets from Benchmark II, which provides it with a more comprehensive understanding of the underlying patterns and relationships in the data. Overall, Chronos-T5 (Mini) appears to be a robust and accurate model for time series forecasting, particularly when compared to local statistical models.", "entity_type": "MODEL", "id": "CHRONOS-T5 (MINI)", "label": "CHRONOS-T5 (MINI)", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,baa887ae552c6b3dac10f74896d8c4a2", "type": "MODEL"}, {"color": "#97c2fc", "description": "Gaussian observations refer to data points that follow a normal distribution", "id": "GAUSSIAN OBSERVATIONS", "label": "GAUSSIAN OBSERVATIONS", "shape": "dot", "size": 10, "source_id": "bca10b04933dc3a7f98a3f1b610e419b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Supervised models are models that are trained on labeled data", "id": "SUPERVISED MODELS", "label": "SUPERVISED MODELS", "shape": "dot", "size": 10, "source_id": "c5dc13d7191b625e7373e79907b5782a", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe entity \"FORECAST\" refers to the prediction of future values or outcomes. This prediction is a key aspect of time series analysis, which involves the study of patterns and trends in data over time. Forecasting is a crucial component of long-term series forecasting, where the goal is to predict future values or outcomes based on historical data and trends.\n\nIn the context of time series forecasting, forecasting techniques such as frequency analysis and multi-head cross-attention are often employed to identify patterns and relationships in the data. These techniques can be used to develop accurate models that can predict future values or outcomes with a high degree of confidence.\n\nOverall, the entity \"FORECAST\" is a critical concept in time series analysis and forecasting, and is used to predict future values or outcomes based on historical data and trends.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which is consistent with the formal and academic language used in the description of the entity \"FORECAST\".\n* The text also mentions that the language used is formal and academic, suggesting that the text is from a research paper or a technical document, which is consistent with the context of time series analysis and forecasting.\n* The text does not provide any contradictory information, and the description of the entity \"FORECAST\" is consistent with the context of time series analysis and forecasting.", "id": "FORECAST", "label": "FORECAST", "shape": "dot", "size": 10, "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26,c60a8238db3d56eff9cc9692e7ac5b1c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the Pedestrian Counts dataset in third person:\n\nThe Pedestrian Counts dataset, also referred to as ped-counts, is a dataset specifically designed for fine-tuning forecasting tasks. This dataset encompasses hourly pedestrian counts recorded by 66 sensors within the city of Melbourne, Australia, commencing in May 2009. The dataset provides valuable insights into pedestrian movement patterns and can be utilized for various applications, including long-term series forecasting and frequency analysis. The data collected from these sensors can be leveraged to develop accurate forecasting models, which can be further fine-tuned using techniques such as multi-head cross-attention. Overall, the Pedestrian Counts dataset is a valuable resource for researchers and practitioners working in the field of time series forecasting and pedestrian movement analysis.", "id": "PEDESTRIAN COUNTS DATASET", "label": "PEDESTRIAN COUNTS DATASET", "shape": "dot", "size": 10, "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26,85902d07a5ac3acacd692810072fa1c6", "type": "DATASET"}, {"color": "#97c2fc", "description": "Requests Minute dataset is a dataset used for fine-tuning forecasting tasks", "id": "REQUESTS MINUTE DATASET", "label": "REQUESTS MINUTE DATASET", "shape": "dot", "size": 10, "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"CHRONOS-GPT2\" is a model that has demonstrated superior performance compared to local statistical models. Specifically, it has achieved a notable ranking in terms of point forecasting performance, placing 5th among other models. This suggests that CHRONOS-GPT2 has been effectively trained and fine-tuned to excel in time series forecasting tasks, potentially leveraging its architecture and training data to make accurate predictions.", "entity_type": "MODEL", "id": "CHRONOS-GPT2", "label": "CHRONOS-GPT2", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,532a6d434dab611a80aef1e94dd2fb45", "type": "MODEL"}, {"color": "#97c2fc", "description": "Moirai-1.0-R (Large) is a model that performs better than local statistical models", "entity_type": "MODEL", "id": "MOIRAI-1.0-R (LARGE)", "label": "MOIRAI-1.0-R (LARGE)", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nAUTO-THETA is a model that has demonstrated superior performance compared to local statistical models. Specifically, it has achieved a notable ranking in terms of point forecasting performance, placing 6th among other models. This suggests that AUTO-THETA is a robust and effective model for time series forecasting tasks, capable of producing accurate predictions.", "entity_type": "MODEL", "id": "AUTO-THETA", "label": "AUTO-THETA", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f,532a6d434dab611a80aef1e94dd2fb45", "type": "MODEL"}, {"color": "#97c2fc", "description": "Relative WQL is a metric used to evaluate the performance of models", "entity_type": "METRIC", "id": "RELATIVE WQL", "label": "RELATIVE WQL", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "type": "METRIC"}, {"color": "#97c2fc", "description": "Relative MASE is a metric used to evaluate the performance of models", "entity_type": "METRIC", "id": "RELATIVE MASE", "label": "RELATIVE MASE", "shape": "dot", "size": 10, "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"OWA\" refers to the \"Overall Weighted Average,\" which is a metric used to evaluate the performance of models in forecasting tasks, particularly in the context of time series forecasting. Specifically, OWA is used to assess the performance of models for short-term time series forecasting, as well as for evaluating the overall performance of time series forecasting models. This metric is utilized to provide a comprehensive evaluation of a model\u0027s performance, taking into account various aspects of its forecasting capabilities.\n\nThe use of OWA as a metric is evident in the table, where it is used to evaluate the performance of models in forecasting tasks. Additionally, OWA is mentioned in the context of academic papers and conferences, such as ICLR, AAAI, and PMLR, which suggests that it is a widely recognized and utilized metric in the field of time series forecasting.\n\nOverall, the summary provides a clear understanding of the entity \"OWA\" and its role in evaluating the performance of time series forecasting models.", "id": "OWA", "label": "OWA", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458,6d66c2ea37e25b646a13d751c05a8e4d,d4551c2839eaa68a7cb7324089956581,d540ee970ce7debedc6b1b95e9c88be8", "type": "METRIC"}, {"color": "#97c2fc", "description": "", "id": "PREDICTED CANDIDATES", "label": "PREDICTED CANDIDATES", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "type": ""}, {"color": "#97c2fc", "description": "Model size refers to the number of parameters in a model", "id": "MODEL SIZE", "label": "MODEL SIZE", "shape": "dot", "size": 10, "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Tay et al. (2021) is a research paper that describes the T5 language model", "id": "TAY ET AL. (2021)", "label": "TAY ET AL. (2021)", "shape": "dot", "size": 10, "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6", "type": "PAPER"}, {"color": "#97c2fc", "description": "Raffel et al. (2020) is a research paper that describes the C4 dataset", "id": "RAFFEL ET AL. (2020)", "label": "RAFFEL ET AL. (2020)", "shape": "dot", "size": 10, "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6", "type": "PAPER"}, {"color": "#97c2fc", "description": "C4 dataset is a large corpus of text data used for training language models", "id": "C4 DATASET", "label": "C4 DATASET", "shape": "dot", "size": 10, "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6", "type": "DATASET"}, {"color": "#97c2fc", "description": "Probabilistic predictions are a concept that is used to evaluate the performance of time series forecasting models", "id": "PROBABILISTIC PREDICTIONS", "label": "PROBABILISTIC PREDICTIONS", "shape": "dot", "size": 10, "source_id": "e37f4063d074e1697e1f539131b3d963", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "EC2 instance is a resource that is used to train time series forecasting models", "id": "EC2 INSTANCE", "label": "EC2 INSTANCE", "shape": "dot", "size": 10, "source_id": "e37f4063d074e1697e1f539131b3d963", "type": "RESOURCE"}, {"color": "#97c2fc", "description": "LLM initialization refers to the practice of initializing a model with pre-trained language model weights", "id": "LLM INITIALIZATION", "label": "LLM INITIALIZATION", "shape": "dot", "size": 10, "source_id": "c7f04fa1168df64cce110e20a1b72b51", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Random initialization refers to the practice of initializing a model with random weights", "id": "RANDOM INITIALIZATION", "label": "RANDOM INITIALIZATION", "shape": "dot", "size": 10, "source_id": "c7f04fa1168df64cce110e20a1b72b51", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Figure 10a is a visual representation of the performance of Chronos-T5 models trained with and without TSMixup augmentations", "id": "FIGURE 10A", "label": "FIGURE 10A", "shape": "dot", "size": 10, "source_id": "c7f04fa1168df64cce110e20a1b72b51", "type": "FIGURE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nPATCH 1 is a specific instance or example of a patch. The primary language of the text related to PATCH 1 is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.", "id": "PATCH 1", "label": "PATCH 1", "shape": "dot", "size": 10, "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nPATCH 2 is a specific instance or example of a patch. The description of PATCH 2 is limited to this single statement, suggesting that it is a well-defined and distinct entity within its context. However, without further information, it is unclear what PATCH 2 refers to or its specific characteristics.\n\nGiven the lack of additional details, it is not possible to provide a more detailed or enriched summary. The description provided is concise and to the point, indicating that PATCH 2 is a singular instance of a patch, but the nature and purpose of this patch remain unclear.", "id": "PATCH 2", "label": "PATCH 2", "shape": "dot", "size": 10, "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Model fine-tuning is a concept mentioned in the text, as indicated by the use of the phrase \"model fine-tuning\"", "id": "MODEL FINE-TUNING", "label": "MODEL FINE-TUNING", "shape": "dot", "size": 10, "source_id": "b27c89cb0db6646b1203b2701e017aeb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "EVENT", "label": "EVENT", "shape": "dot", "size": 10, "source_id": "69457f873272a693c1f813c75ecf030a,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nFew-shot forecasting is a concept that refers to the ability to make accurate predictions with limited data, specifically involving the process of predicting future values or outcomes with a small amount of training data. This task is used in the context of forecasting, where the goal is to make predictions with a small amount of available data, which is a critical aspect of few-shot forecasting.\n\nThe primary language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers. The text is formal and academic in nature, suggesting that it is from a research paper or a technical document.\n\nThe few-shot forecasting concept is likely discussed in the context of time series analysis, as it involves predicting future values or outcomes. The use of terms such as \"time series\" and \"long-term series forecasting\" in the text supports this inference. Additionally, the mention of \"frequency analysis\" and \"multi-head cross-attention\" suggests that the text may be discussing advanced machine learning techniques used in few-shot forecasting.\n\nOverall, few-shot forecasting is a critical concept in the field of forecasting, involving the ability to make accurate predictions with limited data, and is likely discussed in the context of time series analysis and advanced machine learning techniques.", "id": "FEW-SHOT FORECASTING", "label": "FEW-SHOT FORECASTING", "shape": "dot", "size": 10, "source_id": "5fa25d3abec59ccd2718e00c0e0eb440,b90805909f7b1f31ddc03014f161d3ba,e6a6bc6fbd362394320961ac10cdd230", "type": "TASK"}, {"color": "#97c2fc", "description": "Acceleration refers to the rate of change of an object\u0027s velocity", "id": "ACCELERATION", "label": "ACCELERATION", "shape": "dot", "size": 10, "source_id": "2dbfdb45630a023a5a6979b9573a868f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Loss of precision refers to the degradation of accuracy or detail in the data representation", "id": "LOSS OF PRECISION", "label": "LOSS OF PRECISION", "shape": "dot", "size": 10, "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe entity \"RECALL\" is a metric used to evaluate the performance of an anomaly detection model. It is denoted by the abbreviation \"R\" in the table, indicating its significance in the context of model evaluation. As a metric, recall is a crucial aspect of assessing the effectiveness of an anomaly detection model, providing valuable insights into its ability to identify and flag anomalies accurately.\n\nIn the context of machine learning and time series forecasting, recall is an essential metric that helps researchers and practitioners evaluate the performance of their models. By analyzing the recall value, they can determine the model\u0027s ability to detect anomalies and make informed decisions about its deployment and optimization.\n\nOverall, the entity \"RECALL\" plays a vital role in the evaluation of anomaly detection models, and its use is widespread in the field of machine learning and time series forecasting.", "id": "RECALL", "label": "RECALL", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,f6e57fa18831bcc732a631240536777a", "type": "METRIC"}, {"color": "#97c2fc", "description": "Fine-tuned setting refers to the scenario where the model is fine-tuned on the specific task", "id": "FINE-TUNED", "label": "FINE-TUNED", "shape": "dot", "size": 10, "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "type": "SETTING"}, {"color": "#97c2fc", "description": "Ground truth refers to the actual value or label of a data point", "id": "GROUNDTUTH", "label": "GROUNDTUTH", "shape": "dot", "size": 10, "source_id": "a90c6ca26542ea5935f9d8d5bf3688a9", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"80% INTERVAL\" refers to a range of values that contains or is expected to contain 80% of the predicted values. This range is defined as the interval within which 80% of the predicted values are likely to fall. In other words, it is a probabilistic measure that indicates the likelihood of a predicted value falling within a specific range.\n\nThe \"80% INTERVAL\" is a statistical concept used to quantify the uncertainty associated with predictions, particularly in the context of time series forecasting and long-term series forecasting. It is a useful tool for evaluating the reliability and accuracy of predictions, as it provides a clear indication of the range of possible values within which the actual outcome is likely to fall.\n\nThe \"80% INTERVAL\" can be used in various applications, including frequency analysis and multi-head cross-attention models, to improve the accuracy and reliability of predictions. It is a key concept in machine learning and time series analysis, and is often used in conjunction with other statistical techniques, such as confidence intervals and prediction intervals.\n\nOverall, the \"80% INTERVAL\" is a powerful tool for understanding and quantifying the uncertainty associated with predictions, and is an essential concept in the field of machine learning and time series analysis.", "id": "80% INTERVAL", "label": "80% INTERVAL", "shape": "dot", "size": 10, "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7,a90c6ca26542ea5935f9d8d5bf3688a9", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"FREQUENCY\" is a characteristic of time series that can be handled by TimeGPT. It refers to the time interval between data points, which is a fundamental aspect of time series analysis. This information is crucial for understanding and working with time series data, particularly in the context of long-term series forecasting, frequency analysis, and multi-head cross-attention techniques.\n\nIn the context of time series analysis, frequency is a critical component that can be leveraged to improve forecasting accuracy and understanding of underlying patterns. TimeGPT\u0027s ability to handle frequency as a characteristic of time series data suggests its potential applications in various fields, including but not limited to, finance, economics, and signal processing.\n\nOverall, the summary highlights the importance of frequency in time series analysis and its relevance to TimeGPT\u0027s capabilities, providing a clear and concise understanding of the entity \"FREQUENCY\" and its role in time series data.", "id": "FREQUENCY", "label": "FREQUENCY", "shape": "dot", "size": 10, "source_id": "1ddbab2dca370c9ef7b5a724075518cc,518bfcd6711530089fe3914ca16459c2", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Ground truth AR model is the model that was used to generate the time series", "id": "GROUND TRUTH AR MODEL", "label": "GROUND TRUTH AR MODEL", "shape": "dot", "size": 10, "source_id": "9c155897f3e35902f57696e0abaeb161", "type": "MODEL"}, {"color": "#97c2fc", "description": "AR(1) refers to an autoregressive process of order 1", "id": "AR(1)", "label": "AR(1)", "shape": "dot", "size": 10, "source_id": "9c155897f3e35902f57696e0abaeb161", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "AR(2) refers to an autoregressive process of order 2", "id": "AR(2)", "label": "AR(2)", "shape": "dot", "size": 10, "source_id": "9c155897f3e35902f57696e0abaeb161", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "AR(3) refers to an autoregressive process of order 3", "id": "AR(3)", "label": "AR(3)", "shape": "dot", "size": 10, "source_id": "9c155897f3e35902f57696e0abaeb161", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "AR(4) refers to an autoregressive process of order 4", "id": "AR(4)", "label": "AR(4)", "shape": "dot", "size": 10, "source_id": "9c155897f3e35902f57696e0abaeb161", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"KERNEL DENSITY ESTIMATION\" is a statistical method used for estimating the underlying probability distribution of a set of data. It is a technique employed to estimate the density of a distribution, providing valuable insights into the characteristics of the data. This method is particularly useful in understanding the shape and behavior of the data, allowing for informed decisions and predictions to be made.\n\nThe summary is based on the provided descriptions, which are consistent and complementary. The information collected from all the descriptions has been used to create a concise and coherent summary. The entity name \"KERNEL DENSITY ESTIMATION\" has been included to provide context, and relevant details have been added to enhance the understanding of the method.", "id": "KERNEL DENSITY ESTIMATION", "label": "KERNEL DENSITY ESTIMATION", "shape": "dot", "size": 10, "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7,9c155897f3e35902f57696e0abaeb161", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Predictive distribution refers to the distribution of possible outcomes for a given input", "id": "PREDICTIVE DISTRIBUTION", "label": "PREDICTIVE DISTRIBUTION", "shape": "dot", "size": 10, "source_id": "9c155897f3e35902f57696e0abaeb161", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ATM withdrawal data is the source of the NN5 dataset", "id": "ATM WITHDRAWAL DATA", "label": "ATM WITHDRAWAL DATA", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"VARIANCE\" is a statistic used to calculate the spread of a time series. It refers to a measure of the spread or dispersion of a set of data. Specifically, variance is used to quantify the amount of variation or dispersion of a time series, providing insights into the distribution of the data points. This measure is essential in various fields, including statistics, data analysis, and machine learning, where it is used to understand the characteristics of a dataset and make informed decisions.\n\nIn the context of time series analysis, variance is a crucial component in understanding the patterns and trends within a dataset. It helps to identify the level of dispersion or spread of the data points, which can be useful in forecasting and predicting future values. The variance of a time series can be calculated using various methods, including the sample variance and population variance.\n\nOverall, the entity \"VARIANCE\" plays a significant role in data analysis and time series forecasting, providing valuable insights into the characteristics of a dataset and enabling informed decision-making.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which is consistent with the formal and academic tone of the description.\n* The text also mentions technical terms, mathematical equations, and references to academic papers, which are all written in English, further supporting the conclusion that the primary language of the text is English.\n\nNote: The description is not empty, and the information provided is consistent across the descriptions.", "id": "VARIANCE", "label": "VARIANCE", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,85e4fbea9a08a0a663f3c5dc3f3a95a7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The mean value is a statistic that is used to calculate the average value of a time series", "id": "MEAN VALUE", "label": "MEAN VALUE", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536", "type": "STATISTIC"}, {"color": "#97c2fc", "description": "Summary statistics are a type of data that is used to describe the characteristics of a time series", "id": "SUMMARY STATISTICS", "label": "SUMMARY STATISTICS", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536", "type": "DATA TYPE"}, {"color": "#97c2fc", "description": "Series refers to a collection of data points or observations in a time series", "id": "SERIES", "label": "SERIES", "shape": "dot", "size": 10, "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Edge cases refer to the extreme or unusual situations that may occur in a time series", "id": "EDGE CASES", "label": "EDGE CASES", "shape": "dot", "size": 10, "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Research avenues refer to the potential areas of investigation or development that may be explored", "id": "RESEARCH AVEUNUES", "label": "RESEARCH AVEUNUES", "shape": "dot", "size": 10, "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Conformal methods refer to the techniques used to calibrate or adjust the model for a specific task", "id": "CONFIRMAL METHODS", "label": "CONFIRMAL METHODS", "shape": "dot", "size": 10, "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Low-rank adapters refer to the techniques used to fine-tune the model with minimal changes to its parameters", "id": "LOW-RANK ADAPTERS", "label": "LOW-RANK ADAPTERS", "shape": "dot", "size": 10, "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"CONFIDENCE INTERVAL\" refers to a statistical concept that represents the range of values within which a model\u0027s predictions or the true value is likely to lie. This range is often indicated by the use of the \u00b1 symbol in tables, suggesting that it encompasses a margin of error or uncertainty. In the context of statistical modeling, a confidence interval is a crucial tool for evaluating the reliability of predictions and estimates, providing a probabilistic measure of the likely range of values.\n\nThe description highlights the dual nature of confidence intervals, which can be applied to both model predictions and true values. This ambiguity is resolved by considering the context in which the confidence interval is being used. In general, confidence intervals are used to quantify the uncertainty associated with a statistical estimate or prediction, providing a range of values within which the true value is likely to lie.\n\nRelevant information from the nearby text is not provided, but based on the given descriptions, it can be inferred that confidence intervals are a fundamental concept in statistical analysis and modeling, used to evaluate the reliability and uncertainty of predictions and estimates.", "id": "CONFIDENCE INTERVAL", "label": "CONFIDENCE INTERVAL", "shape": "dot", "size": 10, "source_id": "1dc665214307b1656d1a0094a1918ece,42e1bb44edbe787e104f589e74b95d6c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Multivariate forecasting involves predicting the future values of multiple time series based on their past behavior", "id": "MULTIVARIATE FORECASTING", "label": "MULTIVARIATE FORECASTING", "shape": "dot", "size": 10, "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Interest rates are an example of a time series that can influence the forecast for another time series", "id": "INTEREST RATES", "label": "INTEREST RATES", "shape": "dot", "size": 10, "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Housing prices are an example of a time series that can be influenced by interest rates", "id": "HOUSING PRICES", "label": "HOUSING PRICES", "shape": "dot", "size": 10, "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "XGBoost is a machine learning model used for time series forecasting", "id": "XGBOOST", "label": "XGBOOST", "shape": "dot", "size": 10, "source_id": "6771b6279846e780d6807b15184ae008", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity in question is the \"GPU\" (Graphics Processing Unit), which is a type of computer hardware. Specifically, it is a device used to accelerate the training of a model, and it is designed for parallel processing. The GPU is a crucial component in modern computing, enabling faster and more efficient processing of complex tasks, particularly in the context of machine learning and deep learning.\n\nIn the context of machine learning, GPUs are widely used to accelerate the training of models, allowing for faster convergence and improved performance. The GPU\u0027s ability to perform parallel processing makes it an ideal choice for tasks that require simultaneous processing of multiple data points.\n\nOverall, the GPU is a critical component in modern computing, and its role in accelerating the training of models has made it an essential tool in the field of machine learning.\n\nRelevant information from the nearby text is not provided, but based on general knowledge, the following additional information can be added:\n\n* GPUs are commonly used in data centers, cloud computing, and high-performance computing applications.\n* The use of GPUs has led to significant advancements in fields such as computer vision, natural language processing, and predictive analytics.\n* The development of specialized GPUs, such as Tensor Cores and CUDA cores, has further accelerated the training of deep learning models.\n\nNote that this additional information is not directly related to the provided text, but it provides a broader context and understanding of the GPU\u0027s role in modern computing.", "id": "GPU", "label": "GPU", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1,bd73ee0439609823e12a877840c6ebae,fa01b75dccc556af8aca0d6c47e1970f", "type": "DEVICE"}, {"color": "#97c2fc", "description": "V100 refers to a specific type of graphics processing unit", "id": "V100", "label": "V100", "shape": "dot", "size": 10, "source_id": "116332ac4538a1430c83a34fcbec22d1", "type": "DEVICE"}, {"color": "#97c2fc", "description": "CPU cores are devices used to accelerate the training of a model", "id": "CPU CORES", "label": "CPU CORES", "shape": "dot", "size": 10, "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "type": "DEVICE"}, {"color": "#97c2fc", "description": "RAM is a device used to store the data used to train a model", "id": "RAM", "label": "RAM", "shape": "dot", "size": 10, "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "type": "DEVICE"}, {"color": "#97c2fc", "description": "Transformer models are a type of neural network architecture that allows for parallelization of inference on GPUs", "id": "TRANSFORMER MODELS", "label": "TRANSFORMER MODELS", "shape": "dot", "size": 10, "source_id": "bd73ee0439609823e12a877840c6ebae", "type": "MODEL"}, {"color": "#97c2fc", "description": "Anomaly detector is a model that identifies unusual patterns or data points in a dataset", "id": "ANOMALY DETECTOR", "label": "ANOMALY DETECTOR", "shape": "dot", "size": 10, "source_id": "bd73ee0439609823e12a877840c6ebae", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "SALINAS", "label": "SALINAS", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe Journal of Machine Learning Research is a publication that has published papers on various topics, including Arts and time series modeling. Specifically, it is the publication where the paper \"GluonTS: Probabilistic and Neural Time Series Modeling in Python\" was published. This suggests that the journal has a strong focus on machine learning research, including time series analysis and modeling. The publication of papers on Arts also indicates that the journal has a broad scope, covering various areas of machine learning research.\n\nIt is worth noting that the journal\u0027s publication of the paper \"GluonTS: Probabilistic and Neural Time Series Modeling in Python\" implies that it has a strong interest in time series modeling and analysis, which is a key area of research in machine learning. The use of Python in the paper\u0027s title also suggests that the journal may have a focus on practical applications of machine learning research.\n\nOverall, The Journal of Machine Learning Research appears to be a reputable publication that has made significant contributions to the field of machine learning research, including time series analysis and modeling.", "id": "THE JOURNAL OF MACHINE LEARNING RESEARCH", "label": "THE JOURNAL OF MACHINE LEARNING RESEARCH", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1,97c1f318683bb63131ece0a51f096c5b", "type": "PUBLICATION"}, {"color": "#97c2fc", "description": "SEASONALNAIVE is a statistical model used for time series forecasting", "id": "SEASONALNAIVE", "label": "SEASONALNAIVE", "shape": "dot", "size": 10, "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "ARTS", "label": "ARTS", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": ""}, {"color": "#97c2fc", "description": "Konstantinos Benidis is an author of the paper \"Deep Explicit Duration Switching Models for Time Series\"Konstantinos Benidis is an author of the paper \"Deep learning for time series forecasting: Tutorial and literature survey\"", "entity_type": "AUTHOR", "id": "KONSTANTINOS BENIDIS", "label": "KONSTANTINOS BENIDIS", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Deep learning for time series forecasting is a type of time series forecasting model", "id": "DEEP LEARNING FOR TIME SERIES FORECASTING", "label": "DEEP LEARNING FOR TIME SERIES FORECASTING", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\n\"Advances in Neural Information Processing Systems\" (NIPS) is a conference and a publication that hosts and publishes research papers in the field of neural information processing systems. Specifically, it is mentioned that two papers were published in NIPS: \"Deep Explicit Duration Switching Models for Time Series\" and \"Language models are few-shot learners\". These papers suggest that NIPS covers a wide range of topics, including time series analysis and language modeling. The conference and publication are likely to be a prominent platform for researchers and experts in the field to share their work and advancements in neural information processing systems.\n\nThe summary is written in third person and includes the entity name \"Advances in Neural Information Processing Systems\" to provide full context. Relevant information from the nearby text, such as the topics covered by the published papers, has been incorporated to enrich the summary.", "entity_type": "PUBLICATION", "id": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS", "label": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1,66b7675d8c62321e1cc8916401159787", "type": "PUBLICATION"}, {"color": "#97c2fc", "description": "Language models are few-shot learners is a type of time series forecasting model", "id": "LANGUAGE MODELS ARE FEW-SHOT LEARNERS", "label": "LANGUAGE MODELS ARE FEW-SHOT LEARNERS", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "CONFERENCE", "label": "CONFERENCE", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": ""}, {"color": "#97c2fc", "description": "V. Assimakopoulos is an author of the paper \"The theta model: a decomposition approach to forecasting\"", "id": "V ASSIMAKOPOULOS", "label": "V ASSIMAKOPOULOS", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "The theta model is a type of time series forecasting model", "id": "THE THETA MODEL", "label": "THE THETA MODEL", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "MODEL"}, {"color": "#97c2fc", "description": "International Journal of Forecasting is a publication where the paper \"The theta model: a decomposition approach to forecasting\" was published", "id": "INTERNATIONAL JOURNAL OF FORECASTING", "label": "INTERNATIONAL JOURNAL OF FORECASTING", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "PUBLICATION"}, {"color": "#97c2fc", "description": "The tourism forecasting competition is an event where the paper \"The tourism forecasting competition\" was presented", "id": "THE TOURISM FORECASTING COMPETITION", "label": "THE TOURISM FORECASTING COMPETITION", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "EVENT"}, {"color": "#97c2fc", "description": "George Athanasopoulos is an author of the paper \"The tourism forecasting competition\"", "id": "GEORGE ATHANASOPOULOS", "label": "GEORGE ATHANASOPOULOS", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "ACM Comput. Surv. is a publication where the paper \"Deep learning for time series forecasting: Tutorial and literature survey\" was published", "id": "ACM COMPUT SURV", "label": "ACM COMPUT SURV", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "PUBLICATION"}, {"color": "#97c2fc", "description": "Oliver Borchert is an author of the paper \"Multi-objective model selection for time series forecasting\"", "id": "OLIVER BORCHERT", "label": "OLIVER BORCHERT", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Multi-objective model selection is a type of time series forecasting model", "id": "MULTI-OBJECTIVE MODEL SELECTION", "label": "MULTI-OBJECTIVE MODEL SELECTION", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "MODEL"}, {"color": "#97c2fc", "description": "arXiv preprint is a publication where the paper \"Multi-objective model selection for time series forecasting\" was published", "id": "ARXIV PREPRINT", "label": "ARXIV PREPRINT", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "PUBLICATION"}, {"color": "#97c2fc", "description": "Tom B. Brown is an author of the paper \"Language models are few-shot learners\"", "id": "TOM B BROWN", "label": "TOM B BROWN", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Gray is an author of a research paper", "id": "GRAY", "label": "GRAY", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "", "id": "LITWIN", "label": "LITWIN", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": ""}, {"color": "#97c2fc", "description": "Chess is an author of a research paper", "id": "CHESS", "label": "CHESS", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Clark is an author of a research paper", "id": "CLARK", "label": "CLARK", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Berner is an author of a research paper", "id": "BERNER", "label": "BERNER", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "McCandlish is an author of a research paper", "id": "MCCANDLISH", "label": "MCCANDLISH", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Radford is an author of a research paper", "id": "RADFORD", "label": "RADFORD", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Sutskever is an author of a research paper", "id": "SUTSKEVER", "label": "SUTSKEVER", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Amodei is an author of a research paper", "id": "AMODEI", "label": "AMODEI", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Carmona is an author of a research paper", "id": "CARMONA", "label": "CARMONA", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Aubet is an author of a research paper", "id": "AUBET", "label": "AUBET", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Flunkert is an author of a research paper", "id": "FLUNKERT", "label": "FLUNKERT", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Gasthaus is an author of a research paper", "id": "GASTHAUS", "label": "GASTHAUS", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Neural Contextual Anomaly Detection for Time Series is a research paper", "id": "NEURAL CONTEXTUAL ANOMALY DETECTION FOR TIME SERIES", "label": "NEURAL CONTEXTUAL ANOMALY DETECTION FOR TIME SERIES", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "PAPER"}, {"color": "#97c2fc", "description": "arXiv:2107.07702 is a research paper", "id": "ARXIV:2107.07702", "label": "ARXIV:2107.07702", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "PAPER"}, {"color": "#97c2fc", "description": "Challu is an author of a research paper", "id": "CHALLU", "label": "CHALLU", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Olivares is an author of a research paper", "id": "OLIVARES", "label": "OLIVARES", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Oreshkin is an author of a research paper", "id": "ORESHKIN", "label": "ORESHKIN", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Garza Ramirez is an author of a research paper", "id": "GARZA RAMIREZ", "label": "GARZA RAMIREZ", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Canseco is an author of a research paper", "id": "CANSECO", "label": "CANSECO", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Dubrawski is an author of a research paper", "id": "DUBRAWSKI", "label": "DUBRAWSKI", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Chowdhery is an author of a research paper", "id": "CHOWDHERY", "label": "CHOWDHERY", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Narang is an author of a research paper", "id": "NARANG", "label": "NARANG", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Devlin is an author of a research paper", "id": "DEVLIN", "label": "DEVLIN", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "PaLM is a model for language modeling", "id": "PALM", "label": "PALM", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "MODEL"}, {"color": "#97c2fc", "description": "Journal of Machine Learning Research is a journal", "id": "JOURNAL OF MACHINE LEARNING RESEARCH", "label": "JOURNAL OF MACHINE LEARNING RESEARCH", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "JOURNAL"}, {"color": "#97c2fc", "description": "Dao is an author of a research paper", "id": "DAO", "label": "DAO", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "FlashAttention-2 is a model for attention", "id": "FLASHATTENTION-2", "label": "FLASHATTENTION-2", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "MODEL"}, {"color": "#97c2fc", "description": "Darlow is an author of a research paper", "id": "DARLOW", "label": "DARLOW", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Joosen is an author of a research paper", "id": "JOOSSEN", "label": "JOOSSEN", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Asenov is an author of a research paper", "id": "ASENOV", "label": "ASENOV", "shape": "dot", "size": 10, "source_id": "66b7675d8c62321e1cc8916401159787", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nGodahewa et al. is an academic paper or group of researchers that has been mentioned in the text. The authors of this paper, Godahewa et al., are associated with a research paper that discusses the Australian Electricity dataset. This suggests that the paper by Godahewa et al. is related to time series analysis, specifically long-term series forecasting, as the Australian Electricity dataset is a common benchmark for evaluating time series forecasting models.\n\nThe paper by Godahewa et al. likely employs various techniques such as frequency analysis and multi-head cross-attention, which are commonly used in time series forecasting and natural language processing tasks. The authors may have cited other English-language academic papers and authors in their work, further indicating that the primary language of the text is English.\n\nOverall, Godahewa et al. is an academic paper or group of researchers that has been mentioned in the text, and their work is related to time series analysis and forecasting, specifically using the Australian Electricity dataset.", "id": "GODAHEWA ET AL.", "label": "GODAHEWA ET AL.", "shape": "dot", "size": 10, "source_id": "1727ee77a376bbef1c7625a001e4ac3d,6820cea275275e87630a6876e890c525,e5e4a8f03f502fada5b17cba5dc942ba", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Open dataset refers to a dataset that is publicly available", "id": "OPEN DATASET", "label": "OPEN DATASET", "shape": "dot", "size": 10, "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe \"BRAZILIAN CITIES TEMPERATURE\" dataset contains monthly time series representing the weather at 12 different cities in Brazil. This dataset is comprised of temperature data from various Brazilian cities, providing a comprehensive view of the climatic conditions in the region.\n\nThe dataset\u0027s primary focus is on temperature data, which is collected on a monthly basis, allowing for the analysis of long-term series forecasting and frequency analysis. The inclusion of 12 different cities in Brazil enables researchers to identify patterns and trends in temperature data across various geographical locations.\n\nThe dataset\u0027s structure and content suggest that it is suitable for applications such as time series forecasting, climate modeling, and urban planning. The availability of temperature data from multiple cities in Brazil makes it an invaluable resource for researchers and policymakers seeking to understand and mitigate the impacts of climate change in the region.\n\nOverall, the \"BRAZILIAN CITIES TEMPERATURE\" dataset offers a unique opportunity to explore the complexities of temperature data in Brazil, providing insights that can inform decision-making and policy development at local, national, and international levels.", "id": "BRAZILIAN CITIES TEMPERATURE", "label": "BRAZILIAN CITIES TEMPERATURE", "shape": "dot", "size": 10, "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502,d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "Mexico City Bikes is a dataset containing bike usage data from Mexico City", "id": "MEXICO CITY BIKES", "label": "MEXICO CITY BIKES", "shape": "dot", "size": 10, "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "type": "DATASET"}, {"color": "#97c2fc", "description": "Brazilian weather data is the source of the Brazilian Cities Temperature dataset", "id": "BRAZILIAN WEATHER DATA", "label": "BRAZILIAN WEATHER DATA", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data related to the entity \"SOLAR\" can be generated as follows:\n\nThe entity \"SOLAR\" refers to a specific concept or measure related to energy or weather, and it contains data about solar power generation in the US in 2006. This dataset, known as \"Solar,\" is a collection of solar energy data, providing valuable insights into the solar power generation trends in the United States during the year 2006.\n\nThe data is likely to include various metrics such as solar irradiance, temperature, and other environmental factors that influence solar power generation. The dataset may also contain information on the capacity and output of solar power plants, as well as the overall energy production from solar sources during that period.\n\nGiven the context of the entity \"SOLAR\" and the descriptions provided, it is reasonable to infer that the dataset is focused on analyzing and understanding the solar power generation patterns in the US during 2006. This information can be useful for researchers, policymakers, and industry professionals to develop strategies for improving solar power generation and reducing reliance on fossil fuels.\n\nOverall, the entity \"SOLAR\" is associated with a dataset that provides a comprehensive view of solar power generation in the US in 2006, offering valuable insights into the energy landscape of that period.", "id": "SOLAR", "label": "SOLAR", "shape": "dot", "size": 10, "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502,d4bc1397526f236029876d4fbc22a721,ec7705b83cf4fe3aa18662c917b18c1a", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"SPANISH ENERGY AND WEATHER\" dataset contains a comprehensive collection of data related to energy and weather patterns in Spain. This dataset spans a period of 4 years, providing valuable insights into electricity consumption, generation, pricing, and weather conditions. The dataset is a valuable resource for researchers and analysts interested in understanding the complex relationships between energy demand, supply, and weather patterns in Spain.\n\nThe dataset includes a range of variables, such as electricity consumption and generation data, pricing information, and weather data, which can be used to analyze and model various aspects of the Spanish energy market. The 4-year time frame provides a sufficient amount of data to identify trends, patterns, and correlations between different variables, making it an ideal dataset for time series analysis and forecasting.\n\nOverall, the \"SPANISH ENERGY AND WEATHER\" dataset is a rich and valuable resource for anyone interested in understanding the energy and weather dynamics in Spain, and can be used for a variety of applications, including long-term series forecasting, frequency analysis, and multi-head cross-attention models.", "id": "SPANISH ENERGY AND WEATHER", "label": "SPANISH ENERGY AND WEATHER", "shape": "dot", "size": 10, "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502,d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "US solar power data is the source of the Solar dataset", "id": "US SOLAR POWER DATA", "label": "US SOLAR POWER DATA", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "Beijing Multisite Sunspot refers to a specific company or service", "id": "BEIJING MULTISITE SUNSPOT", "label": "BEIJING MULTISITE SUNSPOT", "shape": "dot", "size": 10, "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "type": "COMPANY"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TAXI\" refers to a vehicle for hire. The \"TAXI\" dataset contains taxi usage data from an unknown location, which suggests that it is a collection of data related to the usage patterns of taxis in a specific area. The dataset likely includes information such as the number of taxis in operation, the routes they take, the times of day they are most active, and other relevant metrics.\n\nGiven the context of the dataset, it is likely that the \"TAXI\" dataset is used for time series analysis and forecasting, such as long-term series forecasting, to understand and predict the demand for taxi services. The dataset may also be used for frequency analysis to identify patterns in taxi usage, such as peak hours or days of the week.\n\nOverall, the \"TAXI\" dataset is a valuable resource for understanding the usage patterns of taxis in a specific location, and can be used to inform decisions related to transportation planning, traffic management, and other related fields.", "id": "TAXI", "label": "TAXI", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525,6a222f9ed7fcf5fc945dfc22e16a3502,e067234b24b0625a3f95ecc18035f915", "type": "DATASET"}, {"color": "#97c2fc", "description": "Spain energy and weather data is the source of the Spanish Energy and Weather dataset", "id": "SPAIN ENERGY AND WEATHER DATA", "label": "SPAIN ENERGY AND WEATHER DATA", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe USHCN (United States Historical Climatology Network) is a dataset containing climate data from the United States, which is provided by a United States government agency responsible for climate data. The dataset, referred to as USHCN, encompasses a collection of climate and weather data, making it a valuable resource for researchers and scientists studying climate patterns and trends.\n\nThis summary is derived from the provided descriptions, which are all related to the same entity, USHCN. The descriptions are consistent and provide a clear understanding of the entity\u0027s purpose and characteristics. The information is written in a formal and academic tone, suggesting that it is from a research paper or technical document.\n\nThe summary includes the entity name, USHCN, and provides a concise description of its purpose and characteristics. The information is accurate and consistent with the provided descriptions, and no contradictions were found.", "id": "USHCN", "label": "USHCN", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525,6a222f9ed7fcf5fc945dfc22e16a3502,e067234b24b0625a3f95ecc18035f915", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nNEW YORK is a city located in the United States. It serves as the location for various data sources, including rideshare services and taxi rides. This information suggests that NEW YORK is a significant hub for data collection, particularly in the transportation sector.\n\nTo further enrich this summary, we can consider the context of time series analysis and machine learning, which are often used in conjunction with data from rideshare services and taxi rides. This could imply that NEW YORK is also a key location for research and development in these areas, with potential applications in long-term series forecasting, frequency analysis, and multi-head cross-attention models.\n\nOverall, the summary provides a concise description of NEW YORK as a city with a rich data landscape, particularly in the transportation sector, and potentially a hub for research and development in time series analysis and machine learning.", "id": "NEW YORK", "label": "NEW YORK", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525,a875a1c0bede47a1c4e3823be81c42c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Precipitation is a measure of the amount of rain or snow that falls in a given period", "id": "PRECIPITATION", "label": "PRECIPITATION", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Air quality data is the source of the KDD Cup 2018 dataset", "id": "AIR QUALITY DATA", "label": "AIR QUALITY DATA", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "Air Quality UCL refers to a specific company or service", "id": "AIR QUALITY UCL", "label": "AIR QUALITY UCL", "shape": "dot", "size": 10, "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "type": "COMPANY"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"ETT M2\" is a concept related to energy. It is also a dataset used for pretraining and fine-tuning, suggesting its application in machine learning and model development. Furthermore, \"ETT M2\" refers to a specific concept or measure, which is likely related to its energy context. Given the technical nature of the descriptions, it is possible that \"ETT M2\" is a measure or metric used in energy-related time series forecasting or analysis, such as long-term series forecasting or frequency analysis.\n\nThe use of \"ETT M2\" as a dataset for pretraining and fine-tuning implies its relevance in the development of machine learning models, particularly those that involve time series forecasting or energy-related applications. The fact that it is a specific concept or measure suggests that it may be a well-defined and widely recognized metric in the field of energy or time series analysis.\n\nOverall, the entity \"ETT M2\" appears to be a concept or measure related to energy, with applications in machine learning and time series forecasting. Its use as a dataset for pretraining and fine-tuning suggests its relevance in the development of energy-related models and its potential as a benchmark or evaluation metric in the field.", "id": "ETT M2", "label": "ETT M2", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e,ec7705b83cf4fe3aa18662c917b18c1a", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"M5\" is a forecasting competition that contains products sales data. Specifically, it is a time series dataset of retail sales data, updated daily. This suggests that M5 is a dynamic and constantly evolving dataset, providing a rich source of information for time series analysis and forecasting.\n\nIn the context of machine learning and time series forecasting, M5 is an important entity that offers a unique opportunity for researchers and practitioners to develop and evaluate their models. The daily updates of the dataset ensure that the models can be trained and tested on a wide range of scenarios, making it an ideal platform for long-term series forecasting and frequency analysis.\n\nOverall, M5 is a valuable resource for anyone interested in time series analysis, forecasting, and machine learning, particularly in the retail sales domain.", "id": "M5", "label": "M5", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8,a875a1c0bede47a1c4e3823be81c42c6", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "Mexico City is a city in Mexico", "id": "MEXICO CITY", "label": "MEXICO CITY", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nFRED-MD is a dataset of economic data that contains monthly macro-economic indicators from the Federal Reserve Bank. It is a database of economic data, including macroeconomic and financial data, which provides valuable insights into the economic landscape. FRED-MD is a comprehensive collection of economic indicators, making it a valuable resource for researchers, analysts, and policymakers seeking to understand and forecast economic trends.\n\nThis summary incorporates information from all the descriptions provided, resolving any potential contradictions and presenting a coherent overview of the FRED-MD dataset. The summary highlights the key characteristics of FRED-MD, including its source, content, and potential applications, providing a clear and concise understanding of this economic dataset.", "id": "FRED-MD", "label": "FRED-MD", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8,d4bc1397526f236029876d4fbc22a721,e067234b24b0625a3f95ecc18035f915", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Exchange rate data is the source of the Exchange Rate dataset", "id": "EXCHANGE RATE DATA", "label": "EXCHANGE RATE DATA", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "Federal Reserve Bank data is the source of the FRED-MD dataset", "id": "FEDERAL RESERVE BANK DATA", "label": "FEDERAL RESERVE BANK DATA", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"CLOUD\" refers to a domain used for fine-tuning forecasting tasks, which involves a collection of data or applications stored on remote servers. This entity is utilized in the context of long-term series forecasting, where frequency analysis and multi-head cross-attention techniques may be employed. The use of cloud-based infrastructure enables the storage and processing of large datasets, facilitating the development and deployment of advanced forecasting models.\n\nIn the context of machine learning and time series forecasting, the cloud domain plays a crucial role in enabling the fine-tuning of forecasting tasks, which involves the analysis of large datasets and the application of sophisticated algorithms. The cloud-based infrastructure provides a scalable and flexible platform for data storage, processing, and model deployment, making it an essential component of modern forecasting systems.\n\nOverall, the entity \"CLOUD\" is a critical component of the forecasting ecosystem, enabling the development and deployment of advanced forecasting models that can analyze large datasets and provide accurate predictions.", "id": "CLOUD", "label": "CLOUD", "shape": "dot", "size": 10, "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26,ec7705b83cf4fe3aa18662c917b18c1a", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "M1 (Monthly) is a time series dataset of various economic and financial variables, updated monthly", "id": "M1 (MONTHLY)", "label": "M1 (MONTHLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "M1 (Quarterly) is a time series dataset of various economic and financial variables, updated quarterly", "id": "M1 (QUARTERLY)", "label": "M1 (QUARTERLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "M1 (Yearly) is a time series dataset of various economic and financial variables, updated yearly", "id": "M1 (YEARLY)", "label": "M1 (YEARLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "M3 (Monthly) is a time series dataset of various economic and financial variables, updated monthly", "id": "M3 (MONTHLY)", "label": "M3 (MONTHLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "M3 (Quarterly) is a time series dataset of various economic and financial variables, updated quarterly", "id": "M3 (QUARTERLY)", "label": "M3 (QUARTERLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "M3 (Yearly) is a time series dataset of various economic and financial variables, updated yearly", "id": "M3 (YEARLY)", "label": "M3 (YEARLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "M4 (Quarterly) is a time series dataset of various economic and financial variables, updated quarterly", "id": "M4 (QUARTERLY)", "label": "M4 (QUARTERLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "M4 (Yearly) is a time series dataset of various economic and financial variables, updated yearly", "id": "M4 (YEARLY)", "label": "M4 (YEARLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "NN5 (Daily) is a time series dataset of financial data, updated daily", "id": "NN5 (DAILY)", "label": "NN5 (DAILY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "NN5 (Weekly) is a time series dataset of financial data, updated weekly", "id": "NN5 (WEEKLY)", "label": "NN5 (WEEKLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "Tourism (Monthly) is a time series dataset of tourism-related data, updated monthly", "id": "TOURISM (MONTHLY)", "label": "TOURISM (MONTHLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "Tourism (Quarterly) is a time series dataset of tourism-related data, updated quarterly", "id": "TOURISM (QUARTERLY)", "label": "TOURISM (QUARTERLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "Tourism (Yearly) is a time series dataset of tourism-related data, updated yearly", "id": "TOURISM (YEARLY)", "label": "TOURISM (YEARLY)", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "Smartmeter energy use data in London households is the source of the ETERS dataset", "id": "SMARTMETER ENERGY USE DATA", "label": "SMARTMETER ENERGY USE DATA", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "", "id": "ETERS", "label": "ETERS", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": ""}, {"color": "#97c2fc", "description": "Australia wind farm data is the source of the Wind Farms dataset", "id": "AUSTRALIA WIND FARM DATA", "label": "AUSTRALIA WIND FARM DATA", "shape": "dot", "size": 10, "source_id": "d4bc1397526f236029876d4fbc22a721", "type": "DATASET"}, {"color": "#97c2fc", "description": "NOAA is a United States government agency responsible for providing weather and climate data", "id": "NOAA", "label": "NOAA", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "", "id": "WEATHER SERIES", "label": "WEATHER SERIES", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": ""}, {"color": "#97c2fc", "description": "PM2.5 is a measure of particulate matter in the air", "id": "PM2.5", "label": "PM2.5", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "PM10 is a measure of particulate matter in the air", "id": "PM10", "label": "PM10", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "NO2 is a measure of nitrogen dioxide in the air", "id": "NO2", "label": "NO2", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "CO is a measure of carbon monoxide in the air", "id": "CO", "label": "CO", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "O3 is a measure of ozone in the air", "id": "O3", "label": "O3", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "SO2 is a measure of sulfur dioxide in the air", "id": "SO2", "label": "SO2", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"Beijing is a city located in China, serving as the site for two significant datasets: the Beijing PM2.5 dataset and the Beijing Multi-Site Air-Quality dataset. These datasets are crucial for various research and analysis purposes, particularly in the fields of environmental science and air quality monitoring.\"\n\nThis summary incorporates all the provided descriptions, resolving any potential contradictions and presenting a coherent overview of the entity \"Beijing\" and its associated datasets.", "id": "BEIJING", "label": "BEIJING", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525,85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "London is a city in the United Kingdom", "id": "LONDON", "label": "LONDON", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the \"BEIJING PM2.5 DATASET\" in third person:\n\nThe \"BEIJING PM2.5 DATASET\" is a dataset containing hourly data of PM2.5 levels recorded by the US Embassy in Beijing. This dataset is utilized in a specific project, indicating its relevance and importance in the context of air quality monitoring and analysis.\n\nTo provide a more detailed understanding, the dataset likely includes various features such as time stamps, PM2.5 concentration levels, and potentially other environmental factors that may influence air quality. The use of hourly data suggests that the dataset is suitable for time series analysis and forecasting, which can be crucial in understanding and predicting PM2.5 levels in Beijing.\n\nGiven the context of the dataset, it is likely that the data is used for research purposes, such as developing machine learning models for long-term series forecasting, frequency analysis, or other applications in environmental science. The dataset\u0027s relevance to air quality monitoring and analysis makes it a valuable resource for researchers and scientists working in this field.\n\nOverall, the \"BEIJING PM2.5 DATASET\" is a valuable resource for understanding and analyzing air quality in Beijing, and its utilization in a specific project highlights its importance in the field of environmental science.", "id": "BEIJING PM2.5 DATASET", "label": "BEIJING PM2.5 DATASET", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6,88d96b5f76042688e9dd745eb822d919", "type": "DATASET"}, {"color": "#97c2fc", "description": "Temperature is a measure of heat", "id": "TEMPERATURE", "label": "TEMPERATURE", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Rain is a form of precipitation", "id": "RAIN", "label": "RAIN", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Snow is a form of precipitation", "id": "SNOW", "label": "SNOW", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Snow depth is a measure of the depth of snow on the ground", "id": "SNOW DEPTH", "label": "SNOW DEPTH", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Minimum temperature is the lowest temperature recorded in a given period", "id": "MINTEMP", "label": "MINTEMP", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Maximum temperature is the highest temperature recorded in a given period", "id": "MAXTEMP", "label": "MAXTEMP", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Climate stations are locations where climate data is collected", "id": "CLIMATE STATIONS", "label": "CLIMATE STATIONS", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Australia is a country in the Southern Hemisphere", "id": "AUSTRALIA", "label": "AUSTRALIA", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Bikes refers to bicycles", "id": "BIKES", "label": "BIKES", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Transport and tourism is a domain used to label datasets", "id": "TRANSPORT \u0026 TOURISM", "label": "TRANSPORT \u0026 TOURISM", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "San Francisco traffic is a dataset used for pretraining and fine-tuning", "id": "SAN FRANCISCO TRAFFIC", "label": "SAN FRANCISCO TRAFFIC", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nLyft, a ride-hailing company, is a rideshare service that contains various hourly statistics. The primary language of the text related to Lyft is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document related to Lyft.\n\nThe text contains information about Lyft\u0027s ride-hailing services, including hourly statistics, which implies that the company provides real-time data on its operations. This data can be useful for time series analysis, long-term series forecasting, and frequency analysis, among other applications.\n\nOverall, Lyft is a ride-hailing company that offers rideshare services with hourly statistics, and the text related to the company is written in English, suggesting a formal and academic tone.", "id": "LYFT", "label": "LYFT", "shape": "dot", "size": 10, "source_id": "6820cea275275e87630a6876e890c525,a875a1c0bede47a1c4e3823be81c42c6", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"MELBOURNE\" is a location where sensors are deployed to collect data on pedestrians. This suggests that Melbourne is a site for data collection, likely for the purpose of understanding pedestrian behavior, traffic patterns, or other related aspects. The fact that sensors are collecting data implies that the data is being used for some form of analysis or modeling, possibly involving machine learning or time series forecasting techniques.\n\nGiven the context, it is likely that the data collected in Melbourne is being used to inform urban planning, transportation systems, or other related applications. The specific details of the data collection and analysis are not provided, but the presence of sensors and the focus on pedestrians suggest a focus on understanding human movement and behavior in the city.\n\nOverall, the entity \"MELBOURNE\" is a location where data is being collected on pedestrians, likely for the purpose of informing urban planning, transportation systems, or other related applications.", "id": "MELBOURNE", "label": "MELBOURNE", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131,a875a1c0bede47a1c4e3823be81c42c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "San Francisco Bay area is the location of sensors that collect traffic data", "id": "SAN FRANCISCO BAY AREA", "label": "SAN FRANCISCO BAY AREA", "shape": "dot", "size": 10, "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Pedestrian counts refer to the number of people walking in a given area", "id": "PEDIESTRIAN COUNTS", "label": "PEDIESTRIAN COUNTS", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "M1 is a forecasting competition that contains time series data from various domains", "id": "M1", "label": "M1", "shape": "dot", "size": 10, "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "type": "COMPETITION"}, {"color": "#97c2fc", "description": "M3 is a forecasting competition that contains time series data from various domains", "id": "M3", "label": "M3", "shape": "dot", "size": 10, "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "type": "COMPETITION"}, {"color": "#97c2fc", "description": "Page views refer to the number of times a Wikipedia article is viewed", "id": "PAGE VIEWS", "label": "PAGE VIEWS", "shape": "dot", "size": 10, "source_id": "2565ae205d4c98342168bb67a4f7a309", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "OneFitsAll is a method that leverages a pretrained large language model (LLM) and finetunes the input and output layers for time series forecasting", "id": "ONEFITSSALL", "label": "ONEFITSSALL", "shape": "dot", "size": 10, "source_id": "f0c52387a7b3a5c3850fe6991f0a7c83", "type": "MODEL"}, {"color": "#97c2fc", "description": "Supervised baselines refer to a specific model or algorithm", "id": "SUPERVISED BASELINES", "label": "SUPERVISED BASELINES", "shape": "dot", "size": 10, "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "type": "MODEL"}, {"color": "#97c2fc", "description": "Dataset split refers to the division of data into training and testing sets", "id": "DATASET SPLIT", "label": "DATASET SPLIT", "shape": "dot", "size": 10, "source_id": "51ee17c1f0212d4c94010d8e376f649b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Window length is a parameter used to control the length of the windows used during training", "id": "WINDOW LENGTH", "label": "WINDOW LENGTH", "shape": "dot", "size": 10, "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Sparsity is a characteristic of time series that TimeGPT can handle", "id": "SPARSITY", "label": "SPARSITY", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "CHARACTERISTIC"}, {"color": "#97c2fc", "description": "Point forecasts refer to a single predicted value for a given time series", "id": "POINT FORECASTS", "label": "POINT FORECASTS", "shape": "dot", "size": 10, "source_id": "f0808ad97bc4d26391284145427770e5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Mean absolute scaling error (MASE) is a metric used to evaluate point forecasts", "id": "MEAN ABSOLUTE SCALING ERROR", "label": "MEAN ABSOLUTE SCALING ERROR", "shape": "dot", "size": 10, "source_id": "f0808ad97bc4d26391284145427770e5", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe Seasonal Naive Model is a baseline model used for comparison purposes in time series analysis. It is a type of model employed to estimate the empirical error of a time series, serving as a reference point for evaluating the performance of other models. This model is particularly useful in long-term series forecasting, where its simplicity and ease of implementation make it an attractive choice for researchers and practitioners alike.\n\nThe Seasonal Naive Model\u0027s primary function is to provide a baseline against which more complex models can be compared, allowing for a more accurate assessment of their performance. By estimating the empirical error of a time series, this model helps to establish a benchmark for evaluating the accuracy of other forecasting models.\n\nIn the context of time series analysis, the Seasonal Naive Model is often used in conjunction with other models, such as those that incorporate frequency analysis or multi-head cross-attention mechanisms. Its simplicity and ease of implementation make it an ideal choice for researchers and practitioners seeking to evaluate the performance of more complex models.\n\nOverall, the Seasonal Naive Model is a fundamental component of time series analysis, providing a baseline for comparison and evaluation of more complex models. Its use is widespread in academic and practical applications, including research papers and technical documents, such as those published in conferences like ICLR, AAAI, and PMLR.", "id": "SEASONAL NAIVE MODEL", "label": "SEASONAL NAIVE MODEL", "shape": "dot", "size": 10, "source_id": "f0808ad97bc4d26391284145427770e5,fb67fcff21ac521a5ed8b202412ec1fc", "type": "MODEL"}, {"color": "#97c2fc", "description": "Relative Mean Absolute Error (rMAE) is a metric used to evaluate the performance of a model", "id": "RELATIVE MEAN ABSOLUTE ERROR", "label": "RELATIVE MEAN ABSOLUTE ERROR", "shape": "dot", "size": 10, "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "type": "METRIC"}, {"color": "#97c2fc", "description": "Relative Root Mean Square Error (rRMSE) is a metric used to evaluate the performance of a model", "id": "RELATIVE ROOT MEAN SQUARE ERROR", "label": "RELATIVE ROOT MEAN SQUARE ERROR", "shape": "dot", "size": 10, "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "type": "METRIC"}, {"color": "#97c2fc", "description": "Predicted quantiles refer to the predicted values at specific probability levels for a given time series", "id": "PREDICTED QUANTILES", "label": "PREDICTED QUANTILES", "shape": "dot", "size": 10, "source_id": "f0808ad97bc4d26391284145427770e5", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Lag-Llama model is a model used for forecasting", "id": "LAG-LLAMA MODEL", "label": "LAG-LLAMA MODEL", "shape": "dot", "size": 10, "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "type": "MODEL"}, {"color": "#97c2fc", "description": "Uncertainty is an intrinsic aspect of life, a constant element that humans have tirelessly sought to navigate and comprehend", "id": "UNCERTAINTY", "label": "UNCERTAINTY", "shape": "dot", "size": 10, "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Ground truths are the actual values that a model is trying to predict", "id": "GROUND TRUTHS", "label": "GROUND TRUTHS", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a", "type": "DATA"}, {"color": "#97c2fc", "description": "Output patches are a set of data points that are output by the model", "id": "OUTPUT PATCHES", "label": "OUTPUT PATCHES", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a", "type": "DATA"}, {"color": "#97c2fc", "description": "Data history is a concept mentioned in the text, as indicated by the use of the phrase \"amount of history available\"", "id": "DATA HISTORY", "label": "DATA HISTORY", "shape": "dot", "size": 10, "source_id": "d08a82328191907abfcdb72efab9a6cf", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Azul Garza is one of the authors of the paper introducing TimeGPT", "id": "AZUL GARZA", "label": "AZUL GARZA", "shape": "dot", "size": 10, "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nCRISTIAN CHALLU is a researcher and author who has contributed to the development of TimeGPT, a significant advancement in the field of time series forecasting. As one of the authors of the paper introducing TimeGPT, CRISTIAN CHALLU has played a crucial role in shaping the understanding and application of this technology. Additionally, CRISTIAN CHALLU is also an author of one of the referenced papers, further solidifying their expertise and influence in the field of time series analysis and forecasting.\n\nThis summary combines the information from the description list, highlighting CRISTIAN CHALLU\u0027s contributions to TimeGPT and their role as an author of a referenced paper. The language used is formal and academic, consistent with the tone of the original text.", "id": "CRISTIAN CHALLU", "label": "CRISTIAN CHALLU", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925,f8f6fec6d1d7eda0349d3c52c4c96d97", "type": "PERSON"}, {"color": "#97c2fc", "description": "Max Mergenthaler-Canseco is one of the authors of the paper introducing TimeGPT", "id": "MAX MERGENTHALER-CANSECO", "label": "MAX MERGENTHALER-CANSECO", "shape": "dot", "size": 10, "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "type": "PERSON"}, {"color": "#97c2fc", "description": "Vaswani et al. is a paper that introduced the Transformer architecture", "id": "VASWANI ET AL.", "label": "VASWANI ET AL.", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "PAPER"}, {"color": "#97c2fc", "description": "Conformal predictions are a method used to estimate prediction intervals based on historic errors", "id": "CONFORMAL PREDICTIONS", "label": "CONFORMAL PREDICTIONS", "shape": "dot", "size": 10, "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Historical values refer to the data points that have been observed in the past", "id": "HISTORICAL VALUES", "label": "HISTORICAL VALUES", "shape": "dot", "size": 10, "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "type": "DATA"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"ZERO-SHOT INFERENCE\" refers to a task in machine learning where a model is tested without any fine-tuning or additional training. This ability of a model to make predictions without being trained on specific data is a key aspect of zero-shot inference. In essence, zero-shot inference enables models to generalize and make accurate predictions on unseen data without requiring extensive training or fine-tuning. This capability is crucial in various applications, including natural language processing, computer vision, and time series forecasting, where models need to adapt to new and unseen data.", "id": "ZERO-SHOT INFERENCE", "label": "ZERO-SHOT INFERENCE", "shape": "dot", "size": 10, "source_id": "f8afc5a341360ab82dfbe9ebbb7f8e84,f912df936d0b735fe654d1a9c53caa3b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Benchmark models refer to the models used as a reference or comparison for other models", "id": "BENCHMARK MODELS", "label": "BENCHMARK MODELS", "shape": "dot", "size": 10, "source_id": "f8afc5a341360ab82dfbe9ebbb7f8e84", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"RMAE\" is a metric used to evaluate the performance of time series forecasting models. It is specifically designed to assess the accuracy of models in predicting future values in a time series. The use of \"rMAE\" suggests that it is a variant or a specific type of Mean Absolute Error (MAE) metric, which is commonly used in time series forecasting to measure the average magnitude of the errors in a model\u0027s predictions.\n\nGiven the context of time series forecasting, it is likely that \"RMAE\" is used to evaluate the performance of models in terms of their ability to accurately forecast future values in a time series. The fact that it is mentioned alongside \"time series forecasting models\" further reinforces this interpretation.\n\nOverall, \"RMAE\" appears to be a metric used to evaluate the performance of time series forecasting models, with a focus on assessing the accuracy of these models in predicting future values in a time series.", "id": "RMAE", "label": "RMAE", "shape": "dot", "size": 10, "source_id": "f8afc5a341360ab82dfbe9ebbb7f8e84,f912df936d0b735fe654d1a9c53caa3b", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"RRMSE\" can be described as follows:\n\nRRMSE, or root relative mean squared error, is a metric used to evaluate the performance of time series forecasting models. It is a measure of the average magnitude of the errors produced by a model, relative to the actual values in the time series data. This metric is commonly used in the field of time series analysis and forecasting to assess the accuracy of a model\u0027s predictions.\n\nThe use of RRMSE is particularly relevant in the context of long-term series forecasting, where the goal is to predict future values in a time series based on past trends and patterns. By evaluating a model\u0027s performance using RRMSE, researchers and practitioners can gain insights into its ability to accurately forecast future values and make informed decisions about model selection and improvement.\n\nIn addition to its use in time series forecasting, RRMSE is also a useful metric for evaluating the performance of models in other domains, such as frequency analysis and multi-head cross-attention. However, its application is most commonly associated with time series analysis and forecasting.\n\nOverall, RRMSE is a widely used and important metric in the field of time series analysis and forecasting, providing a quantitative measure of a model\u0027s performance and accuracy in predicting future values in a time series.", "id": "RRMSE", "label": "RRMSE", "shape": "dot", "size": 10, "source_id": "f8afc5a341360ab82dfbe9ebbb7f8e84,f912df936d0b735fe654d1a9c53caa3b", "type": "METRIC"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe \"PYTHON SDK\" is a software development kit designed for interacting with TimeGPT and utilizing its capabilities. This SDK provides a platform for developers to leverage TimeGPT\u0027s features and functionality, enabling them to build applications and solutions that integrate with TimeGPT.\n\nThe summary is based on the two descriptions provided, which are consistent and complementary. The first description highlights the SDK\u0027s role in interacting with TimeGPT, while the second description emphasizes its purpose in utilizing TimeGPT\u0027s capabilities. There are no contradictions in the descriptions, and the information is coherent and relevant to the entity \"PYTHON SDK\".", "id": "PYTHON SDK", "label": "PYTHON SDK", "shape": "dot", "size": 10, "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce,b8ce119147e4d1454453c514e68dc4dc", "type": "SOFTWARE"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"REST API\" can be described as follows:\n\nThe REST API is a software interface that enables interaction and access to TimeGPT. It serves as a medium for users to engage with TimeGPT, facilitating communication and data exchange between the two entities.\n\nThis description is derived from the two provided descriptions, which are identical except for minor wording variations. The information collected from both descriptions is combined to provide a concise and coherent summary of the REST API entity.\n\nNo contradictions were found in the provided descriptions, and the language used is formal and technical, suggesting that the text is from a technical document or research paper. The context of the REST API interacting with TimeGPT is also inferred from the nearby text, which is not explicitly provided but can be assumed based on the given information.", "id": "REST API", "label": "REST API", "shape": "dot", "size": 10, "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce,b8ce119147e4d1454453c514e68dc4dc", "type": "SOFTWARE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nNixtla is a company that is behind the development of TimeGPT, a time series forecasting model. The organization is also affiliated with the authors of a paper that introduced TimeGPT, indicating a strong connection between Nixtla and the research behind TimeGPT. This suggests that Nixtla is not only the company behind TimeGPT but also a hub for research and development in the field of time series forecasting.\n\nThe language used to describe Nixtla and its connection to TimeGPT is formal and academic, indicating that the information is likely from a research paper or technical document. The presence of technical terms, mathematical equations, and references to academic papers further supports this conclusion.\n\nOverall, Nixtla appears to be a key player in the development and research of TimeGPT, a time series forecasting model with potential applications in various fields.", "id": "NIXTLA", "label": "NIXTLA", "shape": "dot", "size": 10, "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce,f8f6fec6d1d7eda0349d3c52c4c96d97", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"REFERENCES\" refers to a list of sources cited in the text, which are used to support or validate information and provide additional context. These references are typically included at the end of a document, such as a research paper or technical document, and are used to acknowledge the work of other authors and researchers. They can include citations to academic papers, books, and other sources, and are often used to provide further information or context to the reader.\n\nIn the context of academic writing, references are a crucial component of the text, as they allow authors to credit the work of others and provide a clear understanding of the sources used to support their arguments or findings. The use of references also enables readers to easily locate and access the original sources, which can be particularly useful in fields such as science, technology, engineering, and mathematics (STEM), where accuracy and reliability are paramount.\n\nOverall, the entity \"REFERENCES\" plays a vital role in academic writing, serving as a means of acknowledging the work of others, providing additional context, and facilitating further research and exploration.", "id": "REFERENCES", "label": "REFERENCES", "shape": "dot", "size": 10, "source_id": "0654926be53a9cf18e4def1c94371576,9f70fa3e2d0ba714627b9ce1a442fd21,c80929fa1aa2a6f9652319844c4ca742,ec3fbfb800fd9bf1d913584fda4ae925", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "San Francisco is the city where Nixtla is located", "id": "SAN FRANCISCO", "label": "SAN FRANCISCO", "shape": "dot", "size": 10, "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "type": "LOCATION"}, {"color": "#97c2fc", "description": "USA is the country where Nixtla is located", "id": "USA", "label": "USA", "shape": "dot", "size": 10, "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "type": "LOCATION"}, {"color": "#97c2fc", "description": "API key is a unique identifier used to access TimeGPT", "id": "API KEY", "label": "API KEY", "shape": "dot", "size": 10, "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Benidis et al. are researchers who have contributed to the field of time series analysis", "id": "BENIDIS ET AL.", "label": "BENIDIS ET AL.", "shape": "dot", "size": 10, "source_id": "6771b6279846e780d6807b15184ae008", "type": "RESEARCHERS"}, {"color": "#97c2fc", "description": "Hybrid forecasting refers to the combination of different forecasting methods or models to improve performance", "id": "HYBRID FORECASTING", "label": "HYBRID FORECASTING", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "A literature review is a document that summarizes and analyzes the existing research on a particular topic", "id": "LITERATURE REVIEW", "label": "LITERATURE REVIEW", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "A review and taxonomy is a document that summarizes and categorizes the existing research on a particular topic", "id": "REVIEW AND TAXONOMY", "label": "REVIEW AND TAXONOMY", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "ESRNN is a type of neural network that is designed for probabilistic forecasting", "id": "ESRNN", "label": "ESRNN", "shape": "dot", "size": 10, "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "type": "MODEL"}, {"color": "#97c2fc", "description": "MQ-Transformer is a model that uses transformer-based models", "id": "MQ-TRANSFORMER", "label": "MQ-TRANSFORMER", "shape": "dot", "size": 10, "source_id": "7d5d82d600620153153772a9bc498ac0", "type": "MODEL"}, {"color": "#97c2fc", "description": "Scaling laws refer to the existence of laws on data and model sizes for Transformer architectures on time series forecasting tasks", "id": "SCALING LAWS", "label": "SCALING LAWS", "shape": "dot", "size": 10, "source_id": "7d5d82d600620153153772a9bc498ac0", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Output embedding is a concept mentioned in the text, as indicated by the use of the phrase \"output embedding\"", "id": "OUTPUT EMBEDDING", "label": "OUTPUT EMBEDDING", "shape": "dot", "size": 10, "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Inputs (shifted right) is a concept mentioned in the text, as indicated by the use of the phrase \"inputs (shifted right)\"", "id": "INPUTS (SHIFTED RIGHT)", "label": "INPUTS (SHIFTED RIGHT)", "shape": "dot", "size": 10, "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Forecasting task is a concept mentioned in the text, as indicated by the use of the phrase \"forecasting task\"", "id": "FORECASTING TASK", "label": "FORECASTING TASK", "shape": "dot", "size": 10, "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Conditional distribution is a concept mentioned in the text, as indicated by the use of the phrase \"conditional distribution\"", "id": "CONDITIONAL DISTRIBUTION", "label": "CONDITIONAL DISTRIBUTION", "shape": "dot", "size": 10, "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Encoder-decoder is a structure used in TimeGPT to process time series", "id": "ENCODER-DECODER", "label": "ENCODER-DECODER", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "STRUCTURE"}, {"color": "#97c2fc", "description": "Residual connections are a technique used in TimeGPT to improve the model\u0027s performance", "id": "RESIDUAL CONNECTIONS", "label": "RESIDUAL CONNECTIONS", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Layer normalization is a technique used in TimeGPT to improve the model\u0027s performance", "id": "LAYER NORMALIZATION", "label": "LAYER NORMALIZATION", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TRANSPORT\" refers to the movement of people or goods from one place to another. This domain is handled by TimeGPT, a model capable of fine-tuning forecasting tasks. Specifically, \"TRANSPORT\" is a domain used for long-term series forecasting, which involves analyzing time series data to predict future trends and patterns. The frequency analysis and multi-head cross-attention techniques are likely employed in this domain to improve forecasting accuracy. The entity \"TRANSPORT\" is a critical area of study in the field of time series forecasting, with applications in various industries such as logistics, supply chain management, and traffic flow prediction.", "id": "TRANSPORT", "label": "TRANSPORT", "shape": "dot", "size": 10, "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26,518bfcd6711530089fe3914ca16459c2,ec7705b83cf4fe3aa18662c917b18c1a", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nPyTorch is an open-source deep learning framework used to implement various models, including TimeGPT. It is a library used in projects that require the development and deployment of artificial intelligence and machine learning models. PyTorch is known for its dynamic computation graph and automatic differentiation capabilities, making it a popular choice among researchers and developers in the field of deep learning.\n\nThe use of PyTorch in the project suggests that the project involves the development of a deep learning model, likely for time series forecasting or a related task. The fact that PyTorch is used to implement TimeGPT implies that the project is focused on developing a model that can generate human-like text based on a given prompt or context.\n\nOverall, PyTorch is a powerful tool for building and training deep learning models, and its use in the project indicates a focus on developing a sophisticated AI model that can perform complex tasks such as time series forecasting and text generation.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which is consistent with the formal and academic tone of the description.\n* The text also mentions technical terms and mathematical equations, which are consistent with the use of PyTorch in deep learning projects.\n* The fact that PyTorch is used to implement TimeGPT suggests that the project is focused on developing a model that can generate human-like text, which is a complex task that requires sophisticated AI capabilities.", "id": "PYTORCH", "label": "PYTORCH", "shape": "dot", "size": 10, "source_id": "42e1bb44edbe787e104f589e74b95d6c,88d96b5f76042688e9dd745eb822d919", "type": "FRAMEWORK"}, {"color": "#97c2fc", "description": "Adam is an optimization algorithm used to train TimeGPT", "id": "ADAM", "label": "ADAM", "shape": "dot", "size": 10, "source_id": "42e1bb44edbe787e104f589e74b95d6c", "type": "OPTIMIZER"}, {"color": "#97c2fc", "description": "Matplotlib is an open-source library used in the project", "id": "MATPLOTLIB", "label": "MATPLOTLIB", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "LIBRARY"}, {"color": "#97c2fc", "description": "Error refers to the difference between the predicted and actual values, as indicated by the use of the \u00b1 symbol in the table", "id": "ERROR", "label": "ERROR", "shape": "dot", "size": 10, "source_id": "1dc665214307b1656d1a0094a1918ece", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Prediction intervals are a measure of the uncertainty associated with a forecast", "id": "PREDICTION INTERVALS", "label": "PREDICTION INTERVALS", "shape": "dot", "size": 10, "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Embedding refers to the process of representing a data point as a vector in a high-dimensional space", "id": "EMBEDDING", "label": "EMBEDDING", "shape": "dot", "size": 10, "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Equation 2 is a mathematical formula used to compute the rMAE metric", "id": "EQUATION 2", "label": "EQUATION 2", "shape": "dot", "size": 10, "source_id": "f912df936d0b735fe654d1a9c53caa3b", "type": "FORMULA"}, {"color": "#97c2fc", "description": "Task-specific dataset refers to a dataset that is tailored to a specific task or domain", "id": "TASK-SPECIFIC DATASET", "label": "TASK-SPECIFIC DATASET", "shape": "dot", "size": 10, "source_id": "55eb54ef455d14cd1a11760924f99eb8", "type": "DATA"}, {"color": "#97c2fc", "description": "Fine-tuning steps refer to the process of adjusting model parameters on a task-specific dataset", "id": "FINE-TUNING STEPS", "label": "FINE-TUNING STEPS", "shape": "dot", "size": 10, "source_id": "55eb54ef455d14cd1a11760924f99eb8", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Model parameters refer to the adjustable values in a model that are adjusted during fine-tuning", "id": "MODEL PARAMETERS", "label": "MODEL PARAMETERS", "shape": "dot", "size": 10, "source_id": "55eb54ef455d14cd1a11760924f99eb8", "type": "DATA"}, {"color": "#97c2fc", "description": "Task-specific task refers to a task that is tailored to a specific domain or context", "id": "TASK-SPECIFIC TASK", "label": "TASK-SPECIFIC TASK", "shape": "dot", "size": 10, "source_id": "55eb54ef455d14cd1a11760924f99eb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Parallel computing-optimized statistical methods refer to methods that are optimized for parallel computing", "id": "PARALLEL COMPUTING-OPTIMIZED STATISTICAL METHODS", "label": "PARALLEL COMPUTING-OPTIMIZED STATISTICAL METHODS", "shape": "dot", "size": 10, "source_id": "55eb54ef455d14cd1a11760924f99eb8", "type": "METHOD"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nBORIS N ORESHKIN is a researcher and author who has contributed to the field of time series forecasting. Specifically, he is the author of a notable paper titled \"N-beats: Neural basis expansion analysis for interpretable time series forecasting.\" This paper is likely a significant contribution to the field, given the technical terms and mathematical equations used in the text, which suggest a formal and academic tone. The use of English language and abbreviations such as \"N-beats\" further supports the conclusion that the paper is written in English and is likely a research paper or technical document. Overall, BORIS N ORESHKIN\u0027s work in time series forecasting, as evident from his paper, demonstrates his expertise in this area.", "id": "BORIS N ORESHKIN", "label": "BORIS N ORESHKIN", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Dmitri Carpov is an author of the paper \"N-beats: Neural basis expansion analysis for interpretable time series forecasting\"", "id": "DMITRI CARPOV", "label": "DMITRI CARPOV", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Nicolas Chapados is an author of the paper \"N-beats: Neural basis expansion analysis for interpretable time series forecasting\"", "id": "NICOLAS CHAPADOS", "label": "NICOLAS CHAPADOS", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Yoshua Bengio is an author of the paper \"N-beats: Neural basis expansion analysis for interpretable time series forecasting\"", "id": "YOSHUA BEN-GIO", "label": "YOSHUA BEN-GIO", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Lag-Llama demonstrates strong zero-shot generalization capabilities compared to a wide range of forecasting models", "id": "FORECASTING MODELS", "label": "FORECASTING MODELS", "shape": "dot", "size": 10, "source_id": "c7167cf92abe7513ccb936ca79871932", "type": "MODEL"}, {"color": "#97c2fc", "description": "Demographic refers to a specific domain or category of data or information", "id": "DEMOGRAPHIC", "label": "DEMOGRAPHIC", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Micro refers to a specific domain or category of data or information", "id": "MICRO", "label": "MICRO", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Macro refers to a specific domain or category of data or information", "id": "MACRO", "label": "MACRO", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Industry refers to a specific domain or category of data or information", "id": "INDUSTRY", "label": "INDUSTRY", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Open time series data refers to a large collection of time series data that is publicly available for use in research and development", "id": "OPEN TIME SERIES DATA", "label": "OPEN TIME SERIES DATA", "shape": "dot", "size": 10, "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "type": "DATA"}, {"color": "#97c2fc", "description": "Unseen time series datasets refer to time series datasets that have not been seen or used in training a model", "id": "UNSEEN TIME SERIES DATASETS", "label": "UNSEEN TIME SERIES DATASETS", "shape": "dot", "size": 10, "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "type": "DATA"}, {"color": "#97c2fc", "description": "State-of-the-art dataset-specific models refer to models that are trained on a specific dataset and are considered to be the best performing models for that dataset", "id": "STATE-OF-THE-ART DATASET-SPECIFIC MODELS", "label": "STATE-OF-THE-ART DATASET-SPECIFIC MODELS", "shape": "dot", "size": 10, "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"FEW-SHOT ADAPTATION\" refers to a concept in machine learning that enables a model to adapt to a new task or dataset with only a few examples or instances. This ability is crucial for models to generalize and perform well on unseen data. The phrase \"few-shot adaptation performance\" is used to describe this concept, indicating its significance in evaluating the effectiveness of models in adapting to new tasks.\n\nThe description of few-shot adaptation highlights its importance in real-world applications where data is limited, and models need to learn quickly from a small number of examples. This concept is closely related to the field of time series analysis, where models need to adapt to changing patterns and trends in data.\n\nIn the context of machine learning, few-shot adaptation is a key aspect of long-term series forecasting, where models need to adapt to new data and patterns over time. The use of techniques such as frequency analysis and multi-head cross-attention can help models to better adapt to new tasks and datasets.\n\nOverall, few-shot adaptation is a critical concept in machine learning that enables models to adapt to new tasks and datasets with only a few examples or instances, making it a valuable tool for real-world applications.\n\nRelevant information from the nearby text:\n\n* The text mentions the use of technical terms, mathematical equations, and references to academic papers, which are all written in English.\n* The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n* The text cites English-language academic papers and authors, which further supports the conclusion that the primary language of the text is English.\n\nNote: The provided information is consistent and does not contain any contradictions. The summary is based on the information provided in the description list and the nearby text.", "id": "FEW-SHOT ADAPTATION", "label": "FEW-SHOT ADAPTATION", "shape": "dot", "size": 10, "source_id": "ca3dfe42c66d68dd6dbad037936b2360,d08a82328191907abfcdb72efab9a6cf", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\n\"THETA MODELS\" are a type of statistical model used for time series forecasting. They are specifically designed for this purpose, as indicated by the use of the phrase \"Theta models\" in the context of time series forecasting. This suggests that THETA MODELS are a well-established and widely recognized approach in the field of time series analysis, with a clear and defined application in forecasting long-term series.\n\nThe use of THETA MODELS in time series forecasting is likely to involve various techniques, such as frequency analysis and multi-head cross-attention, as these are common methods used in time series analysis. Additionally, the mathematical equations and formulas used in THETA MODELS are likely to be written in a standard mathematical notation, consistent with the conventions used in English-language academic papers.\n\nOverall, THETA MODELS appear to be a sophisticated and widely used approach in time series forecasting, with a strong foundation in statistical modeling and a clear application in the field of time series analysis.", "id": "THETA MODELS", "label": "THETA MODELS", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0,ca3dfe42c66d68dd6dbad037936b2360", "type": "MODEL"}, {"color": "#97c2fc", "description": "AutoGluon is a framework used for automating machine learning tasks", "id": "AUTOGLUON", "label": "AUTOGLUON", "shape": "dot", "size": 10, "source_id": "51ee17c1f0212d4c94010d8e376f649b", "type": "FRAMEWORK"}, {"color": "#97c2fc", "description": "Deep neural networks refer to models that use multiple layers of artificial neurons to make predictions", "id": "DEEP NEURAL NETWORKS", "label": "DEEP NEURAL NETWORKS", "shape": "dot", "size": 10, "source_id": "51ee17c1f0212d4c94010d8e376f649b", "type": "MODEL"}, {"color": "#97c2fc", "description": "RNN-based models are a type of neural model used in time series forecasting, as indicated by the use of the phrase \"RNN-based models\"", "id": "RNN-BASED MODELS", "label": "RNN-BASED MODELS", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "MODEL"}, {"color": "#97c2fc", "description": "LSTM-based models are a type of neural model used in time series forecasting, as indicated by the use of the phrase \"LSTM-based models\"", "id": "LSTM-BASED MODELS", "label": "LSTM-BASED MODELS", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "MODEL"}, {"color": "#97c2fc", "description": "Transfer capabilities refer to the ability of a model to perform well on tasks or domains that are different from the ones it was trained on", "id": "TRANSFER CAPABILITIES", "label": "TRANSFER CAPABILITIES", "shape": "dot", "size": 10, "source_id": "c4e47033b8ecc85beacde9d560e66062", "type": "PROPERTY"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity in question is the \"NEURAL NETWORK.\" A neural network is a type of model used in the training process, specifically designed to approximate the unknown distribution of the next future values given the covariates. This implies that neural networks are utilized for predictive modeling, particularly in time series forecasting or long-term series forecasting, where the goal is to forecast future values based on historical data and covariates.\n\nThe use of neural networks in this context involves frequency analysis and multi-head cross-attention mechanisms, which are advanced techniques used in machine learning models to improve their performance and accuracy. The mathematical equations and formulas used in the development of these models are written in standard mathematical notation, indicating a high level of technical complexity and academic rigor.\n\nThe entity \"NEURAL NETWORK\" is commonly referenced in academic conferences and journals, such as ICLR, AAAI, and PMLR, and is often cited in academic papers and research studies. This suggests that neural networks are a widely researched and applied topic in the field of machine learning and artificial intelligence.\n\nOverall, the summary provides a detailed understanding of the entity \"NEURAL NETWORK\" and its applications in predictive modeling, machine learning, and time series forecasting.", "id": "NEURAL NETWORK", "label": "NEURAL NETWORK", "shape": "dot", "size": 10, "source_id": "6ea15432a841705c2e74cbc01f6004e9,ac37bdb991d1513e44e4c5fc7a28b187", "type": "MODEL"}, {"color": "#97c2fc", "description": "Meta learning is a technique used to learn temporal trends in the input training time-series with limited data", "id": "META LEARNING", "label": "META LEARNING", "shape": "dot", "size": 10, "source_id": "6ea15432a841705c2e74cbc01f6004e9", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Loss function is a function used to evaluate the performance of the model", "id": "LOSSES FUNCTION", "label": "LOSSES FUNCTION", "shape": "dot", "size": 10, "source_id": "6ea15432a841705c2e74cbc01f6004e9", "type": "FUNCTION"}, {"color": "#97c2fc", "description": "Second-level frequency refers to a time series with a frequency of 525600", "id": "SECOND-LEVEL FREQUENCY", "label": "SECOND-LEVEL FREQUENCY", "shape": "dot", "size": 10, "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "type": "FREQUENCY"}, {"color": "#97c2fc", "description": "Baptiste Rozi\u00e8re is an author of the paper \"Llama: Open and efficient foundation language models\"", "id": "BAPTISTE ROZIERE", "label": "BAPTISTE ROZIERE", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Parametric distribution head is a concept that projects the model\u2019s features to the parameters of a probability distribution", "id": "PARAMETRIC DISTRIBUTION HEAD", "label": "PARAMETRIC DISTRIBUTION HEAD", "shape": "dot", "size": 10, "source_id": "b305a0d76a0159dc10338467e16d6761", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Student\u2019s t-distribution is a concept that is used as a parametric probability distribution", "id": "STUDENT\u0027S T-DISTRIBUTION", "label": "STUDENT\u0027S T-DISTRIBUTION", "shape": "dot", "size": 10, "source_id": "b305a0d76a0159dc10338467e16d6761", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Normalizing flows is a concept that is used as a parametric probability distribution", "id": "NORMALIZING FLOWS", "label": "NORMALIZING FLOWS", "shape": "dot", "size": 10, "source_id": "b305a0d76a0159dc10338467e16d6761", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Copulas is a concept that is used as a parametric probability distribution", "id": "COPULAS", "label": "COPULAS", "shape": "dot", "size": 10, "source_id": "b305a0d76a0159dc10338467e16d6761", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "RMSNorm is a concept that is used in Lag-Llama", "id": "RMSNORM", "label": "RMSNORM", "shape": "dot", "size": 10, "source_id": "b305a0d76a0159dc10338467e16d6761", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Rotary Positional Encoding is a concept that is used in Lag-Llama", "id": "ROTARY POSITIONAL ENCODING", "label": "ROTARY POSITIONAL ENCODING", "shape": "dot", "size": 10, "source_id": "b305a0d76a0159dc10338467e16d6761", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Model optimization is a process that involves adjusting the model\u0027s parameters to improve its performance", "id": "MODEL OPTIMIZATION", "label": "MODEL OPTIMIZATION", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536", "type": "PROCESS"}, {"color": "#97c2fc", "description": "A parametric distributional head is a component of a model that is used to generate probability distributions", "id": "PARAMETRIC DISTRIBUTIONAL HEAD", "label": "PARAMETRIC DISTRIBUTIONAL HEAD", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536", "type": "MODEL COMPONENT"}, {"color": "#97c2fc", "description": "A scaling heuristic is an algorithm that is used to scale the values of a time series", "id": "SCALING HEURISTIC", "label": "SCALING HEURISTIC", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Time series augmentation is a process that involves modifying a time series to create new data points", "id": "TIME SERIES AUGMENTATION", "label": "TIME SERIES AUGMENTATION", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Freq-mix is an algorithm that is used to mix the frequencies of a time series", "id": "FREQ-MIX", "label": "FREQ-MIX", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe entity \"HYPERPARAMETER SEARCH\" refers to a process that involves searching for the optimal values of a model\u0027s hyperparameters. This process is crucial in machine learning as it enables the selection of the best hyperparameters for a model, thereby improving its performance and accuracy. Hyperparameter search is a critical step in the development of machine learning models, and it is essential to identify the optimal hyperparameters to achieve the desired results.\n\nIn the context of machine learning, hyperparameters are parameters that are set before training a model, and they can significantly impact the model\u0027s performance. The process of hyperparameter search involves trying different combinations of hyperparameters and evaluating the performance of the model for each combination. This process can be time-consuming and computationally expensive, but it is essential to identify the optimal hyperparameters for a model.\n\nOverall, hyperparameter search is a critical component of machine learning model development, and it plays a vital role in achieving the desired results. By selecting the optimal hyperparameters, machine learning models can be improved, and their performance can be enhanced.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which is consistent with the formal and academic language used in the descriptions.\n* The text also mentions that the language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n* The presence of technical terms, mathematical equations, and references to academic papers further supports the conclusion that the text is written in English.\n\nNote: The descriptions provided are consistent and do not contain any contradictions. Therefore, the summary is a straightforward representation of the information provided.", "id": "HYPERPARAMETER SEARCH", "label": "HYPERPARAMETER SEARCH", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,f0c52387a7b3a5c3850fe6991f0a7c83,fa01b75dccc556af8aca0d6c47e1970f", "type": "PROCESS"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe CORPUS OF DATASETS is a collection of datasets that are used to train a model. Specifically, it refers to a large collection of datasets used to train the model, serving as a comprehensive resource for model development and training.\n\nThis summary is derived from the provided descriptions, which are consistent and provide a clear understanding of the CORPUS OF DATASETS. The descriptions highlight the purpose and nature of the CORPUS OF DATASETS, emphasizing its role in model training and development.\n\nThe summary is written in the third person and includes the entity name, providing a clear and concise description of the CORPUS OF DATASETS.", "id": "CORPUS OF DATASETS", "label": "CORPUS OF DATASETS", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536,1727ee77a376bbef1c7625a001e4ac3d", "type": "DATA SET"}, {"color": "#97c2fc", "description": "Pretraining data refers to the amount of data used to train the model", "id": "PRETRAINING DATA", "label": "PRETRAINING DATA", "shape": "dot", "size": 10, "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Zero-shot generalization refers to the ability of the model to perform well on unseen datasets without any fine-tuning", "id": "ZERO-SHOT GENERALIZATION", "label": "ZERO-SHOT GENERALIZATION", "shape": "dot", "size": 10, "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Diverse datasets refers to datasets from different domains", "id": "DIVERSE DATASETS", "label": "DIVERSE DATASETS", "shape": "dot", "size": 10, "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Performance refers to the model\u0027s ability to make accurate predictions", "id": "PERFORMANCE", "label": "PERFORMANCE", "shape": "dot", "size": 10, "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Early stopping criterion is a parameter used to determine when to stop training a model", "id": "EARLY STOPPING CRITERION", "label": "EARLY STOPPING CRITERION", "shape": "dot", "size": 10, "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Cold-start problem is a concept mentioned in the text, as indicated by the use of the phrase \"cold-start problem\"", "id": "COLD-START PROBLEM", "label": "COLD-START PROBLEM", "shape": "dot", "size": 10, "source_id": "d08a82328191907abfcdb72efab9a6cf", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "40% is a percentage, as indicated by the use of the percentage sign in the table", "id": "40 %", "label": "40 %", "shape": "dot", "size": 10, "source_id": "1dc665214307b1656d1a0094a1918ece", "type": "PERCENTAGE"}, {"color": "#97c2fc", "description": "60% is a percentage, as indicated by the use of the percentage sign in the table", "id": "60 %", "label": "60 %", "shape": "dot", "size": 10, "source_id": "1dc665214307b1656d1a0094a1918ece", "type": "PERCENTAGE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"REQUESTS\" entity refers to specific demands or inquiries for a service or resource. The \"Requests\" dataset is a collection of data used to evaluate the performance of Lag-Llama, a model likely used for long-term series forecasting or other related tasks. This dataset is likely used to assess the model\u0027s ability to accurately predict or analyze requests, which can be a crucial aspect of various applications, such as service management, resource allocation, or demand forecasting.\n\nThe use of the \"Requests\" dataset suggests that Lag-Llama is being evaluated on its capacity to handle and process requests, which may involve tasks like frequency analysis, multi-head cross-attention, or other advanced techniques. The entity \"REQUESTS\" is therefore closely related to the performance evaluation of Lag-Llama, and the dataset is a key component in this process.\n\nOverall, the \"REQUESTS\" entity is a critical aspect of the Lag-Llama model\u0027s performance evaluation, and the \"Requests\" dataset plays a vital role in assessing the model\u0027s capabilities in handling requests and related tasks.", "id": "REQUESTS", "label": "REQUESTS", "shape": "dot", "size": 10, "source_id": "bcf830b85e12e1ab9498e3ac3593a88e,ec7705b83cf4fe3aa18662c917b18c1a", "type": "DATASET"}, {"color": "#97c2fc", "description": "Exchange-rate dataset is a dataset used to evaluate Lag-Llama\u0027s performance", "id": "EXCHANGE-RATE", "label": "EXCHANGE-RATE", "shape": "dot", "size": 10, "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "type": "DATASET"}, {"color": "#97c2fc", "description": "OneFitsAll is a model that adapts a pre-trained LLM for forecasting", "id": "ONEFITSAALL", "label": "ONEFITSAALL", "shape": "dot", "size": 10, "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "type": "MODEL"}, {"color": "#97c2fc", "description": "Tay et al. are authors who studied the influence of inductive bias at scale in the NLP community", "id": "TAY ET AL.", "label": "TAY ET AL.", "shape": "dot", "size": 10, "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "type": "AUTHORS"}, {"color": "#97c2fc", "description": "Zhou et al. are authors who proposed the OneFitsAll model", "id": "ZHOU ET AL.", "label": "ZHOU ET AL.", "shape": "dot", "size": 10, "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "type": "AUTHORS"}, {"color": "#97c2fc", "description": "Zhu et al. is the author of the paper OneFitsAll model", "id": "ZHU ET AL.", "label": "ZHU ET AL.", "shape": "dot", "size": 10, "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Appendix refers to the supplementary material provided in the document", "id": "APPENDIX", "label": "APPENDIX", "shape": "dot", "size": 10, "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Subsection 6.1 refers to a specific section of the document discussing the performance of Lag-Llama", "id": "SUBSECTION 6.1", "label": "SUBSECTION 6.1", "shape": "dot", "size": 10, "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "type": "SECTION"}, {"color": "#97c2fc", "description": "State-of-the-art refers to the current best-performing model or approach in a particular field", "id": "STATE-OF-THE-ART", "label": "STATE-OF-THE-ART", "shape": "dot", "size": 10, "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "type": "MODEL"}, {"color": "#97c2fc", "description": "Dataset-specific models are models that are trained on a specific dataset", "id": "DATASET-SPECIFIC MODELS", "label": "DATASET-SPECIFIC MODELS", "shape": "dot", "size": 10, "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "type": "MODEL"}, {"color": "#97c2fc", "description": "State-of-the-art performance refers to the best performance achieved by a model on a particular task", "id": "STATE-OF-THE-ART PERFORMANCE", "label": "STATE-OF-THE-ART PERFORMANCE", "shape": "dot", "size": 10, "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe Monash Time Series Repository Dataset is a dataset mentioned in the text and used for training and testing Lag-Llama. This dataset is likely a collection of time series data, given its association with time series analysis and forecasting. The dataset is specifically mentioned in the context of long-term series forecasting, suggesting that it may contain historical data that can be used to predict future trends.\n\nThe use of the Monash Time Series Repository Dataset for training and testing Lag-Llama implies that it is a valuable resource for evaluating the performance of machine learning models, particularly those designed for time series forecasting. The dataset\u0027s association with frequency analysis and multi-head cross-attention further suggests that it may contain complex and nuanced time series data that requires advanced analytical techniques to model and predict.\n\nOverall, the Monash Time Series Repository Dataset appears to be a significant resource for researchers and practitioners working in the field of time series analysis and forecasting, particularly those interested in developing and evaluating machine learning models for long-term series forecasting.\n\nRelevant information from the nearby text includes:\n\n* The text is written in English, suggesting that the dataset is likely to be used in an English-language academic or research context.\n* The text mentions technical terms and mathematical equations, indicating that the dataset is likely to be used for advanced analytical and modeling tasks.\n* The text references academic papers and authors, suggesting that the dataset may be used in a research or academic setting.\n\nNote that the contradictions in the descriptions are resolved by focusing on the common thread of the Monash Time Series Repository Dataset being used for training and testing Lag-Llama, and its association with time series analysis and forecasting.", "id": "MONASH TIME SERIES REPOSITORY DATASET", "label": "MONASH TIME SERIES REPOSITORY DATASET", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,1727ee77a376bbef1c7625a001e4ac3d", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe Hugging Face datasets are a collection of datasets mentioned in the text, specifically used for training and testing Lag-Llama. These datasets are utilized for long-term series forecasting, frequency analysis, and other related tasks. The datasets are likely used in academic research and technical documents, as indicated by the presence of mathematical equations, formulas, and references to academic papers. The language used in the text is formal and academic, suggesting that the datasets are used in English-language research papers and technical documents. The datasets may be used in conferences and journals such as ICLR, AAAI, and PMLR, and may be cited in papers by English-language authors.\n\nNote: The information provided is based on the given descriptions and the analysis of the language used in the text. The summary is written in a way that is coherent and consistent with the provided information.", "id": "HUGGING FACE DATASETS", "label": "HUGGING FACE DATASETS", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,1727ee77a376bbef1c7625a001e4ac3d", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe \"ONE-FITS-ALL MODEL\" is a machine learning model used for time series forecasting tasks. It is a model specifically designed for this purpose, indicating its primary function is to forecast future values in a time series. The model\u0027s application is evident in the project where it is utilized, suggesting its effectiveness in handling time series data.\n\nGiven the context of the text, which is formal and academic, it is likely that the \"ONE-FITS-ALL MODEL\" is a sophisticated model that incorporates advanced techniques such as frequency analysis and multi-head cross-attention, as mentioned in the text. The model\u0027s ability to handle time series data makes it a valuable tool for researchers and practitioners in the field of time series forecasting.\n\nOverall, the \"ONE-FITS-ALL MODEL\" is a specialized machine learning model designed for time series forecasting tasks, with potential applications in various fields, including research and industry projects.", "id": "ONE-FITS-ALL MODEL", "label": "ONE-FITS-ALL MODEL", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,88d96b5f76042688e9dd745eb822d919", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe N-BEATS MODEL is a machine learning model specifically designed for time series forecasting tasks. It is a model used in the project for this purpose, indicating its application in a particular context. The N-BEATS model is utilized for long-term series forecasting, suggesting its ability to handle complex and dynamic time series data. Additionally, the model incorporates techniques such as frequency analysis, which is a common approach in time series forecasting to understand the underlying patterns and trends in the data.\n\nThe N-BEATS model\u0027s architecture likely involves the use of multi-head cross-attention, a technique commonly employed in transformer-based models for handling sequential data. This suggests that the model is capable of capturing complex relationships and patterns within the time series data.\n\nOverall, the N-BEATS MODEL is a sophisticated machine learning model designed for time series forecasting tasks, leveraging techniques such as frequency analysis and multi-head cross-attention to provide accurate and reliable predictions.", "id": "N-BEATS MODEL", "label": "N-BEATS MODEL", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,88d96b5f76042688e9dd745eb822d919", "type": "MODEL"}, {"color": "#97c2fc", "description": "GluonTS code is used in the project", "id": "GLUONTS CODE", "label": "GLUONTS CODE", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "CODE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nSAHIL is a researcher who has made significant contributions to the development of Lag-Llama. In addition to his research contributions, SAHIL also serves as an advisor to the project, providing valuable feedback on the experiments and the paper. This dual role highlights his expertise and involvement in the project, underscoring his importance in the development of Lag-Llama.\n\nThe information collected from the descriptions suggests that SAHIL is a key figure in the project, with a deep understanding of the technical aspects of Lag-Llama, as well as the ability to provide strategic guidance and feedback. This summary provides a clear and concise overview of SAHIL\u0027s role and contributions to the project.", "id": "SAHIL", "label": "SAHIL", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,88d96b5f76042688e9dd745eb822d919", "type": "PERSON"}, {"color": "#97c2fc", "description": "", "id": "PROJECT", "label": "PROJECT", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nAnderson is a researcher who has made significant contributions to the development of Lag-Llama. In addition to their research contributions, Anderson also serves as an advisor to the project, providing valuable feedback on the experiments and the paper. This dual role highlights Anderson\u0027s expertise and influence in the field, as well as their commitment to the project\u0027s success.\n\nThis summary combines the two descriptions provided, highlighting Anderson\u0027s research contributions and their advisory role, while also providing context for their involvement in the project.", "id": "ANDERSON", "label": "ANDERSON", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,88d96b5f76042688e9dd745eb822d919", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nNicolas is a researcher who has made significant contributions to the development of Lag-Llama. In addition to his research contributions, Nicolas also serves as an advisor to the project, providing valuable feedback on the experiments and the paper. His expertise and guidance have likely played a crucial role in shaping the project\u0027s outcomes and ensuring the quality of the research.\n\nThis summary combines the two descriptions provided, highlighting Nicolas\u0027 dual role as a researcher and advisor to the project. The information is written in a clear and concise manner, providing a comprehensive understanding of Nicolas\u0027 involvement in the project.", "id": "NICOLAS", "label": "NICOLAS", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,88d96b5f76042688e9dd745eb822d919", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of Alexandre:\n\nAlexandre is a researcher who has made significant contributions to the development of Lag-Llama. In addition to his research role, he also serves as an advisor to the project, providing valuable feedback on the experiments and the paper. His expertise and guidance have likely played a crucial role in shaping the project\u0027s outcomes and ensuring the quality of the research.\n\nThis summary combines the two descriptions provided, highlighting Alexandre\u0027s dual role as a researcher and advisor. The information is written in the third person and includes the entity name \"ALEXANDRE\" for context.", "id": "ALEXANDRE", "label": "ALEXANDRE", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,88d96b5f76042688e9dd745eb822d919", "type": "PERSON"}, {"color": "#97c2fc", "description": "The paper is a research paper that is part of the project", "id": "PAPER", "label": "PAPER", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Valentina is an advisor to the project who provided feedback on the experiments and the paper", "id": "VALENTINA", "label": "VALENTINA", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "PERSON"}, {"color": "#97c2fc", "description": "Yuriy is an advisor to the project who provided feedback on the experiments and the paper", "id": "YURIY", "label": "YURIY", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "PERSON"}, {"color": "#97c2fc", "description": "Irina is an advisor to the project who provided feedback on the experiments and the paper", "id": "IRINA", "label": "IRINA", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "PERSON"}, {"color": "#97c2fc", "description": "Viatcheslav Gurev is a person who provided useful discussions during the project", "id": "VIATCHESLAV GUREV", "label": "VIATCHESLAV GUREV", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "PERSON"}, {"color": "#97c2fc", "description": "Experimental setups are used in the project", "id": "EXPERIMENTAL SETUPS", "label": "EXPERIMENTAL SETUPS", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "SETUP"}, {"color": "#97c2fc", "description": "NumPy is an open-source library used in the project", "id": "NUMPY", "label": "NUMPY", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "LIBRARY"}, {"color": "#97c2fc", "description": "Pandas is an open-source library used in the project", "id": "PANDAS", "label": "PANDAS", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "LIBRARY"}, {"color": "#97c2fc", "description": "The Canada CIFAR AI Chair Program is a program that supported the project", "id": "CANADA CIFAR AI CHAIR PROGRAM", "label": "CANADA CIFAR AI CHAIR PROGRAM", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "PROGRAM"}, {"color": "#97c2fc", "description": "The Canada Excellence Research Chairs (CERC) Program is a program that supported the project", "id": "CANADA EXCELLENCE RESEARCH CHAIRS (CERC) PROGRAM", "label": "CANADA EXCELLENCE RESEARCH CHAIRS (CERC) PROGRAM", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "PROGRAM"}, {"color": "#97c2fc", "description": "The Oak Ridge Leadership Computing Facility is a facility that provided compute resources for the project", "id": "OAK RIDGE LEADERSHIP COMPUTING FACILITY", "label": "OAK RIDGE LEADERSHIP COMPUTING FACILITY", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "FACILITY"}, {"color": "#97c2fc", "description": "The Oak Ridge National Laboratory is a laboratory that provided compute resources for the project", "id": "OAK RIDGE NATIONAL LABORATORY", "label": "OAK RIDGE NATIONAL LABORATORY", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "LABORATORY"}, {"color": "#97c2fc", "description": "ServiceNow is a company that provided compute resources for the project", "id": "SERVICE NOW", "label": "SERVICE NOW", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "COMPANY"}, {"color": "#97c2fc", "description": "Mila is an organization that provided compute resources for the project", "id": "MILA", "label": "MILA", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe Australian Electricity Demand dataset is a dataset used in a project, specifically related to the analysis and forecasting of electricity demand in Australia. The dataset is likely used for time series analysis and long-term series forecasting, as indicated by its application in a project. The dataset may involve frequency analysis and potentially utilizes techniques such as multi-head cross-attention, given the technical terms and mathematical equations mentioned in the context. However, no specific details about the dataset\u0027s structure, content, or characteristics are provided in the given descriptions.", "id": "AUSTRALIAN ELECTRICITY DEMAND DATASET", "label": "AUSTRALIAN ELECTRICITY DEMAND DATASET", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6,88d96b5f76042688e9dd745eb822d919", "type": "DATASET"}, {"color": "#97c2fc", "description": "Victoria is one of the five Australian states included in the Australian Electricity Demand dataset", "id": "VICTORIA", "label": "VICTORIA", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "New South Wales is one of the five Australian states included in the Australian Electricity Demand dataset", "id": "NEW SOUTH WALES", "label": "NEW SOUTH WALES", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Queensland is one of the five Australian states included in the Australian Electricity Demand dataset", "id": "QUEENSLAND", "label": "QUEENSLAND", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Tasmania is one of the five Australian states included in the Australian Electricity Demand dataset", "id": "TASMANIA", "label": "TASMANIA", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "South Australia is one of the five Australian states included in the Australian Electricity Demand dataset", "id": "SOUTH AUSTRALIA", "label": "SOUTH AUSTRALIA", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "The US Embassy in Beijing is the location where the PM2.5 levels were recorded in the Beijing PM2.5 dataset", "id": "US EMBASSY", "label": "US EMBASSY", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Beijing Capital International Airport is the location where the meteorological data was recorded in the Beijing PM2.5 dataset", "id": "BEIJING CAPITAL INTERNATIONAL AIRPORT", "label": "BEIJING CAPITAL INTERNATIONAL AIRPORT", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the \"ELECTRICITY HOURLY DATASET\" in third person:\n\nThe \"ELECTRICITY HOURLY DATASET\" is a collection of data used to train and evaluate machine learning models. This dataset captures electricity usage for 321 clients measured at hourly intervals from 2012 to 2014. The dataset is a valuable resource for researchers and practitioners in the field of machine learning and time series forecasting, particularly in the context of long-term series forecasting and frequency analysis.\n\nThe dataset\u0027s hourly measurements from 2012 to 2014 provide a comprehensive view of electricity usage patterns over time, allowing for the application of advanced techniques such as multi-head cross-attention and other machine learning algorithms to identify trends, patterns, and relationships within the data. The dataset\u0027s structure and content make it an ideal candidate for time series analysis and forecasting, enabling the development of accurate models that can predict electricity usage with high precision.\n\nOverall, the \"ELECTRICITY HOURLY DATASET\" is a rich and valuable resource for anyone interested in machine learning, time series forecasting, and electricity usage analysis, offering a unique opportunity to explore and understand the complex relationships between electricity usage and various factors over time.", "id": "ELECTRICITY HOURLY DATASET", "label": "ELECTRICITY HOURLY DATASET", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6,c60a8238db3d56eff9cc9692e7ac5b1c", "type": "DATASET"}, {"color": "#97c2fc", "description": "The Exchange Rate compilation encompasses the daily exchange rates of eight foreign currencies, namely Australia, the United Kingdom, Canada, Switzerland, China, Japan, New Zealand, and Singapore, spanning the period from 1990 to 2016", "id": "EXCHANGE RATE COMPILATION", "label": "EXCHANGE RATE COMPILATION", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "DATASET"}, {"color": "#97c2fc", "description": "Japan is one of the countries included in the Exchange Rate compilation", "id": "JAPAN", "label": "JAPAN", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "New Zealand is one of the countries included in the Exchange Rate compilation", "id": "NEW ZEALAND", "label": "NEW ZEALAND", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Singapore is one of the countries included in the Exchange Rate compilation", "id": "SINGAPORE", "label": "SINGAPORE", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "The London Smart Meters dataset focuses on electrical consumption readings from smart meters in 5,567 households that participated in the UK Power Networks Low Carbon London project between November 2011 and February 2014", "id": "LONDON SMART METERS DATASET", "label": "LONDON SMART METERS DATASET", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "DATASET"}, {"color": "#97c2fc", "description": "UK Power Networks is the organization that participated in the Low Carbon London project", "id": "UK POWER NETWORKS", "label": "UK POWER NETWORKS", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "The Low Carbon London project was a project that aimed to reduce carbon emissions in London", "id": "LOW CARBON LONDON PROJECT", "label": "LOW CARBON LONDON PROJECT", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "PROJECT"}, {"color": "#97c2fc", "description": "The KDD Cup 2018 dataset comprises extensive hourly time series data reflecting air quality levels across 59 stations in Beijing and London from January 2017 to March 2018", "id": "KDD CUP 2018 DATASET", "label": "KDD CUP 2018 DATASET", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "DATASET"}, {"color": "#97c2fc", "description": "The KDD Cup 2018 dataset includes data from 59 stations in Beijing and London", "id": "59 STATIONS", "label": "59 STATIONS", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "The Pedestrian Counts dataset includes data from 66 sensors in the city of Melbourne", "id": "66 SENSORS", "label": "66 SENSORS", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "LOCATION"}, {"color": "#97c2fc", "description": "The Solar dataset comprises 6000 simulated time series for 5-minute solar power and hourly forecasts of photovoltaic power plants in the U.S. in 2006", "id": "SOLAR DATASET", "label": "SOLAR DATASET", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131,85902d07a5ac3acacd692810072fa1c6", "type": "DATASET"}, {"color": "#97c2fc", "description": "Solar power refers to energy generated from the sun", "id": "SOLAR POWER", "label": "SOLAR POWER", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The Sunspot dataset comprises a singular extensive daily time series of sunspot numbers spanning from January 1818 to May 2020", "id": "SUNSPOT DATASET", "label": "SUNSPOT DATASET", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131", "type": "DATASET"}, {"color": "#97c2fc", "description": "Sunspots refer to dark regions on the surface of the sun", "id": "SUNSPOTS", "label": "SUNSPOTS", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The Uber TLC Hourly dataset consists data of 4.5 million Uber pickups in NYC (April-September 2014) and 14.3 million pickups (January-June 2015)", "id": "UBER TLC HOURLY DATASET", "label": "UBER TLC HOURLY DATASET", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131", "type": "DATASET"}, {"color": "#97c2fc", "description": "Uber pickups refer to the number of times a person uses the Uber service", "id": "UBER PICKUPS", "label": "UBER PICKUPS", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The Wind Farms dataset contains minute-frequency time series data tracking the wind power production of 339 wind farms in Australia", "id": "WIND FARMS DATASET", "label": "WIND FARMS DATASET", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131", "type": "DATASET"}, {"color": "#97c2fc", "description": "Wind power production refers to the amount of energy generated from wind", "id": "WIND POWER PRODUCTION", "label": "WIND POWER PRODUCTION", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Training split refers to the portion of data used to train a model", "id": "TRAINING SPLIT", "label": "TRAINING SPLIT", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"TEST SPLIT\" refers to a subset of the dataset that is specifically used for testing purposes. It is a portion of the data that is utilized to evaluate the performance and accuracy of a machine learning model. In other words, the test split is a critical component of the dataset that allows researchers and developers to assess how well their model generalizes to unseen data.\n\nThis summary is derived from the two descriptions provided, which are consistent and complementary. The first description states that the test split is a subset of the dataset used for testing, while the second description elaborates on the purpose of the test split, which is to test a model. By combining these two descriptions, we can gain a deeper understanding of the concept of test split and its significance in the context of machine learning and data analysis.\n\nIn terms of relevant information from the nearby text, it is worth noting that the context of the test split is likely related to the development and evaluation of machine learning models. This is inferred from the presence of technical terms such as \"dataset\" and \"model,\" which are commonly used in the field of machine learning. Additionally, the use of phrases such as \"testing purposes\" and \"evaluate the performance\" suggests that the test split is an essential component of the model development process.", "id": "TEST SPLIT", "label": "TEST SPLIT", "shape": "dot", "size": 10, "source_id": "76cc338e586223647fd3dbe4e7a7c131,c5dc13d7191b625e7373e79907b5782a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Pretraining datasets are datasets that are used to pretrain the model", "id": "PRETRAINING DATASETS", "label": "PRETRAINING DATASETS", "shape": "dot", "size": 10, "source_id": "c5dc13d7191b625e7373e79907b5782a", "type": "DATASET"}, {"color": "#97c2fc", "description": "Validation split is a subset of the dataset that is used for validation", "id": "VALIDATION SPLIT", "label": "VALIDATION SPLIT", "shape": "dot", "size": 10, "source_id": "c5dc13d7191b625e7373e79907b5782a", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"UCI\" is a concept related to air quality. It is also a dataset used for pretraining and fine-tuning. The UCI dataset is likely a collection of data related to air quality, which is used in machine learning models to improve their performance. The use of UCI for pretraining and fine-tuning suggests that it is a valuable resource for researchers and developers working on air quality-related projects.\n\nGiven the context of machine learning and time series forecasting, it is possible that the UCI dataset contains time series data related to air quality, such as pollutant concentrations, temperature, or humidity levels. The dataset may be used to develop and evaluate models for long-term series forecasting, frequency analysis, and other tasks related to air quality prediction.\n\nOverall, the UCI entity is a concept and a dataset that plays a significant role in air quality research and machine learning applications.", "id": "UCI", "label": "UCI", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"BEIJING PM2.5\" is a concept related to air quality. It is a dataset used for pretraining and fine-tuning, indicating its significance in machine learning and time series forecasting applications. The dataset likely contains information on particulate matter (PM2.5) levels in Beijing, which is a critical air quality metric. The use of this dataset for pretraining and fine-tuning suggests its potential in developing accurate models for long-term series forecasting and frequency analysis of air quality patterns in Beijing.\n\nGiven the formal and academic language used in the descriptions, it is likely that the information is from a research paper or technical document related to machine learning, time series forecasting, and air quality analysis. The mention of technical terms such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis\" further supports this conclusion.\n\nOverall, the \"BEIJING PM2.5\" dataset is a valuable resource for researchers and practitioners working on air quality analysis, machine learning, and time series forecasting, particularly in the context of Beijing\u0027s air quality challenges.", "id": "BEIJING PM2.5", "label": "BEIJING PM2.5", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"CPU LIMIT MINUTE\" is a concept and dataset related to cloud computing. It is a concept that refers to a specific aspect of cloud computing, and it is also a dataset used for pretraining and fine-tuning purposes. This suggests that \"CPU LIMIT MINUTE\" is a multifaceted entity that has both theoretical and practical applications in the field of cloud computing.\n\nGiven the context of cloud computing and the use of \"CPU LIMIT MINUTE\" as a dataset, it is likely that this concept is related to the management and optimization of computing resources in cloud environments. The fact that it is used for pretraining and fine-tuning suggests that it may be a key component in the development of machine learning models for cloud computing applications.\n\nOverall, \"CPU LIMIT MINUTE\" appears to be an important concept and dataset in the field of cloud computing, with potential applications in the development of machine learning models and the optimization of computing resources.", "id": "CPU LIMIT MINUTE", "label": "CPU LIMIT MINUTE", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"CPU USAGE MINUTE\" is a concept related to cloud computing. It is also a dataset used for pretraining and fine-tuning, suggesting its significance in machine learning and model development. The dataset is likely used to analyze and predict CPU usage patterns over a minute, which is crucial in cloud computing for optimizing resource allocation and performance.\n\nGiven the context of cloud computing and machine learning, it is likely that the dataset \"CPU USAGE MINUTE\" is used to develop and fine-tune models for long-term series forecasting, frequency analysis, and other related tasks. The use of this dataset for pretraining and fine-tuning implies that it is a valuable resource for researchers and practitioners in the field of cloud computing and machine learning.\n\nOverall, the entity \"CPU USAGE MINUTE\" is a critical concept and dataset in cloud computing, with significant implications for machine learning and model development.", "id": "CPU USAGE MINUTE", "label": "CPU USAGE MINUTE", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"MEMORY LIMIT MINUTE\" is a concept and dataset related to cloud computing. It is a concept that refers to a specific aspect of cloud computing, and it is also a dataset used for pretraining and fine-tuning purposes. This suggests that \"MEMORY LIMIT MINUTE\" is a multifaceted entity that has both theoretical and practical applications in the field of cloud computing.\n\nGiven the context of cloud computing, it is likely that \"MEMORY LIMIT MINUTE\" is related to the management and optimization of memory resources in cloud-based systems. The fact that it is used as a dataset for pretraining and fine-tuning suggests that it may be used to develop and improve machine learning models that are capable of optimizing memory usage in cloud environments.\n\nOverall, \"MEMORY LIMIT MINUTE\" appears to be an important concept and dataset in the field of cloud computing, with potential applications in the development of more efficient and effective cloud-based systems.", "id": "MEMORY LIMIT MINUTE", "label": "MEMORY LIMIT MINUTE", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\"MEMORY USAGE MINUTE\" is a concept related to cloud computing, which refers to the measurement of memory usage over a specific time interval, in this case, one minute. This concept is likely used to monitor and optimize memory allocation in cloud-based systems. Additionally, \"MEMORY USAGE MINUTE\" is also a dataset used for pretraining and fine-tuning, suggesting that it is a collection of data points that can be leveraged to train machine learning models, particularly those related to cloud computing and memory usage.\n\nThis summary combines the two descriptions provided, resolving any potential contradictions and providing a clear and concise overview of the \"MEMORY USAGE MINUTE\" concept and its associated dataset.", "id": "MEMORY USAGE MINUTE", "label": "MEMORY USAGE MINUTE", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "type": "DATASET"}, {"color": "#97c2fc", "description": "Function Delay Minute is a concept related to cloud computing", "id": "FUNCTION DELAY MINUTE", "label": "FUNCTION DELAY MINUTE", "shape": "dot", "size": 10, "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"PLATFORM DELAY MINUTE\" is a concept related to cloud computing. It is also a dataset used for pretraining and fine-tuning, suggesting its significance in the development and improvement of machine learning models, particularly in the context of cloud computing. The dataset likely contains time series data, which is a crucial aspect of cloud computing, as it enables the analysis and forecasting of platform delays in real-time. This information is essential for optimizing cloud computing services, ensuring high availability, and minimizing downtime.\n\nGiven the context of cloud computing and the use of the dataset for pretraining and fine-tuning, it is likely that the \"PLATFORM DELAY MINUTE\" dataset is used in the development of long-term series forecasting models, which are critical for predicting and mitigating platform delays. The dataset may also involve frequency analysis and multi-head cross-attention techniques, which are commonly used in time series forecasting and natural language processing tasks.\n\nOverall, the \"PLATFORM DELAY MINUTE\" is a crucial concept and dataset in the field of cloud computing, with significant implications for the development and improvement of machine learning models, particularly in the context of time series forecasting and platform delay analysis.", "id": "PLATFORM DELAY MINUTE", "label": "PLATFORM DELAY MINUTE", "shape": "dot", "size": 10, "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "type": "DATASET"}, {"color": "#97c2fc", "description": "Instances Minute is a concept related to cloud computing", "id": "INSTANCES MINUTE", "label": "INSTANCES MINUTE", "shape": "dot", "size": 10, "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Context length C is a hyperparameter that determines the length of the context window", "id": "CONTEXT LENGTH C \u2020", "label": "CONTEXT LENGTH C \u2020", "shape": "dot", "size": 10, "source_id": "c5dc13d7191b625e7373e79907b5782a", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Augmentation probability is a hyperparameter that determines the probability of data augmentation", "id": "AUGMENTATION PROBABILITY", "label": "AUGMENTATION PROBABILITY", "shape": "dot", "size": 10, "source_id": "c5dc13d7191b625e7373e79907b5782a", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Frequency masking rate is a hyperparameter that determines the rate of frequency masking", "id": "FREQUENCY MASKING RATE", "label": "FREQUENCY MASKING RATE", "shape": "dot", "size": 10, "source_id": "c5dc13d7191b625e7373e79907b5782a", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Frequency mixing rate is a hyperparameter that determines the rate of frequency mixing", "id": "FREQUENCY MIXING RATE", "label": "FREQUENCY MIXING RATE", "shape": "dot", "size": 10, "source_id": "c5dc13d7191b625e7373e79907b5782a", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "Time series foundation model refers to a type of machine learning model designed for time series data", "id": "TIME SERIES FOUNDATION MODEL", "label": "TIME SERIES FOUNDATION MODEL", "shape": "dot", "size": 10, "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "type": "MODEL"}, {"color": "#97c2fc", "description": "Data repositories refer to collections of data used to train and evaluate machine learning models", "id": "DATA REPOSITORIES", "label": "DATA REPOSITORIES", "shape": "dot", "size": 10, "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Laws refer to mathematical or statistical relationships between variables", "id": "LAWS", "label": "LAWS", "shape": "dot", "size": 10, "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Relations refer to the relationships between variables or data points", "id": "RELATIONS", "label": "RELATIONS", "shape": "dot", "size": 10, "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Target refers to the value or outcome being predicted or forecasted", "id": "TARGET", "label": "TARGET", "shape": "dot", "size": 10, "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"BEIJING PM25\" is a specific measure or indicator of air quality. It is also used as a traffic metric for fine-tuning forecasting tasks. This suggests that BEIJING PM25 is a critical component in assessing and predicting air quality in Beijing, which is essential for urban planning, public health, and environmental monitoring.\n\nGiven the context of forecasting tasks, it is likely that BEIJING PM25 is used in conjunction with time series analysis and long-term series forecasting techniques to predict air quality levels in Beijing. The use of traffic metrics as a predictor variable also implies that BEIJING PM25 may be influenced by factors such as traffic volume, road conditions, and vehicle emissions.\n\nOverall, BEIJING PM25 is a key indicator of air quality in Beijing, and its use in forecasting tasks highlights the importance of accurate and reliable air quality monitoring in urban environments.\n\nRelevant information from the nearby text is not provided, but based on the given descriptions, the following information can be inferred:\n\n* BEIJING PM25 is related to air quality and traffic metrics.\n* It is used in forecasting tasks, likely involving time series analysis and long-term series forecasting.\n* The entity is likely influenced by factors such as traffic volume, road conditions, and vehicle emissions.\n\nThese inferences provide a more comprehensive understanding of the entity \"BEIJING PM25\" and its role in air quality monitoring and forecasting.", "id": "BEIJING PM25", "label": "BEIJING PM25", "shape": "dot", "size": 10, "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26,ec7705b83cf4fe3aa18662c917b18c1a", "type": "TRAFFIC"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"MEMORY LIMIT\" can be described as follows:\n\nThe \"MEMORY LIMIT\" refers to a restriction or a maximum amount of memory available for a process, application, or model. It is a limitation on the amount of memory that can be allocated or utilized by a particular entity, such as a process or a machine learning model. This restriction is typically imposed to prevent excessive memory usage, which can lead to performance issues, crashes, or other problems.\n\nIn the context of machine learning, the \"MEMORY LIMIT\" is particularly relevant, as it can impact the performance and accuracy of models. By setting a memory limit, developers can ensure that their models do not consume excessive memory, which can lead to slower training times, reduced accuracy, or even model crashes.\n\nOverall, the \"MEMORY LIMIT\" is an important concept in computer science and machine learning, as it helps to manage memory usage and prevent potential issues related to excessive memory consumption.\n\nRelevant information from the nearby text is not provided, but based on the given descriptions, this summary aims to provide a comprehensive and coherent description of the entity \"MEMORY LIMIT\".", "id": "MEMORY LIMIT", "label": "MEMORY LIMIT", "shape": "dot", "size": 10, "source_id": "990578022879395d00b7a5b229863c2f,ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "LIMIT", "label": "LIMIT", "shape": "dot", "size": 10, "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "type": ""}, {"color": "#97c2fc", "description": "Memory usage refers to the amount of memory used by a model", "id": "MEMORY USAGE", "label": "MEMORY USAGE", "shape": "dot", "size": 10, "source_id": "990578022879395d00b7a5b229863c2f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Platform delay refers to a delay or slowdown in a system or process", "id": "PLATFORM DELAY", "label": "PLATFORM DELAY", "shape": "dot", "size": 10, "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "CPU usage refers to the amount of CPU resources used by a model", "id": "CPU USAGE", "label": "CPU USAGE", "shape": "dot", "size": 10, "source_id": "990578022879395d00b7a5b229863c2f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ETT ML refers to a specific concept or measure", "id": "ETT ML", "label": "ETT ML", "shape": "dot", "size": 10, "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Technical terms refer to specialized vocabulary used in a particular field or industry", "id": "TECHNICAL TERMS", "label": "TECHNICAL TERMS", "shape": "dot", "size": 10, "source_id": "0654926be53a9cf18e4def1c94371576,9f70fa3e2d0ba714627b9ce1a442fd21,c80929fa1aa2a6f9652319844c4ca742", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Mathematical equations are used to describe mathematical relationships and solve problems", "id": "MATHEMATICAL EQUATIONS", "label": "MATHEMATICAL EQUATIONS", "shape": "dot", "size": 10, "source_id": "0654926be53a9cf18e4def1c94371576,9f70fa3e2d0ba714627b9ce1a442fd21,c80929fa1aa2a6f9652319844c4ca742", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Shaojie Bai is an author of one of the referenced papers", "id": "SHAOJIE BAI", "label": "SHAOJIE BAI", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "J Zico Kolter is an author of one of the referenced papers", "id": "J ZICO KOLTER", "label": "J ZICO KOLTER", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Vladlen Koltun is an author of one of the referenced papers", "id": "VLADLEN KOLTUN", "label": "VLADLEN KOLTUN", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "George EP Box is an author of one of the referenced papers", "id": "GEORGE EP BOX", "label": "GEORGE EP BOX", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Gwilym M Jenkins is an author of one of the referenced papers", "id": "GWILYM M JENKINS", "label": "GWILYM M JENKINS", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Gregory C Reinsel is an author of one of the referenced papers", "id": "GREGORY C REINSEL", "label": "GREGORY C REINSEL", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Greta M Ljung is an author of one of the referenced papers", "id": "GWETA M LJUNG", "label": "GWETA M LJUNG", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Tom Brown is an author of one of the referenced papers", "id": "TOM BROWN", "label": "TOM BROWN", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Benjamin Mann is an author of one of the referenced papers", "id": "BENJAMIN MANN", "label": "BENJAMIN MANN", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Nick Ryder is an author of one of the referenced papers", "id": "NICK RYDER", "label": "NICK RYDER", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Melanie Subbiah is an author of one of the referenced papers", "id": "MELANIE SUBBIAH", "label": "MELANIE SUBBIAH", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jared D Kaplan is an author of one of the referenced papers", "id": "JARED D KAPLAN", "label": "JARED D KAPLAN", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Prafulla Dhariwal is an author of one of the referenced papers", "id": "PRAFULLA DHARIWAL", "label": "PRAFULLA DHARIWAL", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Arvind Neelakantan is an author of one of the referenced papers", "id": "ARVIND NEELAKANTAN", "label": "ARVIND NEELAKANTAN", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Pranav Shyam is an author of one of the referenced papers", "id": "PRANAV SHYAM", "label": "PRANAV SHYAM", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Girish Sastry is an author of one of the referenced papers", "id": "GIRISH SASTRY", "label": "GIRISH SASTRY", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Amanda Askell is an author of one of the referenced papers", "id": "AMANDA ASKELL", "label": "AMANDA ASKELL", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Kin G Olivares is an author of one of the referenced papers", "id": "KIN G OLIVARES", "label": "KIN G OLIVARES", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Federico Garza is an author of one of the referenced papers", "id": "FEDERICO GARZA", "label": "FEDERICO GARZA", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Max Mergenthaler is an author of one of the referenced papers", "id": "MAX MERGENTHALER", "label": "MAX MERGENTHALER", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Artur Dubrawski is an author of one of the referenced papers", "id": "ARTUR DUBRAWSKI", "label": "ARTUR DUBRAWSKI", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Ching Chang is an author of one of the referenced papers", "id": "CHING CHANG", "label": "CHING CHANG", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Wen-Chih Peng is an author of one of the referenced papers", "id": "WEN-CHIH PENG", "label": "WEN-CHIH PENG", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Tien-Fu Chen is an author of one of the referenced papers", "id": "TIEN-FU CHEN", "label": "TIEN-FU CHEN", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of Pin-Yu Chen:\n\nPin-Yu Chen is a researcher who has made significant contributions to the field of machine learning and artificial intelligence. As an author of multiple papers, Chen has demonstrated expertise in various areas, including model reprogramming, molecular representation learning, time series classification, and transferability of lottery tickets.\n\nSpecifically, Chen has published papers on reprogramming language models for molecular representation learning, reprogramming acoustic models for time series classification (Voice2series), and reprogramming pretrained language models for antibody sequence infilling. Additionally, Chen has explored the concept of reprogramming under constraints, revisiting efficient and reliable transferability of lottery tickets.\n\nChen\u0027s work in model reprogramming has the potential to revolutionize the way we approach various tasks, from molecular representation learning to time series classification. His research has been recognized through the publication of papers in reputable conferences and journals, showcasing his expertise and dedication to advancing the field of machine learning.\n\nOverall, Pin-Yu Chen is a prominent researcher and author in the field of machine learning, with a focus on model reprogramming and its applications in various domains.", "id": "PIN-YU CHEN", "label": "PIN-YU CHEN", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,8f4724ff6541b8924f0cebe9872ed040,a4bb4cfc468e2f87ad5d8a4b451bcbbf,a73df99fe49b288e1c8751be2008b191,ec3fbfb800fd9bf1d913584fda4ae925", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nHongyan Hao is a researcher and author who has contributed to the field of recommender systems and large language model reasoning graphs. Specifically, Hongyan Hao is an author of the paper titled \"Enhancing recommender systems with large language model reasoning graphs,\" which suggests their expertise in this area. This information is consistent across the provided descriptions, indicating that Hongyan Hao is indeed associated with this paper and the field of recommender systems.\n\nGiven the context of the descriptions, it can be inferred that Hongyan Hao is a researcher who has published academic work in the field of recommender systems, leveraging large language model reasoning graphs to enhance these systems. This summary provides a concise and coherent overview of Hongyan Hao\u0027s role as an author and researcher in this field.", "id": "HONGYAN HAO", "label": "HONGYAN HAO", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nXin Ouyang is an author of a referenced paper and is specifically associated with the paper titled \"Enhancing recommender systems with large language model reasoning graphs.\" This suggests that Xin Ouyang has expertise in the area of recommender systems and large language models, and has contributed to the development of innovative solutions in this field through their research.\n\nGiven the context of the provided descriptions, it appears that Xin Ouyang is a researcher or academic who has published papers in the field of recommender systems and large language models. The fact that their paper is referenced in a technical document or research paper further supports this conclusion.\n\nOverall, Xin Ouyang is a researcher with a focus on recommender systems and large language models, and has made significant contributions to the field through their published work.", "id": "XIN OUYANG", "label": "XIN OUYANG", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nSimeng Wang is a researcher and author who has contributed to the field of recommender systems and large language model reasoning graphs. Specifically, Simeng Wang is an author of the paper titled \"Enhancing recommender systems with large language model reasoning graphs,\" which suggests their expertise in this area. This information is consistent across the provided descriptions, indicating that Simeng Wang is indeed an author of the referenced paper.", "id": "SIMENG WANG", "label": "SIMENG WANG", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nYan Wang is an author of a referenced paper and has also authored the paper \"Enhancing recommender systems with large language model reasoning graphs.\" This suggests that Yan Wang is a researcher or academic in the field of recommender systems and large language models, with expertise in enhancing these systems using reasoning graphs. The fact that Yan Wang\u0027s work is referenced in other papers and has been published in a notable paper indicates a level of recognition and credibility within the academic community.\n\nGiven the context of the provided text, it appears that the descriptions are not contradictory, but rather complementary, providing additional information about Yan Wang\u0027s academic background and research contributions.", "id": "YAN WANG", "label": "YAN WANG", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nYue Shen is a researcher and author who has contributed to the field of recommender systems and large language model reasoning graphs. Specifically, Yue Shen is an author of the paper titled \"Enhancing recommender systems with large language model reasoning graphs,\" which suggests their expertise in this area. This information is consistent across the provided descriptions, indicating that Yue Shen is indeed an author of the mentioned paper.", "id": "YUE SHEN", "label": "YUE SHEN", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nJinjie Gu is a researcher and author who has contributed to the field of recommender systems and large language model reasoning graphs. Specifically, Jinjie Gu is an author of the paper titled \"Enhancing recommender systems with large language model reasoning graphs,\" which suggests their expertise in this area. This information is consistent across the provided descriptions, indicating that Jinjie Gu is indeed an author of the mentioned paper.", "id": "JINJIE GU", "label": "JINJIE GU", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nQing Cui is a researcher and author who has contributed to the field of recommender systems. Specifically, Qing Cui is an author of the paper titled \"Enhancing recommender systems with large language model reasoning graphs.\" This suggests that Qing Cui\u0027s work focuses on improving recommender systems using large language models and reasoning graphs. The fact that Qing Cui is also an author of one of the referenced papers implies that they have a strong background in research and have published their work in academic papers. Overall, Qing Cui appears to be a knowledgeable and experienced researcher in the field of recommender systems and large language models.", "id": "QING CUI", "label": "QING CUI", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Longfei Li is an author of one of the referenced papers", "id": "LONGFEI LI", "label": "LONGFEI LI", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "CPU limit refers to the maximum amount of CPU resources available for a model", "id": "CPU LIMIT", "label": "CPU LIMIT", "shape": "dot", "size": 10, "source_id": "990578022879395d00b7a5b229863c2f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"DATASET CONTEXT\" refers to a concept mentioned in the text, which is a crucial aspect of training a model. It encompasses the information or data used to train the model, providing the necessary background knowledge for the model to learn from. Specifically, in the context of time series analysis, the dataset context refers to the background information provided to the language model about the input time series. This context is essential for the model to understand the underlying patterns, trends, and relationships within the data, enabling it to make accurate predictions and forecasts. The dataset context can include various types of information, such as the frequency analysis, long-term series forecasting, and multi-head cross-attention, which are all relevant to the task of time series forecasting. Overall, the dataset context plays a vital role in the development and training of machine learning models, particularly in the field of time series analysis.", "id": "DATASET CONTEXT", "label": "DATASET CONTEXT", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,e6a6bc6fbd362394320961ac10cdd230", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time series patterns refer to the knowledge and reasoning capabilities to interpret time series data, as mentioned in the text", "id": "TIME SERIES PATTERNS", "label": "TIME SERIES PATTERNS", "shape": "dot", "size": 10, "source_id": "89b79391630ac478085efea89fad5736", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Multimodal models refer to models that can process and integrate data from multiple sources or modalities, such as text, images, or audio", "id": "MULTIMODAL MODELS", "label": "MULTIMODAL MODELS", "shape": "dot", "size": 10, "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "type": "MODEL"}, {"color": "#97c2fc", "description": "IBM Research is a research organization", "id": "IBM RESEARCH", "label": "IBM RESEARCH", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Alibaba Group is a multinational conglomerate", "id": "ALIBABA GROUP", "label": "ALIBABA GROUP", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Monash University is a university", "id": "MONASH UNIVERSITY", "label": "MONASH UNIVERSITY", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Griffith University is a university", "id": "GRIFFITH UNIVERSITY", "label": "GRIFFITH UNIVERSITY", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "The Hong Kong University of Science and Technology (Guangzhou) is a university", "id": "THE HONG KONG UNIVERSITY OF SCIENCE AND TECHNOLOGY (GUANGZHOU)", "label": "THE HONG KONG UNIVERSITY OF SCIENCE AND TECHNOLOGY (GUANGZHOU)", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Ant Group is a financial technology company", "id": "ANT GROUP", "label": "ANT GROUP", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Shiyu Wang is a researcher", "id": "SHIYU WANG", "label": "SHIYU WANG", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "PERSON"}, {"color": "#97c2fc", "description": "Lintao Ma is a researcher", "id": "LINTAO MA", "label": "LINTAO MA", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "PERSON"}, {"color": "#97c2fc", "description": "Yuan-Fang Li is a researcher", "id": "YUAN-FANG LI", "label": "YUAN-FANG LI", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "PERSON"}, {"color": "#97c2fc", "description": "James Y. Zhang is a researcher", "id": "JAMES Y. ZHANG", "label": "JAMES Y. ZHANG", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "PERSON"}, {"color": "#97c2fc", "description": "Xiaoming Shi is a researcher", "id": "XIAOMING SHI", "label": "XIAOMING SHI", "shape": "dot", "size": 10, "source_id": "8f4724ff6541b8924f0cebe9872ed040", "type": "PERSON"}, {"color": "#97c2fc", "description": "Pre-trained recommender systems are a type of system that uses pre-trained models to make recommendations", "id": "PRE-TRAINED RECOMMENDER SYSTEMS", "label": "PRE-TRAINED RECOMMENDER SYSTEMS", "shape": "dot", "size": 10, "source_id": "a73df99fe49b288e1c8751be2008b191", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Yunyi Zhou is an author who has published research papers on time series forecasting", "id": "YUNYI ZHOU", "label": "YUNYI ZHOU", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Yijia Ruan is an author who has published research papers on time series forecasting", "id": "YIJIA RUAN", "label": "YIJIA RUAN", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Ria Vinod is an author of the paper \"Reprogramming language models for molecular representation learning\"", "id": "RIA VINOD", "label": "RIA VINOD", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nPayel Das is a researcher and author who has contributed to multiple academic papers. Specifically, she is an author of two papers: \"Reprogramming language models for molecular representation learning\" and \"Reprogramming pretrained language models for antibody sequence infilling.\" These papers suggest that Payel Das has expertise in the areas of language models, molecular representation learning, and antibody sequence infilling. Her work likely involves the application of machine learning and artificial intelligence techniques to understand and represent complex biological systems.\n\nGiven the context of the provided descriptions, it appears that Payel Das is a researcher in the field of artificial intelligence and machine learning, with a focus on its applications in molecular biology and bioinformatics. Her work may involve the development of new methods and techniques for representing and analyzing biological data using language models and other machine learning approaches.\n\nOverall, Payel Das is a researcher with expertise in the intersection of artificial intelligence, machine learning, and molecular biology, and her work has been published in academic papers on these topics.", "id": "PAYEL DAS", "label": "PAYEL DAS", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "LLMs have the potential for generalizable forecasting across domains without requiring per-task retraining from scratch", "id": "GENERALIZABILITY", "label": "GENERALIZABILITY", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PROPERTY"}, {"color": "#97c2fc", "description": "LLMs have shown the ability to perform new tasks with only a few examples, enabling data efficiency", "id": "DATA EFFICIENCY", "label": "DATA EFFICIENCY", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PROPERTY"}, {"color": "#97c2fc", "description": "LLMs exhibit sophisticated reasoning and pattern recognition capabilities", "id": "REASONING", "label": "REASONING", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PROPERTY"}, {"color": "#97c2fc", "description": "LLMs are trained once on massive computing and then can be applied to forecasting tasks without learning from scratch", "id": "EASY OPTIMIZATION", "label": "EASY OPTIMIZATION", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PROPERTY"}, {"color": "#97c2fc", "description": "Brown et al. (2020) is a paper that discusses foundation language models", "id": "BROWN ET AL. (2020)", "label": "BROWN ET AL. (2020)", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PAPER"}, {"color": "#97c2fc", "description": "OpenAI (2023) is a paper that discusses GPT-4", "id": "OPENAI (2023)", "label": "OPENAI (2023)", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PAPER"}, {"color": "#97c2fc", "description": "Mirchandani et al. (2023) is a paper that discusses reasoning and pattern recognition capabilities", "id": "MIRCHANDANI ET AL. (2023)", "label": "MIRCHANDANI ET AL. (2023)", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PAPER"}, {"color": "#97c2fc", "description": "Wang et al. (2023) is a paper that discusses reasoning and pattern recognition capabilities", "id": "WANG ET AL. (2023)", "label": "WANG ET AL. (2023)", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PAPER"}, {"color": "#97c2fc", "description": "Chu et al. (2023) is a paper that discusses reasoning and pattern recognition capabilities", "id": "CHU ET AL. (2023)", "label": "CHU ET AL. (2023)", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PAPER"}, {"color": "#97c2fc", "description": "Ma et al. (2023) is a paper that discusses multimodal knowledge", "id": "MA ET AL. (2023)", "label": "MA ET AL. (2023)", "shape": "dot", "size": 10, "source_id": "b67d18d306fde251ee94b0a831d1e075", "type": "PAPER"}, {"color": "#97c2fc", "description": "Declarative prompts are a concept mentioned in the text, as indicated by the use of the phrase \"declarative prompts\"", "id": "DECLARATIVE PROMPTS", "label": "DECLARATIVE PROMPTS", "shape": "dot", "size": 10, "source_id": "b27c89cb0db6646b1203b2701e017aeb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Domain expert knowledge is a concept mentioned in the text, as indicated by the use of the phrase \"domain expert knowledge\"", "id": "DOMAIN EXPERT KNOWLEDGE", "label": "DOMAIN EXPERT KNOWLEDGE", "shape": "dot", "size": 10, "source_id": "b27c89cb0db6646b1203b2701e017aeb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Input context refers to the information provided to the LLM to facilitate the task", "id": "INPUT CONTEXT", "label": "INPUT CONTEXT", "shape": "dot", "size": 10, "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entity \"MULTIVARIATE TIME SERIES\" refers to a set of time series data that are related to each other. This type of data consists of multiple related time series, which are often sourced from various data sources. In essence, multivariate time series data is a collection of time series data that are interconnected and interdependent, requiring a comprehensive approach to analysis and forecasting.\n\nThis summary is derived from the provided descriptions, which collectively paint a picture of multivariate time series as a complex and multifaceted concept. The descriptions highlight the key characteristics of multivariate time series, including their related nature, multiple data sources, and the need for a holistic approach to analysis.\n\nThe language used in the descriptions is formal and technical, suggesting that the context is academic or research-oriented. The use of terms such as \"multivariate time series,\" \"time series data,\" and \"data sources\" further reinforces this interpretation.\n\nOverall, the summary provides a clear and concise understanding of the entity \"MULTIVARIATE TIME SERIES,\" highlighting its key characteristics and the complexities involved in analyzing and forecasting this type of data.", "id": "MULTIVARIATE TIME SERIES", "label": "MULTIVARIATE TIME SERIES", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9,3174231a67593609c727151c9df31d0a,9c6e74299923071f9fc2b7cce49efc90", "type": "DATA"}, {"color": "#97c2fc", "description": "Univariate time series data refers to time series data with a single data source", "id": "UNIVARIATE TIME SERIES", "label": "UNIVARIATE TIME SERIES", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "DATA"}, {"color": "#97c2fc", "description": "A pre-trained and frozen LLM is a component of the model that uses a pre-trained language model", "id": "PRE-TRAINED AND FROZEN LLM", "label": "PRE-TRAINED AND FROZEN LLM", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a", "type": "COMPONENT"}, {"color": "#97c2fc", "description": "Statistical context is a concept mentioned in the text, referring to the statistical information or data used to train a model", "id": "STATISTICAL CONTEXT", "label": "STATISTICAL CONTEXT", "shape": "dot", "size": 10, "source_id": "e6a6bc6fbd362394320961ac10cdd230", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Source data representation space refers to the space in which the source data is represented", "id": "SOURCE DATA REPRESENTATION SPACE", "label": "SOURCE DATA REPRESENTATION SPACE", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Target modalities refer to the modalities of time series and natural language that are being aligned", "id": "TARGET MODALITIES", "label": "TARGET MODALITIES", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Update refers to the process of adjusting the parameters of the model during training", "id": "UPDATE", "label": "UPDATE", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Fine-tune refers to the process of adjusting the parameters of a pre-trained model during training", "id": "FINE-TUNE", "label": "FINE-TUNE", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Optimized refers to the process of adjusting the parameters of the model to improve its performance", "id": "OPTIMIZED", "label": "OPTIMIZED", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Domain-specific models refer to models that are trained on a specific domain or task", "id": "DOMAIN-SPECIFIC MODELS", "label": "DOMAIN-SPECIFIC MODELS", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "MODEL"}, {"color": "#97c2fc", "description": "Resource constraints refer to the limitations on computational resources, memory, and other resources", "id": "RESOURCE CONSTRAINTS", "label": "RESOURCE CONSTRAINTS", "shape": "dot", "size": 10, "source_id": "fececbac281c1e2b13921f378df30919", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "RV is a space used to represent vocabulary", "id": "RV", "label": "RV", "shape": "dot", "size": 10, "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "type": "SPACE"}, {"color": "#97c2fc", "description": "D\u02c6(i) is a space used to represent pre-trained word embeddings", "id": "D\u02c6(I)", "label": "D\u02c6(I)", "shape": "dot", "size": 10, "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "type": "SPACE"}, {"color": "#97c2fc", "description": "V is the vocabulary size", "id": "V", "label": "V", "shape": "dot", "size": 10, "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "type": "SIZE"}, {"color": "#97c2fc", "description": "V \u2032 is a smaller vocabulary size", "id": "V \u2032", "label": "V \u2032", "shape": "dot", "size": 10, "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "type": "SIZE"}, {"color": "#97c2fc", "description": "RV \u2032 is a space used to represent text prototypes", "id": "RV \u2032", "label": "RV \u2032", "shape": "dot", "size": 10, "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "type": "SPACE"}, {"color": "#97c2fc", "description": "E is a model used to represent pre-trained word embeddings", "id": "E", "label": "E", "shape": "dot", "size": 10, "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "type": "MODEL"}, {"color": "#97c2fc", "description": "E\u2032 is a model used to represent text prototypes", "id": "E\u2032", "label": "E\u2032", "shape": "dot", "size": 10, "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "type": "MODEL"}, {"color": "#97c2fc", "description": "D is a space used to represent text prototypes", "id": "D", "label": "D", "shape": "dot", "size": 10, "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "type": "SPACE"}, {"color": "#97c2fc", "description": "Wk Q is a matrix used in multi-head cross-attention", "id": "WK Q", "label": "WK Q", "shape": "dot", "size": 10, "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "type": "MATRIX"}, {"color": "#97c2fc", "description": "Wk K is a matrix used in multi-head cross-attention", "id": "WK K", "label": "WK K", "shape": "dot", "size": 10, "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "type": "MATRIX"}, {"color": "#97c2fc", "description": "Wk V is a matrix used in multi-head cross-attention", "id": "WK V", "label": "WK V", "shape": "dot", "size": 10, "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "type": "MATRIX"}, {"color": "#97c2fc", "description": "Electricity consumption is a concept mentioned in the text, as indicated by the use of the phrase \"electricity consumption usually peaks at noon\"", "id": "ELECTRICITY CONSUMPTION", "label": "ELECTRICITY CONSUMPTION", "shape": "dot", "size": 10, "source_id": "509b431231e669be373f593b31412eed", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Transformer load is a concept mentioned in the text, as indicated by the use of the phrase \"significant increase in transformer load\"", "id": "TRANSFORMER LOAD", "label": "TRANSFORMER LOAD", "shape": "dot", "size": 10, "source_id": "509b431231e669be373f593b31412eed", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Electricity transformer temperature is a concept mentioned in the text, as indicated by the use of the phrase \"Electricity Transformer Temperature (ETT)\"", "id": "ELECTRICITY TRANSFORMER TEMPERATURE (ETT)", "label": "ELECTRICITY TRANSFORMER TEMPERATURE (ETT)", "shape": "dot", "size": 10, "source_id": "509b431231e669be373f593b31412eed", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Prompting is a concept mentioned in the text, as indicated by the use of the phrase \"Prompting serves as a straightforward yet effective approach\"", "id": "PROMPTING", "label": "PROMPTING", "shape": "dot", "size": 10, "source_id": "509b431231e669be373f593b31412eed", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TIME SERIES PATCHES\" is a concept mentioned in the text, which refers to the input time series data used in the TIME-LLM model. Time series patches are related to time series forecasting, as they are used in this context. Additionally, time series patches are connected to Llama-7B, a model used in time series patches, and patch reprogramming, a technique employed to modify or update time series patches.\n\nThe use of time series patches is indicated by the phrase \"reprogram time series patches,\" suggesting that they are a crucial component in the TIME-LLM model. Furthermore, the fact that time series patches are used in time series forecasting implies that they are essential for predicting future values in a time series.\n\nOverall, time series patches are a key concept in the TIME-LLM model, serving as input data for time series forecasting and being related to Llama-7B and patch reprogramming.", "id": "TIME SERIES PATCHES", "label": "TIME SERIES PATCHES", "shape": "dot", "size": 10, "source_id": "509b431231e669be373f593b31412eed,56806630593fdf0c3d95ad7555080dcf,e57d44d23f5a3a82ce9a9b9532d31cbe", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TASK INSTRUCTION\" refers to a concept mentioned in the text, which provides information or guidance to a model. Specifically, task instruction is related to the guidance provided to a language model for the transformation of patch embeddings for specific tasks. This implies that task instruction plays a crucial role in enabling language models to adapt to various tasks and perform transformations on patch embeddings accordingly.\n\nIn the context of machine learning and language models, task instruction is essential for guiding the model\u0027s behavior and enabling it to perform specific tasks. The transformation of patch embeddings is a critical step in this process, as it allows the model to understand and process the input data in a way that is relevant to the task at hand.\n\nOverall, the entity \"TASK INSTRUCTION\" is a key concept in the field of machine learning and natural language processing, and its role in guiding language models and transforming patch embeddings is a critical aspect of its functionality.", "id": "TASK INSTRUCTION", "label": "TASK INSTRUCTION", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,e6a6bc6fbd362394320961ac10cdd230", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Input statistics refer to the additional crucial statistics provided to the language model to facilitate pattern recognition and reasoning", "id": "INPUT STATISTICS", "label": "INPUT STATISTICS", "shape": "dot", "size": 10, "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ET T m2 \u2192 ET T m1 is a concept that refers to a transformation or conversion between ET T m2 and ET T m1", "id": "ET T M2 \u2192 ET T M1", "label": "ET T M2 \u2192 ET T M1", "shape": "dot", "size": 10, "source_id": "b90805909f7b1f31ddc03014f161d3ba", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "SOTA model refers to the state-of-the-art model in forecasting tasks", "id": "SOTA MODEL", "label": "SOTA MODEL", "shape": "dot", "size": 10, "source_id": "6d66c2ea37e25b646a13d751c05a8e4d", "type": "MODEL"}, {"color": "#97c2fc", "description": "Recent SOTA models refer to the state-of-the-art models in forecasting tasks", "id": "RECENT SOTA MODELS", "label": "RECENT SOTA MODELS", "shape": "dot", "size": 10, "source_id": "6d66c2ea37e25b646a13d751c05a8e4d", "type": "MODEL"}, {"color": "#97c2fc", "description": "Reformer (2020) is a model mentioned in the text, as indicated by the use of the name \"Reformer (2020)\"", "id": "REFORMER (2020)", "label": "REFORMER (2020)", "shape": "dot", "size": 10, "source_id": "1d6fb60c5060c25ae4791b03a0513a7f", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "INFORMER (2021)", "label": "INFORMER (2021)", "shape": "dot", "size": 10, "source_id": "1d6fb60c5060c25ae4791b03a0513a7f", "type": ""}, {"color": "#97c2fc", "description": "MSE reduction is a concept that refers to the decrease in mean squared error between the predicted and actual values", "id": "MSE REDUCTION", "label": "MSE REDUCTION", "shape": "dot", "size": 10, "source_id": "7e03744dea80e2138baff03611104fa8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Computational footprint refers to the amount of resources or energy required to perform a computation or task", "id": "COMPUTATIONAL FOOTPRINT", "label": "COMPUTATIONAL FOOTPRINT", "shape": "dot", "size": 10, "source_id": "1902e651467179a9a1d4c4df0035e980", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "LLAMA (32) is a variant of the LLAMA model with 32 parameters", "id": "LLAMA (32)", "label": "LLAMA (32)", "shape": "dot", "size": 10, "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "type": "MODEL"}, {"color": "#97c2fc", "description": "LLAMA (8) is a variant of the LLAMA model with 8 parameters", "id": "LLAMA (8)", "label": "LLAMA (8)", "shape": "dot", "size": 10, "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "type": "MODEL"}, {"color": "#97c2fc", "description": "LLAMA (w/o LLM) is a variant of the LLAMA model without LLM", "id": "LLAMA (W/O LLM)", "label": "LLAMA (W/O LLM)", "shape": "dot", "size": 10, "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "type": "MODEL"}, {"color": "#97c2fc", "description": "Tim Dettmers is an author who has published a paper on efficient finetuning of quantized llms", "id": "TIM DETTMERS", "label": "TIM DETTMERS", "shape": "dot", "size": 10, "source_id": "a73df99fe49b288e1c8751be2008b191", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Siqiao Xue is an author of the paper \"Enhancing recommender systems with large language model reasoning graphs\"", "id": "SIQIAO XUE", "label": "SIQIAO XUE", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "James Y Zhang is an author of the paper \"Enhancing recommender systems with large language model reasoning graphs\"", "id": "JAMES Y ZHANG", "label": "JAMES Y ZHANG", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Cross-modality adaptation refers to the process of adapting a model from one domain to another", "id": "CROSS-MODALITY ADAPTATION", "label": "CROSS-MODALITY ADAPTATION", "shape": "dot", "size": 10, "source_id": "f7266cfccedb1d9840d10afa689a05e9", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Shohreh Deldari is an author who has published a paper on self-supervised representation learning on multimodal and temporal data", "id": "SHOHREH DELDARI", "label": "SHOHREH DELDARI", "shape": "dot", "size": 10, "source_id": "a73df99fe49b288e1c8751be2008b191", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Self-supervised representation learning is a technique for learning representations of data without labeled supervision", "id": "SELF-SUPERVISED REPRESENTATION LEARNING", "label": "SELF-SUPERVISED REPRESENTATION LEARNING", "shape": "dot", "size": 10, "source_id": "a73df99fe49b288e1c8751be2008b191", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Nate Gruver is an author who has published a paper on large language models as zero-shot time series forecasters", "id": "NATE GRUVER", "label": "NATE GRUVER", "shape": "dot", "size": 10, "source_id": "a73df99fe49b288e1c8751be2008b191", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Zero-shot time series forecasters are models that can forecast time series data without any training data", "id": "ZERO-SHOT TIME SERIES FORECASTERS", "label": "ZERO-SHOT TIME SERIES FORECASTERS", "shape": "dot", "size": 10, "source_id": "a73df99fe49b288e1c8751be2008b191", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nSepp Hochreiter is a researcher and author who has published a paper on Long Short-Term Memory (LSTM), a type of recurrent neural network architecture. The paper, which is likely a technical document or research paper, is written in English and contains technical terms, mathematical equations, and references to academic papers. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nThe paper on LSTM is likely related to the field of artificial intelligence, machine learning, or deep learning, given the mention of recurrent neural networks and long-term series forecasting. The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports the conclusion that the text is written in English and is likely from an academic conference or journal.\n\nOverall, Sepp Hochreiter is a researcher who has made significant contributions to the field of artificial intelligence and machine learning through his work on LSTM, and his paper is a valuable resource for researchers and practitioners in the field.", "id": "SEPP HOCHREITER", "label": "SEPP HOCHREITER", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b,a73df99fe49b288e1c8751be2008b191", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "J\u00a8urgen Schmidhuber is an author of the paper on Long short-term memory", "id": "J\u00a8URGEN SCHMIDHUBER", "label": "J\u00a8URGEN SCHMIDHUBER", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Taesung Kim is an author of the paper on Reversible instance normalization for accurate time-series forecasting against distribution shift", "id": "TAESUNG KIM", "label": "TAESUNG KIM", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jinhee Kim is an author of the paper on Reversible instance normalization for accurate time-series forecasting against distribution shift", "id": "JINHEE KIM", "label": "JINHEE KIM", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Yunwon Tae is an author of the paper on Reversible instance normalization for accurate time-series forecasting against distribution shift", "id": "YUNWON TAE", "label": "YUNWON TAE", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Cheonbok Park is an author of the paper on Reversible instance normalization for accurate time-series forecasting against distribution shift", "id": "CHEONBOK PARK", "label": "CHEONBOK PARK", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jang-Ho Choi is an author of the paper on Reversible instance normalization for accurate time-series forecasting against distribution shift", "id": "JANG-HO CHOI", "label": "JANG-HO CHOI", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jaegul Choo is an author of the paper on Reversible instance normalization for accurate time-series forecasting against distribution shift", "id": "JAEGUL CHOO", "label": "JAEGUL CHOO", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Diederik P Kingma is an author of the paper on Adam: A method for stochastic optimization", "id": "DIEDERIK P KINGMA", "label": "DIEDERIK P KINGMA", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jimmy Ba is an author of the paper on Adam: A method for stochastic optimization", "id": "JIMMY BA", "label": "JIMMY BA", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Nikita Kitaev is an author of the paper on Reformer: The efficient transformer", "id": "NIKITA KITAEV", "label": "NIKITA KITAEV", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n\u0141ukasz Kaiser is a researcher and author who has contributed to several significant papers in the field of artificial intelligence and machine learning. Specifically, he is the author of two notable papers: \"Attention is all you need\" and \"Reformer: The efficient transformer.\" These papers demonstrate his expertise in transformer-based models and their applications in natural language processing and other areas. As an author of influential research papers, \u0141ukasz Kaiser plays a crucial role in advancing the field of machine learning and its related technologies.\n\nNote: The entity name \"\\u0141UKASZ KAISER\" is a Polish name, but the descriptions provided suggest that \u0141ukasz Kaiser is a researcher in the field of artificial intelligence and machine learning, which is a global and English-dominated field.", "id": "\u0141UKASZ KAISER", "label": "\u0141UKASZ KAISER", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Anselm Levskaya is an author of the paper on Reformer: The efficient transformer", "id": "ANSLEM LEVSKAYA", "label": "ANSLEM LEVSKAYA", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Aidan N Gomez is an author of the paper \"Attention is all you need\"", "id": "AIDAN N GOMEZ", "label": "AIDAN N GOMEZ", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Illia Polosukhin is an author of the paper \"Attention is all you need\"", "id": "ILLIA POLOSUKHIN", "label": "ILLIA POLOSUKHIN", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Takeshi Kojima is an author of the paper on Large language models are zero-shot reasoners", "id": "TAKEISHI KOJIMA", "label": "TAKEISHI KOJIMA", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Shixiang Shane Gu is an author of the paper on Large language models are zero-shot reasoners", "id": "SHIXIANG SHANE GU", "label": "SHIXIANG SHANE GU", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Machel Reid is an author of the paper on Large language models are zero-shot reasoners", "id": "MACHEL REID", "label": "MACHEL REID", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Yutaka Matsuo is an author of the paper on Large language models are zero-shot reasoners", "id": "YUTAKA MATSUO", "label": "YUTAKA MATSUO", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Yusuke Iwasawa is an author of the paper on Large language models are zero-shot reasoners", "id": "YUSUKE IWASAWA", "label": "YUSUKE IWASAWA", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Michael Leonard is an author of the paper on Promotional analysis and forecasting for demand planning: a practical time series approach", "id": "MICHAEL LEONARD", "label": "MICHAEL LEONARD", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Na Li is an author of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "id": "NA LI", "label": "NA LI", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Donald M Arnold is an author of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "id": "DONALD M ARNOLD", "label": "DONALD M ARNOLD", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Douglas G Down is an author of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "id": "DOUGLAS G DOWN", "label": "DOUGLAS G DOWN", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Rebecca Barty is an author of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "id": "REBECCA BARTY", "label": "REBECCA BARTY", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "John Blake is an author of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "id": "JOHN BLAKE", "label": "JOHN BLAKE", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Fei Chiang is an author of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "id": "FEI CHIANG", "label": "FEI CHIANG", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Tom Courtney is an author of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "id": "TOM COURTNEY", "label": "TOM COURTNEY", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Marianne Waito is an author of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "id": "MARIANNE WAITO", "label": "MARIANNE WAITO", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nRick Trifunov is a researcher and author who has contributed to the field of machine learning and inventory optimization. Specifically, he is the author of a paper titled \"From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization.\" This paper likely explores the application of machine learning and statistical modeling techniques to improve demand forecasting and inventory management for red blood cells. As an author of this paper, Rick Trifunov has demonstrated expertise in integrating machine learning, statistical modeling, and inventory optimization to address real-world problems in healthcare logistics.\n\nThe paper\u0027s focus on integrating machine learning, statistical modeling, and inventory optimization suggests that Rick Trifunov has a strong background in both machine learning and operations research. His work in this area has the potential to improve the efficiency and effectiveness of inventory management systems, particularly in the context of healthcare logistics.\n\nOverall, Rick Trifunov\u0027s research and expertise in machine learning and inventory optimization make him a valuable contributor to the field of operations research and healthcare logistics.", "id": "RICK TRIFUNOV", "label": "RICK TRIFUNOV", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nNANCY M HEDDLE is a researcher and author who has contributed to the field of machine learning and inventory optimization. Specifically, she is an author of a paper titled \"From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization.\" This paper likely explores the application of machine learning and statistical modeling techniques to improve demand forecasting and inventory management for red blood cells. The work is likely relevant to the healthcare industry, particularly in the context of blood supply chain management.\n\nThe paper may involve the use of time series analysis, frequency analysis, and multi-head cross-attention techniques, as these are common methods used in machine learning and statistical modeling. The research may also draw on the work of other authors and conferences, such as ICLR, AAAI, and PMLR, which are prominent in the field of artificial intelligence and machine learning.\n\nOverall, NANCY M HEDDLE\u0027s work in this area demonstrates her expertise in integrating machine learning, statistical modeling, and inventory optimization to improve decision-making in complex systems, such as the blood supply chain.", "id": "NANCY M HEDDLE", "label": "NANCY M HEDDLE", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nHengbo Liu is a researcher and author who has contributed to the field of time series forecasting. Specifically, he is the author of two papers: \"Sadi: A self-adaptive decomposed interpretable framework for electric load forecasting under extreme events\" and \"Sadi: A self-adaptive decomposed interpretable framework for time series forecasting.\" These papers suggest that Hengbo Liu\u0027s work focuses on developing interpretable and adaptive frameworks for time series forecasting, particularly in the context of extreme events and electric load forecasting. His research appears to be grounded in the use of advanced machine learning techniques, such as multi-head cross-attention, and frequency analysis, as indicated by the presence of technical terms and mathematical equations in the descriptions. Overall, Hengbo Liu\u0027s work has the potential to contribute significantly to the field of time series forecasting and its applications in various domains.", "id": "HENGBO LIU", "label": "HENGBO LIU", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Xin Liu is an author of the paper \"Large language models are few-shot health learners\"", "id": "XIN LIU", "label": "XIN LIU", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Daniel McDuff is an author of the paper \"Large language models are few-shot health learners\"", "id": "DANIEL MCDUFF", "label": "DANIEL MCDUFF", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nYong Liu is a researcher and author who has contributed to several papers in the field of time series analysis and forecasting. Specifically, he is an author of two notable papers: \"Non-stationary transformers: Exploring the stationarity in time series forecasting\" and \"Timesnet: Temporal 2d-variation modeling for general time series analysis\". These papers demonstrate his expertise in developing innovative methods for time series analysis, including the use of transformers and temporal 2D-variation modeling. His work has likely had a significant impact on the field of time series forecasting, and his contributions are a testament to his expertise and dedication to research in this area.", "id": "YONG LIU", "label": "YONG LIU", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of Haixu Wu:\n\nHaixu Wu is a researcher and author who has contributed to several papers in the field of time series forecasting. Specifically, he is an author of the papers \"Non-stationary transformers: Exploring the stationarity in time series forecasting\", \"Timesnet: Temporal 2d-variation modeling for general time series analysis\", and \"Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting\". These papers demonstrate his expertise in time series analysis and forecasting, particularly in the areas of non-stationary transformers, temporal 2D-variation modeling, and decomposition transformers with auto-correlation.\n\nThe papers suggest that Haixu Wu has a strong background in machine learning and deep learning, as well as a deep understanding of time series data and its complexities. His work has likely focused on developing new methods and techniques for time series forecasting, including the use of transformers and other deep learning architectures.\n\nOverall, Haixu Wu appears to be a prominent researcher in the field of time series forecasting, with a strong publication record and a focus on developing innovative methods for time series analysis and forecasting.\n\nRelevant information from the nearby text:\n\n* The primary language of the text is English, which suggests that Haixu Wu\u0027s papers are written in English and are likely to be published in international conferences and journals.\n* The text contains technical terms, mathematical equations, and references to academic papers, which are all written in English and suggest a formal and academic tone.\n* The use of English-language abbreviations such as \"ICLR\", \"AAAI\", and \"PMLR\" further supports the conclusion that the text is written in English.\n\nNote: The contradictions in the description list are resolved by providing a comprehensive summary that includes all the relevant information from the descriptions.", "id": "HAIXU WU", "label": "HAIXU WU", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Ziyang Ma is an author of the paper \"Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition\"", "id": "ZIYANG MA", "label": "ZIYANG MA", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Wen Wu is an author of the paper \"Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition\"", "id": "WEN WU", "label": "WEN WU", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Spyros Makridakis is an author of the paper \"The m4 competition: Results, findings, conclusion and way forward\"Spyros Makridakis is an author of the paper \"The m3-competition: results, conclusions and implications\"", "entity_type": "AUTHOR", "id": "SPYROS MAKRIDAKIS", "label": "SPYROS MAKRIDAKIS", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Michele Hibon is an author of the paper \"The m3-competition: results, conclusions and implications\"", "id": "MICHELE HIBON", "label": "MICHELE HIBON", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Evangelos Spiliotis is an author of the paper \"The m4 competition: Results, findings, conclusion and way forward\"", "id": "EVANGELOS SPILIOTIS", "label": "EVANGELOS SPILIOTIS", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Igor Melnyk is an author of the paper \"Reprogramming pretrained language models for antibody sequence infilling\"", "id": "IGOR MELNYK", "label": "IGOR MELNYK", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Vijil Chenthamarakshan is an author of the paper \"Reprogramming pretrained language models for antibody sequence infilling\"", "id": "VIJIL CHENTHAMARAKSHAN", "label": "VIJIL CHENTHAMARAKSHAN", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Suvir Mirchandani is an author of the paper \"Large language models as general pattern machines\"", "id": "SUVIR MIRCHANDANI", "label": "SUVIR MIRCHANDANI", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Fei Xia is an author of the paper \"Large language models as general pattern machines\"", "id": "FEI XIA", "label": "FEI XIA", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Pete Florence is an author of the paper \"Large language models as general pattern machines\"", "id": "PETE FLORENCE", "label": "PETE FLORENCE", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "", "id": "XIA", "label": "XIA", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": ""}, {"color": "#97c2fc", "description": "Danny Driess is an author of the paper \"Large language models as general pattern machines\"", "id": "DANNY DRIESS", "label": "DANNY DRIESS", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Montserrat Gonzalez Arenas is an author of the paper \"Large language models as general pattern machines\"", "id": "MONTSERRAT GONZALEZ ARENAS", "label": "MONTSERRAT GONZALEZ ARENAS", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Kanishka Rao is an author of the paper \"Large language models as general pattern machines\"", "id": "KANISHKA RAO", "label": "KANISHKA RAO", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Andy Zeng is an author of the paper \"Large language models as general pattern machines\"", "id": "ANDY ZENG", "label": "ANDY ZENG", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Diganta Misra is an author of the paper \"Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets\"", "id": "DIGANTA MISRA", "label": "DIGANTA MISRA", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Agam Goyal is an author of the paper \"Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets\"", "id": "AGAM GOYAL", "label": "AGAM GOYAL", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Bharat Runwal is an author of the paper \"Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets\"", "id": "BHARAT RUNWAL", "label": "BHARAT RUNWAL", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Pin Yu Chen is an author of the paper \"Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets\"", "id": "PIN YU CHEN", "label": "PIN YU CHEN", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Dorsa Sadigh is an author of the paper \"Large language models as general pattern machines\"", "id": "DORSA SADIGH", "label": "DORSA SADIGH", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Adam Paszke is an author of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "id": "ADAM PASZKE", "label": "ADAM PASZKE", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Sam Gross is an author of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "id": "SAM GROSS", "label": "SAM GROSS", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Francisco Massa is an author of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "id": "FRANCISCO MASSA", "label": "FRANCISCO MASSA", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Adam Lerer is an author of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "id": "ADAM LERER", "label": "ADAM LERER", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "James Bradbury is an author of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "id": "JAMES BRADBURY", "label": "JAMES BRADBURY", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Gregory Chanan is an author of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "id": "GREGORY CHANAN", "label": "GREGORY CHANAN", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Trevor Killeen is an author of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "id": "TREVOR KILLEEN", "label": "TREVOR KILLEEN", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Zeming Lin is an author of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "id": "ZEMING LIN", "label": "ZEMING LIN", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Natalia Gimelshein is an author of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "id": "NATALIA GIMELSHEIN", "label": "NATALIA GIMELSHEIN", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Luca Antiga is an author of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "id": "LUCA ANTIGA", "label": "LUCA ANTIGA", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Alec Radford is an author of the paper \"Language models are unsupervised multitask learners\"", "id": "ALEC RADFORD", "label": "ALEC RADFORD", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jeffrey Wu is an author of the paper \"Language models are unsupervised multitask learners\"", "id": "JEFFREY WU", "label": "JEFFREY WU", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Rewon Child is an author of the paper \"Language models are unsupervised multitask learners\"", "id": "REWON CHILD", "label": "REWON CHILD", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Naman Goyal is an author of the paper \"Llama: Open and efficient foundation language models\"", "id": "NAMAN GOYAL", "label": "NAMAN GOYAL", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Eric Hambro is an author of the paper \"Llama: Open and efficient foundation language models\"", "id": "ERIC HAMBRO", "label": "ERIC HAMBRO", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Faisal Azhar is an author of the paper \"Llama: Open and efficient foundation language models\"", "id": "FAISAL AZHAR", "label": "FAISAL AZHAR", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Maria Tsimpoukelli is an author of the paper \"Multimodal few-shot learning with frozen language models\"", "id": "MARIA TSIMPOUKELLI", "label": "MARIA TSIMPOUKELLI", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jacob L Menick is an author of the paper \"Multimodal few-shot learning with frozen language models\"", "id": "JACOB L MENICK", "label": "JACOB L MENICK", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Serkan Cabi is an author of the paper \"Multimodal few-shot learning with frozen language models\"", "id": "SERKAN CABI", "label": "SERKAN CABI", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "SM Eslami is an author of the paper \"Multimodal few-shot learning with frozen language models\"", "id": "SM ESLAMI", "label": "SM ESLAMI", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Oriol Vinyals is an author of the paper \"Multimodal few-shot learning with frozen language models\"", "id": "ORIOL VINYALS", "label": "ORIOL VINYALS", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Felix Hill is an author of the paper \"Multimodal few-shot learning with frozen language models\"", "id": "FELIX HILL", "label": "FELIX HILL", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9,8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Ashish Vaswani is an author of the paper \"Attention is all you need\"", "id": "ASHISH VASWANI", "label": "ASHISH VASWANI", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Noam Shazeer is an author of the paper \"Attention is all you need\"", "id": "NOAM SHAZEEER", "label": "NOAM SHAZEEER", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Niki Parmar is an author of the paper \"Attention is all you need\"", "id": "NIKI PARMAR", "label": "NIKI PARMAR", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jakob Uszkoreit is an author of the paper \"Attention is all you need\"", "id": "JAKOB USZKOREIT", "label": "JAKOB USZKOREIT", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Llion Jones is an author of the paper \"Attention is all you need\"", "id": "LLION JONES", "label": "LLION JONES", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nHao Xue is an author who has contributed to the field of time series forecasting. Specifically, Hao Xue is the author of the paper titled \"Prompt-based time series forecasting: A new task and dataset.\" This paper is likely a research contribution to the field of time series analysis, which involves the study of sequences of data points measured at regular time intervals. The paper may have introduced a new task and dataset for time series forecasting, which is a crucial aspect of predicting future values in a time series based on past observations.\n\nAs an author, Hao Xue is likely a researcher or academic in the field of machine learning, artificial intelligence, or data science, with expertise in time series analysis and forecasting. The paper may have employed various techniques, such as frequency analysis, multi-head cross-attention, or other advanced methods, to tackle the challenges of time series forecasting.\n\nOverall, Hao Xue\u0027s work on prompt-based time series forecasting has likely made a significant contribution to the field, providing new insights and approaches for predicting future values in time series data.", "id": "HAO XUE", "label": "HAO XUE", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,98c6b5003112ab7110e45414a2fa468b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nFLORA D SALIM is an author who has been mentioned in the text, as indicated by the citation of their paper. Specifically, FLORA D SALIM is an author of the paper titled \"Prompt-based time series forecasting: A new task and dataset.\" This suggests that FLORA D SALIM has made significant contributions to the field of time series forecasting, particularly in the area of prompt-based forecasting. The paper, which FLORA D SALIM co-authored, introduces a new task and dataset for time series forecasting, indicating that FLORA D SALIM is a prominent researcher in this field.\n\nThe language of the text in which FLORA D SALIM\u0027s paper is mentioned is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. The text is formal and academic in tone, suggesting that it is from a research paper or technical document. The presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports the conclusion that the primary language of the text is English.\n\nOverall, FLORA D SALIM is a notable author in the field of time series forecasting, with a specific focus on prompt-based forecasting. Their work, as represented by the paper \"Prompt-based time series forecasting: A new task and dataset,\" has contributed to the advancement of knowledge in this area.", "id": "FLORA D SALIM", "label": "FLORA D SALIM", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,98c6b5003112ab7110e45414a2fa468b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Self-supervised contrastive pre-training is a concept mentioned in the text, as indicated by the use of the phrase \"Self-supervised contrastive pre-training for time series via time-frequency consistency\"", "id": "SELF-SUPERVISED CONTRASTIVE PRE-TRAINING", "label": "SELF-SUPERVISED CONTRASTIVE PRE-TRAINING", "shape": "dot", "size": 10, "source_id": "98c6b5003112ab7110e45414a2fa468b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Multimodal large language models are a concept mentioned in the text, as indicated by the use of the phrase \"A survey on multimodal large language models\"", "id": "MULTIMODAL LARGE LANGUAGE MODELS", "label": "MULTIMODAL LARGE LANGUAGE MODELS", "shape": "dot", "size": 10, "source_id": "98c6b5003112ab7110e45414a2fa468b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Ailing Zeng is an author mentioned in the text, as indicated by the citation of their paper", "id": "AILING ZENG", "label": "AILING ZENG", "shape": "dot", "size": 10, "source_id": "98c6b5003112ab7110e45414a2fa468b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Qiang Xu is an author mentioned in the text, as indicated by the citation of their paper", "id": "QIANG XU", "label": "QIANG XU", "shape": "dot", "size": 10, "source_id": "98c6b5003112ab7110e45414a2fa468b", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Less is more is a concept mentioned in the text, as indicated by the use of the phrase \"Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures\"", "id": "LESS IS MORE", "label": "LESS IS MORE", "shape": "dot", "size": 10, "source_id": "98c6b5003112ab7110e45414a2fa468b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "AAAI Conference is a conference where research papers are presented", "id": "AAAI CONFERENCE", "label": "AAAI CONFERENCE", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "EVENT"}, {"color": "#97c2fc", "description": "", "id": "LONG SEQUENCE TIME-SERIES FORECASTING", "label": "LONG SEQUENCE TIME-SERIES FORECASTING", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": ""}, {"color": "#97c2fc", "description": "Proceedings of the AAAI Conference on Artificial Intelligence is a document that contains research papers presented at the AAAI Conference", "id": "PROCEEDINGS OF THE AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE", "label": "PROCEEDINGS OF THE AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Tian Zhou 2023a is a research paper on power general time series analysis by pre-trained LM", "id": "TIAN ZHOU 2023A", "label": "TIAN ZHOU 2023A", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "PAPER"}, {"color": "#97c2fc", "description": "Peisong Niu is an author who has published research papers on time series forecasting", "id": "PEISONG NIU", "label": "PEISONG NIU", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Representation aggregation refers to the process of combining multiple representations of a time series into a single representation", "id": "REPRESENTATION AGGREGATION", "label": "REPRESENTATION AGGREGATION", "shape": "dot", "size": 10, "source_id": "f7266cfccedb1d9840d10afa689a05e9", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Wu et al. are authors mentioned in the text, as indicated by the use of the name \"Wu et al.\"", "id": "WU ET AL.", "label": "WU ET AL.", "shape": "dot", "size": 10, "source_id": "74527a4337ed6919731be520311ae774", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Makridakis et al. are authors mentioned in the text, as indicated by the use of the name \"Makridakis et al.\"", "id": "MAKRIDAKIS ET AL.", "label": "MAKRIDAKIS ET AL.", "shape": "dot", "size": 10, "source_id": "74527a4337ed6919731be520311ae774", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Makridakis is an author mentioned in the text, as indicated by the use of the name \"Makridakis\"", "id": "MAKRIDAKIS", "label": "MAKRIDAKIS", "shape": "dot", "size": 10, "source_id": "74527a4337ed6919731be520311ae774", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Each entry within the ETT datasets includes six power load features", "id": "POWER LOAD FEATURES", "label": "POWER LOAD FEATURES", "shape": "dot", "size": 10, "source_id": "50eeacd99c68b2581be90310bedcbc2c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The target variable in the ETT datasets is termed \u201coil temperature\u201d", "id": "OIL TEMPERATURE", "label": "OIL TEMPERATURE", "shape": "dot", "size": 10, "source_id": "50eeacd99c68b2581be90310bedcbc2c", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Patch dimensions are a parameter used in the TIME-LLM model", "id": "PATCH DIMENSIONS", "label": "PATCH DIMENSIONS", "shape": "dot", "size": 10, "source_id": "1b48e9ca066ac5ba037066bb762d3458", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "LTF - ETTh1 is a configuration used for long-term forecasting with the ETTh1 dataset", "id": "LTF - ETTH1", "label": "LTF - ETTH1", "shape": "dot", "size": 10, "source_id": "306c29917039736b5e882376c7647704", "type": "CONFIGURATION"}, {"color": "#97c2fc", "description": "LTF - ETTh2 is a configuration used for long-term forecasting with the ETTh2 dataset", "id": "LTF - ETTH2", "label": "LTF - ETTH2", "shape": "dot", "size": 10, "source_id": "306c29917039736b5e882376c7647704", "type": "CONFIGURATION"}, {"color": "#97c2fc", "description": "LTF - ETTm1 is a configuration used for long-term forecasting with the ETTm1 dataset", "id": "LTF - ETTM1", "label": "LTF - ETTM1", "shape": "dot", "size": 10, "source_id": "306c29917039736b5e882376c7647704", "type": "CONFIGURATION"}, {"color": "#97c2fc", "description": "LTF - ETTm2 is a configuration used for long-term forecasting with the ETTm2 dataset", "id": "LTF - ETTM2", "label": "LTF - ETTM2", "shape": "dot", "size": 10, "source_id": "306c29917039736b5e882376c7647704", "type": "CONFIGURATION"}, {"color": "#97c2fc", "description": "LTF - Weather is a configuration used for long-term forecasting with the Weather dataset", "id": "LTF - WEATHER", "label": "LTF - WEATHER", "shape": "dot", "size": 10, "source_id": "306c29917039736b5e882376c7647704", "type": "CONFIGURATION"}, {"color": "#97c2fc", "description": "LTF - Electricity is a configuration used for long-term forecasting with the Electricity dataset", "id": "LTF - ELECTRICITY", "label": "LTF - ELECTRICITY", "shape": "dot", "size": 10, "source_id": "306c29917039736b5e882376c7647704", "type": "CONFIGURATION"}, {"color": "#97c2fc", "description": "LTF - Traffic is a configuration used for long-term forecasting with the Traffic dataset", "id": "LTF - TRAFFIC", "label": "LTF - TRAFFIC", "shape": "dot", "size": 10, "source_id": "306c29917039736b5e882376c7647704", "type": "CONFIGURATION"}, {"color": "#97c2fc", "description": "LTF - ILI is a configuration used for long-term forecasting with the ILI dataset", "id": "LTF - ILI", "label": "LTF - ILI", "shape": "dot", "size": 10, "source_id": "306c29917039736b5e882376c7647704", "type": "CONFIGURATION"}, {"color": "#97c2fc", "description": "STF - M3-Quarterly is a configuration used for short-term forecasting with the M3-Quarterly dataset", "id": "STF - M3-QUARTERLY", "label": "STF - M3-QUARTERLY", "shape": "dot", "size": 10, "source_id": "306c29917039736b5e882376c7647704", "type": "CONFIGURATION"}, {"color": "#97c2fc", "description": "STF - M4 is a configuration used for short-term forecasting with the M4 dataset", "id": "STF - M4", "label": "STF - M4", "shape": "dot", "size": 10, "source_id": "306c29917039736b5e882376c7647704", "type": "CONFIGURATION"}, {"color": "#97c2fc", "description": "LLAMAs refer to the language models used in the TIME-LLM model", "id": "LLAMAS", "label": "LLAMAS", "shape": "dot", "size": 10, "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "type": "MODEL"}, {"color": "#97c2fc", "description": "SOTA refers to the state-of-the-art performance of a model", "id": "SOTA", "label": "SOTA", "shape": "dot", "size": 10, "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Velocity refers to the rate of change of an object\u0027s position", "id": "VELOCITY", "label": "VELOCITY", "shape": "dot", "size": 10, "source_id": "2dbfdb45630a023a5a6979b9573a868f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Velocity 1 refers to a specific instance or example of velocity", "id": "VELOCITY 1", "label": "VELOCITY 1", "shape": "dot", "size": 10, "source_id": "2dbfdb45630a023a5a6979b9573a868f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Velocity 2 refers to a specific instance or example of velocity", "id": "VELOCITY 2", "label": "VELOCITY 2", "shape": "dot", "size": 10, "source_id": "2dbfdb45630a023a5a6979b9573a868f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Velocity 3 refers to a specific instance or example of velocity", "id": "VELOCITY 3", "label": "VELOCITY 3", "shape": "dot", "size": 10, "source_id": "2dbfdb45630a023a5a6979b9573a868f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Acceleration 1 refers to a specific instance or example of acceleration", "id": "ACCELERATION 1", "label": "ACCELERATION 1", "shape": "dot", "size": 10, "source_id": "2dbfdb45630a023a5a6979b9573a868f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Acceleration 2 refers to a specific instance or example of acceleration", "id": "ACCELERATION 2", "label": "ACCELERATION 2", "shape": "dot", "size": 10, "source_id": "2dbfdb45630a023a5a6979b9573a868f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Acceleration 3 refers to a specific instance or example of acceleration", "id": "ACCELERATION 3", "label": "ACCELERATION 3", "shape": "dot", "size": 10, "source_id": "2dbfdb45630a023a5a6979b9573a868f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"TABLE\" contains various data and information, as described in the text. The text itself is written in English, indicating that the language of the table\u0027s content is also English. The table is likely to be a part of a research paper or a technical document, given the formal and academic tone of the language used.\n\nThe table may include technical terms, mathematical equations, and references to academic papers, which are all written in English. This suggests that the table is related to topics such as time series analysis, long-term series forecasting, frequency analysis, and multi-head cross-attention, which are commonly discussed in English-language academic papers.\n\nThe presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports the idea that the table is related to academic conferences and journals. Additionally, the citation of English-language academic papers and authors implies that the table is written in English.\n\nOverall, the \"TABLE\" entity contains various data and information, likely related to technical and academic topics, and is written in English.", "id": "TABLE", "label": "TABLE", "shape": "dot", "size": 10, "source_id": "15c3350fad556f666d93817c5036109c,deb67d5386710136cf24bcbf135a66c4", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Table 3 is a specific table mentioned in the text", "id": "TABLE 3", "label": "TABLE 3", "shape": "dot", "size": 10, "source_id": "deb67d5386710136cf24bcbf135a66c4", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Based on the provided information, the entity \"RESULTS\" can be described as follows:\n\nThe entity \"RESULTS\" refers to the data and statistics presented in the table, as well as the outcomes and findings presented in the text. In the context of a research paper or technical document, the results are likely to include the data and statistics presented in tables, figures, and charts, as well as the interpretations and conclusions drawn from the data.\n\nGiven the formal and academic language used in the text, it is likely that the results are presented in a structured and organized manner, with clear headings, subheadings, and captions to facilitate understanding. The results may also include references to academic papers, authors, and conferences, as mentioned in the text.\n\nOverall, the entity \"RESULTS\" encompasses both the quantitative data and statistics presented in the table, as well as the qualitative outcomes and findings presented in the text, providing a comprehensive summary of the research or analysis conducted.", "id": "RESULTS", "label": "RESULTS", "shape": "dot", "size": 10, "source_id": "15c3350fad556f666d93817c5036109c,deb67d5386710136cf24bcbf135a66c4", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Experimental conditions refer to the settings and variables used in the study", "id": "EXPERIMENTAL CONDITIONS", "label": "EXPERIMENTAL CONDITIONS", "shape": "dot", "size": 10, "source_id": "deb67d5386710136cf24bcbf135a66c4", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "M4-Yearly is a dataset used for evaluating the performance of models in short-term forecasting", "id": "M4-YEARLY", "label": "M4-YEARLY", "shape": "dot", "size": 10, "source_id": "ef2ef6c95c36f51706c54ca3f2893641", "type": "DATASET"}, {"color": "#97c2fc", "description": "M4-Hourly is a dataset used for evaluating the performance of models in short-term forecasting", "id": "M4-HOURLY", "label": "M4-HOURLY", "shape": "dot", "size": 10, "source_id": "ef2ef6c95c36f51706c54ca3f2893641", "type": "DATASET"}, {"color": "#97c2fc", "description": "M4-Daily is a dataset used for evaluating the performance of models in short-term forecasting", "id": "M4-DAILY", "label": "M4-DAILY", "shape": "dot", "size": 10, "source_id": "ef2ef6c95c36f51706c54ca3f2893641", "type": "DATASET"}, {"color": "#97c2fc", "description": "M4-Weekly is a dataset used for evaluating the performance of models in short-term forecasting", "id": "M4-WEEKLY", "label": "M4-WEEKLY", "shape": "dot", "size": 10, "source_id": "ef2ef6c95c36f51706c54ca3f2893641", "type": "DATASET"}, {"color": "#97c2fc", "description": "Long-term forecasting is a concept that refers to the process of predicting future values or outcomes over a long period of time", "id": "LONG-TERM FORECASTING", "label": "LONG-TERM FORECASTING", "shape": "dot", "size": 10, "source_id": "b90805909f7b1f31ddc03014f161d3ba", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "", "id": "TABLE 17", "label": "TABLE 17", "shape": "dot", "size": 10, "source_id": "b90805909f7b1f31ddc03014f161d3ba", "type": ""}, {"color": "#97c2fc", "description": "ETTM1-96 is a time series dataset", "id": "ETTM1-96", "label": "ETTM1-96", "shape": "dot", "size": 10, "source_id": "e6a6bc6fbd362394320961ac10cdd230", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "ETTM1-192 is a time series dataset", "id": "ETTM1-192", "label": "ETTM1-192", "shape": "dot", "size": 10, "source_id": "e6a6bc6fbd362394320961ac10cdd230", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "ETTH1-96 is a time series dataset", "id": "ETTH1-96", "label": "ETTH1-96", "shape": "dot", "size": 10, "source_id": "e6a6bc6fbd362394320961ac10cdd230", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "ETTH1-192 is a time series dataset", "id": "ETTH1-192", "label": "ETTH1-192", "shape": "dot", "size": 10, "source_id": "e6a6bc6fbd362394320961ac10cdd230", "type": "TIME SERIES"}, {"color": "#97c2fc", "description": "96 is a specific time point or interval", "id": "96", "label": "96", "shape": "dot", "size": 10, "source_id": "a014d14e003d0998b877eacffa8ebfc4", "type": "TIME"}, {"color": "#97c2fc", "description": "192 is a specific time point or interval", "id": "192", "label": "192", "shape": "dot", "size": 10, "source_id": "a014d14e003d0998b877eacffa8ebfc4", "type": "TIME"}, {"color": "#97c2fc", "description": "336 is a specific time point or interval", "id": "336", "label": "336", "shape": "dot", "size": 10, "source_id": "a014d14e003d0998b877eacffa8ebfc4", "type": "TIME"}, {"color": "#97c2fc", "description": "720 is a specific time point or interval", "id": "720", "label": "720", "shape": "dot", "size": 10, "source_id": "a014d14e003d0998b877eacffa8ebfc4", "type": "TIME"}, {"color": "#97c2fc", "description": "Full short-term time series forecasting results is a concept mentioned in the text, as indicated by the use of the phrase \"Full short-term time series forecasting results\"", "id": "FULL SHORT-TERM TIME SERIES FORECASTING RESULTS", "label": "FULL SHORT-TERM TIME SERIES FORECASTING RESULTS", "shape": "dot", "size": 10, "source_id": "f49330b6fd81d86d14e7a9d4b8e45576", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Forecasting horizons is a concept mentioned in the text, as indicated by the use of the phrase \"forecasting horizons are in [6, 48]\"", "id": "FORECASTING HORIZONS", "label": "FORECASTING HORIZONS", "shape": "dot", "size": 10, "source_id": "f49330b6fd81d86d14e7a9d4b8e45576", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Tab. 17 is a document mentioned in the text, as indicated by the use of the phrase \"The full ablation results are in Tab. 17\"", "id": "TAB. 17", "label": "TAB. 17", "shape": "dot", "size": 10, "source_id": "7e97089185883c456c798ddc5ec86373", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Tab. 18 is a document mentioned in the text, as indicated by the use of the phrase \"Our results are given in Tab. 18\"", "id": "TAB. 18", "label": "TAB. 18", "shape": "dot", "size": 10, "source_id": "7e97089185883c456c798ddc5ec86373", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Running time is a resource mentioned in the text, as indicated by the use of the phrase \"running time (seconds per iteration)\"", "id": "RUNNING TIME", "label": "RUNNING TIME", "shape": "dot", "size": 10, "source_id": "7e97089185883c456c798ddc5ec86373", "type": "RESOURCE"}, {"color": "#97c2fc", "description": "ETTh1-96 is a concept that refers to a specific type of data or model that predicts 96 steps ahead", "id": "ET T H1-96", "label": "ET T H1-96", "shape": "dot", "size": 10, "source_id": "b90805909f7b1f31ddc03014f161d3ba", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ETTh1-192 is a concept that refers to a specific type of data or model that predicts 192 steps ahead", "id": "ET T H1-192", "label": "ET T H1-192", "shape": "dot", "size": 10, "source_id": "b90805909f7b1f31ddc03014f161d3ba", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ETTm1-96 is a concept that refers to a specific type of data or model that predicts 96 steps ahead", "id": "ET T M1-96", "label": "ET T M1-96", "shape": "dot", "size": 10, "source_id": "b90805909f7b1f31ddc03014f161d3ba", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ETTm1-192 is a concept that refers to a specific type of data or model that predicts 192 steps ahead", "id": "ET T M1-192", "label": "ET T M1-192", "shape": "dot", "size": 10, "source_id": "b90805909f7b1f31ddc03014f161d3ba", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Deep transformer networks are a type of neural network architecture that uses self-attention mechanisms to process sequential data", "id": "DEEP TRANSFORMER NETWORKS", "label": "DEEP TRANSFORMER NETWORKS", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"MULTIVARIATE TIME SERIES DATA\" refers to a type of data that consists of multiple variables measured over time. This data is characterized by its multivariate nature, where multiple variables are observed and recorded simultaneously over a period of time. The primary language of the text related to this entity is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nThe language used is formal and academic, suggesting that the text is from a research paper or a technical document. The presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports the conclusion that the primary language of the text is English.\n\nIn terms of the characteristics of multivariate time series data, it is worth noting that this type of data is commonly used in various fields, including finance, economics, and engineering. The analysis of multivariate time series data often involves techniques such as frequency analysis, long-term series forecasting, and multi-head cross-attention, which are used to identify patterns and relationships within the data.\n\nOverall, the entity \"MULTIVARIATE TIME SERIES DATA\" refers to a complex and multifaceted type of data that is commonly used in various fields and analyzed using a range of techniques.", "id": "MULTIVARIATE TIME SERIES DATA", "label": "MULTIVARIATE TIME SERIES DATA", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3,78ee4a3d7a2bffd4405a03d94a4f6cb1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nSHRESHTH TULI is a researcher who has made significant contributions to the field of machine learning and time series forecasting. Specifically, he is associated with the development of the TranAD model, a notable achievement in the realm of time series analysis. Furthermore, SHRESHTH TULI is an author who has published a paper, as indicated by the citation \"Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings\". This suggests that he has expertise in the area of time series forecasting and has shared his knowledge through academic publications. Overall, SHRESHTH TULI is a notable figure in the field of time series analysis and machine learning, with a strong background in research and publication.", "id": "SHRESHTH TULI", "label": "SHRESHTH TULI", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3,5277ea101e78f34ea2c62fa10007b2ac,9b35a2c607e0f4b8cbc9179f424f180a,a4a241e471ad258932241bc441b96155,fa91ecd90e1c12d64176de581c6220c7", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nImperial College London is a renowned institution that plays a significant role in research and education. As an organization, it provides essential funding and support for various research and educational initiatives. Notably, some of the authors affiliated with Imperial College London contribute to the academic community through their research and publications. The institution\u0027s academic environment is likely formal and academic, as suggested by the presence of technical terms, mathematical equations, and references to academic papers. These indicators are consistent with the language and content typically found in English-language research papers and technical documents. Overall, Imperial College London is a prominent institution that supports research, education, and academic pursuits, with a strong presence in the English-language academic community.", "id": "IMPERIAL COLLEGE LONDON", "label": "IMPERIAL COLLEGE LONDON", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3,a39d9d28126733c33db624ccc25f5782", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "London, UK is a location where some of the authors are affiliated", "id": "LONDON, UK", "label": "LONDON, UK", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "type": "LOCATION"}, {"color": "#97c2fc", "description": "A type of operating system", "id": "WINDOWS 11 OS", "label": "WINDOWS 11 OS", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "ENTITY"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nGiuliano Casale is a researcher who has made significant contributions to the field of machine learning and time series forecasting. Specifically, he is an author who contributed to the development of the TranAD model, a notable achievement in the realm of time series analysis. Additionally, Giuliano Casale is mentioned as an author in the text, as indicated by the citation \"Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings\". Furthermore, he is also listed as an author of the paper, highlighting his expertise and involvement in the research. Overall, Giuliano Casale\u0027s work has been recognized and acknowledged in the academic community, solidifying his position as a prominent researcher in the field.\n\nRelevant information from the nearby text:\n\n* The text is written in English, as indicated by the use of technical terms, mathematical equations, and references to academic papers.\n* The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n* The text mentions the TranAD model, which is a significant contribution to the field of time series analysis.\n* The citation \"Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings\" suggests that Giuliano Casale is a co-author of the paper, highlighting his collaboration with other researchers in the field.\n\nNote: The contradictions in the description list have been resolved by combining the information to provide a comprehensive summary of Giuliano Casale\u0027s contributions and involvement in the research.", "id": "GIULIANO CASALE", "label": "GIULIANO CASALE", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3,5277ea101e78f34ea2c62fa10007b2ac,9b35a2c607e0f4b8cbc9179f424f180a,a4a241e471ad258932241bc441b96155,bd73ee0439609823e12a877840c6ebae,fa91ecd90e1c12d64176de581c6220c7", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nNicholas R. Jennings is a researcher who has made significant contributions to the development of the TranAD model. He is also an author mentioned in the text, as indicated by the citation \"Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings\". Furthermore, Nicholas R. Jennings is an author of the paper, and one of the authors of the paper, highlighting his involvement in the research and academic community.\n\nThe mention of the TranAD model suggests that Nicholas R. Jennings\u0027 work is related to time series analysis and forecasting, as the TranAD model is likely a model developed for long-term series forecasting. The citation of his work also implies that he is a prominent figure in the field of artificial intelligence and machine learning, as indicated by the mention of conferences and journals such as ICLR, AAAI, and PMLR.\n\nOverall, Nicholas R. Jennings is a researcher and author who has made notable contributions to the development of the TranAD model and the field of artificial intelligence and machine learning.", "id": "NICHOLAS R. JENNINGS", "label": "NICHOLAS R. JENNINGS", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3,5277ea101e78f34ea2c62fa10007b2ac,9b35a2c607e0f4b8cbc9179f424f180a,a4a241e471ad258932241bc441b96155,bd73ee0439609823e12a877840c6ebae,fa91ecd90e1c12d64176de581c6220c7", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Shreshth Tuli is one of the authors of the paper", "id": "SHRESTH TULI", "label": "SHRESTH TULI", "shape": "dot", "size": 10, "source_id": "bd73ee0439609823e12a877840c6ebae", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Loughborough University is an organization where one of the authors is affiliated", "id": "LOUGHBOROUGH UNIVERSITY", "label": "LOUGHBOROUGH UNIVERSITY", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "President\u0027s PhD scholarship is an award given to students who are pursuing a PhD degree", "id": "PRESIDENT\u0027S PHD SCHOLARSHIP", "label": "PRESIDENT\u0027S PHD SCHOLARSHIP", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "AWARD"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMODEL-AGNOSTIC META LEARNING is a technique used in TranAD to improve generalization and adaptability. It is a type of machine learning approach that allows for the training of models using limited data. This technique is particularly useful in scenarios where data is scarce, and the model needs to adapt quickly to new situations.\n\nThe use of MODEL-AGNOSTIC META LEARNING enables models to learn how to learn from limited data, making them more adaptable and generalizable. This approach is essential in various applications, including but not limited to, time series forecasting, where long-term series forecasting and frequency analysis are crucial.\n\nThe mathematical equations and formulas used in MODEL-AGNOSTIC META LEARNING are written in a standard mathematical notation, indicating that the language used is formal and academic, typical of English-language academic papers. The presence of English-language abbreviations such as ICLR, AAAI, and PMLR further supports the conclusion that the primary language of the text is English.\n\nOverall, MODEL-AGNOSTIC META LEARNING is a powerful technique that enables models to learn from limited data, making them more adaptable and generalizable. Its applications are vast, and it is particularly useful in time series forecasting and other scenarios where data is scarce.", "id": "MODEL-AGNOSTIC META LEARNING", "label": "MODEL-AGNOSTIC META LEARNING", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3,bd73ee0439609823e12a877840c6ebae", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTRANAAD is a transformer-based anomaly detection model that utilizes self-conditioning and an adversarial training process to amplify errors and gain training stability. This approach enables the model to effectively identify anomalies and improve its overall performance.\n\nThe model\u0027s architecture is based on the transformer framework, which is a type of neural network designed for natural language processing tasks. By leveraging self-conditioning and adversarial training, TRANAAD is able to learn complex patterns and relationships in the data, allowing it to detect anomalies with high accuracy.\n\nThe use of self-conditioning in TRANAAD involves the model learning to predict its own outputs, which helps to improve its stability and reduce the impact of noise in the data. The adversarial training process, on the other hand, involves the model being trained to minimize the difference between its predictions and the actual values, which helps to amplify errors and improve the model\u0027s ability to detect anomalies.\n\nOverall, TRANAAD is a powerful anomaly detection model that has the potential to be applied in a wide range of domains, including but not limited to, time series forecasting, frequency analysis, and multi-head cross-attention.", "id": "TRANAAD", "label": "TRANAAD", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782,bd73ee0439609823e12a877840c6ebae", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided descriptions, a comprehensive summary of the entity \"ADVERSARIAL TRAINING\" can be generated as follows:\n\nADVERSARIAL TRAINING refers to a type of training method that involves adding noise or perturbations to the data, with the goal of amplifying errors and improving the robustness of the model. This technique is used in TranAD to amplify reconstruction errors and improve anomaly detection, ultimately leading to training stability. Furthermore, adversarial training is a type of training approach that involves training a model to be robust to adversarial examples, which are inputs that are specifically designed to cause the model to make mistakes. By incorporating this type of training, models can become more resilient to such examples and provide more accurate results in real-world applications.\n\nThis summary is based on the information collected from all the descriptions, and any contradictions have been resolved to provide a single, coherent summary. The entity name \"ADVERSARIAL TRAINING\" is included to provide full context, and relevant information from the nearby text has been incorporated to enrich the summary.", "id": "ADVERSARIAL TRAINING", "label": "ADVERSARIAL TRAINING", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3,78ee4a3d7a2bffd4405a03d94a4f6cb1,a39d9d28126733c33db624ccc25f5782,bd73ee0439609823e12a877840c6ebae", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Focus score-based self-conditioning is a type of self-conditioning approach that uses focus scores to enable robust multi-modal feature extraction", "id": "FOCUS SCORE-BASED SELF-CONDITIONING", "label": "FOCUS SCORE-BASED SELF-CONDITIONING", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Graph-attention network is a type of neural network layer used for modeling feature and temporal correlations", "id": "GRAPH-ATTENTION NETWORK", "label": "GRAPH-ATTENTION NETWORK", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420", "type": "LAYER"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nOPENGAUSS is a method for anomaly detection that utilizes a Long-Short-Term-Memory (LSTM) based neural network to forecast data with an input time-series. This approach is combined with a non-parametric dynamic thresholding method for detecting anomalies from prediction errors. OPENGAUSS is specifically designed to identify anomalies in data by leveraging the forecasting capabilities of the LSTM-based neural network and the dynamic thresholding approach. As a type of neural network model, OPENGAUSS is used for anomaly detection, making it a valuable tool in various applications where identifying unusual patterns or outliers is crucial.", "id": "OPENGAUSS", "label": "OPENGAUSS", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420,0c4c072869e10b0b4bd4bc19a60a23a3", "type": "MODEL"}, {"color": "#97c2fc", "description": "Tree-based LSTM is a type of neural network layer used for capturing temporal trends even with noisy data", "id": "TREE-BASED LSTM", "label": "TREE-BASED LSTM", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420", "type": "LAYER"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nSAND is a method for anomaly detection that utilizes a combination of clustering and statistical analysis to identify anomalies. Specifically, SAND is a type of algorithm that leverages clustering and database read-write history to detect outliers. This approach enables SAND to effectively identify unusual patterns and deviations in data, making it a valuable tool for anomaly detection.\n\nThe summary is based on the information provided in the description list, which presents two distinct descriptions of SAND. To resolve any potential contradictions, the summary combines the key elements of both descriptions, highlighting the use of clustering and statistical analysis, as well as the incorporation of database read-write history. This comprehensive summary provides a clear understanding of the SAND method and its application in anomaly detection.", "id": "SAND", "label": "SAND", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3,203f9117f8528750ca0c22a768a02cd9", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "MAML", "label": "MAML", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": ""}, {"color": "#97c2fc", "description": "Section 3 is a part of the document that outlines the working of the TranAD model", "id": "SECTION 3", "label": "SECTION 3", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Section 4 is a part of the document that shows a performance evaluation of the proposed method", "id": "SECTION 4", "label": "SECTION 4", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Section 6 is a part of the document that concludes the paper", "id": "SECTION 6", "label": "SECTION 6", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Classical methods are algorithms that are based on traditional techniques such as k-Mean clustering, Support Vector Machines, and regression models", "id": "CLASSICAL METHODS", "label": "CLASSICAL METHODS", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Multi-scale convolutional recursive encoder is a model used for anomaly detection in multivariate time series data", "id": "MULTI-SCALE CONVOLUTIONAL RECURSIVE ENCODER", "label": "MULTI-SCALE CONVOLUTIONAL RECURSIVE ENCODER", "shape": "dot", "size": 10, "source_id": "ffbbbf29ffb8d038e241f023079cb0a2", "type": "MODEL"}, {"color": "#97c2fc", "description": "Broad level trends refer to general patterns or changes that occur over time", "id": "BROAD LEVEL TRENDS", "label": "BROAD LEVEL TRENDS", "shape": "dot", "size": 10, "source_id": "1902e651467179a9a1d4c4df0035e980", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time-series data used to train the TranAD model", "id": "TRAINING TIME-SERIES", "label": "TRAINING TIME-SERIES", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "DATA"}, {"color": "#97c2fc", "description": "Normalized Discounted Cumulative Gain is a metric used to evaluate the performance of an anomaly detection model", "id": "NDCG", "label": "NDCG", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "type": "METRIC"}, {"color": "#97c2fc", "description": "", "id": "ANAD", "label": "ANAD", "shape": "dot", "size": 10, "source_id": "1413d358c623ac2d4c70be6547eb218b", "type": ""}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"POSITION ENCODING\" is a technique used in various machine learning models to add positional information to input data. It is a concept used in the window encoder to encode the input window, allowing the model to understand the position of each token in a sequence. Specifically, position encoding is a technique used in transformer models to encode the position of each token in a sequence, enabling the model to capture the temporal relationships between tokens.\n\nIn the context of transformer models, position encoding is a crucial component that helps the model understand the position of each token in a sequence, which is essential for tasks such as language translation, text classification, and long-term series forecasting. The use of position encoding in transformer models has been widely adopted in the field of natural language processing and has shown significant improvements in model performance.\n\nOverall, position encoding is a powerful technique that enables machine learning models to capture the positional relationships between tokens in a sequence, which is essential for many natural language processing tasks.\n\nRelevant information from the nearby text:\n\n* The use of position encoding in transformer models has been widely adopted in the field of natural language processing.\n* Position encoding is a crucial component that helps the model understand the position of each token in a sequence.\n* The use of position encoding has shown significant improvements in model performance for tasks such as language translation, text classification, and long-term series forecasting.\n\nNote: The provided descriptions are not contradictory, and the summary is a coherent and concise description of the entity \"POSITION ENCODING\".", "id": "POSITION ENCODING", "label": "POSITION ENCODING", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e,4d2961a17ee532a47fa53a41f53ebdd3,bd73ee0439609823e12a877840c6ebae", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Input Matrices are the data used as input to the neural network", "id": "INPUT MATRICES", "label": "INPUT MATRICES", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "Transformer based encoder-decoder is a type of neural network architecture that allows for quick model training and high detection performance", "id": "TRANSFORMER BASED ENCODER-DECODER", "label": "TRANSFORMER BASED ENCODER-DECODER", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "MODEL"}, {"color": "#97c2fc", "description": "Attention weights are parameters used in the TranAD model", "id": "ATTENTION WEIGHTS", "label": "ATTENTION WEIGHTS", "shape": "dot", "size": 10, "source_id": "a65c0f1eae6e779357739df141f75d36", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "State-of-the-art methods are algorithms that are currently considered to be the best in their field", "id": "STATE-OF-THE-ART METHODS", "label": "STATE-OF-THE-ART METHODS", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Simple transformers are a type of algorithm that underperform compared to TranAD", "id": "SIMPLE TRANSFORMERS", "label": "SIMPLE TRANSFORMERS", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Prediction scores are a metric used to evaluate the performance of a model", "id": "PREDICTION SCORES", "label": "PREDICTION SCORES", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "METRIC"}, {"color": "#97c2fc", "description": "Training time overheads are a metric used to evaluate the efficiency of a model", "id": "TRAINING TIME OVERHEADS", "label": "TRAINING TIME OVERHEADS", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "METRIC"}, {"color": "#97c2fc", "description": "Section 2 is a part of the document that overviews related work", "id": "SECTION 2", "label": "SECTION 2", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "The VLDB community is a group of researchers and practitioners who work on time series anomaly detection", "id": "VLDB COMMUNITY", "label": "VLDB COMMUNITY", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "COMMUNITY"}, {"color": "#97c2fc", "description": "K-Mean clustering is a type of algorithm that groups similar data points together", "id": "K-MEAN CLUSTERING", "label": "K-MEAN CLUSTERING", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Support Vector Machines is a type of algorithm that is used for classification and regression tasks", "id": "SUPPORT VECTOR MACHINES", "label": "SUPPORT VECTOR MACHINES", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Wavelet theory is a type of algorithm that is used for signal processing and analysis", "id": "WAVELET THEORY", "label": "WAVELET THEORY", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Hilbert transform is a type of algorithm that is used for signal processing and analysis", "id": "HILBERT TRANSFORM", "label": "HILBERT TRANSFORM", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Principal Component Analysis is a type of algorithm that is used for dimensionality reduction", "id": "PRINCIPAL COMPONENT ANALYSIS", "label": "PRINCIPAL COMPONENT ANALYSIS", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Process regression is a type of algorithm that is used for predicting continuous outcomes", "id": "PROCESS REGRESSION", "label": "PROCESS REGRESSION", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe entity \"TIME SERIES DISCORDS\" refers to the most unusual time series subsequences. Time series discords are a concept that can be used to identify and analyze unusual patterns or anomalies within a time series dataset. This concept is likely used in the context of time series analysis and forecasting, where understanding and modeling unusual patterns is crucial for making accurate predictions.\n\nThe description of time series discords suggests that they are subsequences of a time series that exhibit unusual behavior, such as sudden changes or outliers. This implies that time series discords can be used to identify and isolate unusual patterns within a time series, which can be useful for a variety of applications, including anomaly detection, quality control, and predictive modeling.\n\nOverall, the concept of time series discords appears to be a useful tool for analyzing and understanding unusual patterns within time series data, and can be used in a variety of contexts, including time series forecasting and anomaly detection.", "id": "TIME SERIES DISCORDS", "label": "TIME SERIES DISCORDS", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9,ffbbbf29ffb8d038e241f023079cb0a2", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMATRIX PROFILING is a method used for anomaly and motif discovery. It is a type of algorithm that detects time series discords, which are unusual patterns or anomalies in time series data. This approach is particularly useful for identifying irregularities or unusual behavior in data that can be indicative of underlying patterns or motifs.\n\nThe primary application of MATRIX PROFILING is in the realm of time series analysis, where it is used to identify anomalies and motifs in long-term series forecasting. By employing frequency analysis and other techniques, MATRIX PROFILING can help uncover hidden patterns and relationships within the data, enabling more accurate predictions and better decision-making.\n\nThe use of MATRIX PROFILING is not limited to a specific domain or industry, but rather can be applied to a wide range of fields where time series data is relevant. Its ability to detect anomalies and motifs makes it a valuable tool for researchers and practitioners seeking to gain insights from complex data sets.\n\nOverall, MATRIX PROFILING is a powerful algorithm for anomaly and motif discovery in time series data, offering a range of benefits and applications in various fields.", "id": "MATRIX PROFILING", "label": "MATRIX PROFILING", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9,ffbbbf29ffb8d038e241f023079cb0a2", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nF1* is a metric used to evaluate the performance of anomaly detection models. This metric is utilized to assess the effectiveness of anomaly detection models, as indicated by its presence in a table. The use of F1* in this context suggests that it is a key performance indicator for evaluating the accuracy and precision of anomaly detection models.\n\nGiven the technical nature of the descriptions, it is likely that F1* is a metric commonly used in the field of machine learning and data science, particularly in the context of anomaly detection and predictive modeling. The fact that it is used in a table implies that it is a quantifiable measure that can be used to compare the performance of different anomaly detection models.\n\nOverall, F1* appears to be a critical metric for evaluating the performance of anomaly detection models, and its use in the table suggests that it is a widely accepted and utilized metric in the field of machine learning and data science.", "id": "F1*", "label": "F1*", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,2b44aebc638544dcf835db30c4270d09", "type": "METRIC"}, {"color": "#97c2fc", "description": "Kate Highnam is a person who provided constructive comments on improving the manuscript writing", "id": "KATE HIGHNAM", "label": "KATE HIGHNAM", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "PERSON"}, {"color": "#97c2fc", "description": "Python is a programming language used to implement the MERLIN baseline", "id": "PYTHON", "label": "PYTHON", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "LANGUAGE"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"NDT\" is a non-parametric dynamic error thresholding strategy used for setting a threshold for anomaly labeling. This method utilizes moving averages of the error sequence to determine the threshold. Notably, NDT has been previously used as a method for anomaly threshold selection, indicating its established application in this area.", "id": "NDT", "label": "NDT", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420,ffbbbf29ffb8d038e241f023079cb0a2", "type": "STRATEGY"}, {"color": "#97c2fc", "description": "Peak Over Threshold (POT) is a method for automated anomaly threshold selection", "id": "PEAK OVER THRESHOLD (POT)", "label": "PEAK OVER THRESHOLD (POT)", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420", "type": "METHOD"}, {"color": "#97c2fc", "description": "Planar Normalizing Flow is a type of neural network model used for generating reconstruction probabilities", "id": "PLANAR NORMALIZING FLOW", "label": "PLANAR NORMALIZING FLOW", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "ARIATIONAL AUTOENCODER", "label": "ARIATIONAL AUTOENCODER", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420", "type": ""}, {"color": "#97c2fc", "description": "Multi-Scale Convectional Recursive Encoder-Decoder (MSCRED) is a type of neural network model used for anomaly detection", "id": "MULTI-SCALE CONVECTIONAL RECURSIVE ENCODER-DECODER (MSCRED)", "label": "MULTI-SCALE CONVECTIONAL RECURSIVE ENCODER-DECODER (MSCRED)", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420", "type": "MODEL"}, {"color": "#97c2fc", "description": "ConvLSTM is a type of neural network layer used for capturing complex inter-modal correlations and temporal information", "id": "CONVLSTM", "label": "CONVLSTM", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420", "type": "LAYER"}, {"color": "#97c2fc", "description": "LSTM based GAN is a type of neural network model used for modeling time-series distribution", "id": "LSTM BASED GAN", "label": "LSTM BASED GAN", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420", "type": "MODEL"}, {"color": "#97c2fc", "description": "GRU is a type of neural network layer used for aiding detection without severe overheads", "id": "GRU", "label": "GRU", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420", "type": "LAYER"}, {"color": "#97c2fc", "description": "Convolutional autoencoding memory network is a type of neural network model used for anomaly detection", "id": "CONVOLUTIONAL AUTOENCODING MEMORY NETWORK", "label": "CONVOLUTIONAL AUTOENCODING MEMORY NETWORK", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nHitAnomaly is a method for anomaly detection and diagnosis, specifically designed to identify and diagnose anomalies in data. It is a type of neural network model used for anomaly detection, indicating that it employs a machine learning approach to identify patterns and anomalies in data. The use of a neural network model suggests that HitAnomaly is capable of complex pattern recognition and learning from data, making it a sophisticated tool for anomaly detection and diagnosis.\n\nGiven the context of the descriptions, it appears that HitAnomaly is a specialized tool for identifying and diagnosing anomalies in data, and its neural network architecture enables it to perform complex pattern recognition and learning tasks. The fact that it is a method for anomaly detection and diagnosis suggests that it is a valuable tool for various applications, including but not limited to, quality control, fraud detection, and predictive maintenance.\n\nOverall, HitAnomaly is a powerful tool for anomaly detection and diagnosis, leveraging the capabilities of neural networks to identify and diagnose anomalies in data.", "id": "HITANOMALY", "label": "HITANOMALY", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420,1902e651467179a9a1d4c4df0035e980", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"VANILLA TRANSFORMERS\" refers to a type of neural network that utilizes self-attention mechanisms to process sequential data. This neural network model is specifically designed to handle sequential data, leveraging the self-attention mechanism to process and analyze the input data. \n\nIn addition to its primary function, the \"VANILLA TRANSFORMERS\" model is also used for anomaly detection, indicating its versatility and applicability in various machine learning tasks. The use of self-attention mechanisms in this model allows it to effectively process and analyze sequential data, making it a valuable tool in the field of artificial intelligence and machine learning.\n\nOverall, the \"VANILLA TRANSFORMERS\" entity is a type of neural network model that combines the strengths of self-attention mechanisms with the ability to process sequential data, making it a powerful tool for a range of applications, including anomaly detection.", "id": "VANILLA TRANSFORMERS", "label": "VANILLA TRANSFORMERS", "shape": "dot", "size": 10, "source_id": "0259287b914980606371cd1161c6a420,1902e651467179a9a1d4c4df0035e980", "type": "MODEL"}, {"color": "#97c2fc", "description": "Temporal trends refer to patterns or changes that occur over time", "id": "TEMPORAL TRENDS", "label": "TEMPORAL TRENDS", "shape": "dot", "size": 10, "source_id": "1902e651467179a9a1d4c4df0035e980", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Noisy data refers to data that contains errors or inconsistencies", "id": "NOISY DATA", "label": "NOISY DATA", "shape": "dot", "size": 10, "source_id": "1902e651467179a9a1d4c4df0035e980", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Local contextual window is a concept mentioned in the text, as indicated by the use of the phrase \"local contextual window\"", "id": "LOCAL CONTEXTUAL WINDOW", "label": "LOCAL CONTEXTUAL WINDOW", "shape": "dot", "size": 10, "source_id": "477c5b7f23f4e00030ab389788a4c88d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Anomaly label is a concept mentioned in the text, as indicated by the use of the phrase \"anomaly label\"", "id": "ANOMALY LABEL", "label": "ANOMALY LABEL", "shape": "dot", "size": 10, "source_id": "477c5b7f23f4e00030ab389788a4c88d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Complete sequence refers to the entire input sequence up to the t-th timestamp", "id": "COMPLETE SEQUENCE", "label": "COMPLETE SEQUENCE", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Encoder is a component of the transformer model that encodes the input sequence", "id": "ENCODER", "label": "ENCODER", "shape": "dot", "size": 10, "source_id": "f2e78b25a535b82e2743a0ea4052eb9a", "type": "MODEL COMPONENT"}, {"color": "#97c2fc", "description": "Multihead attention is a concept used in the decoder to perform attention operations", "id": "MULTIHEAD ATTENTION", "label": "MULTIHEAD ATTENTION", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Masking is a concept used in the window encoder to hide window sequences for future timestamps", "id": "MASK", "label": "MASK", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Scaled-dot product attention is a type of attention mechanism used in the transformer model", "id": "SCALEDDOT PRODUCT ATTENTION", "label": "SCALEDDOT PRODUCT ATTENTION", "shape": "dot", "size": 10, "source_id": "f2e78b25a535b82e2743a0ea4052eb9a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Feed-forward layer is a component of the transformer model that applies a non-linear transformation to the input", "id": "FEED-FORWARD LAYER", "label": "FEED-FORWARD LAYER", "shape": "dot", "size": 10, "source_id": "f2e78b25a535b82e2743a0ea4052eb9a", "type": "MODEL COMPONENT"}, {"color": "#97c2fc", "description": "Feed-Forward Layers are a type of neural network layer", "id": "FEED-FORWARD LAYERS", "label": "FEED-FORWARD LAYERS", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Scaled-Dot Product Attention is a type of attention mechanism used in neural networks", "id": "SCALED-DOT PRODUCT ATTENTION", "label": "SCALED-DOT PRODUCT ATTENTION", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "GAN is a type of generative adversarial network", "id": "GAN", "label": "GAN", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "MODEL"}, {"color": "#97c2fc", "description": "Time-Efficient GAN Style Adversarial Training is a method used to train GAN models", "id": "TIME-EFFICIENT GAN STYLE ADVERSARIAL TRAINING", "label": "TIME-EFFICIENT GAN STYLE ADVERSARIAL TRAINING", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "METHOD"}, {"color": "#97c2fc", "description": "Transformer Encoders are a type of neural network layer", "id": "TRANSFORMER ENCODERS", "label": "TRANSFORMER ENCODERS", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "MODEL"}, {"color": "#97c2fc", "description": "Decoders are a type of neural network layer", "id": "DECODERS", "label": "DECODERS", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "MODEL"}, {"color": "#97c2fc", "description": "\ud835\udc4a is an input matrix", "id": "\ud835\udc4a", "label": "\ud835\udc4a", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "\ud835\udc36 is an input matrix", "id": "\ud835\udc36", "label": "\ud835\udc36", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "\ud835\udc44 is an input matrix", "id": "\ud835\udc44", "label": "\ud835\udc44", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "\ud835\udc3e is an input matrix", "id": "\ud835\udc3e", "label": "\ud835\udc3e", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "\ud835\udc49 is an input matrix", "id": "\ud835\udc49", "label": "\ud835\udc49", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "\ud835\udc56 is an index used to iterate over the input matrices", "id": "\ud835\udc56", "label": "\ud835\udc56", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "INDEX"}, {"color": "#97c2fc", "description": "\u210e is the number of heads used in the multi-head self attention mechanism", "id": "\u210e", "label": "\u210e", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "INDEX"}, {"color": "#97c2fc", "description": "\ud835\udc44\ud835\udc56 is a transformed input matrix", "id": "\ud835\udc44\ud835\udc56", "label": "\ud835\udc44\ud835\udc56", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "\ud835\udc3e\ud835\udc56 is a transformed input matrix", "id": "\ud835\udc3e\ud835\udc56", "label": "\ud835\udc3e\ud835\udc56", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "\ud835\udc49\ud835\udc56 is a transformed input matrix", "id": "\ud835\udc49\ud835\udc56", "label": "\ud835\udc49\ud835\udc56", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "\ud835\udc3c1 is an input matrix used in the first encoder", "id": "\ud835\udc3c1", "label": "\ud835\udc3c1", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "\ud835\udc3c11 is a transformed input matrix", "id": "\ud835\udc3c11", "label": "\ud835\udc3c11", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "\ud835\udc3c12 is a transformed input matrix", "id": "\ud835\udc3c12", "label": "\ud835\udc3c12", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "\ud835\udc3c2 is an input matrix used in the window encoder", "id": "\ud835\udc3c2", "label": "\ud835\udc3c2", "shape": "dot", "size": 10, "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "type": "DATA"}, {"color": "#97c2fc", "description": "Encoded-decoder network is a type of neural network used in the TranAD model", "id": "ENCODED-DECODER NETWORK", "label": "ENCODED-DECODER NETWORK", "shape": "dot", "size": 10, "source_id": "a65c0f1eae6e779357739df141f75d36", "type": "MODEL"}, {"color": "#97c2fc", "description": "Auto-regressive inference is a concept used in the TranAD model", "id": "AUTO-REGRESSIVE INFERENCE", "label": "AUTO-REGRESSIVE INFERENCE", "shape": "dot", "size": 10, "source_id": "a65c0f1eae6e779357739df141f75d36", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Phase 1 is a concept used in the TranAD model, representing the first phase of inference", "id": "PHASE 1", "label": "PHASE 1", "shape": "dot", "size": 10, "source_id": "a65c0f1eae6e779357739df141f75d36", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Phase 2 is a concept used in the TranAD model, representing the second phase of inference", "id": "PHASE 2", "label": "PHASE 2", "shape": "dot", "size": 10, "source_id": "a65c0f1eae6e779357739df141f75d36", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Focus score is a concept used in the TranAD model, representing the deviations of the reconstructed output from the given input", "id": "FOCUS SCORE", "label": "FOCUS SCORE", "shape": "dot", "size": 10, "source_id": "a65c0f1eae6e779357739df141f75d36", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Meta step-size is a parameter used in the meta-optimization process", "id": "META STEP-SIZE", "label": "META STEP-SIZE", "shape": "dot", "size": 10, "source_id": "6ea15432a841705c2e74cbc01f6004e9", "type": "PARAMETER"}, {"color": "#97c2fc", "description": "The Generalized Pareto Distribution is a statistical distribution used to fit the data distribution and choose the threshold automatically", "id": "GENERALIZED PARETO DISTRIBUTION", "label": "GENERALIZED PARETO DISTRIBUTION", "shape": "dot", "size": 10, "source_id": "1b51ec337efd822ca3a0b3eb819c1b91", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The Peak Over Threshold method is a statistical method used to choose the threshold automatically and dynamically", "id": "PEAK OVER THRESHOLD", "label": "PEAK OVER THRESHOLD", "shape": "dot", "size": 10, "source_id": "1b51ec337efd822ca3a0b3eb819c1b91", "type": "METHOD"}, {"color": "#97c2fc", "description": "Recordings of the electrical activity of the heart", "id": "ELECTROCARDIOGRAM RECORDINGS", "label": "ELECTROCARDIOGRAM RECORDINGS", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "DATA"}, {"color": "#97c2fc", "description": "", "id": "MIT-BIH SUPRAVENTRICULAR ARRHYTHMIA DATABASE", "label": "MIT-BIH SUPRAVENTRICULAR ARRHYTHMIA DATABASE", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": ""}, {"color": "#97c2fc", "description": "Individuals who have undergone electrocardiogram recordings", "id": "PATIENTS", "label": "PATIENTS", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "ENTITY"}, {"color": "#97c2fc", "description": "Abnormalities or irregularities in the electrocardiogram recordings", "id": "ANOMALIES", "label": "ANOMALIES", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Abnormal heartbeats originating from the atria", "id": "SUPRAVENTRICULAR CONTRACTIONS", "label": "SUPRAVENTRICULAR CONTRACTIONS", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Abnormal heartbeats occurring before the normal heartbeat", "id": "PREMATURE HEARTBEATS", "label": "PREMATURE HEARTBEATS", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "A dataset of soil samples and telemetry information using the Mars rover by NASA", "id": "SOIL MOISTURE ACTIVE PASSIVE DATASET", "label": "SOIL MOISTURE ACTIVE PASSIVE DATASET", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "DATABASE"}, {"color": "#97c2fc", "description": "Samples of soil collected from various locations", "id": "SOIL SAMPLES", "label": "SOIL SAMPLES", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "DATA"}, {"color": "#97c2fc", "description": "Data collected from the Mars rover\u0027s sensors and instruments", "id": "TELEMETRY INFORMATION", "label": "TELEMETRY INFORMATION", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "DATA"}, {"color": "#97c2fc", "description": "A robotic vehicle designed to explore the planet Mars", "id": "MARS ROVER", "label": "MARS ROVER", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "ENTITY"}, {"color": "#97c2fc", "description": "The United States space agency responsible for the Mars rover mission", "id": "NASA", "label": "NASA", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "A dataset similar to the SMAP dataset but corresponds to the sensor and actuator data for the Mars rover itself", "id": "MARS SCIENCE LABORATORY DATASET", "label": "MARS SCIENCE LABORATORY DATASET", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "DATABASE"}, {"color": "#97c2fc", "description": "Data collected from the Mars rover\u0027s sensors and actuators", "id": "SENSOR AND ACTUATOR DATA", "label": "SENSOR AND ACTUATOR DATA", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "DATA"}, {"color": "#97c2fc", "description": "Data used to evaluate the performance of the TranAD model", "id": "VALIDATION DATA", "label": "VALIDATION DATA", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "DATA"}, {"color": "#97c2fc", "description": "A deep learning model for parallel processing", "id": "PARALLEL TRANSFORMER", "label": "PARALLEL TRANSFORMER", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "MODEL"}, {"color": "#97c2fc", "description": "A type of central processing unit", "id": "INTEL I7-10700K CPU", "label": "INTEL I7-10700K CPU", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "ENTITY"}, {"color": "#97c2fc", "description": "A type of random access memory", "id": "64GB RAM", "label": "64GB RAM", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "ENTITY"}, {"color": "#97c2fc", "description": "A type of graphics processing unit", "id": "NVIDIA RTX 3080", "label": "NVIDIA RTX 3080", "shape": "dot", "size": 10, "source_id": "fa91ecd90e1c12d64176de581c6220c7", "type": "ENTITY"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity \"AUC*\" is a metric used to evaluate the performance of anomaly detection models. This metric is utilized to assess the effectiveness of anomaly detection models, as indicated by its presence in a table. The use of \"AUC*\" in this context suggests that it is a key performance indicator for anomaly detection models, allowing researchers and practitioners to evaluate and compare the performance of different models.\n\nIt is worth noting that the language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. This further supports the conclusion that \"AUC*\" is a metric used in the context of anomaly detection models, as described in English-language academic papers and research.\n\nOverall, the summary provides a clear understanding of the entity \"AUC*\" and its role in evaluating the performance of anomaly detection models.", "id": "AUC*", "label": "AUC*", "shape": "dot", "size": 10, "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,2b44aebc638544dcf835db30c4270d09", "type": "METRIC"}, {"color": "#97c2fc", "description": "Friedman test is a statistical test used to compare the performance of multiple models", "id": "FREIDMAN TEST", "label": "FREIDMAN TEST", "shape": "dot", "size": 10, "source_id": "70a97858b727e5af1466ae5eb9183921", "type": "STATISTICAL TEST"}, {"color": "#97c2fc", "description": "Wilcoxon paired signed-rank test is a statistical test used to compare the performance of multiple models", "id": "WILCOXON PAIRED SIGNED-RANK TEST", "label": "WILCOXON PAIRED SIGNED-RANK TEST", "shape": "dot", "size": 10, "source_id": "70a97858b727e5af1466ae5eb9183921", "type": "STATISTICAL TEST"}, {"color": "#97c2fc", "description": "Critical difference analysis is a statistical analysis used to assess the significance of the differences among the performance of the models", "id": "CRITICAL DIFFERENCE ANALYSIS", "label": "CRITICAL DIFFERENCE ANALYSIS", "shape": "dot", "size": 10, "source_id": "70a97858b727e5af1466ae5eb9183921", "type": "STATISTICAL ANALYSIS"}, {"color": "#97c2fc", "description": "H@100% is a metric used to evaluate the performance of anomaly detection methods", "id": "H@100%", "label": "H@100%", "shape": "dot", "size": 10, "source_id": "1413d358c623ac2d4c70be6547eb218b", "type": "METRIC"}, {"color": "#97c2fc", "description": "H@150% is a metric used to evaluate the performance of anomaly detection methods", "id": "H@150%", "label": "H@150%", "shape": "dot", "size": 10, "source_id": "1413d358c623ac2d4c70be6547eb218b", "type": "METRIC"}, {"color": "#97c2fc", "description": "N@100% is a metric used to evaluate the performance of anomaly detection methods", "id": "N@100%", "label": "N@100%", "shape": "dot", "size": 10, "source_id": "1413d358c623ac2d4c70be6547eb218b", "type": "METRIC"}, {"color": "#97c2fc", "description": "N@150% is a metric used to evaluate the performance of anomaly detection methods", "id": "N@150%", "label": "N@150%", "shape": "dot", "size": 10, "source_id": "1413d358c623ac2d4c70be6547eb218b", "type": "METRIC"}, {"color": "#97c2fc", "description": "ROC/AUC score is a measure of model performance, calculated as the area under the receiver operating characteristic curve", "id": "ROC/AUC SCORE", "label": "ROC/AUC SCORE", "shape": "dot", "size": 10, "source_id": "9b35a2c607e0f4b8cbc9179f424f180a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Training times refer to the amount of time it takes to train a model", "id": "TRAINING TIMES", "label": "TRAINING TIMES", "shape": "dot", "size": 10, "source_id": "9b35a2c607e0f4b8cbc9179f424f180a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Root causes refer to the underlying reasons or explanations for a particular phenomenon or event", "id": "ROOT CAUSES", "label": "ROOT CAUSES", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Our implementation is a software used for numerical computation and data analysis", "id": "IMPLEMENTATION", "label": "IMPLEMENTATION", "shape": "dot", "size": 10, "source_id": "0e3a4048d3d693cb8fd969fd606f1e8a", "type": "SOFTWARE"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entity in question is \"MATLAB CODE.\" \n\nThe description provided indicates that \"MATLAB code is an implementation, as indicated by the use of the abbreviation \u0027\ud835\udc65\u0027 in the text.\" \n\nGiven that the text is written in English and contains technical terms and mathematical equations, it can be inferred that the MATLAB code is likely a technical implementation of a mathematical or computational concept, possibly related to time series analysis or long-term series forecasting, given the mention of \"time series\" in the text.\n\nOverall, the summary suggests that the MATLAB code is a technical implementation of a mathematical or computational concept, possibly related to time series analysis or long-term series forecasting, as indicated by the use of technical terms and mathematical equations in the text.", "id": "MATLAB CODE", "label": "MATLAB CODE", "shape": "dot", "size": 10, "source_id": "0e3a4048d3d693cb8fd969fd606f1e8a,f6e57fa18831bcc732a631240536777a", "type": "IMPLEMENTATION"}, {"color": "#97c2fc", "description": "", "id": "SERIES DATA", "label": "SERIES DATA", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": ""}, {"color": "#97c2fc", "description": "State-of-the-art models refer to the best-performing models in a particular field or domain", "id": "STATE-OF-THE-ART MODELS", "label": "STATE-OF-THE-ART MODELS", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "MODEL"}, {"color": "#97c2fc", "description": "Bidirectional neural networks are a type of neural network architecture that allows for model generalization to diverse temporal trends in data", "id": "BIDIRECTIONAL NEURAL NETWORKS", "label": "BIDIRECTIONAL NEURAL NETWORKS", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "MODEL"}, {"color": "#97c2fc", "description": "Cost-benefit analysis is a technique used to evaluate the costs and benefits of a particular decision or action", "id": "COST-BENEFIT ANALYSIS", "label": "COST-BENEFIT ANALYSIS", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "GitHub is a platform used to host and share code and other software", "id": "GITHUB", "label": "GITHUB", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "PLATFORM"}, {"color": "#97c2fc", "description": "BSD-3 licence is a type of open-source licence used to govern the use and distribution of software", "id": "BSD-3 LICENCE", "label": "BSD-3 LICENCE", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "LICENCE"}, {"color": "#97c2fc", "description": "MATLAB is a programming language used to implement the original MERLIN code", "id": "MATLAB", "label": "MATLAB", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "LANGUAGE"}, {"color": "#97c2fc", "description": "Grid-search is a technique used to find optimal hyperparameters by maximizing a particular metric", "id": "GRID-SEARCH", "label": "GRID-SEARCH", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Ours is the proposed model or system being compared to the original model", "id": "OURS", "label": "OURS", "shape": "dot", "size": 10, "source_id": "b01517b8d09acabed7145d9ffa4a409b", "type": "MODEL"}, {"color": "#97c2fc", "description": "", "id": "ORIGINAL", "label": "ORIGINAL", "shape": "dot", "size": 10, "source_id": "b01517b8d09acabed7145d9ffa4a409b", "type": ""}, {"color": "#97c2fc", "description": "Deviation refers to the difference or variation between the original and proposed models", "id": "DEVIATION", "label": "DEVIATION", "shape": "dot", "size": 10, "source_id": "b01517b8d09acabed7145d9ffa4a409b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Our implementation is an implementation, as indicated by the use of the abbreviation \"\ud835\udc66\" in the text", "id": "OUR IMPLEMENTATION", "label": "OUR IMPLEMENTATION", "shape": "dot", "size": 10, "source_id": "f6e57fa18831bcc732a631240536777a", "type": "IMPLEMENTATION"}, {"color": "#97c2fc", "description": "Server Machine Dataset is a public benchmark dataset used for time series anomaly detection", "id": "SERVER MACHINE DATASET", "label": "SERVER MACHINE DATASET", "shape": "dot", "size": 10, "source_id": "59e8e02df5f942071e733def7884b457", "type": "DATASET"}, {"color": "#97c2fc", "description": "Mars Science Laboratory Rover is a public benchmark dataset used for time series anomaly detection", "id": "MARS SCIENCE LABORATORY ROVER", "label": "MARS SCIENCE LABORATORY ROVER", "shape": "dot", "size": 10, "source_id": "59e8e02df5f942071e733def7884b457", "type": "DATASET"}, {"color": "#97c2fc", "description": "Pooled Server Metrics is a public benchmark dataset used for time series anomaly detection", "id": "POOLED SERVER METRICS", "label": "POOLED SERVER METRICS", "shape": "dot", "size": 10, "source_id": "59e8e02df5f942071e733def7884b457", "type": "DATASET"}, {"color": "#97c2fc", "description": "Sliding windows is a technique used in data preprocessing, as indicated by the description of segmenting signals into non-overlapped windows", "id": "SLIDING WINDOWS", "label": "SLIDING WINDOWS", "shape": "dot", "size": 10, "source_id": "9ec74c919ec0aea919a7f2916213ea16", "type": "PROCESS"}, {"color": "#97c2fc", "description": "AUC-PR is a metric used in the research, as indicated by the description of its calculation and use in evaluating the performance of the RESTAD model", "id": "AUC-PR", "label": "AUC-PR", "shape": "dot", "size": 10, "source_id": "9ec74c919ec0aea919a7f2916213ea16", "type": "METRIC"}, {"color": "#97c2fc", "description": "VUS-ROC is a metric used in the research, as indicated by the description of its calculation and use in evaluating the performance of the RESTAD model", "id": "VUS-ROC", "label": "VUS-ROC", "shape": "dot", "size": 10, "source_id": "9ec74c919ec0aea919a7f2916213ea16", "type": "METRIC"}, {"color": "#97c2fc", "description": "VUS-PR is a metric used in the research, as indicated by the description of its calculation and use in evaluating the performance of the RESTAD model", "id": "VUS-PR", "label": "VUS-PR", "shape": "dot", "size": 10, "source_id": "9ec74c919ec0aea919a7f2916213ea16", "type": "METRIC"}, {"color": "#97c2fc", "description": "Foundation models (FMs) are a class of deep models that are pre-trained on vast amounts of data, thus equipped with a wide range of general knowledge and patterns", "id": "FOUNDATION MODELS (FMS)", "label": "FOUNDATION MODELS (FMS)", "shape": "dot", "size": 10, "source_id": "79b26808691b6333aafce43c5de531f7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "AutoTimes is a pre-trained model used in time series analysis", "id": "AUTOTIMES", "label": "AUTOTIMES", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "UniTime is a pre-trained model used in time series analysis", "id": "UNITIME", "label": "UNITIME", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Yu et al. is a research paper on time series analysis", "id": "YU ET AL.", "label": "YU ET AL.", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "PAPER"}, {"color": "#97c2fc", "description": "Chen et al. is a research paper on time series analysis", "id": "CHEN ET AL.", "label": "CHEN ET AL.", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "PAPER"}, {"color": "#97c2fc", "description": "Xie et al. is a research paper on time series analysis", "id": "XIE ET AL.", "label": "XIE ET AL.", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "PAPER"}, {"color": "#97c2fc", "description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nLIU ET AL. is a research paper that focuses on time series analysis and forecasting. Specifically, the paper discusses the applicability of the encoder-only model to time series forecasting, which is a key aspect of long-term series forecasting. The research paper utilizes various technical terms and mathematical equations, including frequency analysis and multi-head cross-attention, to explore the potential of the encoder-only model in this context. The paper\u0027s formal and academic tone, as well as its use of English-language abbreviations and citations, suggest that it is written in English and is likely a contribution to a technical document or research paper in the field of time series analysis and forecasting.", "id": "LIU ET AL.", "label": "LIU ET AL.", "shape": "dot", "size": 10, "source_id": "7dbc961fe1959f844ebac283498e7fb0,bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "PAPER"}, {"color": "#97c2fc", "description": "Lag-Llama is a pioneering effort as a forecasting foundation model", "id": "LAG-LAGGPT", "label": "LAG-LAGGPT", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "TimeSiam is a pre-trained model used in time series analysis", "id": "TIMESIAM", "label": "TIMESIAM", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "Das et al. is a research paper on time series analysis", "id": "DAS ET AL.", "label": "DAS ET AL.", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "PAPER"}, {"color": "#97c2fc", "description": "UniTS is a pre-trained model used in time series analysis", "id": "UNITSTS", "label": "UNITSTS", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "MTSMAE is a pre-trained model used in time series analysis", "id": "MTSMAE", "label": "MTSMAE", "shape": "dot", "size": 10, "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "type": "MODEL"}, {"color": "#97c2fc", "description": "UniTS is a model mentioned in the text, as indicated by its abbreviation and description", "id": "UNITTS", "label": "UNITTS", "shape": "dot", "size": 10, "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "type": "MODEL"}, {"color": "#97c2fc", "description": "An encoder-decoder model is a type of neural network model that uses an encoder to process input and a decoder to generate output.", "id": "ENCODER-DECODER MODEL", "label": "ENCODER-DECODER MODEL", "shape": "dot", "size": 10, "source_id": "7dbc961fe1959f844ebac283498e7fb0", "type": "MODEL"}, {"color": "#97c2fc", "description": "An encoder-only model is a type of neural network model that uses an encoder to process input.", "id": "ENCODER-ONLY MODEL", "label": "ENCODER-ONLY MODEL", "shape": "dot", "size": 10, "source_id": "7dbc961fe1959f844ebac283498e7fb0", "type": "MODEL"}, {"color": "#97c2fc", "description": "Ansari et al. is a research paper that analyzes the applicability of the encoder-decoder framework to decoder-only models.", "id": "ANSARI ET AL.", "label": "ANSARI ET AL.", "shape": "dot", "size": 10, "source_id": "7dbc961fe1959f844ebac283498e7fb0", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Patching with channel independence is a technique used for time series tokenization", "id": "PATCHING WITH CHANNEL INDEPENDENCE", "label": "PATCHING WITH CHANNEL INDEPENDENCE", "shape": "dot", "size": 10, "source_id": "53f80422fbf85856ecf940d5c2450665", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Pre-training is a technique used to train a machine learning model on a large corpus of data before fine-tuning it for a specific task", "id": "PRE-TRAINING TECHNIQUE", "label": "PRE-TRAINING TECHNIQUE", "shape": "dot", "size": 10, "source_id": "479fc10bea12b01a37fbc5cef21eea76", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Adaptation is a technique used to fine-tune a pre-trained machine learning model for a specific task or dataset", "id": "ADAPTATION TECHNIQUE", "label": "ADAPTATION TECHNIQUE", "shape": "dot", "size": 10, "source_id": "479fc10bea12b01a37fbc5cef21eea76", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The Guangzhou-HKUST(GZ) Joint Funding Program is a funding program that supports research in various fields, including time series analysis", "id": "GUANGZHOU-HKUST(GZ) JOINT FUNDING PROGRAM", "label": "GUANGZHOU-HKUST(GZ) JOINT FUNDING PROGRAM", "shape": "dot", "size": 10, "source_id": "479fc10bea12b01a37fbc5cef21eea76", "type": "PROGRAM"}, {"color": "#97c2fc", "description": "The Guangzhou Municipal Science and Technology Project is a funding program that supports research in various fields, including time series analysis", "id": "GUANGZHOU MUNICIPAL SCIENCE AND TECHNOLOGY PROJECT", "label": "GUANGZHOU MUNICIPAL SCIENCE AND TECHNOLOGY PROJECT", "shape": "dot", "size": 10, "source_id": "479fc10bea12b01a37fbc5cef21eea76", "type": "PROGRAM"}, {"color": "#97c2fc", "description": "Google Research is an organization mentioned in the text, as indicated by the use of the phrase \"Google Research\"", "id": "GOOGLE RESEARCH", "label": "GOOGLE RESEARCH", "shape": "dot", "size": 10, "source_id": "323c49cf157623a685283fcfc9b05491", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Web search queries are a type of time-series data collected from search engine queries", "id": "WEB SEARCH QUERIES", "label": "WEB SEARCH QUERIES", "shape": "dot", "size": 10, "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "type": "DATA"}, {"color": "#97c2fc", "description": "Wikipedia page visits are a type of time-series data collected from user interactions with Wikipedia pages", "id": "WIKIPEDIA PAGE VISITS", "label": "WIKIPEDIA PAGE VISITS", "shape": "dot", "size": 10, "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "type": "DATA"}, {"color": "#97c2fc", "description": "LLAMA-2 is a pre-trained language model", "id": "LLAMA-2", "label": "LLAMA-2", "shape": "dot", "size": 10, "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe INPUT PATCHES are a set of data points that are used as input to the model. Specifically, they are a type of input used in the TimesFM model, as described in the text. This suggests that INPUT PATCHES play a crucial role in the TimesFM model, which is likely a time series forecasting model given the context of the text.\n\nGiven the formal and academic language used in the text, it is likely that the INPUT PATCHES are a key component in a research paper or technical document related to time series forecasting. The use of technical terms such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis\" further supports this conclusion.\n\nOverall, the INPUT PATCHES are a critical input to the TimesFM model, which is likely used for time series forecasting purposes.", "id": "INPUT PATCHES", "label": "INPUT PATCHES", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a,ee8135f4497807724f665a5cdaa775fb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Positional encodings are a type of encoding used in the TimesFM model, as described in the text", "id": "POSITIONAL ENCODINGS", "label": "POSITIONAL ENCODINGS", "shape": "dot", "size": 10, "source_id": "ee8135f4497807724f665a5cdaa775fb", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Time-point refers to a single data point in a time series, often used for forecasting and analysis", "id": "TIME-POINT", "label": "TIME-POINT", "shape": "dot", "size": 10, "source_id": "4ab34d32601d452f14b4a1c31415292d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The M4 monthly dataset is a subset of the TimesFM pretraining dataset", "id": "M4 MONTHLY", "label": "M4 MONTHLY", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The M4 quarterly dataset is a subset of the TimesFM pretraining dataset", "id": "M4 QUARTERLY", "label": "M4 QUARTERLY", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The M4 yearly dataset is a subset of the TimesFM pretraining dataset", "id": "M4 YEARLY", "label": "M4 YEARLY", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The Wiki hourly dataset is a subset of the TimesFM pretraining dataset", "id": "WIKI HOURLY", "label": "WIKI HOURLY", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The Wiki weekly dataset is a subset of the TimesFM pretraining dataset", "id": "WIKI WEEKLY", "label": "WIKI WEEKLY", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The Wiki monthly dataset is a subset of the TimesFM pretraining dataset", "id": "WIKI MONTHLY", "label": "WIKI MONTHLY", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The Trends hourly dataset is a subset of the TimesFM pretraining dataset", "id": "TRENDS HOURLY", "label": "TRENDS HOURLY", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The Trends daily dataset is a subset of the TimesFM pretraining dataset", "id": "TRENDS DAILY", "label": "TRENDS DAILY", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The Trends weekly dataset is a subset of the TimesFM pretraining dataset", "id": "TRENDS WEEKLY", "label": "TRENDS WEEKLY", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "The Trends monthly dataset is a subset of the TimesFM pretraining dataset", "id": "TRENDS MONTHLY", "label": "TRENDS MONTHLY", "shape": "dot", "size": 10, "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "type": "DATASET"}, {"color": "#97c2fc", "description": "SM-GP is a model mentioned in the text, specifically WA13", "id": "SM-GP", "label": "SM-GP", "shape": "dot", "size": 10, "source_id": "2bc70082c142ee2ef5d6a941611505b7", "type": "MODEL"}, {"color": "#97c2fc", "description": "Tensor-cores refer to the processing units used in a TPUv5e6 setup", "id": "TENSOR-CORES", "label": "TENSOR-CORES", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "NNSK22 is a paper that discusses autoregressive decoding in time-series forecasting", "id": "NNSK22", "label": "NNSK22", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "PAPER"}, {"color": "#97c2fc", "description": "DKL+23 is a paper that discusses autoregressive decoding in time-series forecasting", "id": "DKL+23", "label": "DKL+23", "shape": "dot", "size": 10, "source_id": "f7ce89346ea6715560163ef3a630a5df", "type": "PAPER"}, {"color": "#97c2fc", "description": "Police presence refers to the number of law enforcement officers or personnel present in a given area or community", "id": "POLICE PRESENCE", "label": "POLICE PRESENCE", "shape": "dot", "size": 10, "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Probabilistic loss functions are a type of loss function that takes into account uncertainty or probability", "id": "PROBABILISTIC LOSS FUNCTIONS", "label": "PROBABILISTIC LOSS FUNCTIONS", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "MLP structures refer to a type of neural network architecture that uses multiple layers of perceptrons", "id": "MLP STRUCTURES", "label": "MLP STRUCTURES", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Efficient linear state space models refer to a type of model that uses linear equations to describe the behavior of a system", "id": "EFFICIENT LINEAR STATE SPACE MODELS", "label": "EFFICIENT LINEAR STATE SPACE MODELS", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Mamba is a type of efficient linear state space model", "id": "MAMBA", "label": "MAMBA", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "MODEL"}, {"color": "#97c2fc", "description": "Interpretability refers to the ability to understand or explain the behavior or decisions of a model", "id": "INTERPRETABILITY", "label": "INTERPRETABILITY", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "LOCO is a method used to attribute feature importances to different lags in the context supplied to the model", "id": "LOCO", "label": "LOCO", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "METHOD"}, {"color": "#97c2fc", "description": "SHAP is a method used to attribute feature importances to different lags in the context supplied to the model", "id": "SHAP", "label": "SHAP", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "METHOD"}, {"color": "#97c2fc", "description": "GBW+21 is a paper that discusses the use of MAE as a metric", "id": "GBW+21", "label": "GBW+21", "shape": "dot", "size": 10, "source_id": "9b150491b487d5b2da482652e2fe509d", "type": "PAPER"}, {"color": "#97c2fc", "description": "A proper model card is a document that provides a detailed and transparent description of a machine learning model", "id": "PROPER MODEL CARD", "label": "PROPER MODEL CARD", "shape": "dot", "size": 10, "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Pretrain data loader is a tool used to prepare the data for pretraining the models", "id": "PRETRAIN DATA LOADER", "label": "PRETRAIN DATA LOADER", "shape": "dot", "size": 10, "source_id": "39365aee753fb73130c208fdf4046bb7", "type": "TOOL"}, {"color": "#97c2fc", "description": "AirPassengersDataset is a dataset used for time-series forecasting tasks", "id": "AIRPASSSENGERSDATASET", "label": "AIRPASSSENGERSDATASET", "shape": "dot", "size": 10, "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "DATASET"}, {"color": "#97c2fc", "description": "AusBeerDataset is a dataset used for time-series forecasting tasks", "id": "AUSBERRDATASET", "label": "AUSBERRDATASET", "shape": "dot", "size": 10, "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "DATASET"}, {"color": "#97c2fc", "description": "GasRateCO2Dataset is a dataset used for time-series forecasting tasks", "id": "GASRATECO2DATASET", "label": "GASRATECO2DATASET", "shape": "dot", "size": 10, "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "DATASET"}, {"color": "#97c2fc", "description": "MonthlyMilkDataset is a dataset used for time-series forecasting tasks", "id": "MONTHLYMILKDATASET", "label": "MONTHLYMILKDATASET", "shape": "dot", "size": 10, "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "DATASET"}, {"color": "#97c2fc", "description": "SunspotsDataset is a dataset used for time-series forecasting tasks", "id": "SUNSPOTSDATASET", "label": "SUNSPOTSDATASET", "shape": "dot", "size": 10, "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "DATASET"}, {"color": "#97c2fc", "description": "WineDataset is a dataset used for time-series forecasting tasks", "id": "WINEDATASET", "label": "WINEDATASET", "shape": "dot", "size": 10, "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "DATASET"}, {"color": "#97c2fc", "description": "WoolyDataset is a dataset used for time-series forecasting tasks", "id": "WOOLYDATASET", "label": "WOOLYDATASET", "shape": "dot", "size": 10, "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "DATASET"}, {"color": "#97c2fc", "description": "HeartRateDataset is a dataset used for time-series forecasting tasks", "id": "HEARTRATEDATASET", "label": "HEARTRATEDATASET", "shape": "dot", "size": 10, "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "type": "DATASET"}, {"color": "#97c2fc", "description": "FFNN is a model used for evaluation", "id": "FFNN", "label": "FFNN", "shape": "dot", "size": 10, "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "type": "MODEL"}, {"color": "#97c2fc", "description": "Traffic hourly dataset refers to a specific dataset used for time series forecasting and analysis", "id": "TRAFFIC HOURLY DATASET", "label": "TRAFFIC HOURLY DATASET", "shape": "dot", "size": 10, "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "type": "DATASET"}, {"color": "#97c2fc", "description": "Seoul is a city in Republic of Korea", "id": "SEOUL", "label": "SEOUL", "shape": "dot", "size": 10, "source_id": "3e0985c172a09bb6c99e305b9f40a513", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Republic of Korea is a country in East Asia", "id": "REPUBLIC OF KOREA", "label": "REPUBLIC OF KOREA", "shape": "dot", "size": 10, "source_id": "3e0985c172a09bb6c99e305b9f40a513", "type": "LOCATION"}, {"color": "#97c2fc", "description": "Yungi Jeong is an author of the paper", "id": "YUNGI JEONG", "label": "YUNGI JEONG", "shape": "dot", "size": 10, "source_id": "3e0985c172a09bb6c99e305b9f40a513", "type": "PERSON"}, {"color": "#97c2fc", "description": "Eunseok Yang is an author of the paper", "id": "EUNSEOK YANG", "label": "EUNSEOK YANG", "shape": "dot", "size": 10, "source_id": "3e0985c172a09bb6c99e305b9f40a513", "type": "PERSON"}, {"color": "#97c2fc", "description": "Jung Hyun Ryu is an author of the paper", "id": "JUNG HYUN RYU", "label": "JUNG HYUN RYU", "shape": "dot", "size": 10, "source_id": "3e0985c172a09bb6c99e305b9f40a513", "type": "PERSON"}, {"color": "#97c2fc", "description": "Imseong Park is an author of the paper", "id": "IMSEONG PARK", "label": "IMSEONG PARK", "shape": "dot", "size": 10, "source_id": "3e0985c172a09bb6c99e305b9f40a513", "type": "PERSON"}, {"color": "#97c2fc", "description": "Myungjoo Kang is an author of the paper", "id": "MYUNGJOO KANG", "label": "MYUNGJOO KANG", "shape": "dot", "size": 10, "source_id": "3e0985c172a09bb6c99e305b9f40a513", "type": "PERSON"}, {"color": "#97c2fc", "description": "ViT is a type of Transformer model used for computer vision tasks", "id": "VISION TRANSFORMER (VIT)", "label": "VISION TRANSFORMER (VIT)", "shape": "dot", "size": 10, "source_id": "1303ca4c43652bb8052df34d21c78eca", "type": "MODEL"}, {"color": "#97c2fc", "description": "Dimension is the number of features in an attention head", "id": "DIMENSION", "label": "DIMENSION", "shape": "dot", "size": 10, "source_id": "a7604286fb84b26c53950861f9aca4b1", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Lai et al. (2021) is a research paper or publication", "id": "LAI ET AL. (2021)", "label": "LAI ET AL. (2021)", "shape": "dot", "size": 10, "source_id": "6917a14af275e30abe2d30a8f8a9a4e6", "type": "PAPER"}, {"color": "#97c2fc", "description": "Workshop refers to a smaller conference or meeting within a larger conference", "id": "WORKSHOP", "label": "WORKSHOP", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23", "type": "EVENT"}, {"color": "#97c2fc", "description": "Institute of Information \u0026 Communications Technology Planning \u0026 Evaluation is a government agency that supports research and development in the field of information and communications technology", "id": "INSTITUTE OF INFORMATION \u0026 COMMUNICATIONS TECHNOLOGY PLANNING \u0026 EVALUATION", "label": "INSTITUTE OF INFORMATION \u0026 COMMUNICATIONS TECHNOLOGY PLANNING \u0026 EVALUATION", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Korea government refers to the government of South Korea", "id": "KOREA GOVERNMENT", "label": "KOREA GOVERNMENT", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "MSIT refers to the Ministry of Science and ICT, a government agency in South Korea", "id": "MSIT", "label": "MSIT", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "NRF refers to the National Research Foundation of Korea, a government agency that supports research and development in the field of science and technology", "id": "NRF", "label": "NRF", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "ICT R\u0026D program refers to a research and development program in the field of information and communications technology", "id": "ICT R\u0026D PROGRAM", "label": "ICT R\u0026D PROGRAM", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23", "type": "PROGRAM"}, {"color": "#97c2fc", "description": "MOTIE refers to the Ministry of Trade, Industry and Energy, a government agency in South Korea", "id": "MOTIE", "label": "MOTIE", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "Artificial intelligence graduate school program refers to a program that provides education and training in the field of artificial intelligence", "id": "ARTIFICIAL INTELLIGENCE GRADUATE SCHOOL PROGRAM", "label": "ARTIFICIAL INTELLIGENCE GRADUATE SCHOOL PROGRAM", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23", "type": "PROGRAM"}, {"color": "#97c2fc", "description": "Synthesizing criteria refers to the rules or guidelines used to create a synthetic dataset", "id": "SYNTHESIZING CRITERIA", "label": "SYNTHESIZING CRITERIA", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Lai et al. refers to a research paper or authors", "id": "LAI ET AL.", "label": "LAI ET AL.", "shape": "dot", "size": 10, "source_id": "71f873c12230e6ede47583d81eb85e23", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Kernel functions are used to generate synthetic time series by randomly composing them", "id": "KERNEL FUNCTIONS", "label": "KERNEL FUNCTIONS", "shape": "dot", "size": 10, "source_id": "f7e3207706b170296cb036538a709689", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Real-valued distribution heads refer to a type of distribution used in time series analysis, often used in deep learning models", "id": "REAL-VALUED DISTRIBUTION HEADS", "label": "REAL-VALUED DISTRIBUTION HEADS", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Regression via classification refers to a technique used in time series analysis, often used in deep learning models", "id": "REGRESSION VIA CLASSIFICATION", "label": "REGRESSION VIA CLASSIFICATION", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "CNN-based Inception model is a type of deep learning model used in time series analysis", "id": "CNN-BASED INCPTION MODEL", "label": "CNN-BASED INCPTION MODEL", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8", "type": "MODEL"}, {"color": "#97c2fc", "description": "Wu et al. (2023) is a research paper that develops a task-generic backbone based on the Inception model", "id": "WU ET AL. (2023)", "label": "WU ET AL. (2023)", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8", "type": "PAPER"}, {"color": "#97c2fc", "description": "Zhou et al. (2023a) is a research paper that studies general purpose models applicable across time series tasks", "id": "ZHOU ET AL. (2023A)", "label": "ZHOU ET AL. (2023A)", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8", "type": "PAPER"}, {"color": "#97c2fc", "description": "Szegedy et al. (2015) is a research paper that introduces the Inception model", "id": "SZEGEDY ET AL. (2015)", "label": "SZEGEDY ET AL. (2015)", "shape": "dot", "size": 10, "source_id": "6eb4c16edf2eedfd03721efb199478d8", "type": "PAPER"}, {"color": "#97c2fc", "description": "The covariance function defines the joint variability of the function values at an arbitrary pair of points", "id": "COVARIANCE FUNCTION", "label": "COVARIANCE FUNCTION", "shape": "dot", "size": 10, "source_id": "53dbeea6d05c3695460c71f89f468def", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Aggregated relative scores are a measure used to evaluate the performance of models on Benchmark II", "id": "AGGREGATED RELATIVE SCORES", "label": "AGGREGATED RELATIVE SCORES", "shape": "dot", "size": 10, "source_id": "532a6d434dab611a80aef1e94dd2fb45", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "AutoARIMA is a model that places 9th in terms of point forecasting performance", "id": "AUTO-ARIMA", "label": "AUTO-ARIMA", "shape": "dot", "size": 10, "source_id": "532a6d434dab611a80aef1e94dd2fb45", "type": "MODEL"}, {"color": "#97c2fc", "description": "AutoETS is a model that places 11th in terms of point forecasting performance", "id": "AUTO-ETS", "label": "AUTO-ETS", "shape": "dot", "size": 10, "source_id": "532a6d434dab611a80aef1e94dd2fb45", "type": "MODEL"}, {"color": "#97c2fc", "description": "Synthetic data proportion refers to the percentage of synthetic data used in a model\u0027s training data", "id": "SYNTHETIC DATA PROPORTION", "label": "SYNTHETIC DATA PROPORTION", "shape": "dot", "size": 10, "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Appendix D is a section of the document that discusses the properties of metrics", "id": "APPENDIX D", "label": "APPENDIX D", "shape": "dot", "size": 10, "source_id": "a90c6ca26542ea5935f9d8d5bf3688a9", "type": "DOCUMENT"}, {"color": "#97c2fc", "description": "Sine wave refers to a type of wave that oscillates at a constant frequency", "id": "SINE WAVE", "label": "SINE WAVE", "shape": "dot", "size": 10, "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Multi-modal refers to a distribution that has multiple modes or peaks", "id": "MULTI-MODAL", "label": "MULTI-MODAL", "shape": "dot", "size": 10, "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Kernel density estimate refers to a method for estimating the underlying probability distribution of a set of data", "id": "KERNEL DENSITY ESTIMATE", "label": "KERNEL DENSITY ESTIMATE", "shape": "dot", "size": 10, "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Imry Kissos is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "IMRY KISSOS", "label": "IMRY KISSOS", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Laurent Callot is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "LAURENT CALLOT", "label": "LAURENT CALLOT", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Baris Kurt is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "BARIS KURT", "label": "BARIS KURT", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Valentin Flunkert is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "VALENTIN FLUNKERT", "label": "VALENTIN FLUNKERT", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "David Salinas is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "DAVID SALINAS", "label": "DAVID SALINAS", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Boran Han is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "BORAN HAN", "label": "BORAN HAN", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Xiaoyong Jin is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "XIAOYONG JIN", "label": "XIAOYONG JIN", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Luke Huan is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "LUKE HUAN", "label": "LUKE HUAN", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Youngsuk Park is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "YOUNGSUK PARK", "label": "YOUNGSUK PARK", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Gaurav Gupta is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "GAURAV GUPTA", "label": "GAURAV GUPTA", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Karthick Gopalswamy is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "KARTHICK GOPALSWAMY", "label": "KARTHICK GOPALSWAMY", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Tim Januschowski is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "TIM JANUSCHOWSKI", "label": "TIM JANUSCHOWSKI", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Jan Gasthaus is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "JAN GASTHAUS", "label": "JAN GASTHAUS", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Bing Xiang is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "BING XIANG", "label": "BING XIANG", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Kashif Rasul is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "KASHIF RASUL", "label": "KASHIF RASUL", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Juba Nait Saada is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "JUBA NAIT SAADA", "label": "JUBA NAIT SAADA", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Matthias Karlbauer is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "MATTHIAS KARLBAUER", "label": "MATTHIAS KARLBAUER", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Mononito Goswami is a researcher who contributed to this work with insightful discussions and valuable feedback", "id": "MONONITO GOSWAMI", "label": "MONONITO GOSWAMI", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96", "type": "PERSON"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nGerald Woo is a researcher who has made significant contributions to the field of time-series forecasting. He is the author of the paper \"Etsformer: Exponential smoothing transformers for time-series forecasting,\" which showcases his expertise in this area. Additionally, Gerald Woo has provided valuable feedback and insights through his discussions, further highlighting his involvement in the research work. His work on Etsformer demonstrates his ability to develop innovative solutions for time-series forecasting, and his contributions to the field are expected to have a lasting impact.", "id": "GERALD WOO", "label": "GERALD WOO", "shape": "dot", "size": 10, "source_id": "6c273e9f841addeed2a33240ddaa3d96,8c4fb3f97d731ab00c60399045cd97bd", "type": "PERSON"}, {"color": "#97c2fc", "description": "Jasper Schulz is an author of the paper \"GluonTS: Probabilistic and Neural Time Series Modeling in Python\"", "id": "JASPER SCHULZ", "label": "JASPER SCHULZ", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Rob J. Hyndman is an author of the paper \"The tourism forecasting competition\"", "id": "ROB J HYNDMAN", "label": "ROB J HYNDMAN", "shape": "dot", "size": 10, "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Variables are values or quantities that can change or vary, often used in statistical analysis or modeling", "id": "VARIABLES", "label": "VARIABLES", "shape": "dot", "size": 10, "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Top-100k English Wikipedia articles refer to the top 100k most viewed English Wikipedia articles", "id": "TOP-100K ENGLISH WIKIPEDIA ARTICLES", "label": "TOP-100K ENGLISH WIKIPEDIA ARTICLES", "shape": "dot", "size": 10, "source_id": "2565ae205d4c98342168bb67a4f7a309", "type": "DATA SET"}, {"color": "#97c2fc", "description": "N-BEATS is a deep learning model used for forecasting", "id": "NB-EATS", "label": "NB-EATS", "shape": "dot", "size": 10, "source_id": "2565ae205d4c98342168bb67a4f7a309", "type": "MODEL"}, {"color": "#97c2fc", "description": "ForecastPFN7 is a deep learning model used for forecasting", "id": "FORECASTPFN7", "label": "FORECASTPFN7", "shape": "dot", "size": 10, "source_id": "2565ae205d4c98342168bb67a4f7a309", "type": "MODEL"}, {"color": "#97c2fc", "description": "NVIDIA V100 is a hardware component used for deep learning", "id": "NVIDIA V100", "label": "NVIDIA V100", "shape": "dot", "size": 10, "source_id": "2565ae205d4c98342168bb67a4f7a309", "type": "HARDWARE"}, {"color": "#97c2fc", "description": "V100 16GB is a hardware component used for deep learning", "id": "V100 16GB", "label": "V100 16GB", "shape": "dot", "size": 10, "source_id": "2565ae205d4c98342168bb67a4f7a309", "type": "HARDWARE"}, {"color": "#97c2fc", "description": "Intel-based EC2 instances are software libraries used for cloud computing", "id": "INTEL-BASED EC2 INSTANCES", "label": "INTEL-BASED EC2 INSTANCES", "shape": "dot", "size": 10, "source_id": "2565ae205d4c98342168bb67a4f7a309", "type": "SOFTWARE"}, {"color": "#97c2fc", "description": "Stationarity is a characteristic of time series that TimeGPT can handle", "id": "STATIONARITY", "label": "STATIONARITY", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "CHARACTERISTIC"}, {"color": "#97c2fc", "description": "Heteroscedasticity is a characteristic of time series that TimeGPT can handle", "id": "HETEROSCEDASTICITY", "label": "HETEROSCEDASTICITY", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "CHARACTERISTIC"}, {"color": "#97c2fc", "description": "Publicly available time series is a dataset used to train TimeGPT", "id": "PUBLICLY AVAILABLE TIME SERIES", "label": "PUBLICLY AVAILABLE TIME SERIES", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "DATASET"}, {"color": "#97c2fc", "description": "Demographics is a domain that TimeGPT can handle", "id": "DEMOGRAPHICS", "label": "DEMOGRAPHICS", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "IoT sensor data is a domain that TimeGPT can handle", "id": "IOT SENSOR DATA", "label": "IOT SENSOR DATA", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Web traffic is a domain that TimeGPT can handle", "id": "WEB TRAFFIC", "label": "WEB TRAFFIC", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Sales is a domain that TimeGPT can handle", "id": "SALES", "label": "SALES", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Banking is a domain that TimeGPT can handle", "id": "BANKING", "label": "BANKING", "shape": "dot", "size": 10, "source_id": "518bfcd6711530089fe3914ca16459c2", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Robustness refers to a model\u0027s ability to perform well in a wide range of scenarios", "id": "ROBUSTNESS", "label": "ROBUSTNESS", "shape": "dot", "size": 10, "source_id": "42e1bb44edbe787e104f589e74b95d6c", "type": "PROPERTY"}, {"color": "#97c2fc", "description": "Generalization refers to a model\u0027s ability to perform well on new, unseen data", "id": "GENERALIZATION", "label": "GENERALIZATION", "shape": "dot", "size": 10, "source_id": "42e1bb44edbe787e104f589e74b95d6c", "type": "PROPERTY"}, {"color": "#97c2fc", "description": "Autocorrelation refers to the correlation between a time series and a lagged version of itself", "id": "AUTOCORRELATION", "label": "AUTOCORRELATION", "shape": "dot", "size": 10, "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Vaswani et al. (2017) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Vaswani et al.\"", "id": "VASWANI ET AL. (2017)", "label": "VASWANI ET AL. (2017)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Nie et al. (2023a) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Nie et al.\"", "id": "NIE ET AL. (2023A)", "label": "NIE ET AL. (2023A)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Wu et al. (2020a;b) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Wu et al.\"", "id": "WU ET AL. (2020A;B)", "label": "WU ET AL. (2020A;B)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Lim et al. (2021) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Lim et al.\"", "id": "LIM ET AL. (2021)", "label": "LIM ET AL. (2021)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Li et al. (2023) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Li et al.\"", "id": "LI ET AL. (2023)", "label": "LI ET AL. (2023)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Ashok et al. (2023) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Ashok et al.\"", "id": "ASHOK ET AL. (2023)", "label": "ASHOK ET AL. (2023)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Oreshkin et al. (2020a) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Oreshkin et al.\"", "id": "ORESHKIN ET AL. (2020A)", "label": "ORESHKIN ET AL. (2020A)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Zhou et al. (2021a) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Zhou et al.\"", "id": "ZHOU ET AL. (2021A)", "label": "ZHOU ET AL. (2021A)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Wu et al. (2021) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Wu et al.\"", "id": "WU ET AL. (2021)", "label": "WU ET AL. (2021)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Woo et al. (2023) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Woo et al.\"", "id": "WOO ET AL. (2023)", "label": "WOO ET AL. (2023)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Liu et al. (2022b) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Liu et al.\"", "id": "LIU ET AL. (2022B)", "label": "LIU ET AL. (2022B)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Zhou et al. (2022) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Zhou et al.\"", "id": "ZHOU ET AL. (2022)", "label": "ZHOU ET AL. (2022)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Liu et al. (2022a) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Liu et al.\"", "id": "LIU ET AL. (2022A)", "label": "LIU ET AL. (2022A)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Ni et al. (2023) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Ni et al.\"", "id": "NI ET AL. (2023)", "label": "NI ET AL. (2023)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Li et al. (2019) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Li et al.\"", "id": "LI ET AL. (2019)", "label": "LI ET AL. (2019)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Gulati et al. (2020) is an academic paper mentioned in the text, as indicated by the use of the phrase \"Gulati et al.\"", "id": "GULATI ET AL. (2020)", "label": "GULATI ET AL. (2020)", "shape": "dot", "size": 10, "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "type": "ACADEMIC PAPER"}, {"color": "#97c2fc", "description": "Protein design is a scientific domain where foundation models have been applied", "entity_type": "DOMAIN", "id": "PROTEIN DESIGN", "label": "PROTEIN DESIGN", "shape": "dot", "size": 10, "source_id": "c4e47033b8ecc85beacde9d560e66062", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Web data is a domain where foundation models have been applied", "id": "WEB DATA", "label": "WEB DATA", "shape": "dot", "size": 10, "source_id": "c4e47033b8ecc85beacde9d560e66062", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Scientific domains are areas where foundation models have been applied", "id": "SCIENTIFIC DOMAINS", "label": "SCIENTIFIC DOMAINS", "shape": "dot", "size": 10, "source_id": "c4e47033b8ecc85beacde9d560e66062", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "A parametric distribution is used to model the unknown joint distribution of the future values of a time series", "id": "PARAMETRIC DISTRIBUTION", "label": "PARAMETRIC DISTRIBUTION", "shape": "dot", "size": 10, "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "An autoregressive model is used to approximate the distribution in Eq. (2)", "id": "AUTOREGRESSIVE MODEL", "label": "AUTOREGRESSIVE MODEL", "shape": "dot", "size": 10, "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "type": "MODEL"}, {"color": "#97c2fc", "description": "The chain rule of probability is used to approximate the distribution in Eq. (2)", "id": "CHAIN RULE OF PROBABILITY", "label": "CHAIN RULE OF PROBABILITY", "shape": "dot", "size": 10, "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Freq-mask is an algorithm that is used to mask the frequencies of a time series", "id": "FREQ-MASK", "label": "FREQ-MASK", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Transportation is a domain that consists of time series datasets related to transportation", "id": "TRANSPORTATION", "label": "TRANSPORTATION", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Cloud operations is a domain that consists of time series datasets related to cloud operations", "id": "CLOUD OPERATIONS", "label": "CLOUD OPERATIONS", "shape": "dot", "size": 10, "source_id": "00007df4774d6122e3848802a24f9536", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "DynOptTheta is a statistical forecasting method", "id": "DYNOPTTHETA", "label": "DYNOPTTHETA", "shape": "dot", "size": 10, "source_id": "51ee17c1f0212d4c94010d8e376f649b", "type": "MODEL"}, {"color": "#97c2fc", "description": "Downstream dataset refers to the dataset used to evaluate the performance of Lag-Llama", "id": "DOWNSTREAM DATASET", "label": "DOWNSTREAM DATASET", "shape": "dot", "size": 10, "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "type": "DATA"}, {"color": "#97c2fc", "description": "Val is a researcher who contributed to the development of Lag-Llama", "id": "VAL", "label": "VAL", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "PERSON"}, {"color": "#97c2fc", "description": "Transformer-based time series models are a type of model used for time series forecasting tasks", "id": "TRANSFORMER-BASED TIME SERIES MODELS", "label": "TRANSFORMER-BASED TIME SERIES MODELS", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "MODEL"}, {"color": "#97c2fc", "description": "Catch-22 feature-based dataset analysis is a model used for time series forecasting tasks", "id": "CATCH-22 FEATURE-BASED DATASET ANALYSIS", "label": "CATCH-22 FEATURE-BASED DATASET ANALYSIS", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "MODEL"}, {"color": "#97c2fc", "description": "Air quality dataset is a dataset used for training and testing Lag-Llama", "id": "AIR QUALITY DATASET", "label": "AIR QUALITY DATASET", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "DATASET"}, {"color": "#97c2fc", "description": "Huawei dataset is a dataset used for training and testing Lag-Llama", "id": "HUAWEI DATASET", "label": "HUAWEI DATASET", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "DATASET"}, {"color": "#97c2fc", "description": "Azure/Borg/Alibaba datasets are a collection of datasets used for training and testing Lag-Llama", "id": "AZURE/BORG/ALIBABA DATASETS", "label": "AZURE/BORG/ALIBABA DATASETS", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "DATASET"}, {"color": "#97c2fc", "description": "Stochastic Weight Averaging (SWA) is a technique used for improving the performance of models", "id": "STOCHASTIC WEIGHT AVERAGING (SWA)", "label": "STOCHASTIC WEIGHT AVERAGING (SWA)", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Early stopping techniques are a type of technique used for improving the performance of models", "id": "EARLY STOPPING TECHNIQUES", "label": "EARLY STOPPING TECHNIQUES", "shape": "dot", "size": 10, "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "type": "TECHNIQUE"}, {"color": "#97c2fc", "description": "Compute Canada is an organization that provided compute resources for the project", "id": "COMPUTE CANADA", "label": "COMPUTE CANADA", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "ORGANIZATION"}, {"color": "#97c2fc", "description": "The Air Quality UC Irvine Repository dataset is a dataset used in the project", "id": "AIR QUALITY UC IRVINE REPOSITORY DATASET", "label": "AIR QUALITY UC IRVINE REPOSITORY DATASET", "shape": "dot", "size": 10, "source_id": "88d96b5f76042688e9dd745eb822d919", "type": "DATASET"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"BEIJING MULTI-SITE AIR-QUALITY DATASET\" is a comprehensive dataset comprising hourly measurements of six primary air pollutants and six corresponding meteorological variables at various locations in Beijing over a period of four years. This dataset is utilized in a project and serves as a valuable resource for researchers and scientists studying air quality and its associated factors.\n\nThe dataset includes measurements of six primary air pollutants, which are likely to be of significant interest to researchers in the field of air quality. Additionally, the inclusion of six corresponding meteorological variables provides valuable context and allows for a more nuanced understanding of the relationships between air quality and environmental factors.\n\nThe dataset\u0027s focus on hourly measurements over a four-year period suggests that it is well-suited for time series analysis and long-term series forecasting. This could be particularly useful for identifying trends and patterns in air quality over time, as well as for predicting future air quality conditions.\n\nOverall, the \"BEIJING MULTI-SITE AIR-QUALITY DATASET\" is a valuable resource for researchers and scientists seeking to understand and improve air quality in Beijing and beyond. Its comprehensive nature and focus on hourly measurements make it an ideal dataset for a range of applications, from time series analysis to long-term series forecasting.", "id": "BEIJING MULTI-SITE AIR-QUALITY DATASET", "label": "BEIJING MULTI-SITE AIR-QUALITY DATASET", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6,88d96b5f76042688e9dd745eb822d919", "type": "DATASET"}, {"color": "#97c2fc", "description": "The ETTh1, ETTh2, ETTm1, ETTm2 datasets contain 2 years worth of data obtained from two Electricity Transformers at hourly and 15-minute frequencies curated to help predict if electrical transformers\u2019 oil is at a safe temperature", "id": "ETTH1, ETTH2, ETTM1, ETTM2 DATASETS", "label": "ETTH1, ETTH2, ETTM1, ETTM2 DATASETS", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "DATASET"}, {"color": "#97c2fc", "description": "The Huawei cloud datasets contain serverless traces", "id": "HUAWEI CLOUD DATASETS", "label": "HUAWEI CLOUD DATASETS", "shape": "dot", "size": 10, "source_id": "85902d07a5ac3acacd692810072fa1c6", "type": "DATASET"}, {"color": "#97c2fc", "description": "An embedding layer is a type of layer used in the model to transform the input data", "id": "EMBEDDING LAYER", "label": "EMBEDDING LAYER", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Condensed text prototypes are a set of data points that are used to reprogram the input patches", "id": "CONDENSED TEXT PROTOTYPES", "label": "CONDENSED TEXT PROTOTYPES", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a", "type": "DATA"}, {"color": "#97c2fc", "description": "Prompt prefixes are a set of data points that are used to direct the transformation of input patches", "id": "PROMPT PREFIXES", "label": "PROMPT PREFIXES", "shape": "dot", "size": 10, "source_id": "3174231a67593609c727151c9df31d0a", "type": "DATA"}, {"color": "#97c2fc", "description": "LLAMA 32 is a variant of the LLAMA model with 32 parameters", "id": "LLAMA 32", "label": "LLAMA 32", "shape": "dot", "size": 10, "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "type": "MODEL"}, {"color": "#97c2fc", "description": "LLAMA 8 is a variant of the LLAMA model with 8 parameters", "id": "LLAMA 8", "label": "LLAMA 8", "shape": "dot", "size": 10, "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "type": "MODEL"}, {"color": "#97c2fc", "description": "LLM (32) is a variant of the LLM model with 32 parameters", "id": "LLM (32)", "label": "LLM (32)", "shape": "dot", "size": 10, "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "type": "MODEL"}, {"color": "#97c2fc", "description": "LLM (8) is a variant of the LLM model with 8 parameters", "id": "LLM (8)", "label": "LLM (8)", "shape": "dot", "size": 10, "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "type": "MODEL"}, {"color": "#97c2fc", "description": "LLM (w/o LLM) is a variant of the LLM model without LLM", "id": "LLM (W/O LLM)", "label": "LLM (W/O LLM)", "shape": "dot", "size": 10, "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "type": "MODEL"}, {"color": "#97c2fc", "description": "Long short-term memory is a type of recurrent neural network architecture", "id": "LONG SHORT-TERM MEMORY", "label": "LONG SHORT-TERM MEMORY", "shape": "dot", "size": 10, "source_id": "a73df99fe49b288e1c8751be2008b191", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nLinxiao Yang is a researcher and author who has contributed to the field of time series forecasting. Specifically, Linxiao Yang is an author of two papers: \"Sadi: A self-adaptive decomposed interpretable framework for electric load forecasting under extreme events\" and \"Sadi: A self-adaptive decomposed interpretable framework for time series forecasting.\" These papers demonstrate Linxiao Yang\u0027s expertise in developing interpretable frameworks for time series forecasting, particularly in the context of extreme events and electric load forecasting.\n\nThe papers, which are likely written in English, contain technical terms, mathematical equations, and references to academic papers, indicating a formal and academic tone. The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further suggests that the papers are written in English and are likely published in English-language academic conferences and journals.\n\nOverall, Linxiao Yang\u0027s work in time series forecasting, as reflected in these papers, highlights his expertise in developing interpretable frameworks for complex forecasting tasks, particularly in the context of extreme events and electric load forecasting.", "id": "LINXIAO YANG", "label": "LINXIAO YANG", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nRui Xia is a researcher and author who has contributed to the development of a self-adaptive decomposed interpretable framework for electric load forecasting under extreme events, as well as a framework for time series forecasting. Specifically, Rui Xia is the author of the paper \"Sadi: A self-adaptive decomposed interpretable framework for electric load forecasting under extreme events\" and another paper on \"Sadi: A self-adaptive decomposed interpretable framework for time series forecasting\". This suggests that Rui Xia has expertise in the area of time series forecasting and has developed innovative frameworks for addressing complex forecasting challenges, particularly in the context of extreme events.\n\nThe use of technical terms such as \"self-adaptive decomposed interpretable framework\" and \"time series forecasting\" indicates that Rui Xia\u0027s work is grounded in advanced mathematical and computational techniques, likely involving machine learning and statistical modeling. The fact that Rui Xia has published papers on this topic in academic conferences and journals (as indicated by the presence of abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\") further supports the conclusion that Rui Xia is a respected researcher in the field of time series forecasting and electric load forecasting.\n\nOverall, Rui Xia\u0027s work has the potential to contribute significantly to the development of more accurate and reliable forecasting models, particularly in the context of extreme events, and has the potential to have a positive impact on various industries and applications that rely on accurate forecasting.", "id": "RUI XIA", "label": "RUI XIA", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nYi Wang is a researcher and author who has contributed to the field of time series forecasting. Specifically, Yi Wang is an author of two papers: \"Sadi: A self-adaptive decomposed interpretable framework for electric load forecasting under extreme events\" and \"Sadi: A self-adaptive decomposed interpretable framework for time series forecasting.\" These papers suggest that Yi Wang\u0027s work focuses on developing interpretable and adaptive frameworks for time series forecasting, particularly in the context of extreme events and electric load forecasting. The use of technical terms such as \"self-adaptive decomposed interpretable framework\" and \"time series forecasting\" indicates that Yi Wang\u0027s work is grounded in machine learning and data analysis. Overall, Yi Wang appears to be a researcher with expertise in time series forecasting and machine learning, and has made significant contributions to the field through his published papers.", "id": "YI WANG", "label": "YI WANG", "shape": "dot", "size": 10, "source_id": "97c1f318683bb63131ece0a51f096c5b,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of Liang Sun:\n\nLiang Sun is a researcher and author who has made significant contributions to the field of time series forecasting. He is the author of several research papers, including \"Sadi: A self-adaptive decomposed interpretable framework for electric load forecasting under extreme events\" and \"Transformers in time series: A survey\". Additionally, he has published a paper on \"Sadi: A self-adaptive decomposed interpretable framework for time series forecasting\", which suggests that his work focuses on developing interpretable and adaptive frameworks for time series forecasting.\n\nLiang Sun\u0027s research interests and expertise lie in the area of time series forecasting, with a particular emphasis on developing self-adaptive and interpretable frameworks for electric load forecasting and other applications. His work has been published in reputable academic conferences and journals, indicating a high level of academic rigor and quality.\n\nOverall, Liang Sun is a prominent researcher in the field of time series forecasting, with a strong publication record and a focus on developing innovative and interpretable methods for time series analysis.\n\nRelevant information from the nearby text:\n\n* The text mentions that Liang Sun\u0027s work involves the use of transformers in time series forecasting, which suggests that he is familiar with the application of deep learning techniques in this area.\n* The text also mentions that Liang Sun\u0027s work focuses on developing interpretable and adaptive frameworks for time series forecasting, which suggests that he is interested in developing methods that can provide insights into the underlying mechanisms of time series data.\n* The text does not provide any information about Liang Sun\u0027s educational background, work experience, or other personal details.", "id": "LIANG SUN", "label": "LIANG SUN", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,97c1f318683bb63131ece0a51f096c5b,a4bb4cfc468e2f87ad5d8a4b451bcbbf,a69a914fb6c895c7202532b69ad3e094", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Geza Kovacs is an author of the paper \"Large language models are few-shot health learners\"", "id": "GEZA KOVACS", "label": "GEZA KOVACS", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Isaac Galatzer-Levy is an author of the paper \"Large language models are few-shot health learners\"", "id": "ISAAC GALATZER-LEVY", "label": "ISAAC GALATZER-LEVY", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jacob Sunshine is an author of the paper \"Large language models are few-shot health learners\"", "id": "JACOB SUNSHINE", "label": "JACOB SUNSHINE", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jiening Zhan is an author of the paper \"Large language models are few-shot health learners\"", "id": "JIENING ZHAN", "label": "JIENING ZHAN", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Ming-Zher Poh is an author of the paper \"Large language models are few-shot health learners\"", "id": "MING-ZHER POH", "label": "MING-ZHER POH", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Shun Liao is an author of the paper \"Large language models are few-shot health learners\"", "id": "SHUN LIAO", "label": "SHUN LIAO", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Paolo Di Achille is an author of the paper \"Large language models are few-shot health learners\"", "id": "PAOLO DI ACHILLE", "label": "PAOLO DI ACHILLE", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Shwetak Patel is an author of the paper \"Large language models are few-shot health learners\"", "id": "SHWETAK PATEL", "label": "SHWETAK PATEL", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nJianmin Wang is a researcher and author who has contributed to several papers in the field of time series forecasting. Specifically, he is an author of two notable papers: \"Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting\" and \"Non-stationary transformers: Exploring the stationarity in time series forecasting\". These papers demonstrate his expertise in developing innovative models for long-term series forecasting, including the use of decomposition transformers and non-stationary transformers. His work has likely been presented at conferences such as ICLR, AAAI, or PMLR, and has been published in academic journals. Overall, Jianmin Wang is a prominent figure in the field of time series forecasting, with a focus on developing effective models for long-term series forecasting.", "id": "JIANMIN WANG", "label": "JIANMIN WANG", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMingsheng Long is a researcher and author who has contributed to several papers in the field of time series forecasting. Specifically, he is an author of two notable papers: \"Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting\" and \"Non-stationary transformers: Exploring the stationarity in time series forecasting\". These papers demonstrate his expertise in developing innovative models for long-term series forecasting, including the use of decomposition transformers and non-stationary transformers. His work has likely had a significant impact on the field of time series forecasting, and his contributions are a testament to his expertise in this area.\n\nRelevant details from the nearby text include:\n\n* The primary language of the text is English, which suggests that the papers written by Mingsheng Long are also in English.\n* The use of technical terms, mathematical equations, and references to academic papers in the text indicates that Mingsheng Long\u0027s work is grounded in academic research and is likely to be of interest to experts in the field of time series forecasting.\n* The fact that Mingsheng Long has authored multiple papers on time series forecasting suggests that he is a prolific researcher in this area and is likely to be a leading expert in the field.\n\nOverall, Mingsheng Long is a notable researcher and author who has made significant contributions to the field of time series forecasting through his work on decomposition transformers and non-stationary transformers.", "id": "MINGSHENG LONG", "label": "MINGSHENG LONG", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Zhisheng Zheng is an author of the paper \"Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition\"", "id": "ZHISHENG ZHENG", "label": "ZHISHENG ZHENG", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Yiwei Guo is an author of the paper \"Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition\"", "id": "YIWEI GUO", "label": "YIWEI GUO", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Qian Chen is an author of the paper \"Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition\"", "id": "QIAN CHEN", "label": "QIAN CHEN", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Shiliang Zhang is an author of the paper \"Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition\"", "id": "SHILIANG ZHANG", "label": "SHILIANG ZHANG", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Xie Chen is an author of the paper \"Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition\"", "id": "XIE CHEN", "label": "XIE CHEN", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Vassilios Assimakopoulos is an author of the paper \"The m4 competition: Results, findings, conclusion and way forward\"", "id": "VASSILIOS ASSIMAKOPOULOS", "label": "VASSILIOS ASSIMAKOPOULOS", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Amit Dhurandhar is an author of the paper \"Reprogramming pretrained language models for antibody sequence infilling\"", "id": "AMIT DHURANDHAR", "label": "AMIT DHURANDHAR", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Inkit Padhi is an author of the paper \"Reprogramming pretrained language models for antibody sequence infilling\"", "id": "INKIT PADHI", "label": "INKIT PADHI", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Devleena Das is an author of the paper \"Reprogramming pretrained language models for antibody sequence infilling\"", "id": "DEVLEENA DAS", "label": "DEVLEENA DAS", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Dorsa Sadigh is an author of the paper \"Large language models as general pattern machines\"", "id": "DORSASADIGH", "label": "DORSASADIGH", "shape": "dot", "size": 10, "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "David Luan is an author of the paper \"Language models are unsupervised multitask learners\"", "id": "DAVID LUAN", "label": "DAVID LUAN", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Dario Amodei is an author of the paper \"Language models are unsupervised multitask learners\"", "id": "DARIO AMODEI", "label": "DARIO AMODEI", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Ilya Sutskever is an author of the paper \"Language models are unsupervised multitask learners\"", "id": "ILYA SUTSKEVER", "label": "ILYA SUTSKEVER", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Stephen H Schneider is an author of the paper \"Climate modeling\"", "id": "STEPHEN H SCHNEIDER", "label": "STEPHEN H SCHNEIDER", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Robert E Dickinson is an author of the paper \"Climate modeling\"", "id": "ROBERT E DICKINSON", "label": "ROBERT E DICKINSON", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Yihong Tang is an author of the paper \"Domain adversarial spatial-temporal network: a transferable framework for short-term traffic forecasting across cities\"", "id": "YIHONG TANG", "label": "YIHONG TANG", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Ao Qu is an author of the paper \"Domain adversarial spatial-temporal network: a transferable framework for short-term traffic forecasting across cities\"", "id": "AO QU", "label": "AO QU", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Andy HF Chow is an author of the paper \"Domain adversarial spatial-temporal network: a transferable framework for short-term traffic forecasting across cities\"", "id": "ANDY HF CHOW", "label": "ANDY HF CHOW", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "William HK Lam is an author of the paper \"Domain adversarial spatial-temporal network: a transferable framework for short-term traffic forecasting across cities\"", "id": "WILLIAM HK LAM", "label": "WILLIAM HK LAM", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "SC Wong is an author of the paper \"Domain adversarial spatial-temporal network: a transferable framework for short-term traffic forecasting across cities\"", "id": "SC WONG", "label": "SC WONG", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Wei Ma is an author of the paper \"Domain adversarial spatial-temporal network: a transferable framework for short-term traffic forecasting across cities\"", "id": "WEI MA", "label": "WEI MA", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Hugo Touvron is an author of the paper \"Llama: Open and efficient foundation language models\"", "id": "HUGO TOUVRON", "label": "HUGO TOUVRON", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Thibaut Lavril is an author of the paper \"Llama: Open and efficient foundation language models\"", "id": "THIBAUT LAVRIL", "label": "THIBAUT LAVRIL", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Gautier Izacard is an author of the paper \"Llama: Open and efficient foundation language models\"", "id": "GAUTIER IZACARD", "label": "GAUTIER IZACARD", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Xavier Martinet is an author of the paper \"Llama: Open and efficient foundation language models\"", "id": "XAVIER MARTINET", "label": "XAVIER MARTINET", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Marie-Anne Lachaux is an author of the paper \"Llama: Open and efficient foundation language models\"", "id": "MARIE-ANNE LACHAUX", "label": "MARIE-ANNE LACHAUX", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Timoth\u00e9e Lacroix is an author of the paper \"Llama: Open and efficient foundation language models\"", "id": "TIMOTHEE LACROIX", "label": "TIMOTHEE LACROIX", "shape": "dot", "size": 10, "source_id": "81b15ffb0d853301758618e61757cca9", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Chaoli Zhang is an author of the paper \"Transformers in time series: A survey\"", "id": "CHAOLI ZHANG", "label": "CHAOLI ZHANG", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Weiqi Chen is an author of the paper \"Transformers in time series: A survey\"", "id": "WEIQI CHEN", "label": "WEIQI CHEN", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Junchi Yan is an author of the paper \"Transformers in time series: A survey\"", "id": "JUNCHI YAN", "label": "JUNCHI YAN", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Chenghao Liu is an author of the paper \"Etsformer: Exponential smoothing transformers for time-series forecasting\"", "id": "CHENGHAO LIU", "label": "CHENGHAO LIU", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Doyen Sahoo is an author of the paper \"Etsformer: Exponential smoothing transformers for time-series forecasting\"", "id": "DOYEN SAHOO", "label": "DOYEN SAHOO", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Akshat Kumar is an author of the paper \"Etsformer: Exponential smoothing transformers for time-series forecasting\"", "id": "AKSHAT KUMAR", "label": "AKSHAT KUMAR", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Steven Hoi is an author of the paper \"Etsformer: Exponential smoothing transformers for time-series forecasting\"", "id": "STEVEN HOI", "label": "STEVEN HOI", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Jiehui Xu is an author of the paper \"Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting\"", "id": "JIEHUI XU", "label": "JIEHUI XU", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Tengge Hu is an author of the paper \"Timesnet: Temporal 2d-variation modeling for general time series analysis\"", "id": "TENGGE HU", "label": "TENGGE HU", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Hang Zhou is an author of the paper \"Timesnet: Temporal 2d-variation modeling for general time series analysis\"", "id": "HANG ZHOU", "label": "HANG ZHOU", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Chao-Han Huck Yang is an author of the paper \"Voice2series: Reprogramming acoustic models for time series classification\"", "id": "CHAO-HAN HUCK YANG", "label": "CHAO-HAN HUCK YANG", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Yun-Yun Tsai is an author of the paper \"Voice2series: Reprogramming acoustic models for time series classification\"", "id": "YUN-YUN TSAI", "label": "YUN-YUN TSAI", "shape": "dot", "size": 10, "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Fast multivariate time series forecasting is a concept mentioned in the text, as indicated by the use of the phrase \"Fast multivariate time series forecasting with light sampling-oriented mlp structures\"", "id": "FAST MULTIVARIATE TIME SERIES FORECASTING", "label": "FAST MULTIVARIATE TIME SERIES FORECASTING", "shape": "dot", "size": 10, "source_id": "98c6b5003112ab7110e45414a2fa468b", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Xue Wang is an author who has published research papers on time series forecasting", "id": "XUE WANG", "label": "XUE WANG", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Rong Jin is an author who has published research papers on time series forecasting", "id": "RONG JIN", "label": "RONG JIN", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Ge Jin is an author who has published research papers on time series forecasting", "id": "GE JIN", "label": "GE JIN", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Yuchen Huang is an author who has published research papers on time series forecasting", "id": "YUCHEN HUANG", "label": "YUCHEN HUANG", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "Sheng Li is an author who has published research papers on time series forecasting", "id": "SHENG LI", "label": "SHENG LI", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "PTSE is a model that uses a multi-model ensemble method for probabilistic time series forecasting", "id": "PTSE", "label": "PTSE", "shape": "dot", "size": 10, "source_id": "a69a914fb6c895c7202532b69ad3e094", "type": "MODEL"}, {"color": "#97c2fc", "description": "R2DL is a model that reprograms amino acids using word embeddings", "id": "R2DL", "label": "R2DL", "shape": "dot", "size": 10, "source_id": "f7266cfccedb1d9840d10afa689a05e9", "type": "MODEL"}, {"color": "#97c2fc", "description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"REPROBERT\" is a model that utilizes word embeddings to reprogram amino acids. This model is related to the patch reprogramming approach, which is also employed in REPROBERT. The patch reprogramming approach is a key component of REPROBERT, enabling it to reprogram amino acids effectively.\n\nThe language of the text describing REPROBERT is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. The formal and academic tone of the text further supports this conclusion.\n\nOverall, REPROBERT is a model that leverages word embeddings and the patch reprogramming approach to reprogram amino acids, and its description is written in English.", "id": "REPROBERT", "label": "REPROBERT", "shape": "dot", "size": 10, "source_id": "56806630593fdf0c3d95ad7555080dcf,f7266cfccedb1d9840d10afa689a05e9", "type": "MODEL"}, {"color": "#97c2fc", "description": "Word embeddings are related to amino acids as amino acids are used in word embeddingsWord embeddings are a type of representation used in natural language processing", "entity_type": "AMINO ACIDS", "id": "WORD EMBEDDINGS", "label": "WORD EMBEDDINGS", "shape": "dot", "size": 10, "source_id": "56806630593fdf0c3d95ad7555080dcf", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Amino acids are the building blocks of proteinsAmino acids are related to vocabulary as vocabulary is used in amino acids", "entity_type": "VOCABULARY", "id": "AMINO ACIDS", "label": "AMINO ACIDS", "shape": "dot", "size": 10, "source_id": "56806630593fdf0c3d95ad7555080dcf", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Patch reprogramming approach is related to time series patches as time series patches are used in patch reprogramming approachPatch reprogramming approach is a technique used to modify or update a model or system", "entity_type": "TIME SERIES PATCHES", "id": "PATCH REPROGRAMMING APPROACH", "label": "PATCH REPROGRAMMING APPROACH", "shape": "dot", "size": 10, "source_id": "56806630593fdf0c3d95ad7555080dcf", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Llama-7B is related to patch reprogramming approach as patch reprogramming approach is used in Llama-7B", "id": "LAMMA-7B", "label": "LAMMA-7B", "shape": "dot", "size": 10, "source_id": "56806630593fdf0c3d95ad7555080dcf", "type": "PATCH REPROGRAMMING APPROACH"}, {"color": "#97c2fc", "description": "Hibon is an author mentioned in the text, as indicated by the use of the name \"Hibon\"", "id": "HIBON", "label": "HIBON", "shape": "dot", "size": 10, "source_id": "74527a4337ed6919731be520311ae774", "type": "AUTHOR"}, {"color": "#97c2fc", "description": "For short-term forecasting on M4 benchmark, we adopt the symmetric mean absolute percentage error (SMAPE)", "id": "SYMMETRIC MEAN ABSOLUTE PERCENTAGE ERROR", "label": "SYMMETRIC MEAN ABSOLUTE PERCENTAGE ERROR", "shape": "dot", "size": 10, "source_id": "50eeacd99c68b2581be90310bedcbc2c", "type": "METRIC"}, {"color": "#97c2fc", "description": "For short-term forecasting on M4 benchmark, we adopt the overall weighted average (OWA)", "id": "OVERALL WEIGHTED AVERAGE", "label": "OVERALL WEIGHTED AVERAGE", "shape": "dot", "size": 10, "source_id": "50eeacd99c68b2581be90310bedcbc2c", "type": "METRIC"}, {"color": "#97c2fc", "description": "Vehicle refers to a means of transportation, such as a car or truck", "id": "VEHICLE", "label": "VEHICLE", "shape": "dot", "size": 10, "source_id": "2dbfdb45630a023a5a6979b9573a868f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Pedestrian refers to a person walking in a given area", "id": "PEDESTRIAN", "label": "PEDESTRIAN", "shape": "dot", "size": 10, "source_id": "2dbfdb45630a023a5a6979b9573a868f", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "M4 datasets are used for evaluating the performance of models", "id": "M4 DATASETS", "label": "M4 DATASETS", "shape": "dot", "size": 10, "source_id": "ab91381dd032db318e5aec1ad5b914a6", "type": "DATASET"}, {"color": "#97c2fc", "description": "Tab. 19 is a table that compares the performance of TIME-LLM and PatchTST on long-term forecasting tasks", "id": "TAB 19", "label": "TAB 19", "shape": "dot", "size": 10, "source_id": "ab91381dd032db318e5aec1ad5b914a6", "type": "TABLE"}, {"color": "#97c2fc", "description": "Tab. 20 is a table that compares the performance of TIME-LLM and N-HITS on long-term forecasting tasks", "id": "TAB 20", "label": "TAB 20", "shape": "dot", "size": 10, "source_id": "ab91381dd032db318e5aec1ad5b914a6", "type": "TABLE"}, {"color": "#97c2fc", "description": "Fig. 7 is a figure that compares the forecasting results of TIME-LLM and other models", "id": "FIG 7", "label": "FIG 7", "shape": "dot", "size": 10, "source_id": "ab91381dd032db318e5aec1ad5b914a6", "type": "FIGURE"}, {"color": "#97c2fc", "description": "Fig. 8 is a figure that compares the forecasting results of TIME-LLM and other models", "id": "FIG 8", "label": "FIG 8", "shape": "dot", "size": 10, "source_id": "ab91381dd032db318e5aec1ad5b914a6", "type": "FIGURE"}, {"color": "#97c2fc", "description": "Fig. 9 is a figure that compares the forecasting results of TIME-LLM and other models in few-shot scenarios", "id": "FIG 9", "label": "FIG 9", "shape": "dot", "size": 10, "source_id": "ab91381dd032db318e5aec1ad5b914a6", "type": "FIGURE"}, {"color": "#97c2fc", "description": "Fig. 10 is a figure that compares the forecasting results of TIME-LLM and other models in zero-shot scenarios", "id": "FIG 10", "label": "FIG 10", "shape": "dot", "size": 10, "source_id": "ab91381dd032db318e5aec1ad5b914a6", "type": "FIGURE"}, {"color": "#97c2fc", "description": "Others refers to a type of dataset used in short-term forecasting", "id": "OTHERS", "label": "OTHERS", "shape": "dot", "size": 10, "source_id": "27efab80b405b365d8e9dd9834dd1ca8", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "ST is a model mentioned in the text", "entity_type": "MODEL", "id": "ST", "label": "ST", "shape": "dot", "size": 10, "source_id": "b13b2cc422483985c354844b166a0151", "type": "MODEL"}, {"color": "#97c2fc", "description": "Industrial applications refer to the use of technology in industrial settings", "id": "INDUSTRIAL APPLICATIONS", "label": "INDUSTRIAL APPLICATIONS", "shape": "dot", "size": 10, "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "type": "DOMAIN"}, {"color": "#97c2fc", "description": "Time series anomaly detection is a problem that involves detecting anomalies in time series data", "id": "TIME SERIES ANOMALY DETECTION", "label": "TIME SERIES ANOMALY DETECTION", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "PROBLEM"}, {"color": "#97c2fc", "description": "Regression models are algorithms that are used to predict continuous outcomes", "id": "REGRESSION MODELS", "label": "REGRESSION MODELS", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Hidden Markov chains are a type of algorithm that is used for modeling sequential data", "id": "HIDDEN MARKOV CHAINS", "label": "HIDDEN MARKOV CHAINS", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "GraphAn is a type of algorithm that converts time series inputs to graphs and uses graph distance metrics to detect outliers", "id": "GRAPHAN", "label": "GRAPHAN", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Isolation forest is a type of algorithm that uses an ensemble of isolation trees to detect outliers", "id": "ISOLATION FOREST", "label": "ISOLATION FOREST", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "CPOD is a type of algorithm that uses clustering and database read-write history to detect outliers", "id": "CPOD", "label": "CPOD", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "ELLE is a type of algorithm that uses clustering and database read-write history to detect outliers", "id": "ELLE", "label": "ELLE", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Matrix profiling variants are types of algorithms that are used for anomaly and motif discovery", "id": "MATRIX PROFILING VARIANTS", "label": "MATRIX PROFILING VARIANTS", "shape": "dot", "size": 10, "source_id": "203f9117f8528750ca0c22a768a02cd9", "type": "ALGORITHM"}, {"color": "#97c2fc", "description": "Encoder-decoder networks are a type of neural network that uses an encoder to process input data and a decoder to generate output data", "id": "ENCODER-DECODER NETWORKS", "label": "ENCODER-DECODER NETWORKS", "shape": "dot", "size": 10, "source_id": "1902e651467179a9a1d4c4df0035e980", "type": "MODEL"}, {"color": "#97c2fc", "description": "Past input windows is a concept mentioned in the text, as indicated by the use of the phrase \"past input windows\"", "id": "PAST INPUT WINDOWS", "label": "PAST INPUT WINDOWS", "shape": "dot", "size": 10, "source_id": "477c5b7f23f4e00030ab389788a4c88d", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Activation function refers to the Sigmoid function used in the decoder", "id": "ACTIVATION FUNCTION", "label": "ACTIVATION FUNCTION", "shape": "dot", "size": 10, "source_id": "4bdf596e75e1cb06d11b25e95491037e", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "The NDT thresholding method is a technique used in anomaly detection, but it has poor efficiency", "id": "NDT THRESHOLDING METHOD", "label": "NDT THRESHOLDING METHOD", "shape": "dot", "size": 10, "source_id": "8e075f1de7293e0cd1724bd167c263be", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "Discords refer to anomalies or outliers in a dataset", "id": "DISCORDS", "label": "DISCORDS", "shape": "dot", "size": 10, "source_id": "9b35a2c607e0f4b8cbc9179f424f180a", "type": "CONCEPT"}, {"color": "#97c2fc", "description": "AUC score is a metric used to evaluate the performance of a model on anomaly detection tasks", "id": "AUC SCORE", "label": "AUC SCORE", "shape": "dot", "size": 10, "source_id": "78ee4a3d7a2bffd4405a03d94a4f6cb1", "type": "METRIC"}, {"color": "#97c2fc", "description": "Discord lengths refer to the length of a particular sequence or time series", "id": "DISCORD LENGTHS", "label": "DISCORD LENGTHS", "shape": "dot", "size": 10, "source_id": "a39d9d28126733c33db624ccc25f5782", "type": "CONCEPT"}]);
                  edges = new vis.DataSet([{"description": "RESTAD is designed for time series anomaly detection", "from": "TIME SERIES", "source_id": "308a1b7226a66096a6e686d5e99848ad", "to": "RESTAD", "width": 1.0}, {"description": "Based on the provided data, a comprehensive summary of the relationship between \"TIME SERIES\" and \"ANOMALY DETECTION\" can be generated as follows:\n\n\"TIME SERIES\" and \"ANOMALY DETECTION\" are closely related entities, as anomaly detection is a crucial feature and task in time series forecasting. Anomaly detection involves identifying unusual patterns or outliers in time series data, which is a technique used to identify unusual patterns or data points in time series data. This relationship is bidirectional, as time series is used for anomaly detection, and anomaly detection is often used to identify anomalies in time series data. Furthermore, time series anomaly detection is essential across various domains, highlighting the significance of this relationship. Overall, the connection between \"TIME SERIES\" and \"ANOMALY DETECTION\" is fundamental, with anomaly detection being a critical aspect of time series analysis and forecasting.", "from": "TIME SERIES", "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2,308a1b7226a66096a6e686d5e99848ad,3e0985c172a09bb6c99e305b9f40a513,477c5b7f23f4e00030ab389788a4c88d,59e8e02df5f942071e733def7884b457,6383536333b1a09cc9e6f8d4ea5e9bce,736a43d5fef4417bac9757301b4721c3,9125a7d3794226f109debe90e5db041c,99b3fe01a22282fe6d02bf29dacf8875", "to": "ANOMALY DETECTION", "width": 9.0}, {"description": "Time series is related to deep learning as deep learning is particularly well-suited for time series analysis", "from": "TIME SERIES", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "DEEP LEARNING", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n**Summary of TIME SERIES and FOUNDATION MODEL**\n\nTIME SERIES refers to a sequence of data points measured at regular time intervals. This concept is closely related to FOUNDATION MODEL, as foundation models can be fine-tuned for time series forecasting. In fact, foundation models are pre-trained on vast amounts of data, which enables them to analyze and understand time series data. This understanding can be leveraged to improve the accuracy of time series forecasting, making foundation models a valuable tool in this domain.\n\nThe relationship between TIME SERIES and FOUNDATION MODEL is bidirectional, with foundation models being able to analyze and understand time series data, and foundation models being fine-tuned for time series forecasting. This synergy between the two concepts has the potential to revolutionize the field of time series forecasting, enabling more accurate and reliable predictions.\n\nOverall, the connection between TIME SERIES and FOUNDATION MODEL is one of mutual benefit, with each concept enhancing the capabilities of the other. By leveraging the strengths of both concepts, researchers and practitioners can develop more effective time series forecasting models that can better capture the complexities of real-world data.", "from": "TIME SERIES", "source_id": "55eb54ef455d14cd1a11760924f99eb8,5bb0db923f70e5f431183c6ab7dc25dc,b8ce119147e4d1454453c514e68dc4dc", "to": "FOUNDATION MODEL", "width": 3.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe entity \"TIME SERIES\" is closely related to \"FOUNDATION MODELS\" in the context of machine learning and time series forecasting. Foundation models, which are pre-trained models that can be fine-tuned for specific tasks, are utilized in time series analysis. They can be adapted to specific tasks with relatively small amounts of task-specific data, making them a valuable tool for time series forecasting.\n\nFoundation models are particularly useful for time series forecasting as they can be fine-tuned for time series analysis, allowing them to capture the complex patterns and relationships within time series data. This fine-tuning process enables foundation models to learn from the task-specific data and improve their performance on time series forecasting tasks.\n\nIn summary, the relationship between \"TIME SERIES\" and \"FOUNDATION MODELS\" is one of mutual benefit, with foundation models being used to improve time series forecasting and time series data being used to fine-tune and adapt foundation models for specific tasks.\n\nRelevant information from the nearby text:\n\n* The use of foundation models in time series analysis is a key aspect of their application in machine learning and time series forecasting.\n* The ability of foundation models to be fine-tuned for specific tasks makes them a valuable tool for time series forecasting.\n* The use of time series data to fine-tune and adapt foundation models is a key aspect of their application in time series analysis.\n\nNote: The provided information is consistent and does not contain any contradictions, making it possible to create a comprehensive summary.", "from": "TIME SERIES", "source_id": "79b26808691b6333aafce43c5de531f7,bb10b1a7f98a4f869b7abbc5f9632dd1,ca3dfe42c66d68dd6dbad037936b2360", "to": "FOUNDATION MODELS", "width": 3.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"TIME SERIES\" and \"TRAJECTORY\" are closely related concepts in the field of data analysis. According to the descriptions, both \"TIME SERIES\" and \"TRAJECTORY\" are sequences of data points. Specifically, it is stated that \"Time series is related to trajectory as both are sequences of data points\" and \"Time series is related to trajectory as trajectory is a type of time series.\" This suggests that a trajectory can be considered a specific type of time series, characterized by its sequence of data points.\n\nIn essence, the relationship between \"TIME SERIES\" and \"TRAJECTORY\" can be summarized as follows: a trajectory is a type of time series, and both concepts involve sequences of data points. This relationship is fundamental to understanding the underlying structure and patterns in data, particularly in the context of data analysis and forecasting.\n\nGiven the formal and academic language used in the descriptions, it is likely that this summary is relevant to research papers or technical documents in the field of data analysis, particularly those focusing on time series analysis and trajectory modeling.", "from": "TIME SERIES", "source_id": "53f80422fbf85856ecf940d5c2450665,736a43d5fef4417bac9757301b4721c3", "to": "TRAJECTORY", "width": 2.0}, {"description": "Time series is related to spatial time series as spatial time series is a subtype of time series", "from": "TIME SERIES", "source_id": "736a43d5fef4417bac9757301b4721c3,79c0a74b91761402b67c3574db02e8e7", "to": "SPATIAL TIME SERIES", "width": 2.0}, {"description": "Time series is related to events as events can be analyzed using time series techniques", "from": "TIME SERIES", "source_id": "736a43d5fef4417bac9757301b4721c3", "to": "EVENTS", "width": 1.0}, {"description": "Diffusion-based models and time series are both used for generative tasks, but diffusion-based models are more effective", "from": "TIME SERIES", "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "to": "DIFFUSION-BASED MODELS", "width": 1.0}, {"description": "Time series is related to prediction as prediction is used to forecast future values in a time series", "from": "TIME SERIES", "source_id": "9125a7d3794226f109debe90e5db041c", "to": "PREDICTION", "width": 1.0}, {"description": "Time series is related to imputation as imputation is used to fill in missing data points in a time series", "from": "TIME SERIES", "source_id": "9125a7d3794226f109debe90e5db041c", "to": "IMPUTATION", "width": 1.0}, {"description": "Time series is related to masked autoencoding as masked autoencoding is used in pre-training time series foundation models", "from": "TIME SERIES", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "MASKED AUTOENCODING", "width": 1.0}, {"description": "Time series is related to raster as both are types of data representation", "from": "TIME SERIES", "source_id": "53f80422fbf85856ecf940d5c2450665", "to": "RASTER", "width": 1.0}, {"description": "Time series is related to text as both are types of data", "from": "TIME SERIES", "source_id": "53f80422fbf85856ecf940d5c2450665", "to": "TEXT", "width": 1.0}, {"description": "Time series is related to embeddings as embeddings are used to represent time series data", "from": "TIME SERIES", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "EMBEDDINGS", "width": 1.0}, {"description": "Time series is related to context as context is used to make predictions in time series forecasting", "from": "TIME SERIES", "source_id": "6f6e27166a1506482bfcaf5585322595", "to": "CONTEXT", "width": 1.0}, {"description": "Patch based modeling is related to time series as time series is used in patch based modeling", "from": "TIME SERIES", "source_id": "f98d2bec31738be3f7be750b5fe6180e", "to": "PATCH BASED MODELING", "width": 1.0}, {"description": "Time series is related to token as token is used in time series", "from": "TIME SERIES", "source_id": "f98d2bec31738be3f7be750b5fe6180e", "to": "TOKEN", "width": 1.0}, {"description": "Time series is related to patch as patch is a subset of data points in a time series", "from": "TIME SERIES", "source_id": "4ab34d32601d452f14b4a1c31415292d", "to": "PATCH", "width": 1.0}, {"description": "Time series is related to horizon length as horizon length is a concept used in time-series forecasting", "from": "TIME SERIES", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "HORIZON LENGTH", "width": 1.0}, {"description": "Time series is related to context length as context length is a concept used in time-series forecasting", "from": "TIME SERIES", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "CONTEXT LENGTH", "width": 1.0}, {"description": "Time series is related to date derived features as date derived features are often used to extract meaningful information from time series data", "from": "TIME SERIES", "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "to": "DATE DERIVED FEATURES", "width": 1.0}, {"description": "Time series is related to vector as vector is often used to represent time series data in a mathematical format", "from": "TIME SERIES", "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "to": "VECTOR", "width": 1.0}, {"description": "Time series is related to time-points as time-points are specific points in time that make up a time series", "from": "TIME SERIES", "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "to": "TIME-POINTS", "width": 1.0}, {"description": "Self-supervised transformer is used for time series analysis", "from": "TIME SERIES", "source_id": "3e0985c172a09bb6c99e305b9f40a513", "to": "SELF-SUPERVISED TRANSFORMER", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nAnomalyBERT is a model related to TIME SERIES, as it is specifically designed to detect anomalies in time series data. This is evident from the descriptions provided, which state that AnomalyBERT is used for time series anomaly detection. The model\u0027s primary function is to identify unusual patterns or outliers in time series data, which is a critical task in various fields such as finance, healthcare, and energy management.\n\nThe use of AnomalyBERT for time series anomaly detection suggests that it can be applied to various long-term series forecasting tasks, where identifying anomalies is crucial for making accurate predictions. The model\u0027s ability to detect anomalies can also be useful in frequency analysis, which is a common technique used in time series analysis to understand the underlying patterns and trends in the data.\n\nOverall, AnomalyBERT is a valuable tool for TIME SERIES analysis, and its application in time series anomaly detection can lead to improved forecasting accuracy and better decision-making in various domains.", "from": "TIME SERIES", "source_id": "3e0985c172a09bb6c99e305b9f40a513,587ca8c6cc86a93299e9540153babbc6,71f873c12230e6ede47583d81eb85e23", "to": "ANOMALYBERT", "width": 3.0}, {"description": "Local outlier factor is a method that can be used for time series anomaly detection", "from": "TIME SERIES", "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "to": "LOCAL OUTLIER FACTOR", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"TIME SERIES\" is closely related to the entity \"DATA DEGRADATION SCHEME\". Specifically, time series data is being degraded, suggesting that the data degradation scheme is applied to time series data. Furthermore, time series is used to detect anomalies in time series data, which implies that the data degradation scheme is utilized to identify and potentially mitigate these anomalies.\n\nThis relationship between time series and data degradation scheme is a crucial aspect of understanding how data is processed and analyzed in this context. The use of time series to detect anomalies in degraded data highlights the importance of data quality and the need for effective data degradation schemes to ensure accurate analysis and decision-making.\n\nOverall, the summary provides a clear understanding of the interconnection between time series and data degradation scheme, highlighting the key role of time series in detecting anomalies and the importance of data degradation schemes in maintaining data quality.", "from": "TIME SERIES", "source_id": "6917a14af275e30abe2d30a8f8a9a4e6,71f873c12230e6ede47583d81eb85e23", "to": "DATA DEGRADATION SCHEME", "width": 2.0}, {"description": "The text is likely from a technical document, as indicated by the presence of technical terms and mathematical equations", "from": "TIME SERIES", "source_id": "4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "to": "TECHNICAL DOCUMENT", "width": 13.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe primary entity of interest is \"TIME SERIES,\" which is a concept mentioned in the text. It is related to \"LONG-TERM SERIES FORECASTING,\" a technique used to predict future values in the time series. This is evident from the use of the phrase \"time series forecasting\" in the text, which suggests that the entity \"TIME SERIES\" is closely tied to the entity \"LONG-TERM SERIES FORECASTING.\"\n\nThe text also indicates that time series forecasting is a method used to predict future values in the time series, which further reinforces the relationship between the two entities. The language used in the text is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nOverall, the summary can be written as follows:\n\n\"The entity \u0027TIME SERIES\u0027 is a concept mentioned in the text, which is closely related to \u0027LONG-TERM SERIES FORECASTING.\u0027 Time series forecasting is a method used to predict future values in the time series, as indicated by the use of the phrase \u0027time series forecasting\u0027 in the text.\"\n\nThis summary is written in third person and includes information collected from all the descriptions, resolving any potential contradictions. It also includes relevant information from the nearby text to provide a comprehensive understanding of the entities and their relationships.", "from": "TIME SERIES", "source_id": "15c3350fad556f666d93817c5036109c,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "to": "LONG-TERM SERIES FORECASTING", "width": 14.0}, {"description": "Dataset is related to time series as it is used to train and test the model", "from": "TIME SERIES", "source_id": "71f873c12230e6ede47583d81eb85e23", "to": "DATASET", "width": 1.0}, {"description": "Time series is related to anomaly as it is used to detect anomalies in time series data", "from": "TIME SERIES", "source_id": "71f873c12230e6ede47583d81eb85e23", "to": "ANOMALY", "width": 1.0}, {"description": "Time series is related to transformer as it is used to detect anomalies in time series data", "from": "TIME SERIES", "source_id": "71f873c12230e6ede47583d81eb85e23", "to": "TRANSFORMER", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Summary of TIME SERIES and CHRONOS**\n\nTIME SERIES and CHRONOS are two entities that are closely related. CHRONOS is a framework specifically designed for pretrained probabilistic time series models. It is also a language modeling framework for time series forecasting, which implies that it is used for predicting future values in a time series based on past data. Furthermore, CHRONOS is utilized for forecasting time series data, indicating its primary application.\n\nThe relationship between TIME SERIES and CHRONOS is that CHRONOS is a tool for working with TIME SERIES data, specifically for forecasting purposes. In other words, CHRONOS is a framework that enables the use of language modeling techniques for time series forecasting, making it a valuable resource for analyzing and predicting TIME SERIES data.\n\nOverall, the connection between TIME SERIES and CHRONOS is that CHRONOS is a framework that leverages language modeling to facilitate time series forecasting, making it an essential tool for working with TIME SERIES data.\n\n**Key Points:**\n\n* CHRONOS is a framework for pretrained probabilistic time series models.\n* CHRONOS is a language modeling framework for time series forecasting.\n* CHRONOS is used for forecasting time series data.\n* TIME SERIES is related to CHRONOS as CHRONOS is a language modeling framework for time series forecasting.\n\n**Relevant Information:**\n\n* The primary language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers.\n* The text is formal and academic, suggesting that it is from a research paper or a technical document.", "from": "TIME SERIES", "source_id": "42c90ca40b234c098c55632ec70038a1,6eb4c16edf2eedfd03721efb199478d8,bca10b04933dc3a7f98a3f1b610e419b,fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "CHRONOS", "width": 4.0}, {"description": "Time series is a concept that is modeled by probabilistic time series models", "from": "TIME SERIES", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "PROBABILISTIC TIME SERIES MODELS", "width": 1.0}, {"description": "Time series is related to deep forecasters as deep forecasters are used to predict future values in a time series", "from": "TIME SERIES", "source_id": "0bef137d159ccc86cdee0a8be788bd26", "to": "DEEP FORECASTERS", "width": 1.0}, {"description": "Language models are used to predict future patterns in time series", "from": "TIME SERIES", "source_id": "f7e3207706b170296cb036538a709689", "to": "LANGUAGE MODEL", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe TIME SERIES entity is a type of data that can be tokenized into discrete bins through simple scaling and quantization of real values. This process involves breaking down the time series into smaller units, such as tokens, as described in the tokenization scheme of Lag-Llama. The tokenization of time series is a crucial step in various applications, including long-term series forecasting, frequency analysis, and multi-head cross-attention models.\n\nIn the context of Lag-Llama\u0027s tokenization scheme, the TIME SERIES entity is broken down into smaller units, allowing for more efficient processing and analysis. This process enables the application of various machine learning and deep learning techniques, such as multi-head cross-attention, to time series data.\n\nThe TOKENIZATION entity, on the other hand, refers to the process of breaking down a time series into smaller units, such as tokens. This process is essential for various applications, including time series forecasting, frequency analysis, and multi-head cross-attention models.\n\nOverall, the TIME SERIES entity and the TOKENIZATION entity are closely related, as the tokenization of time series is a crucial step in various applications. The Lag-Llama tokenization scheme provides a framework for breaking down time series into smaller units, enabling the application of various machine learning and deep learning techniques.", "from": "TIME SERIES", "source_id": "55099ca8e21d1db3bdaf7bd04e590b72,f7e3207706b170296cb036538a709689", "to": "TOKENIZATION", "width": 2.0}, {"description": "Time series forecasting can be addressed with data augmentation techniques to enhance model robustness and generalization", "from": "TIME SERIES", "source_id": "42c90ca40b234c098c55632ec70038a1", "to": "DATA AUGMENTATION", "width": 1.0}, {"description": "Time series forecasting can be addressed with TSMixup as a data augmentation technique to generate new time series", "from": "TIME SERIES", "source_id": "42c90ca40b234c098c55632ec70038a1", "to": "TSMIXUP", "width": 1.0}, {"description": "Time series forecasting can be addressed with KernelSynth as a data augmentation technique to generate synthetic time series", "from": "TIME SERIES", "source_id": "42c90ca40b234c098c55632ec70038a1", "to": "KERNELSYNTH", "width": 1.0}, {"description": "Time series is related to pattern recognition as pattern recognition is used to identify patterns in time series data", "from": "TIME SERIES", "source_id": "1f1221583d838c2407fe9864225e9eda", "to": "PATTERN RECOGNITION", "width": 1.0}, {"description": "Time series is related to normalization as normalization is used to map time series values into a suitable range", "from": "TIME SERIES", "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "to": "NORMALIZATION", "width": 1.0}, {"description": "Time series is related to quantization as quantization is often used in time series analysis to reduce the precision of the data", "from": "TIME SERIES", "source_id": "c5c841baa0bc205103ac433d446da3b6", "to": "QUANTIZATION", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"TIME SERIES\" is closely related to the entity \"FORECASTING.\" The relationship between these two entities is multifaceted, as described in the provided descriptions. \n\nForecasting is a fundamental feature of time series, as it involves predicting future values or trends based on past data. This process is often used to analyze time series data and make predictions about future values or trends. In essence, forecasting is a process of predicting future values or trends in a time series, which is a key aspect of time series analysis.\n\nThe descriptions provided highlight the interdependence of time series and forecasting, with forecasting being a critical component of time series analysis. The use of forecasting in time series analysis enables the prediction of future values or trends, which is essential for various applications, including business, finance, and economics.\n\nIn summary, the entity \"TIME SERIES\" is inextricably linked to the entity \"FORECASTING,\" with forecasting being a key feature of time series analysis. The relationship between these two entities is characterized by the use of forecasting to predict future values or trends in a time series, which is a critical aspect of time series analysis.\n\nRelevant information from the nearby text, including the use of technical terms, mathematical equations, and references to academic papers, further supports the conclusion that time series and forecasting are closely related entities. The presence of English-language abbreviations and citations of English-language academic papers also suggests that the text is written in English, which is consistent with the formal and academic tone of the descriptions provided.", "from": "TIME SERIES", "source_id": "171fb6df1bf9905b151dfb846d75d0f8,6383536333b1a09cc9e6f8d4ea5e9bce,63e284ec7e76fa859cc46b72d3746654,b8ce119147e4d1454453c514e68dc4dc,cf0d73cbe44e03ef7300f5c53b72090a,e900311a447985c0967f87fff49c58b8,f6fac8e5c6fd12724fe3aa84a2e1cfa6", "to": "FORECASTING", "width": 7.0}, {"description": "Time series is related to hyperparameters as hyperparameters are used to optimize the forecasting of time series", "from": "TIME SERIES", "source_id": "f72ba51a17dd00cff43bcdda22c273e8", "to": "HYPERPARAMETERS", "width": 1.0}, {"description": "T5 is related to time series as it is a type of model that can be used for time series forecasting", "from": "TIME SERIES", "source_id": "c522ea3766ae5129c11b833252340695", "to": "T5", "width": 1.0}, {"description": "Chronos-T5 is related to time series as it is a type of model that can be used for time series forecasting", "from": "TIME SERIES", "source_id": "c522ea3766ae5129c11b833252340695", "to": "CHRONOS-T5", "width": 1.0}, {"description": "Time series is related to AR process as AR process is used to model time series data", "from": "TIME SERIES", "source_id": "9c155897f3e35902f57696e0abaeb161", "to": "AR PROCESS", "width": 1.0}, {"description": "Time series is related to model deployment as model deployment is used to make time series forecasting models available for use", "from": "TIME SERIES", "source_id": "116332ac4538a1430c83a34fcbec22d1", "to": "MODEL DEPLOYMENT", "width": 1.0}, {"description": "Time series is related to forecasting pipelines as forecasting pipelines are used to generate forecasts using time series data", "from": "TIME SERIES", "source_id": "116332ac4538a1430c83a34fcbec22d1", "to": "FORECASTING PIPELINES", "width": 1.0}, {"description": "Algorithm 2 is related to time series as it generates synthetic time series data", "from": "TIME SERIES", "source_id": "e5e4a8f03f502fada5b17cba5dc942ba", "to": "ALGORITHM 2", "width": 1.0}, {"description": "Algorithm 1 is related to time series as it takes in time series datasets and outputs an augmented time series", "from": "TIME SERIES", "source_id": "e5e4a8f03f502fada5b17cba5dc942ba", "to": "ALGORITHM 1", "width": 1.0}, {"description": "Predictions are related to time series as predictions are made for a given time series", "from": "TIME SERIES", "source_id": "f0808ad97bc4d26391284145427770e5", "to": "PREDICTIONS", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTime series is closely related to TimeGPT, a pre-trained foundation model specifically designed for time series forecasting. TimeGPT is equipped with features that enable it to effectively handle time series data, including uncertainty quantification and fine-tuning capabilities. The model takes historical values of time series as inputs to produce forecasts, leveraging its design to accurately predict future values. Overall, TimeGPT\u0027s primary function is to forecast time series data, making it a valuable tool for time series forecasting applications.", "from": "TIME SERIES", "source_id": "42e1bb44edbe787e104f589e74b95d6c,6383536333b1a09cc9e6f8d4ea5e9bce,f8f6fec6d1d7eda0349d3c52c4c96d97,fb67fcff21ac521a5ed8b202412ec1fc", "to": "TIMEGPT", "width": 4.0}, {"description": "Transformers are related to time series as they are commonly used for time series forecasting", "from": "TIME SERIES", "source_id": "b8ce119147e4d1454453c514e68dc4dc", "to": "TRANSFORMERS", "width": 1.0}, {"description": "Zalando is related to time series as they have contributed to the development of time series models", "from": "TIME SERIES", "source_id": "b8ce119147e4d1454453c514e68dc4dc", "to": "ZALANDO", "width": 1.0}, {"description": "OpenAI is related to time series as they have contributed to the development of time series models", "from": "TIME SERIES", "source_id": "b8ce119147e4d1454453c514e68dc4dc", "to": "OPENAI", "width": 1.0}, {"description": "Alibaba is related to time series as they have contributed to the development of time series models", "from": "TIME SERIES", "source_id": "b8ce119147e4d1454453c514e68dc4dc", "to": "ALIBABA", "width": 1.0}, {"description": "Amazon is related to time series as they have contributed to the development of time series models", "from": "TIME SERIES", "source_id": "b8ce119147e4d1454453c514e68dc4dc", "to": "AMAZON", "width": 1.0}, {"description": "Kunz et al. are related to time series as they have contributed to the development of time series models", "from": "TIME SERIES", "source_id": "b8ce119147e4d1454453c514e68dc4dc", "to": "KUNZ ET AL.", "width": 1.0}, {"description": "Brown et al. are related to time series as they have contributed to the development of time series models", "from": "TIME SERIES", "source_id": "b8ce119147e4d1454453c514e68dc4dc", "to": "BROWN ET AL.", "width": 1.0}, {"description": "Eisenach et al. are related to time series as they have contributed to the development of time series models", "from": "TIME SERIES", "source_id": "b8ce119147e4d1454453c514e68dc4dc", "to": "EISENACH ET AL.", "width": 1.0}, {"description": "Zeng et al. are related to time series as they have contributed to the development of time series models", "from": "TIME SERIES", "source_id": "b8ce119147e4d1454453c514e68dc4dc", "to": "ZENG ET AL.", "width": 1.0}, {"description": "Uncertainty quantification is a feature of time series forecasting", "from": "TIME SERIES", "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce", "to": "UNCERTAINTY QUANTIFICATION", "width": 1.0}, {"description": "Fine-tuning is a feature of time series forecasting", "from": "TIME SERIES", "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce", "to": "FINE-TUNING", "width": 1.0}, {"description": "Calendar variables are used in time series forecasting", "from": "TIME SERIES", "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce", "to": "CALENDAR VARIABLES", "width": 1.0}, {"description": "Exogenous variables are used in time series forecasting", "from": "TIME SERIES", "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce", "to": "EXOGENOUS VARIABLES", "width": 1.0}, {"description": "Lag-Llama is related to time series as it is a model for time series forecasting", "from": "TIME SERIES", "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "to": "LAG-LAGA", "width": 1.0}, {"description": "Time series is related to probabilistic time series forecasting as it is the task of predicting the future values of a time series", "from": "TIME SERIES", "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "to": "PROBABILISTIC TIME SERIES FORECASTING", "width": 1.0}, {"description": "The foundation model approach is related to time series as it is applied to time series data", "from": "TIME SERIES", "source_id": "c4e47033b8ecc85beacde9d560e66062", "to": "FOUNDATION MODEL APPROACH", "width": 1.0}, {"description": "A predictive model is used to predict the values of a time series", "from": "TIME SERIES", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "PREDICTIVE MODEL", "width": 1.0}, {"description": "A time series is used to evaluate the performance of the predictive model", "from": "TIME SERIES", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "TEST DATASET", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"TIME SERIES\" is closely related to the entity \"DATASETS.\" Specifically, datasets are associated with time series as time series are utilized to train and test models. Furthermore, datasets contain time series data, which is a crucial aspect of their composition.\n\nThis summary is derived from the provided descriptions, which collectively convey the interconnectedness of time series and datasets. The descriptions suggest that time series are not only used to train and test models but are also a fundamental component of datasets. This relationship is essential in understanding the context and application of time series and datasets in various fields, such as machine learning and data analysis.\n\nIn resolving any potential contradictions, it is clear that the descriptions are consistent in their portrayal of the relationship between time series and datasets. The summary provided above accurately reflects the information contained in the descriptions, highlighting the integral role of time series in datasets and the importance of datasets in housing time series data.", "from": "TIME SERIES", "source_id": "51ee17c1f0212d4c94010d8e376f649b,6c11bd339c9630f1d61f2024e90bce5e", "to": "DATASETS", "width": 2.0}, {"description": "Time series is related to pretraining corpus as pretraining corpus is used to train the model", "from": "TIME SERIES", "source_id": "51ee17c1f0212d4c94010d8e376f649b", "to": "PRETRAINING CORPUS", "width": 1.0}, {"description": "Time series is related to probabilistic forecasting as probabilistic forecasting is used for time series", "from": "TIME SERIES", "source_id": "f0c52387a7b3a5c3850fe6991f0a7c83", "to": "PROBABILISTIC FORECASTING", "width": 1.0}, {"description": "Inductive bias is related to time series as time series analysis can be affected by inductive bias", "from": "TIME SERIES", "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "to": "INDUCTIVE BIAS", "width": 1.0}, {"description": "Time series characteristics are used to select features for classification ability", "from": "TIME SERIES", "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "to": "CANONICAL TIME SERIES CHARACTERISTICS", "width": 1.0}, {"description": "Time series is related to Lag-Llama as Lag-Llama is used for time series forecasting in the text", "from": "TIME SERIES", "source_id": "e995e5477f470244a4a6afb9417f6d96", "to": "LAG-LLAMA", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"TIME SERIES\" is a sequence of data points measured at regular time intervals, and it is closely related to the entity \"TIME SERIES FORECASTING\". Time series forecasting is a process that involves predicting future values in a time series, which is a crucial aspect of understanding and analyzing the behavior of the time series. This process is directly related to the time series itself, as it involves predicting future values of the time series.\n\nIn essence, time series forecasting is a critical component of time series analysis, and it relies heavily on the understanding of the underlying patterns and trends in the time series. The process of time series forecasting can be achieved through various methods, including frequency analysis, multi-head cross-attention, and other advanced techniques.\n\nOverall, the relationship between time series and time series forecasting is fundamental, and a deep understanding of this connection is essential for effective time series analysis and forecasting.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which is consistent with the formal and academic tone of the descriptions provided.\n* The text also mentions the use of technical terms, mathematical equations, and references to academic papers, which further supports the conclusion that the text is from a research paper or a technical document.\n* The descriptions provided are consistent with the topic of time series analysis and forecasting, and they do not contain any contradictory information.\n\nTherefore, the summary generated is a coherent and accurate representation of the information provided.", "from": "TIME SERIES", "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8,8f4724ff6541b8924f0cebe9872ed040", "to": "TIME SERIES FORECASTING", "width": 2.0}, {"description": "Time series is related to large language models as large language models can be used to predict future values of a time series", "from": "TIME SERIES", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "LARGE LANGUAGE MODELS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"TIME SERIES\" is closely related to \"TIME-LLM\", a reprogramming framework designed to repurpose Large Language Models (LLMs) for general time series forecasting. TIME-LLM is specifically optimized for time series forecasting, leveraging its capabilities to provide accurate predictions. In essence, TIME-LLM is tailored to address the complexities of time series data, making it a valuable tool for time series forecasting applications.\n\nThis summary is derived from the descriptions provided, which collectively convey the relationship between TIME SERIES and TIME-LLM. The descriptions highlight TIME-LLM\u0027s purpose of repurposing LLMs for time series forecasting and its optimization for this specific task. By combining these insights, a clear understanding of the connection between TIME SERIES and TIME-LLM can be established.", "from": "TIME SERIES", "source_id": "41fe893a178ebc8790ef4da83da5ab6e,8f4724ff6541b8924f0cebe9872ed040,fececbac281c1e2b13921f378df30919", "to": "TIME-LLM", "width": 3.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"TIME SERIES\" is closely related to the entity \"LLM\" (Large Language Model). The relationship between these two entities is centered around the application of LLM in time series forecasting and analysis. Specifically, LLM is utilized for predicting future values or outcomes based on historical data, which is a key aspect of time series analysis. In turn, time series is relevant to LLM as it can be used for time series forecasting, indicating a reciprocal relationship between the two entities.\n\nThis summary is based on the provided descriptions, which collectively convey the connection between LLM and time series. The descriptions are consistent and do not contain any contradictions, allowing for a coherent and unified summary to be generated.", "from": "TIME SERIES", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,c84edbea28fbbed451e8d0b7df4ffb7c,ec3fbfb800fd9bf1d913584fda4ae925", "to": "LLM", "width": 3.0}, {"description": "Time series has a trend that refers to the overall direction or pattern in a time series", "from": "TIME SERIES", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "TREND", "width": 1.0}, {"description": "Time series has a lag that refers to the delay or time difference between two events or values in a time series", "from": "TIME SERIES", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "LAG", "width": 1.0}, {"description": "Time Series Library is related to time series as it is a software library for time series analysis", "from": "TIME SERIES", "source_id": "f6fac8e5c6fd12724fe3aa84a2e1cfa6", "to": "TIME SERIES LIBRARY", "width": 1.0}, {"description": "Time series is related to dependency discovery as dependency discovery is used to identify relationships between variables in a time series", "from": "TIME SERIES", "source_id": "f7266cfccedb1d9840d10afa689a05e9", "to": "DEPENDENCY DISCOVERY", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"TIME SERIES\" is closely related to the entity \"ELECTRICITY\". This relationship is established through the fact that electricity usage can be measured and analyzed over time, thereby making it a type of time series data. In other words, electricity is a specific type of time series data that can be used to analyze and understand patterns and trends in electricity usage over time.\n\nThis connection is further reinforced by the fact that time series analysis is a crucial aspect of understanding and predicting electricity usage patterns. By analyzing time series data related to electricity, researchers and analysts can gain valuable insights into energy consumption patterns, identify trends and anomalies, and make informed decisions to optimize energy distribution and consumption.\n\nThe use of time series analysis in the context of electricity is also supported by the presence of technical terms and mathematical equations related to frequency analysis and multi-head cross-attention, which are commonly used in time series forecasting and analysis. Additionally, the mention of academic conferences and journals such as ICLR, AAAI, and PMLR suggests that the relationship between time series and electricity is a topic of ongoing research and study in the academic community.\n\nOverall, the relationship between TIME SERIES and ELECTRICITY is one of mutual dependence, with electricity being a specific type of time series data and time series analysis being a crucial tool for understanding and predicting electricity usage patterns.", "from": "TIME SERIES", "source_id": "171fb6df1bf9905b151dfb846d75d0f8,c84edbea28fbbed451e8d0b7df4ffb7c,fd1092903d83bf6e90a6caa371d7c514", "to": "ELECTRICITY", "width": 3.0}, {"description": "Average is related to time series as average is a measure of central tendency that can be applied to time series data", "from": "TIME SERIES", "source_id": "cf0d73cbe44e03ef7300f5c53b72090a", "to": "AVERAGE", "width": 1.0}, {"description": "Time series is related to frequency analysis as frequency analysis can be used to analyze the frequency of occurrence of different values or patterns in a time series", "from": "TIME SERIES", "source_id": "cf0d73cbe44e03ef7300f5c53b72090a", "to": "FREQUENCY ANALYSIS", "width": 1.0}, {"description": "Zero-shot forecasting is related to time series as it is a concept that refers to a sequence of data points measured at regular time intervals", "from": "TIME SERIES", "source_id": "41fe893a178ebc8790ef4da83da5ab6e", "to": "ZERO-SHOT FORECASTING", "width": 1.0}, {"description": "Time series is related to weather as weather is a type of time series data", "from": "TIME SERIES", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c,fd1092903d83bf6e90a6caa371d7c514", "to": "WEATHER", "width": 2.0}, {"description": "Time series is related to traffic as traffic is a type of time series data", "from": "TIME SERIES", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c,fd1092903d83bf6e90a6caa371d7c514", "to": "TRAFFIC", "width": 2.0}, {"description": "Models were used to analyze the time series data", "from": "TIME SERIES", "source_id": "15c3350fad556f666d93817c5036109c", "to": "MODELS", "width": 1.0}, {"description": "Time series is related to data preprocessing as data preprocessing is applied to time series data", "from": "TIME SERIES", "source_id": "477c5b7f23f4e00030ab389788a4c88d", "to": "DATA PREPROCESSING", "width": 1.0}, {"description": "Time series is related to the training process as the model is trained on a dataset of time series data", "from": "TIME SERIES", "source_id": "6ea15432a841705c2e74cbc01f6004e9", "to": "TRAINING PROCESS", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nRESTAD is a model primarily designed for anomaly detection. Specifically, it is an unsupervised anomaly detection model, indicating that it operates without the need for labeled training data. This suggests that RESTAD is capable of identifying anomalies or unusual patterns in data without prior knowledge of what constitutes an anomaly.\n\nThe entity ANOMALY DETECTION is closely related to RESTAD, as it is the primary application or purpose of the model. Anomaly detection is a critical task in various fields, including data analysis, security, and quality control, where identifying unusual patterns or outliers can be crucial for decision-making.\n\nOverall, RESTAD is a specialized model designed to perform unsupervised anomaly detection, making it a valuable tool for data analysts and researchers working in the field of ANOMALY DETECTION.", "from": "ANOMALY DETECTION", "source_id": "308a1b7226a66096a6e686d5e99848ad,99b3fe01a22282fe6d02bf29dacf8875", "to": "RESTAD", "width": 2.0}, {"description": "Anomaly detection is related to reconstruction error as reconstruction error is a measure of how well a model can reconstruct a given input", "from": "ANOMALY DETECTION", "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "to": "RECONSTRUCTION ERROR", "width": 1.0}, {"description": "Anomaly detection is related to AnomalyTrans as AnomalyTrans is a model that uses the concept of association discrepancy to improve unsupervised anomaly detection", "from": "ANOMALY DETECTION", "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "to": "ANOMALY TRANS", "width": 1.0}, {"description": "Anomaly detection is related to RBF enhanced anomaly detection as RBF enhanced anomaly detection is a method for improving anomaly detection", "from": "ANOMALY DETECTION", "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2", "to": "RBF ENHANCED ANOMALY DETECTION", "width": 1.0}, {"description": "The composite anomaly score is used for anomaly detection", "from": "ANOMALY DETECTION", "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "to": "COMPOSITE ANOMALY SCORE", "width": 1.0}, {"description": "Self-supervised transformer is used for anomaly detection", "from": "ANOMALY DETECTION", "source_id": "3e0985c172a09bb6c99e305b9f40a513", "to": "SELF-SUPERVISED TRANSFORMER", "width": 1.0}, {"description": "AnomalyBERT is used for anomaly detection", "from": "ANOMALY DETECTION", "source_id": "3e0985c172a09bb6c99e305b9f40a513,587ca8c6cc86a93299e9540153babbc6", "to": "ANOMALYBERT", "width": 2.0}, {"description": "Data degradation scheme is used for anomaly detection", "from": "ANOMALY DETECTION", "source_id": "3e0985c172a09bb6c99e305b9f40a513", "to": "DATA DEGRADATION SCHEME", "width": 1.0}, {"description": "Anomaly detection is a task that can be handled using recurrent neural networks", "from": "ANOMALY DETECTION", "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "to": "RECURRENT NEURAL NETWORKS", "width": 1.0}, {"description": "Anomaly detection is related to LOF as LOF is a classical algorithm for density-based outlier detection", "from": "ANOMALY DETECTION", "source_id": "1303ca4c43652bb8052df34d21c78eca", "to": "LOCAL OUTLIER FACTOR (LOF)", "width": 1.0}, {"description": "Anomaly detection can be performed using the Chronos-T5 model", "from": "ANOMALY DETECTION", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "CHRONOS-T5", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTranAD is a model specifically designed for anomaly detection, which involves identifying unusual patterns or data points. This model is utilized for the purpose of detecting anomalies, indicating its primary function is to identify and flag data points that deviate from the expected behavior or patterns in the data.\n\nThe TranAD model is closely related to the concept of ANOMALY DETECTION, which is a broader field of study that focuses on identifying and understanding unusual or unexpected events or patterns in data. The TranAD model is a key component in this field, providing a practical solution for detecting anomalies in various types of data.\n\nOverall, TranAD is a specialized model designed to support the ANOMALY DETECTION process, enabling users to identify and analyze unusual patterns or data points in their data.", "from": "ANOMALY DETECTION", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3,78ee4a3d7a2bffd4405a03d94a4f6cb1", "to": "TRANAD", "width": 2.0}, {"description": "Anomaly detection is related to time series as time series is used as input for anomaly detection", "from": "ANOMALY DETECTION", "source_id": "bd73ee0439609823e12a877840c6ebae", "to": "TIME-SERIES", "width": 1.0}, {"description": "Anomaly detection is related to non-parametric dynamic thresholding as it is an approach for detecting anomalies from prediction errors", "from": "ANOMALY DETECTION", "source_id": "bd73ee0439609823e12a877840c6ebae", "to": "NON-PARAMETRIC DYNAMIC THRESHOLDING", "width": 1.0}, {"description": "Anomaly detection and diagnosis are related as both are processes that identify anomalies", "from": "ANOMALY DETECTION", "source_id": "1902e651467179a9a1d4c4df0035e980", "to": "ANOMALY DIAGNOSIS", "width": 1.0}, {"description": "The datasets used in the experiments are related to anomaly detection as they are used to evaluate the performance of the TranAD approach", "from": "ANOMALY DETECTION", "source_id": "9c6e74299923071f9fc2b7cce49efc90", "to": "DATASETS", "width": 1.0}, {"description": "Anomaly detection is related to transformer networks as the TranAD approach uses transformer networks to detect anomalies", "from": "ANOMALY DETECTION", "source_id": "9c6e74299923071f9fc2b7cce49efc90", "to": "TRANSFORMER NETWORKS", "width": 1.0}, {"description": "Meta-learning is used in TranAD to improve its anomaly detection performance, particularly on limited training data", "from": "ANOMALY DETECTION", "source_id": "78ee4a3d7a2bffd4405a03d94a4f6cb1", "to": "META-LEARNING", "width": 1.0}, {"description": "Radial Basis Function (RBF) is used in the Transformer model", "from": "RADIAL BASIS FUNCTION (RBF)", "source_id": "308a1b7226a66096a6e686d5e99848ad", "to": "TRANSFORMER", "width": 1.0}, {"description": "Transformer is used in the research lab", "from": "TRANSFORMER", "source_id": "308a1b7226a66096a6e686d5e99848ad", "to": "PATTERN RECOGNITION LAB", "width": 1.0}, {"description": "LSTM and Transformer are both types of neural network architectures used for anomaly detection", "from": "TRANSFORMER", "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "to": "LSTM", "width": 1.0}, {"description": "USAD and Transformer are both types of anomaly detection models", "from": "TRANSFORMER", "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "to": "USAD", "width": 1.0}, {"description": "Transformer and AnomalyTrans are both types of neural network architectures used for anomaly detection", "from": "TRANSFORMER", "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "to": "ANOMALYTRANS", "width": 1.0}, {"description": "RESTAD is an adaptation of the Transformer architecture for unsupervised anomaly detection", "from": "TRANSFORMER", "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "to": "RESTAD", "width": 1.0}, {"description": "The Transformer architecture has been widely used in natural language processing tasks, such as language translation and text classification.", "from": "TRANSFORMER", "source_id": "7dbc961fe1959f844ebac283498e7fb0", "to": "NATURAL LANGUAGE PROCESSING", "width": 1.0}, {"description": "VAE is related to Transformer as it is a type of neural network used for anomaly detection and other tasks", "from": "TRANSFORMER", "source_id": "1303ca4c43652bb8052df34d21c78eca", "to": "VARIATIONAL AUTOENCODER (VAE)", "width": 1.0}, {"description": "Transformer is related to attention mechanism as it is a concept used in the Transformer model", "from": "TRANSFORMER", "source_id": "1303ca4c43652bb8052df34d21c78eca", "to": "ATTENTION MECHANISM", "width": 1.0}, {"description": "The Transformer architecture is used in ViT as the main body of the model", "from": "TRANSFORMER", "source_id": "a5d60c1a355b77187003182f6df57646", "to": "VIT", "width": 1.0}, {"description": "The paper was presented at ICLR 2023, which is a conference focused on machine learning", "from": "TRANSFORMER", "source_id": "a5d60c1a355b77187003182f6df57646", "to": "ICLR 2023", "width": 1.0}, {"description": "Dataset is related to transformer as it is used to train and test the model", "from": "TRANSFORMER", "source_id": "71f873c12230e6ede47583d81eb85e23", "to": "DATASET", "width": 1.0}, {"description": "AnomalyBERT is related to transformer as it uses a transformer-based model", "from": "TRANSFORMER", "source_id": "71f873c12230e6ede47583d81eb85e23", "to": "ANOMALYBERT", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities in question are \"TRANSFORMER\" and \"TIMEGPT\". \n\nAccording to the descriptions, TimeGPT is based on the Transformer architecture and is related to Transformer as TimeGPT is a Transformer-based model. This indicates that TimeGPT is a variant or an extension of the Transformer architecture, leveraging its core principles and components.\n\nGiven the information collected from all the descriptions, it appears that TimeGPT is a model that builds upon the Transformer architecture, suggesting a strong connection between the two entities. The Transformer architecture is a well-known neural network architecture used in natural language processing and other applications, and TimeGPT\u0027s reliance on it implies that it inherits its strengths and capabilities.\n\nOverall, the summary highlights the relationship between TimeGPT and the Transformer architecture, emphasizing TimeGPT\u0027s status as a Transformer-based model.", "from": "TRANSFORMER", "source_id": "518bfcd6711530089fe3914ca16459c2,89e5f6a93205d5e87ad7e7641c0bbb91", "to": "TIMEGPT", "width": 2.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe Transformer and Self-Attention entities are closely related in the context of neural network architectures used in natural language processing and time series forecasting. Specifically, Self-Attention is a mechanism used in the Transformer architecture, which is a type of neural network architecture itself. This suggests a hierarchical relationship between the two entities, where Self-Attention is a component of the Transformer architecture.\n\nThis relationship is further supported by the fact that both Transformer and Self-Attention are used in applications such as time series forecasting, where the ability to capture long-term dependencies and relationships between different elements is crucial. The use of Self-Attention in the Transformer architecture enables it to effectively model these relationships and make accurate predictions.\n\nOverall, the Transformer and Self-Attention entities are intimately connected, with Self-Attention being a key component of the Transformer architecture and both being used in a variety of applications, including time series forecasting.\n\nRelevant information from the nearby text that has been incorporated into this summary includes:\n\n* The use of technical terms such as \"neural network architectures,\" \"natural language processing,\" and \"time series forecasting,\" which suggests a technical and academic context.\n* The mention of the Transformer architecture as a type of neural network architecture, which implies a high level of complexity and sophistication.\n* The use of the phrase \"long-term dependencies and relationships,\" which suggests that the Transformer and Self-Attention entities are being used to model complex and nuanced patterns in data.\n\nThis summary provides a clear and concise description of the relationship between the Transformer and Self-Attention entities, and highlights their importance in the context of time series forecasting and natural language processing.", "from": "TRANSFORMER", "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91,f7266cfccedb1d9840d10afa689a05e9", "to": "SELF-ATTENTION", "width": 2.0}, {"description": "Transformer is related to time series forecasting as transformer is used in time series forecasting", "from": "TRANSFORMER", "source_id": "f7266cfccedb1d9840d10afa689a05e9", "to": "TIME SERIES FORECASTING", "width": 1.0}, {"description": "Pattern Recognition Lab is located at Delft University of Technology", "from": "PATTERN RECOGNITION LAB", "source_id": "308a1b7226a66096a6e686d5e99848ad", "to": "DELFT UNIVERSITY OF TECHNOLOGY", "width": 1.0}, {"description": "Ramin Ghorbani is affiliated with Delft University of Technology", "from": "DELFT UNIVERSITY OF TECHNOLOGY", "source_id": "308a1b7226a66096a6e686d5e99848ad", "to": "RAMIN GHORBANI", "width": 1.0}, {"description": "Marcel J.T. Reinders is affiliated with Delft University of Technology", "from": "DELFT UNIVERSITY OF TECHNOLOGY", "source_id": "308a1b7226a66096a6e686d5e99848ad", "to": "MARCEL J.T. REINDERS", "width": 1.0}, {"description": "David M.J. Tax is affiliated with Delft University of Technology", "from": "DELFT UNIVERSITY OF TECHNOLOGY", "source_id": "308a1b7226a66096a6e686d5e99848ad", "to": "DAVID M.J. TAX", "width": 1.0}, {"description": "Ramin Ghorbani and Marcel J.T. Reinders are co-authors of the paper", "from": "RAMIN GHORBANI", "source_id": "308a1b7226a66096a6e686d5e99848ad", "to": "MARCEL J.T. REINDERS", "width": 1.0}, {"description": "Ramin Ghorbani and David M.J. Tax are co-authors of the paper", "from": "RAMIN GHORBANI", "source_id": "308a1b7226a66096a6e686d5e99848ad", "to": "DAVID M.J. TAX", "width": 1.0}, {"description": "Marcel J.T. Reinders and David M.J. Tax are co-authors of the paper", "from": "MARCEL J.T. REINDERS", "source_id": "308a1b7226a66096a6e686d5e99848ad", "to": "DAVID M.J. TAX", "width": 1.0}, {"description": "Radial basis function is related to RESTAD as RESTAD is a model that combines reconstruction error with a radial basis function to improve anomaly detection", "from": "RESTAD", "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "to": "RADIAL BASIS FUNCTION", "width": 1.0}, {"description": "RESTAD is related to input signal as input signal refers to the original data that is being analyzed for anomalies", "from": "RESTAD", "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "to": "INPUT SIGNAL", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nRESTAD uses reconstruction error as a measure of the difference between the original input data and the reconstructed data. Specifically, RESTAD utilizes reconstruction error to compute a distinctive anomaly score, indicating its direct relationship with this metric. This connection highlights the significance of reconstruction error in the context of RESTAD, showcasing its role in evaluating the performance and accuracy of the model.\n\nIn essence, RESTAD\u0027s reliance on reconstruction error underscores its focus on measuring the discrepancy between the original and reconstructed data, which is a critical aspect of its anomaly detection capabilities. By leveraging reconstruction error, RESTAD can effectively identify and quantify anomalies, making it a valuable tool in various applications where anomaly detection is crucial.\n\nThe entity \"RECONSTRUCTION ERROR\" is a key concept in this context, serving as a measure of the difference between the original input data and the reconstructed data. Its relationship with RESTAD is fundamental, as RESTAD uses reconstruction error to compute its anomaly score. This connection highlights the importance of reconstruction error in the context of RESTAD, emphasizing its role in evaluating the model\u0027s performance and accuracy.\n\nOverall, the summary provides a clear understanding of the relationship between RESTAD and reconstruction error, highlighting the significance of this metric in the context of RESTAD\u0027s anomaly detection capabilities.", "from": "RESTAD", "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2,59e8e02df5f942071e733def7884b457", "to": "RECONSTRUCTION ERROR", "width": 2.0}, {"description": "RESTAD is related to benchmark datasets as RESTAD is evaluated on benchmark datasets", "from": "RESTAD", "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2", "to": "BENCHMARK DATASETS", "width": 1.0}, {"description": "RESTAD uses multi-head attention as a key component of its architecture", "from": "RESTAD", "source_id": "59e8e02df5f942071e733def7884b457", "to": "MULTI-HEAD ATTENTION", "width": 1.0}, {"description": "RESTAD uses dropout as a regularization technique to prevent overfitting", "from": "RESTAD", "source_id": "59e8e02df5f942071e733def7884b457", "to": "DROPOUT", "width": 1.0}, {"description": "RESTAD uses norm as a measure of the magnitude or size of the input data", "from": "RESTAD", "source_id": "59e8e02df5f942071e733def7884b457", "to": "NORM", "width": 1.0}, {"description": "RESTAD uses feed-forward as a type of neural network architecture", "from": "RESTAD", "source_id": "59e8e02df5f942071e733def7884b457", "to": "FEED-FORWARD", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nRESTAD is a proposed model that integrates an RBF (Radial Basis Function) layer into the Transformer architecture. The RBF layer is a type of neural network layer used in RESTAD. By combining the vanilla Transformer with the RBF layer and a composite anomaly score, RESTAD aims to improve long-term series forecasting capabilities.\n\nThe use of the RBF layer in RESTAD is likely to enable frequency analysis and multi-head cross-attention mechanisms, which are commonly used in time series forecasting tasks. The integration of the RBF layer into the Transformer architecture may also allow for more accurate and robust anomaly detection.\n\nOverall, RESTAD appears to be a novel approach to long-term series forecasting that leverages the strengths of both the Transformer architecture and the RBF layer.", "from": "RESTAD", "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7,59e8e02df5f942071e733def7884b457,99b3fe01a22282fe6d02bf29dacf8875", "to": "RBF LAYER", "width": 3.0}, {"description": "RESTAD uses RBF score as a measure of the similarity between the input data and the reconstructed data", "from": "RESTAD", "source_id": "59e8e02df5f942071e733def7884b457", "to": "RBF SCORE", "width": 1.0}, {"description": "The proposed model RESTAD combines the vanilla Transformer with the RBF layer and composite anomaly score", "from": "RESTAD", "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7", "to": "COMPOSITE ANOMALY SCORE", "width": 1.0}, {"description": "The vanilla Transformer is used as a baseline model for comparison with the proposed model RESTAD", "from": "RESTAD", "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7", "to": "VANILLA TRANSFORMER", "width": 1.0}, {"description": "AUC-ROC is a metric used to evaluate the performance of the RESTAD model", "from": "RESTAD", "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "to": "AUC-ROC", "width": 1.0}, {"description": "Dutch Research Council (NWO) provided funding for the RESTAD project", "from": "RESTAD", "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "to": "DUTCH RESEARCH COUNCIL", "width": 1.0}, {"description": "Similarity scores are related to reconstruction error as similarity scores and reconstruction error are both used in anomaly detection", "from": "RECONSTRUCTION ERROR", "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2", "to": "SIMILARITY SCORES", "width": 1.0}, {"description": "The RBF score is combined with the reconstruction error to form the composite anomaly score", "from": "RECONSTRUCTION ERROR", "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "to": "RBF SCORE", "width": 1.0}, {"description": "The reconstruction error is used to form the composite anomaly score", "from": "RECONSTRUCTION ERROR", "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "to": "COMPOSITE ANOMALY SCORE", "width": 1.0}, {"description": "Based on the provided information, the entity names are \"RECONSTRUCTION ERROR\" and \"RBF LAYER\". The description list provides two descriptions related to these entities.\n\nAfter analyzing the descriptions, it appears that they are not contradictory but rather complementary. The RBF layer is described as integrating RBF similarity scores with reconstruction error, and also combining the reconstruction error and dissimilarity score to form a composite anomaly score.\n\nHere is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe \"RBF LAYER\" is a component that plays a crucial role in integrating and combining various scores to form a composite anomaly score. Specifically, it integrates RBF similarity scores with \"RECONSTRUCTION ERROR\" to form a composite score. This suggests that the RBF layer is designed to leverage the strengths of both RBF similarity scores and reconstruction error to improve anomaly detection or classification.\n\nThe use of reconstruction error in the RBF layer is likely to provide a measure of how well a model can reconstruct or replicate the input data, which can be an important factor in anomaly detection. By combining this with RBF similarity scores, the RBF layer can provide a more comprehensive and robust anomaly score.\n\nOverall, the RBF layer appears to be a critical component in a larger system or model that is designed to detect or classify anomalies, and its integration with reconstruction error is a key aspect of its functionality.", "from": "RECONSTRUCTION ERROR", "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7,99b3fe01a22282fe6d02bf29dacf8875", "to": "RBF LAYER", "width": 2.0}, {"description": "The reconstruction error and dissimilarity score are combined to form a composite anomaly score", "from": "RECONSTRUCTION ERROR", "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7", "to": "DISSIMILARITY SCORE", "width": 1.0}, {"description": "Reconstruction error is combined with RBF similarity scores in the RESTAD model", "from": "RECONSTRUCTION ERROR", "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "to": "RBF SIMILARITY SCORES", "width": 1.0}, {"description": "AnomalyTrans is related to association discrepancy as association discrepancy is a measure of how similar a time point is to its adjacent time points", "from": "ANOMALY TRANS", "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "to": "ASSOCIATION DISCREPANCY", "width": 1.0}, {"description": "Association discrepancy is related to radial basis function as radial basis function is a type of non-linear transformation that can be used to improve anomaly detection", "from": "ASSOCIATION DISCREPANCY", "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "to": "RADIAL BASIS FUNCTION", "width": 1.0}, {"description": "Input signal is related to reconstructed signal as reconstructed signal refers to the output of a model that is attempting to reconstruct the input signal", "from": "INPUT SIGNAL", "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "to": "RECONSTRUCTED SIGNAL", "width": 1.0}, {"description": "Reconstructed signal is related to subtle anomaly as subtle anomaly refers to a data point that is slightly different from the typical patterns in the data", "from": "RECONSTRUCTED SIGNAL", "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "to": "SUBTLE ANOMALY", "width": 1.0}, {"description": "Subtle anomaly is related to significant anomaly as significant anomaly refers to a data point that is significantly different from the typical patterns in the data", "from": "SUBTLE ANOMALY", "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "to": "SIGNIFICANT ANOMALY", "width": 1.0}, {"description": "Time point is related to RBF center as RBF center refers to the center of a radial basis function", "from": "TIME POINT", "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "to": "RBF CENTER", "width": 1.0}, {"description": "RBF center is related to influence radius as influence radius refers to the range of values that are affected by a radial basis function", "from": "RBF CENTER", "source_id": "d0e314f46695e89479ce8e6543a2e1b5", "to": "INFLUENCE RADIUS", "width": 1.0}, {"description": "RBF enhanced anomaly detection is related to RESTAD framework as RESTAD framework incorporates RBF neurons into the Transformer architecture", "from": "RBF ENHANCED ANOMALY DETECTION", "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2", "to": "RESTAD FRAMEWORK", "width": 1.0}, {"description": "RESTAD framework is related to Transformer model as RESTAD framework uses the Transformer architecture", "from": "RESTAD FRAMEWORK", "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2", "to": "TRANSFORMER MODEL", "width": 1.0}, {"description": "Transformer model is related to RBF neurons as RBF neurons are used in the Transformer architecture", "from": "TRANSFORMER MODEL", "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2", "to": "RBF NEURONS", "width": 1.0}, {"description": "Transformer model is related to STEP as STEP is a model that uses unsupervised pre-trained Transformer blocks to model temporal relationship from long-term history time series", "from": "TRANSFORMER MODEL", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "STEP", "width": 1.0}, {"description": "Transformer model is related to TranAD model as TranAD model is a type of transformer model used for anomaly detection in time-series data", "from": "TRANSFORMER MODEL", "source_id": "f2e78b25a535b82e2743a0ea4052eb9a", "to": "TRANAAD MODEL", "width": 1.0}, {"description": "Weights are used in the Transformer model, as initialized in step 1", "from": "TRANSFORMER MODEL", "source_id": "a65c0f1eae6e779357739df141f75d36", "to": "WEIGHTS", "width": 1.0}, {"description": "Transformer model is used to predict the reconstruction of each input time-series window", "from": "TRANSFORMER MODEL", "source_id": "a65c0f1eae6e779357739df141f75d36", "to": "INPUT TIME-SERIES WINDOW", "width": 1.0}, {"description": "Based on the provided information, the primary entity of interest is the \"TRANSFORMER MODEL\" and its relation to the \"ANOMALY SCORE\". \n\nThe \"TRANSFORMER MODEL\" is utilized to calculate the \"ANOMALY SCORE\", which is a measure of how anomalous a data point is. This suggests that the transformer model plays a crucial role in identifying anomalies within a dataset.\n\nIn essence, the transformer model is used to compute the anomaly score, which is a key metric in determining the level of abnormality in a data point. This relationship between the transformer model and anomaly score is fundamental to understanding how anomalies are detected and analyzed.\n\nGiven the technical nature of the descriptions, it is likely that this information is from a research paper or technical document related to machine learning and time series forecasting. The use of technical terms such as \"transformer model\" and \"anomaly score\" further supports this conclusion.\n\nOverall, the \"TRANSFORMER MODEL\" is a critical component in calculating the \"ANOMALY SCORE\", which is a measure of how anomalous a data point is. This relationship is essential in understanding how anomalies are detected and analyzed in a dataset.", "from": "TRANSFORMER MODEL", "source_id": "1b51ec337efd822ca3a0b3eb819c1b91,6ea15432a841705c2e74cbc01f6004e9", "to": "ANOMALY SCORE", "width": 2.0}, {"description": "Transformer model is related to algorithm 2 as it is used in the inference procedure", "from": "TRANSFORMER MODEL", "source_id": "6ea15432a841705c2e74cbc01f6004e9", "to": "ALGORITHM 2", "width": 1.0}, {"description": "RBF neurons are related to similarity scores as RBF neurons compute similarity scores between data points and reference points", "from": "RBF NEURONS", "source_id": "2cd8d6d8f61b953c1922dd3a39c0dec2", "to": "SIMILARITY SCORES", "width": 1.0}, {"description": "Linear projection is related to multi-head attention as multi-head attention uses linear projection to process sequential data", "from": "MULTI-HEAD ATTENTION", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "LINEAR PROJECTION", "width": 1.0}, {"description": "Multi-head attention is related to handcrafted dataset descriptions as handcrafted dataset descriptions are used to process dataset information", "from": "MULTI-HEAD ATTENTION", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "HANDCRAFTED DATASET DESCRIPTIONS", "width": 1.0}, {"description": "Multi-Head Attention is related to multi-head self attention as it is a type of attention mechanism used in neural networks", "from": "MULTI-HEAD ATTENTION", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "MULTI-HEAD SELF ATTENTION", "width": 1.0}, {"description": "Number of heads is related to dropout as dropout is a hyper-parameter", "from": "DROPOUT", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "NUM HEADS", "width": 1.0}, {"description": "Dropout is related to Monash baselines as Monash baselines use dropout", "from": "DROPOUT", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "MONASH BASELINES", "width": 1.0}, {"description": "Weight decay is related to dropout as both are hyperparameters that determine the optimization strategy", "from": "DROPOUT", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "WEIGHT DECAY", "width": 1.0}, {"description": "The RBF layer uses the K-means initialization method", "from": "RBF LAYER", "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "to": "K-MEANS", "width": 1.0}, {"description": "The RBF layer combines the reconstruction error and dissimilarity score to form a composite anomaly score", "from": "RBF LAYER", "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7", "to": "DISSIMILARITY SCORE", "width": 1.0}, {"description": "K-means is used as an initialization strategy for the RBF layer parameters", "from": "INITIALIZATION", "source_id": "59e8e02df5f942071e733def7884b457", "to": "K-MEANS", "width": 1.0}, {"description": "Random is used as an initialization strategy for the RBF layer parameters", "from": "INITIALIZATION", "source_id": "59e8e02df5f942071e733def7884b457", "to": "RANDOM", "width": 1.0}, {"description": "Language model weights are used to initialize Chronos models, as shown in Figure 8", "from": "INITIALIZATION", "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6", "to": "LANGUAGE MODEL", "width": 1.0}, {"description": "Models initialized with language model weights initially exhibit a faster decrease in training loss, but they ultimately converge to a higher final loss", "from": "INITIALIZATION", "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6", "to": "TRAINING LOSS", "width": 1.0}, {"description": "Server Metrics (PSM) is related to data preprocessing as it is used in the research, as indicated by the reference to it in the text", "from": "DATA PREPROCESSING", "source_id": "9ec74c919ec0aea919a7f2916213ea16", "to": "SERVER METRICS (PSM)", "width": 1.0}, {"description": "Data preprocessing is related to the RESTAD model as it is a step involved in the research, as indicated by the description of normalizing features and segmenting signals", "from": "DATA PREPROCESSING", "source_id": "9ec74c919ec0aea919a7f2916213ea16", "to": "RESTAD MODEL", "width": 1.0}, {"description": "Data preprocessing is related to normalization as normalization is a step in data preprocessing", "from": "DATA PREPROCESSING", "source_id": "477c5b7f23f4e00030ab389788a4c88d", "to": "NORMALIZATION", "width": 1.0}, {"description": "Scale is related to normalization as normalization is the process of mapping time series values into a suitable range for quantization", "from": "NORMALIZATION", "source_id": "6eb4c16edf2eedfd03721efb199478d8", "to": "SCALE", "width": 1.0}, {"description": "Normalization is related to mean scaling as mean scaling is a normalization technique that involves applying an affine transformation to the time series", "from": "NORMALIZATION", "source_id": "6eb4c16edf2eedfd03721efb199478d8", "to": "MEAN SCALING", "width": 1.0}, {"description": "Chronos is related to normalization as normalization is used to map time series values into a suitable range", "from": "NORMALIZATION", "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "to": "CHRONOS", "width": 1.0}, {"description": "Normalization is related to affine transformation as affine transformation is used in normalization", "from": "NORMALIZATION", "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "to": "AFFINE TRANSFORMATION", "width": 1.0}, {"description": "Normalization is related to time series windows as normalization is applied to time series windows", "from": "NORMALIZATION", "source_id": "477c5b7f23f4e00030ab389788a4c88d", "to": "TIME SERIES WINDOWS", "width": 1.0}, {"description": "RESTAD model is related to vanilla Transformer as it is a deep learning model used as a baseline in the research, as indicated by the description of its architecture and implementation", "from": "RESTAD MODEL", "source_id": "9ec74c919ec0aea919a7f2916213ea16", "to": "VANILLA TRANSFORMER", "width": 1.0}, {"description": "Vanilla Transformer is related to RBF kernel layer as it is a component of the RESTAD model, as indicated by the description of its placement in the model architecture", "from": "VANILLA TRANSFORMER", "source_id": "9ec74c919ec0aea919a7f2916213ea16", "to": "RBF KERNEL LAYER", "width": 1.0}, {"description": "RBF kernel layer is related to data embedding as it is a component of the RESTAD model, as indicated by the description of its architecture and implementation", "from": "RBF KERNEL LAYER", "source_id": "9ec74c919ec0aea919a7f2916213ea16", "to": "DATA EMBEDDING", "width": 1.0}, {"description": "Data embedding is related to multi-head self-attention as it is a component of the RESTAD model, as indicated by the description of its architecture and implementation", "from": "DATA EMBEDDING", "source_id": "9ec74c919ec0aea919a7f2916213ea16", "to": "MULTI-HEAD SELF-ATTENTION", "width": 1.0}, {"description": "Multi-head self-attention is related to feed-forward networks as it is a component of the RESTAD model, as indicated by the description of its architecture and implementation", "from": "MULTI-HEAD SELF-ATTENTION", "source_id": "9ec74c919ec0aea919a7f2916213ea16", "to": "FEED-FORWARD NETWORKS", "width": 1.0}, {"description": "Feed-forward networks are related to Adam optimizer as it is an optimization algorithm used in the research, as indicated by the description of its use in training the RESTAD model", "from": "FEED-FORWARD NETWORKS", "source_id": "9ec74c919ec0aea919a7f2916213ea16", "to": "ADAM OPTIMIZER", "width": 1.0}, {"description": "TimesNet is related to feed-forward networks as feed-forward networks are a type of neural network used for classification and regression tasks", "from": "FEED-FORWARD NETWORKS", "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "to": "TIMESNET", "width": 1.0}, {"description": "Feed-forward networks are used in models like N-BEATS [Oreshkin et al., 2019, Olivares et al., 2022]", "from": "FEED-FORWARD NETWORKS", "source_id": "7d5d82d600620153153772a9bc498ac0", "to": "N-BEATS", "width": 1.0}, {"description": "Feed-forward networks are used in models like NHITS [Challu et al., 2023]", "from": "FEED-FORWARD NETWORKS", "source_id": "7d5d82d600620153153772a9bc498ac0", "to": "NHITS", "width": 1.0}, {"description": "Adam optimizer is related to hyperparameters as they are parameters of the RESTAD model, as indicated by the description of their determination through systematic search", "from": "ADAM OPTIMIZER", "source_id": "9ec74c919ec0aea919a7f2916213ea16", "to": "HYPERPARAMETERS", "width": 1.0}, {"description": "Adam optimizer is related to TIME-LLM as Adam optimizer is used for model training", "from": "ADAM OPTIMIZER", "source_id": "1b48e9ca066ac5ba037066bb762d3458", "to": "TIME-LLM", "width": 1.0}, {"description": "Hyperparameters are related to anomaly scores as they are used in identifying anomalies, as indicated by the description of their use in the research", "from": "HYPERPARAMETERS", "source_id": "9ec74c919ec0aea919a7f2916213ea16", "to": "ANOMALY SCORES", "width": 1.0}, {"description": "Hyperparameters are related to training corpus as the training corpus is used to optimize the hyperparameters", "from": "HYPERPARAMETERS", "source_id": "f72ba51a17dd00cff43bcdda22c273e8", "to": "TRAINING CORPUS", "width": 1.0}, {"description": "Training data is related to hyperparameters as hyperparameters were optimized to improve TimeGPT\u0027s performance", "from": "HYPERPARAMETERS", "source_id": "42e1bb44edbe787e104f589e74b95d6c", "to": "TRAINING DATA", "width": 1.0}, {"description": "Hyperparameters are related to GPU cluster as the GPU cluster was used to train TimeGPT with optimized hyperparameters", "from": "HYPERPARAMETERS", "source_id": "42e1bb44edbe787e104f589e74b95d6c", "to": "GPU CLUSTER", "width": 1.0}, {"description": "Hyperparameters are used to train Lag-Llama in the text, as indicated by the results", "from": "HYPERPARAMETERS", "source_id": "e995e5477f470244a4a6afb9417f6d96", "to": "LAG-LLAMA", "width": 1.0}, {"description": "TIME-LLM is related to hyperparameters as hyperparameters are used to tune the model\u0027s performance", "from": "HYPERPARAMETERS", "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "to": "TIME-LLM", "width": 1.0}, {"description": "Hyperparameters are related to backbone model layers as backbone model layers are a type of hyperparameter", "from": "HYPERPARAMETERS", "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "to": "BACKBONE MODEL LAYERS", "width": 1.0}, {"description": "Hyperparameters are related to text prototypes as text prototypes are a type of hyperparameter", "from": "HYPERPARAMETERS", "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "to": "TEXT PROTOTYPES", "width": 1.0}, {"description": "Hyperparameters are related to time series input length as time series input length is a type of hyperparameter", "from": "HYPERPARAMETERS", "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "to": "TIME SERIES INPUT LENGTH", "width": 1.0}, {"description": "Hyperparameters are related to patch reprogramming cross-attention heads as patch reprogramming cross-attention heads are a type of hyperparameter", "from": "HYPERPARAMETERS", "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "to": "PATCH REPROGRAMMING CROSS-ATTENTION HEADS", "width": 1.0}, {"description": "AnomalyBERT is related to anomaly scores as it predicts anomaly scores", "from": "ANOMALY SCORES", "source_id": "ee651b9bedb0cbcb6272a67deda44bb0", "to": "ANOMALYBERT", "width": 1.0}, {"description": "Anomaly prediction process is related to anomaly scores as it predicts anomaly scores", "from": "ANOMALY SCORES", "source_id": "ee651b9bedb0cbcb6272a67deda44bb0", "to": "ANOMALY PREDICTION PROCESS", "width": 1.0}, {"description": "Anomaly score is related to threshold as it is used to determine whether a data point is an anomaly", "from": "THRESHOLD", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "ANOMALY SCORE", "width": 1.0}, {"description": "Sliding window strategy is related to F1-score as F1-score is used to evaluate the performance of a machine learning model that uses sliding window strategy", "from": "F1-SCORE", "source_id": "6917a14af275e30abe2d30a8f8a9a4e6", "to": "SLIDING WINDOW STRATEGY", "width": 1.0}, {"description": "F1-score is related to true positives as it is used to evaluate the performance of an anomaly detection model", "from": "F1-SCORE", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "TRUE POSITIVES", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe \"MSL\" and \"SMD\" datasets are two entities that are closely related in the context of anomaly detection. Both datasets are used in an experiment, as indicated by their presence in a table. Specifically, the SMD dataset is utilized to evaluate the performance of anomaly detection models using the MSL metric. This suggests that the MSL dataset serves as a benchmark or reference point for assessing the effectiveness of anomaly detection models, while the SMD dataset is used to test and validate these models.\n\nIn terms of their relationship, the two datasets are interconnected, with the SMD dataset relying on the MSL metric to evaluate model performance. This implies that the MSL dataset plays a crucial role in the evaluation process, providing a standard against which the performance of anomaly detection models is measured.\n\nOverall, the \"MSL\" and \"SMD\" datasets are two key entities in the context of anomaly detection, with the MSL dataset serving as a benchmark and the SMD dataset being used to evaluate the performance of anomaly detection models.", "from": "MSL", "source_id": "22df9b37bd353b7b484fb07edeeb66fd,6917a14af275e30abe2d30a8f8a9a4e6,69afb4bcb8c1e03975c837102e1d0b32", "to": "SMD", "width": 3.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entities \"MSL\" and \"SMAP\" are both real-world benchmark datasets collected from NASA spacecraft. These datasets are used for anomaly detection, as indicated by their presence in the table, and are related to each other as they are both datasets used in the experiment. Specifically, SMAP and MSL are related as they are both datasets used in the experiments, suggesting a connection between the two in the context of the research or study being conducted.\n\nGiven the context of the text, it appears that both MSL and SMAP are concepts mentioned in the text, and are likely used in a comparative or contrasting manner to evaluate their performance or effectiveness in anomaly detection. The fact that they are both benchmark datasets collected from NASA spacecraft further emphasizes their relevance and importance in the field of anomaly detection.\n\nOverall, the summary highlights the connection between MSL and SMAP as datasets used for anomaly detection, and their relationship as related datasets used in the experiment.", "from": "MSL", "source_id": "6917a14af275e30abe2d30a8f8a9a4e6,69afb4bcb8c1e03975c837102e1d0b32,9c6e74299923071f9fc2b7cce49efc90,d16a81565fcd654ab21768510dcc5d7f,fbc83a616e9fa96c245138bca69c177f", "to": "SMAP", "width": 5.0}, {"description": "AnomalyBERT outperforms previous methods on MSL dataset", "from": "MSL", "source_id": "19a1a12db412295d0ddd19ffffb78332", "to": "ANOMALYBERT", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nMSL and WADI are two real-world benchmark datasets used for anomaly detection. They are both utilized in the context of anomaly detection, as indicated by their presence in a table. These datasets are employed to evaluate and compare the performance of various anomaly detection models and techniques.\n\nThe summary includes information from all the descriptions, resolving any potential contradictions. The use of the phrase \"real-world benchmark datasets\" in the second description provides additional context and clarity to the summary, while the mention of their presence in a table in the first description provides a specific example of their application in anomaly detection.\n\nThe summary is written in the third person and includes the entity names, providing a clear and concise description of the datasets. Relevant information from the nearby text has been incorporated to enrich the summary, including the context of anomaly detection and the use of these datasets as benchmarks.", "from": "MSL", "source_id": "69afb4bcb8c1e03975c837102e1d0b32,fbc83a616e9fa96c245138bca69c177f", "to": "WADI", "width": 2.0}, {"description": "Hundman et al. (2018) is a research paper that describes the MSL dataset", "from": "MSL", "source_id": "fbc83a616e9fa96c245138bca69c177f", "to": "HUNDMAN ET AL. (2018)", "width": 1.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entities \"MSL\" and \"SWAT\" are two datasets used for anomaly detection. They are related to each other as they are both datasets used in the experiments and are concepts mentioned in the text. Specifically, they are both used for anomaly detection, as indicated by their presence in the table. This suggests that the datasets are used to identify and analyze anomalies in a particular context, which may be related to water treatment or other industrial processes.\n\nFurther analysis of the text reveals that the datasets are likely used in the context of time series analysis and forecasting, as they are mentioned alongside other technical terms such as \"long-term series forecasting\" and \"frequency analysis\". The use of these terms suggests that the datasets are used to analyze and predict patterns in time series data, which is a common application of anomaly detection.\n\nOverall, the summary of the data is as follows:\n\n\"MSL and SWAT are two datasets used for anomaly detection in the context of time series analysis and forecasting. They are related to each other as they are both datasets used in the experiments and are concepts mentioned in the text.\"\n\nThis summary is written in third person and includes the entity names, providing a clear and concise description of the data.", "from": "MSL", "source_id": "69afb4bcb8c1e03975c837102e1d0b32,9c6e74299923071f9fc2b7cce49efc90,d16a81565fcd654ab21768510dcc5d7f", "to": "SWAT", "width": 3.0}, {"description": "NAB and MSL are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "MSL", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "NAB", "width": 1.0}, {"description": "UCR and MSL are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "MSL", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "UCR", "width": 1.0}, {"description": "MBA and MSL are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "MSL", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "MBA", "width": 1.0}, {"description": "MSL and MSDS are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "MSL", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "MSDS", "width": 1.0}, {"description": "MSL is related to P as P is a metric used to evaluate the models on the MSL dataset", "from": "MSL", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "P", "width": 1.0}, {"description": "MSL is related to R as R is a metric used to evaluate the models on the MSL dataset", "from": "MSL", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "R", "width": 1.0}, {"description": "MSL is related to AUC as AUC is a metric used to evaluate the models on the MSL dataset", "from": "MSL", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "AUC", "width": 1.0}, {"description": "MSL is related to F1 as F1 is a metric used to evaluate the models on the MSL dataset", "from": "MSL", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "F1", "width": 1.0}, {"description": "MSL is related to Time as Time is a metric used to evaluate the models on the MSL dataset", "from": "MSL", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "TIME", "width": 1.0}, {"description": "The SMD dataset is used to evaluate the performance of anomaly detection models using the PSM metric", "from": "PSM", "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "to": "SMD", "width": 1.0}, {"description": "LSTM and USAD are both types of anomaly detection models", "from": "LSTM", "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "to": "USAD", "width": 1.0}, {"description": "GNN is related to LSTM as LSTM is used to process sequential data", "from": "LSTM", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "GNN", "width": 1.0}, {"description": "LSTM is related to ECG as ECG is used to process medical data", "from": "LSTM", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "ECG", "width": 1.0}, {"description": "LGBM and LSTM are both machine learning models used for forecasting and prediction tasks", "from": "LSTM", "source_id": "f912df936d0b735fe654d1a9c53caa3b", "to": "LGBM", "width": 1.0}, {"description": "LSTM and DeepAR are both types of neural networks used for sequence prediction and forecasting tasks", "from": "LSTM", "source_id": "f912df936d0b735fe654d1a9c53caa3b", "to": "DEEPAR", "width": 1.0}, {"description": "LSTM-NDT is related to LSTM as LSTM is an auto-regressive neural network that learns order dependence in sequential data", "from": "LSTM", "source_id": "ffbbbf29ffb8d038e241f023079cb0a2", "to": "LSTM-NDT", "width": 1.0}, {"description": "LSTM is related to DAGMM as DAGMM uses a deep autoencoding Gaussian mixture model for dimension reduction in the feature space and recurrent networks for temporal modeling", "from": "LSTM", "source_id": "ffbbbf29ffb8d038e241f023079cb0a2", "to": "DAGMM", "width": 1.0}, {"description": "LSTM-VAE and USAD are both models mentioned in the text, as indicated by the use of their names", "from": "USAD", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "LSTM-VAE", "width": 1.0}, {"description": "Based on the provided information, the primary entities of interest are \"USAD\" and \"MSCRED\", which are both deep learning models for anomaly detection. \n\nThese models are mentioned in the text and are used for anomaly detection, with a comparison of their performance indicating that \"USAD\" has a higher F1 score than \"MSCRED\". \n\nThe language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. The text is formal and academic, suggesting that it is from a research paper or a technical document.\n\nIn summary, \"USAD\" and \"MSCRED\" are two deep learning models for anomaly detection, with \"USAD\" performing better than \"MSCRED\" based on their F1 scores. The text is written in English and appears to be from a research paper or technical document.", "from": "USAD", "source_id": "2b44aebc638544dcf835db30c4270d09,d5c8b72da09cebefa0c26285ad5272eb,ee42bd06cc3770a3384df85cc16fef54", "to": "MSCRED", "width": 3.0}, {"description": "THOC and USAD are both models mentioned in the text, as indicated by the use of their names", "from": "USAD", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "THOC", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nUSAD and GDN are two deep learning models used for anomaly detection. They are both mentioned in the text and are utilized for identifying anomalies in data. Both models employ attention mechanisms to focus on specific modes of the data, allowing them to effectively detect anomalies. The use of attention mechanisms in USAD and GDN enables them to concentrate on the most relevant information, thereby improving their anomaly detection capabilities.\n\nThe primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nThe descriptions provided are consistent and do not contain any contradictions. Therefore, the summary is a coherent and accurate representation of the information provided.\n\nRelevant information from the nearby text is not explicitly mentioned, but based on the context, it can be inferred that the text is related to machine learning and time series forecasting, as indicated by the use of terms such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis.\"", "from": "USAD", "source_id": "00973c1c3c962d9234a38037709824b1,0259287b914980606371cd1161c6a420,2b44aebc638544dcf835db30c4270d09,8e075f1de7293e0cd1724bd167c263be,ee42bd06cc3770a3384df85cc16fef54", "to": "GDN", "width": 5.0}, {"description": "USAD and AnomalyBERT are both models mentioned in the text, as indicated by the use of their names", "from": "USAD", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "ANOMALYBERT", "width": 1.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nUSAD and MTAD-GAT are two deep learning models for anomaly detection in multivariate time series data. Both models have been evaluated in the text and have been used for anomaly detection. They are related to each other as they are both models mentioned in the text. However, there is a notable difference in their performance, with MTAD-GAT having a higher F1 score than USAD in some cases. Nevertheless, USAD outperforms MTAD-GAT when compared across all datasets and also when given the complete WADI dataset.\n\nThe models use attention mechanisms to focus on specific modes of the data, which is a key aspect of their anomaly detection capabilities. Both USAD and MTAD-GAT are methods for anomaly detection and diagnosis, and they have been used for this purpose in the text. Overall, the two models are similar in their application but differ in their performance and effectiveness.\n\nIt is worth noting that the descriptions provided are not contradictory, but rather complementary, and they all point to the same conclusion: USAD and MTAD-GAT are two deep learning models for anomaly detection in multivariate time series data, with some differences in their performance and effectiveness.", "from": "USAD", "source_id": "00973c1c3c962d9234a38037709824b1,1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,70a97858b727e5af1466ae5eb9183921,8e075f1de7293e0cd1724bd167c263be,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb", "to": "MTAD-GAT", "width": 9.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nMAD-GAN and USAD are two deep learning models for anomaly detection. Both models have been evaluated in the text, with their performance metrics indicating their effectiveness in detecting anomalies. Specifically, MAD-GAN and USAD are both models used for anomaly detection, with MAD-GAN outperforming USAD across all datasets, including the complete WADI dataset. Notably, USAD has a higher F1 score than MAD-GAN, suggesting that while MAD-GAN may be more accurate in certain scenarios, USAD excels in other areas. Overall, both models demonstrate their potential in anomaly detection, with MAD-GAN showcasing its superiority in a broader range of scenarios.\n\nThe provided descriptions collectively paint a picture of two models, MAD-GAN and USAD, that are both designed for anomaly detection. While they share some similarities, their performance metrics and evaluation results reveal distinct differences in their capabilities. By considering the entirety of the descriptions, a nuanced understanding of the relationship between MAD-GAN and USAD can be gained, highlighting their respective strengths and weaknesses in the context of anomaly detection.", "from": "USAD", "source_id": "00973c1c3c962d9234a38037709824b1,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,70a97858b727e5af1466ae5eb9183921,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb", "to": "MAD-GAN", "width": 6.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nUSAD and CAE-M are two deep learning models designed for anomaly detection. Both models are utilized for identifying anomalies, with CAE-M demonstrating a higher F1 score compared to USAD. This suggests that CAE-M is more effective in detecting anomalies than USAD.\n\nThe summary is written in third person and includes the entity names, providing the full context. Relevant information from the nearby text has been incorporated to enrich the summary. The contradictions in the descriptions have been resolved to provide a single, coherent summary.", "from": "USAD", "source_id": "00973c1c3c962d9234a38037709824b1,2b44aebc638544dcf835db30c4270d09,d5c8b72da09cebefa0c26285ad5272eb", "to": "CAE-M", "width": 3.0}, {"description": "Based on the provided information, the primary entities are \"USAD\" and \"TRANAD\", which are both deep learning models used for anomaly detection. \n\nThe descriptions provided indicate that these models have been compared in the text, suggesting that they are being evaluated or contrasted in some way. The use of their names in a table further supports this, as it implies that they are being presented or analyzed together.\n\nGiven the context of anomaly detection, it is likely that these models are being used to identify unusual patterns or outliers in data. The fact that they are deep learning models suggests that they are using complex algorithms and neural networks to perform this task.\n\nOverall, the summary of the data is as follows:\n\n\"USAD and TRANAD are two deep learning models used for anomaly detection, which have been compared and presented together in the text. They are likely being evaluated or contrasted in some way, and are being used to identify unusual patterns or outliers in data.\"\n\nThis summary takes into account all of the provided descriptions and resolves any potential contradictions. It also includes relevant information from the nearby text, such as the fact that they are deep learning models and are being used for anomaly detection.", "from": "USAD", "source_id": "00973c1c3c962d9234a38037709824b1,2b44aebc638544dcf835db30c4270d09,a4a241e471ad258932241bc441b96155", "to": "TRANAD", "width": 3.0}, {"description": "MERLIN and USAD are both models used for anomaly detection, with USAD having a higher F1 score than MERLIN", "from": "USAD", "source_id": "d5c8b72da09cebefa0c26285ad5272eb", "to": "MERLIN", "width": 1.0}, {"description": "LSTM-NDT and USAD are both models used for anomaly detection, with USAD having a higher F1 score than LSTM-NDT", "from": "USAD", "source_id": "d5c8b72da09cebefa0c26285ad5272eb", "to": "LSTM-NDT", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"USAD\" and \"DAGMM\" are both deep learning models used for anomaly detection. According to the information, both models are utilized for this purpose, as indicated by their names in the table. Furthermore, a comparison between the two models reveals that \"USAD\" has a higher F1 score than \"DAGMM\", suggesting that \"USAD\" may be more effective in detecting anomalies. The F1 score is a measure of a model\u0027s accuracy in detecting both true positives and true negatives, making it a crucial metric in evaluating anomaly detection models.\n\nIt is worth noting that both \"USAD\" and \"DAGMM\" are deep learning models, which implies that they utilize complex neural network architectures to learn patterns and relationships in data. This is consistent with the use of deep learning models for anomaly detection, which often involves learning a normal behavior or pattern in data and then identifying instances that deviate from this norm.\n\nOverall, the summary provides a clear understanding of the entities \"USAD\" and \"DAGMM\" and their roles in anomaly detection, as well as a comparison of their performance based on the F1 score metric.", "from": "USAD", "source_id": "2b44aebc638544dcf835db30c4270d09,d5c8b72da09cebefa0c26285ad5272eb", "to": "DAGMM", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nUSAD and OmniAnomaly are both deep learning models designed for anomaly detection. They are utilized for identifying anomalies in data, with a focus on detecting unusual patterns or outliers. According to the available information, USAD has demonstrated a higher F1 score compared to OmniAnomaly, indicating its potential for more accurate anomaly detection. The models\u0027 performance is likely evaluated using metrics such as precision, recall, and F1 score, which are commonly used in anomaly detection tasks. The use of deep learning architectures in both models suggests that they leverage complex neural networks to learn patterns and relationships in the data, enabling them to detect anomalies effectively. Overall, USAD and OmniAnomaly are two distinct models with different performance characteristics, both of which are employed for anomaly detection purposes.", "from": "USAD", "source_id": "2b44aebc638544dcf835db30c4270d09,d5c8b72da09cebefa0c26285ad5272eb", "to": "OMNIANOMALY", "width": 2.0}, {"description": "AnomalyTrans and DCDetector are both types of anomaly detection models", "from": "ANOMALYTRANS", "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "to": "DCDETECTOR", "width": 1.0}, {"description": "DCDetector and RESTAD (R) are both types of anomaly detection models", "from": "DCDETECTOR", "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "to": "RESTAD (R)", "width": 1.0}, {"description": "RESTAD (R) and RESTAD (K) are both types of anomaly detection models", "from": "RESTAD (R)", "source_id": "22df9b37bd353b7b484fb07edeeb66fd", "to": "RESTAD (K)", "width": 1.0}, {"description": "AnomalyBERT outperforms previous methods on SMD dataset", "from": "SMD", "source_id": "19a1a12db412295d0ddd19ffffb78332", "to": "ANOMALYBERT", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe datasets \"SMAP\" and \"SMD\" are both real-world benchmark datasets used for anomaly detection. They are utilized in the context of anomaly detection, as indicated by their presence in a table. These datasets are likely employed in various applications, such as identifying unusual patterns or outliers in data, and are used to evaluate the performance of anomaly detection models.\n\nIt is worth noting that the use of these datasets for anomaly detection suggests that they contain data with anomalies or irregularities that need to be identified and addressed. The fact that they are real-world benchmark datasets implies that they are widely used and recognized in the field of anomaly detection, and are likely to be used as a standard for evaluating the performance of anomaly detection models.\n\nOverall, the datasets \"SMAP\" and \"SMD\" are important resources for researchers and practitioners working in the field of anomaly detection, and are likely to play a significant role in the development and evaluation of anomaly detection models.", "from": "SMD", "source_id": "69afb4bcb8c1e03975c837102e1d0b32,fbc83a616e9fa96c245138bca69c177f", "to": "SMAP", "width": 2.0}, {"description": "Su et al. (2019) is a research paper that describes the SMD dataset", "from": "SMD", "source_id": "fbc83a616e9fa96c245138bca69c177f", "to": "SU ET AL. (2019)", "width": 1.0}, {"description": "SMD is a collection of sub-datasets from 28 different machines provided by a large Internet company", "from": "SMD", "source_id": "5809618fe3a2014c2140601fcf103022", "to": "SUB-DATASETS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nWADI and SMD are two entities that are related to each other in the context of anomaly detection and experimental datasets. They are both datasets used for anomaly detection, as indicated by their presence in the table, and are related as they are both datasets used in the experiments. Furthermore, they are also referred to as systems, as indicated by the use of their abbreviations in the table.\n\nGiven the context of the text, it appears that WADI and SMD are likely datasets or systems used in the field of anomaly detection, possibly in the context of time series analysis or machine learning. The use of their abbreviations in the table suggests that they are being used as part of a larger experiment or study, and their presence in the table indicates that they are being used for anomaly detection purposes.\n\nOverall, the summary provides a clear understanding of the relationship between WADI and SMD, and highlights their roles as datasets or systems used in the context of anomaly detection and experimental research.", "from": "SMD", "source_id": "69afb4bcb8c1e03975c837102e1d0b32,9c6e74299923071f9fc2b7cce49efc90,f6e57fa18831bcc732a631240536777a", "to": "WADI", "width": 3.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"SMD\" and \"MSDS\" are related datasets and metrics used in the context of anomaly detection. They are both utilized in experiments and are systems that evaluate the performance of anomaly detection methods. Specifically, they are datasets used for anomaly detection, as indicated by their presence in the table, and are also metrics used to evaluate the performance of anomaly detection methods.\n\nThe use of these entities in the context of anomaly detection suggests that they are likely used in applications where identifying unusual patterns or outliers is crucial. The fact that they are both datasets and metrics implies that they are used to both collect and analyze data, as well as to evaluate the effectiveness of anomaly detection methods.\n\nThe presence of \"SMD\" and \"MSDS\" in the table, along with their use as metrics, suggests that they are likely used in a variety of applications, including but not limited to, finance, healthcare, and cybersecurity. The use of these entities in experiments also implies that they are being used to test and evaluate the performance of anomaly detection methods in a controlled environment.\n\nOverall, the summary of the data suggests that \"SMD\" and \"MSDS\" are related datasets and metrics used in the context of anomaly detection, and are likely used in a variety of applications to evaluate the performance of anomaly detection methods.\n\nRelevant information from the nearby text that was used to enrich the summary includes:\n\n* The presence of technical terms and mathematical equations in the text, which suggests that the context is technical and academic.\n* The use of English-language abbreviations and citations, which suggests that the text is written in English and is likely from a research paper or technical document.\n* The fact that the text is formal and academic, which suggests that the context is likely a research paper or technical document.\n\nThis information was used to provide context and clarify the meaning of the entities \"SMD\" and \"MSDS\", and to suggest potential applications and uses for these entities.", "from": "SMD", "source_id": "1413d358c623ac2d4c70be6547eb218b,69afb4bcb8c1e03975c837102e1d0b32,9c6e74299923071f9fc2b7cce49efc90,f6e57fa18831bcc732a631240536777a", "to": "MSDS", "width": 4.0}, {"description": "The DAGMM model has poor performance for longer sequences like SMD", "from": "SMD", "source_id": "8e075f1de7293e0cd1724bd167c263be", "to": "DAGMM MODEL", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"SMD\" and \"MERLIN\" are related in a context of model evaluation and anomaly detection. MERLIN is utilized to identify anomalies within the data related to SMD, which serves as a metric to assess the performance of models. This suggests that SMD is a key component in evaluating the effectiveness of models, and MERLIN plays a crucial role in detecting irregularities or outliers within the SMD data.\n\nThis relationship highlights the importance of SMD in model evaluation and the complementary role of MERLIN in identifying anomalies within the SMD data. The connection between these two entities underscores the significance of robust model evaluation and anomaly detection in various applications.\n\nThe information provided does not contain any contradictions, and the summary is based on the explicit relationships described between the entities \"SMD\" and \"MERLIN\".", "from": "SMD", "source_id": "00973c1c3c962d9234a38037709824b1,f6e57fa18831bcc732a631240536777a", "to": "MERLIN", "width": 2.0}, {"description": "LSTM-NDT is related to SMD as SMD is a metric used to evaluate model performance", "from": "SMD", "source_id": "00973c1c3c962d9234a38037709824b1", "to": "LSTM-NDT", "width": 1.0}, {"description": "TranAD achieves the best rank across all models with a significant statistical difference when given the complete SMD dataset", "from": "SMD", "source_id": "70a97858b727e5af1466ae5eb9183921", "to": "TRANAD", "width": 1.0}, {"description": "NAB and SMD are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "SMD", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "NAB", "width": 1.0}, {"description": "UCR and SMD are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "SMD", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "UCR", "width": 1.0}, {"description": "MBA and SMD are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "SMD", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "MBA", "width": 1.0}, {"description": "Based on the provided information, the primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nThe entities \"SMD\" and \"SWAT\" are both datasets and systems used for anomaly detection. They are mentioned in a table, suggesting that they are being compared or analyzed in some way.\n\nGiven the context, it appears that \"SMD\" and \"SWAT\" are likely water treatment systems, as they are commonly used in the field of water treatment and anomaly detection. The use of these systems in anomaly detection suggests that they are being monitored for unusual patterns or behavior.\n\nIn terms of their use in anomaly detection, both \"SMD\" and \"SWAT\" are likely being used to identify and respond to potential security threats or equipment failures. This could involve monitoring sensor data, analyzing patterns, and making predictions about future behavior.\n\nOverall, the summary of the data is as follows:\n\n\"SMD and SWAT are two water treatment systems used for anomaly detection. They are both datasets and systems that are being used to identify and respond to potential security threats or equipment failures in water treatment plants. The use of these systems in anomaly detection suggests that they are being monitored for unusual patterns or behavior, and are likely being used to make predictions about future behavior and respond to potential threats.\"\n\nThis summary is based on the information provided in the description list and the context of the text. It is written in third person and includes the entity names for clarity.", "from": "SMD", "source_id": "69afb4bcb8c1e03975c837102e1d0b32,f6e57fa18831bcc732a631240536777a", "to": "SWAT", "width": 2.0}, {"description": "The SMD dataset and MSL dataset are used for testing the anomaly detection performance of the model", "from": "SMD DATASET", "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7", "to": "MSL DATASET", "width": 1.0}, {"description": "The SMD dataset and PSM dataset are used for testing the anomaly detection performance of the model", "from": "SMD DATASET", "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7", "to": "PSM DATASET", "width": 1.0}, {"description": "WADI dataset is related to SMD dataset as both are types of datasets used to evaluate the performance of the model", "from": "SMD DATASET", "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "to": "WADI DATASET", "width": 1.0}, {"description": "SMD dataset is related to Tracer dataset as both are types of datasets used to evaluate the performance of the model", "from": "SMD DATASET", "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "to": "TRACER DATASET", "width": 1.0}, {"description": "The MSL dataset and PSM dataset are used for testing the anomaly detection performance of the model", "from": "MSL DATASET", "source_id": "529ee9dbb2f4807b9c21bbf190dbd1f7", "to": "PSM DATASET", "width": 1.0}, {"description": "WADI and MSL datasets are used to evaluate AnomalyBERT", "from": "MSL DATASET", "source_id": "587ca8c6cc86a93299e9540153babbc6", "to": "WADI DATASET", "width": 1.0}, {"description": "MSL and SMAP datasets are used to evaluate AnomalyBERT", "from": "MSL DATASET", "source_id": "587ca8c6cc86a93299e9540153babbc6", "to": "SMAP DATASET", "width": 1.0}, {"description": "Unsupervised anomaly detection is a task that involves identifying unusual patterns or outliers in data without labeled training data, and adversarial memory autoencoders are a type of neural network architecture used in this task", "from": "UNSUPERVISED ANOMALY DETECTION", "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "to": "ADVERSARIAL MEMORY AUTOENCODERS", "width": 1.0}, {"description": "Adversarial memory autoencoders and LSTM-based variational autoencoders are both types of neural network architectures used in anomaly detection", "from": "ADVERSARIAL MEMORY AUTOENCODERS", "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "to": "LSTM-BASED VARIATIONAL AUTOENCODER", "width": 1.0}, {"description": "IEEE Robotics and Automation Letters and Computers, Materials \u0026 Continua are both journals that publish research papers on computer science and materials science", "from": "IEEE ROBOTICS AND AUTOMATION LETTERS", "source_id": "99b3fe01a22282fe6d02bf29dacf8875", "to": "COMPUTERS MATERIALS \u0026 CONTINUA", "width": 1.0}, {"description": "Foundation models are related to time series analysis as they can be fine-tuned for various tasks, including time series analysis", "from": "TIME SERIES ANALYSIS", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "FOUNDATION MODELS", "width": 1.0}, {"description": "Time series analysis is related to data mining as data mining is used to analyze and understand time series data", "from": "TIME SERIES ANALYSIS", "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "to": "DATA MINING", "width": 1.0}, {"description": "Methodology is related to time series analysis as methodology is used to analyze and understand time series data", "from": "TIME SERIES ANALYSIS", "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "to": "METHODLOGY", "width": 1.0}, {"description": "The foundation model paradigm is used in time series analysis", "from": "TIME SERIES ANALYSIS", "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "to": "FOUNDATION MODEL PARADIGM", "width": 1.0}, {"description": "Time series analysis often requires ground truth labels to train a model", "from": "TIME SERIES ANALYSIS", "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "to": "GROUND TRUTH LABELS", "width": 1.0}, {"description": "Time series analysis is related to neural forecasting methods as neural forecasting methods are used for time series forecasting", "from": "TIME SERIES ANALYSIS", "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "to": "NEURAL FORECASTING METHODS", "width": 1.0}, {"description": "Time series analysis is related to ICLR 2023 as the paper was presented at the conference", "from": "TIME SERIES ANALYSIS", "source_id": "98c6b5003112ab7110e45414a2fa468b", "to": "ICLR 2023", "width": 1.0}, {"description": "Foundation models for time series analysis are a type of foundation model that is specifically designed for time series analysis", "from": "FOUNDATION MODELS FOR TIME SERIES ANALYSIS", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "FOUNDATION MODELS", "width": 1.0}, {"description": "Yuxuan Liang is an author of the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\"", "from": "FOUNDATION MODELS FOR TIME SERIES ANALYSIS", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "YUXUAN LIANG", "width": 1.0}, {"description": "Haomin Wen is an author of the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\"", "from": "FOUNDATION MODELS FOR TIME SERIES ANALYSIS", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "HAOMIN WEN", "width": 1.0}, {"description": "Yuqi Nie is an author of the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\"", "from": "FOUNDATION MODELS FOR TIME SERIES ANALYSIS", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "YUQI NIE", "width": 1.0}, {"description": "Yushan Jiang is an author of the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\"", "from": "FOUNDATION MODELS FOR TIME SERIES ANALYSIS", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "YUSHAN JIANG", "width": 1.0}, {"description": "Ming Jin is an author of the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\"", "from": "FOUNDATION MODELS FOR TIME SERIES ANALYSIS", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "MING JIN", "width": 1.0}, {"description": "Dongjin Song is an author of the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\"", "from": "FOUNDATION MODELS FOR TIME SERIES ANALYSIS", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "DONGJIN SONG", "width": 1.0}, {"description": "Shirui Pan is an author of the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\"", "from": "FOUNDATION MODELS FOR TIME SERIES ANALYSIS", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "SHIRUI PAN", "width": 1.0}, {"description": "Qingsong Wen is an author of the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\"", "from": "FOUNDATION MODELS FOR TIME SERIES ANALYSIS", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "QINGSONG WEN", "width": 1.0}, {"description": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining is a conference where the paper \"Foundation Models for Time Series Analysis: A Tutorial and Survey\" was presented", "from": "FOUNDATION MODELS FOR TIME SERIES ANALYSIS", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING", "width": 1.0}, {"description": "KDD \u201924 is related to Yuxuan Liang as Yuxuan Liang is an author of a research paper presented at KDD \u201924", "from": "YUXUAN LIANG", "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "to": "KDD \u201924", "width": 1.0}, {"description": "Yuxuan Liang is related to Zhixuan Chu as Zhixuan Chu is a co-author of Yuxuan Liang", "from": "YUXUAN LIANG", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "ZHIXUAN CHU", "width": 1.0}, {"description": "Yuqi Nie and Nam H Nguyen are authors of the paper \"A time series is worth 64 words: Long-term forecasting with transformers\"", "from": "YUQI NIE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "NAM H NGUYEN", "width": 1.0}, {"description": "Yuqi Nie and Phanwadee Sinthong are authors of the paper \"A time series is worth 64 words: Long-term forecasting with transformers\"", "from": "YUQI NIE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "PHANWADEE SINTHONG", "width": 1.0}, {"description": "Yuqi Nie and Jayant Kalagnanam are authors of the paper \"A time series is worth 64 words: Long-term forecasting with transformers\"", "from": "YUQI NIE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "JAYANT KALAGNANAM", "width": 1.0}, {"description": "Ming Jin is related to Zhixuan Chu as Zhixuan Chu is a co-author of Ming Jin", "from": "MING JIN", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "ZHIXUAN CHU", "width": 1.0}, {"description": "Ming Jin and Huan Yee Koh are related as they are co-authors of the paper on A survey on graph neural networks for time series", "from": "MING JIN", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "HUAN YEE KOH", "width": 1.0}, {"description": "Ming Jin and Qingsong Wen are related as they are co-authors of the paper on A survey on graph neural networks for time series", "from": "MING JIN", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "QINGSONG WEN", "width": 1.0}, {"description": "Ming Jin and Daniele Zambon are related as they are co-authors of the paper on A survey on graph neural networks for time series", "from": "MING JIN", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "DANIELE ZAMBON", "width": 1.0}, {"description": "Ming Jin and Cesare Alippi are related as they are co-authors of the paper on A survey on graph neural networks for time series", "from": "MING JIN", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "CESARE ALIPPI", "width": 1.0}, {"description": "Ming Jin and Geoffrey I Webb are related as they are co-authors of the paper on A survey on graph neural networks for time series", "from": "MING JIN", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "GEOFFREY I WEBB", "width": 1.0}, {"description": "Ming Jin and Irwin King are related as they are co-authors of the paper on A survey on graph neural networks for time series", "from": "MING JIN", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "IRWIN KING", "width": 1.0}, {"description": "Ming Jin and Shirui Pan are related as they are co-authors of the paper on A survey on graph neural networks for time series", "from": "MING JIN", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "SHIRUI PAN", "width": 1.0}, {"description": "Shirui Pan is related to Zhixuan Chu as Zhixuan Chu is a co-author of Shirui Pan", "from": "SHIRUI PAN", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "ZHIXUAN CHU", "width": 1.0}, {"description": "Qingsong Wen is related to Zhixuan Chu as Zhixuan Chu is a co-author of Qingsong Wen", "from": "QINGSONG WEN", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "ZHIXUAN CHU", "width": 1.0}, {"description": "Barcelona is the location of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "from": "ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING", "source_id": "7a36c78af074ba651b0404f11cdf481d", "to": "BARCELONA", "width": 1.0}, {"description": "Based on the provided information, the primary entity is \"BARCELONA\" and the secondary entity is \"KDD \u002724\". \n\nAfter analyzing the descriptions, it is clear that there is a direct relationship between the two entities. The descriptions state that \"Barcelona is related to KDD \u002724 as KDD \u002724 was held in Barcelona\" and \"KDD \u002724 is related to Barcelona as the conference was held in Barcelona\". \n\nThis indicates that KDD \u002724, a conference, was held in Barcelona, a city. Therefore, the relationship between the two entities is that KDD \u002724 was a conference held in Barcelona.\n\nHere is a comprehensive summary of the data in third person:\n\nBARCELONA is a city that hosted the KDD \u002724 conference. KDD \u002724, a conference, was held in BARCELONA, indicating a direct relationship between the two entities. The conference was likely attended by researchers and professionals in the field of data science and artificial intelligence, as suggested by the presence of technical terms and references to academic papers in the descriptions.", "from": "BARCELONA", "source_id": "736a43d5fef4417bac9757301b4721c3,ef8fd6170b08af3bd99c9df44ffa8b57", "to": "KDD \u201924", "width": 2.0}, {"description": "Barcelona is related to Spain as Barcelona is located in Spain", "from": "BARCELONA", "source_id": "ef8fd6170b08af3bd99c9df44ffa8b57", "to": "SPAIN", "width": 1.0}, {"description": "Foundation models are related to deep learning as deep learning is a key component of foundation models", "from": "DEEP LEARNING", "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "to": "FOUNDATION MODEL", "width": 1.0}, {"description": "Deep learning is related to transformers as transformers are a type of neural network architecture used in deep learning", "from": "DEEP LEARNING", "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "to": "TRANSFORMERS", "width": 1.0}, {"description": "Deep learning is a subfield of machine learning that involves the use of artificial neural networks to analyze data", "from": "DEEP LEARNING", "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "to": "MACHINE LEARNING", "width": 1.0}, {"description": "Predictions are related to deep learning as deep learning has achieved widespread acclaim for generative models in other fundamental domains of the human condition", "from": "DEEP LEARNING", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "PREDICTIONS", "width": 1.0}, {"description": "Deep learning is related to forecasting science as forecasting science has fallen short of fulfilling the promises of genuinely universal pre-trained models", "from": "DEEP LEARNING", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "FORECASTING SCIENCE", "width": 1.0}, {"description": "Pre-trained models are related to deep learning as deep learning is used to train pre-trained models", "from": "DEEP LEARNING", "source_id": "6771b6279846e780d6807b15184ae008", "to": "PRE-TRAINED MODEL", "width": 1.0}, {"description": "Deep learning is related to time series forecasting as deep learning models are used for time series forecasting", "from": "DEEP LEARNING", "source_id": "6771b6279846e780d6807b15184ae008", "to": "TIME SERIES FORECASTING", "width": 1.0}, {"description": "Transformer networks are related to deep learning as they are a type of deep learning model", "from": "DEEP LEARNING", "source_id": "9c6e74299923071f9fc2b7cce49efc90", "to": "TRANSFORMER NETWORKS", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities in question are \"FOUNDATION MODELS\" and \"LARGE LANGUAGE MODELS.\" \n\nFOUNDATION MODELS are related to LARGE LANGUAGE MODELS, as LARGE LANGUAGE MODELS can be used as a starting point for FOUNDATION MODELS. Furthermore, FOUNDATION MODELS include LARGE LANGUAGE MODELS, which are utilized in natural language processing. This indicates a hierarchical relationship between the two entities, with LARGE LANGUAGE MODELS serving as a foundational component of FOUNDATION MODELS.\n\nIn essence, FOUNDATION MODELS leverage the capabilities of LARGE LANGUAGE MODELS to perform more complex tasks, while LARGE LANGUAGE MODELS themselves are a crucial part of FOUNDATION MODELS. This interdependence highlights the significance of LARGE LANGUAGE MODELS in the development and application of FOUNDATION MODELS in natural language processing.", "from": "FOUNDATION MODELS", "source_id": "0bef137d159ccc86cdee0a8be788bd26,f92205091f8d3b7e1e60b9bc80883a62", "to": "LARGE LANGUAGE MODELS", "width": 2.0}, {"description": "Foundation models include advanced models, which are used in computer vision", "from": "FOUNDATION MODELS", "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "to": "COMPUTER VISION", "width": 1.0}, {"description": "Foundation models can be used in spatial time series analysis, as they can handle spatial data effectively", "from": "FOUNDATION MODELS", "source_id": "79b26808691b6333aafce43c5de531f7", "to": "SPATIAL TIME SERIES", "width": 1.0}, {"description": "Foundation models can be used in trajectory analysis, as they can handle movement or change over time effectively", "from": "FOUNDATION MODELS", "source_id": "79b26808691b6333aafce43c5de531f7", "to": "TRAJECTORIES", "width": 1.0}, {"description": "Foundation models can be used in event analysis, as they can handle specific occurrences or happenings effectively", "from": "FOUNDATION MODELS", "source_id": "79b26808691b6333aafce43c5de531f7", "to": "EVENTS", "width": 1.0}, {"description": "Transformers are used as the backbone of foundation models, as they can handle sequential data effectively", "from": "FOUNDATION MODELS", "source_id": "79b26808691b6333aafce43c5de531f7", "to": "TRANSFORMERS", "width": 1.0}, {"description": "BERT is a type of foundation model that has revolutionized text understanding and generation tasks", "from": "FOUNDATION MODELS", "source_id": "79b26808691b6333aafce43c5de531f7", "to": "BERT", "width": 1.0}, {"description": "GPT-3 is a type of foundation model that has revolutionized text understanding and generation tasks", "from": "FOUNDATION MODELS", "source_id": "79b26808691b6333aafce43c5de531f7", "to": "GPT-3", "width": 1.0}, {"description": "CLIP is a type of foundation model that has propelled advancements in image recognition, object detection, and more", "from": "FOUNDATION MODELS", "source_id": "79b26808691b6333aafce43c5de531f7", "to": "CLIP", "width": 1.0}, {"description": "SAM is a type of foundation model that has propelled advancements in image recognition, object detection, and more", "from": "FOUNDATION MODELS", "source_id": "79b26808691b6333aafce43c5de531f7", "to": "SAM", "width": 1.0}, {"description": "Foundation models are related to FMs as FMs are a type of foundation model", "from": "FOUNDATION MODELS", "source_id": "736a43d5fef4417bac9757301b4721c3", "to": "FMS", "width": 1.0}, {"description": "Foundation models are related to data modality as data modality is used to train or adapt foundation models", "from": "FOUNDATION MODELS", "source_id": "736a43d5fef4417bac9757301b4721c3", "to": "DATA MODALITY", "width": 1.0}, {"description": "Foundation models are related to adaptation as adaptation is the process of modifying or updating foundation models", "from": "FOUNDATION MODELS", "source_id": "736a43d5fef4417bac9757301b4721c3", "to": "ADAPTATION", "width": 1.0}, {"description": "KDD \u201924 is related to foundation models as the paper presented at KDD \u201924 discusses foundation models", "from": "FOUNDATION MODELS", "source_id": "736a43d5fef4417bac9757301b4721c3", "to": "KDD \u201924", "width": 1.0}, {"description": "Foundation models are related to foundation models on time series analysis as foundation models on time series analysis are a specific application of foundation models", "from": "FOUNDATION MODELS", "source_id": "79c0a74b91761402b67c3574db02e8e7", "to": "FOUNDATION MODELS ON TIME SERIES ANALYSIS", "width": 1.0}, {"description": "SimMTM explores cross-domain applications, where pre-trained models via masked time series modeling exhibit superior fine-tuning performance in forecasting and classification tasks", "from": "FOUNDATION MODELS", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "SIMMTM", "width": 1.0}, {"description": "Timer advances the field by facilitating general time series analysis through single, large-scale pre-trained models", "from": "FOUNDATION MODELS", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "TIMER", "width": 1.0}, {"description": "UniTS advances the field by facilitating general time series analysis through single, large-scale pre-trained models", "from": "FOUNDATION MODELS", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "UNITS", "width": 1.0}, {"description": "OFA adapts pre-trained models, such as LLMs, for broad time series analysis", "from": "FOUNDATION MODELS", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "OFA", "width": 1.0}, {"description": "TEST adapts pre-trained models, such as LLMs, for broad time series analysis", "from": "FOUNDATION MODELS", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "TEST", "width": 1.0}, {"description": "TFM utilizes graph structures and algorithms to analyze the behavior and interactions within transportation systems", "from": "FOUNDATION MODELS", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "TFM", "width": 1.0}, {"description": "ST-LLM combines spatio-temporal information with a partially frozen LLM to improve traffic predictions", "from": "FOUNDATION MODELS", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "ST-LLM", "width": 1.0}, {"description": "DiffSTG applies denoising diffusion models to spatio-temporal graphs for probabilistic traffic forecasting", "from": "FOUNDATION MODELS", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "DIFFSTG", "width": 1.0}, {"description": "STEP links spatio-temporal GNNs with a pre-trained transformer for enhanced forecasting by learning from extensive historical data", "from": "FOUNDATION MODELS", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "STEP", "width": 1.0}, {"description": "Foundation models are related to trajectory data as trajectory data is a type of temporal dataset used in time series forecasting models", "from": "FOUNDATION MODELS", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "TRAJECTORY DATA", "width": 1.0}, {"description": "Foundation models are related to healthcare records as healthcare records are a type of temporal dataset used in time series forecasting models", "from": "FOUNDATION MODELS", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "HEALTHCARE RECORDS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"FOUNDATION MODELS\" and \"PRE-TRAINING\" are closely related concepts in the field of machine learning. Foundation models are pre-trained models that have been trained on a large dataset and can be fine-tuned for specific downstream tasks. This pre-training phase is the initial phase of training a model, where the model is trained on a general dataset to learn general patterns and features. Foundation models, therefore, are essentially pre-trained models that have been trained on a broad range of tasks and can be adapted for more specific tasks through fine-tuning.\n\nThis relationship between foundation models and pre-training is a fundamental concept in the development of artificial intelligence and machine learning models. By pre-training a model on a large dataset, it can learn general features and patterns that can be applied to a wide range of tasks, making it a more efficient and effective approach to model development.\n\nIn summary, foundation models and pre-training are interconnected concepts, where pre-training is the initial phase of training a model, and foundation models are the pre-trained models that can be fine-tuned for specific downstream tasks.", "from": "FOUNDATION MODELS", "source_id": "9125a7d3794226f109debe90e5db041c,de8e097ce66fbc954bd529888cbc15ea", "to": "PRE-TRAINING", "width": 2.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nFoundation models are closely related to self-supervised learning, as they are trained using this approach. Specifically, self-supervised learning is utilized to train foundation models on large datasets without the need for labeled data. This relationship highlights the significance of self-supervised learning in the development and training of foundation models, enabling them to learn and improve from unlabeled data.\n\nThis summary is written in third person and includes the entity names, \"FOUNDATION MODELS\" and \"SELF-SUPERVISED LEARNING\", to provide context. The information is also enriched with relevant details from the provided descriptions, resolving any potential contradictions and presenting a coherent summary.", "from": "FOUNDATION MODELS", "source_id": "9125a7d3794226f109debe90e5db041c,c4e47033b8ecc85beacde9d560e66062", "to": "SELF-SUPERVISED LEARNING", "width": 2.0}, {"description": "Foundation models are related to generative pre-training as generative pre-training is used to train foundation models to generate new data points", "from": "FOUNDATION MODELS", "source_id": "9125a7d3794226f109debe90e5db041c", "to": "GENERATIVE PRE-TRAINING", "width": 1.0}, {"description": "Foundation models are related to contrastive learning as contrastive learning is used to train foundation models to distinguish between similar and dissimilar data points", "from": "FOUNDATION MODELS", "source_id": "9125a7d3794226f109debe90e5db041c", "to": "CONTRASTIVE LEARNING", "width": 1.0}, {"description": "Foundation models are related to time series foundation models as time series foundation models are pre-trained models that can be fine-tuned for specific time series tasks", "from": "FOUNDATION MODELS", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "TIME SERIES FOUNDATION MODELS", "width": 1.0}, {"description": "Zero-shot forecasters are a type of foundation model", "from": "FOUNDATION MODELS", "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "to": "ZERO-SHOT FORECASTERS", "width": 1.0}, {"description": "Foundation models are trained exclusively on time-series data", "from": "FOUNDATION MODELS", "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "to": "TIME-SERIES DATA", "width": 1.0}, {"description": "Foundation models rely on their capabilities to generalize across domains, particularly in new datasets that were not available during training, which is related to transfer learning", "from": "FOUNDATION MODELS", "source_id": "7d5d82d600620153153772a9bc498ac0", "to": "TRANSFER LEARNING", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entity \"FOUNDATION MODELS\" refers to pre-trained models that capture wide-ranging and generic features. These models are particularly beneficial for fine-tuning, as they have already learned to extract common patterns and relationships from large datasets. Foundation models are closely related to \"TRANSFORMER-BASED ARCHITECTURES,\" as they are often built using transformer-based architectures, which enable them to capture complex and nuanced relationships between different pieces of information.\n\nIn particular, foundation models are pre-trained models that capture wide-ranging and generic features, and are particularly beneficial for fine-tuning. This suggests that they have been trained on a large and diverse dataset, and have learned to extract common patterns and relationships that can be applied to a wide range of tasks.\n\nThe relationship between foundation models and transformer-based architectures is further reinforced by the fact that foundation models are often built using transformer-based architectures. This suggests that the transformer architecture is well-suited for capturing the complex and nuanced relationships that foundation models are designed to extract.\n\nOverall, the summary suggests that foundation models are a type of pre-trained model that is particularly beneficial for fine-tuning, and are closely related to transformer-based architectures. They are designed to capture wide-ranging and generic features, and are often built using transformer-based architectures.", "from": "FOUNDATION MODELS", "source_id": "55eb54ef455d14cd1a11760924f99eb8,f912df936d0b735fe654d1a9c53caa3b", "to": "TRANSFORMER-BASED ARCHITECTURES", "width": 2.0}, {"description": "Fine-tuning is a process of adjusting model parameters on a task-specific dataset to tailor the model\u0027s pre-existing knowledge toward the requirements of the new task", "from": "FOUNDATION MODELS", "source_id": "55eb54ef455d14cd1a11760924f99eb8", "to": "FINE-TUNING", "width": 1.0}, {"description": "Foundation models are related to Lag-Llama as Lag-Llama is a type of foundation model", "from": "FOUNDATION MODELS", "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "to": "LAG-LAGA", "width": 1.0}, {"description": "Time series forecasting is related to foundation models as foundation models are an emerging paradigm of self-supervised learning", "from": "FOUNDATION MODELS", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "TIME SERIES FORECASTING", "width": 1.0}, {"description": "Foundation models are related to Assimakopoulos \u0026 Nikolopoulos (2000) as Assimakopoulos \u0026 Nikolopoulos (2000) is an academic paper mentioned in the text", "from": "FOUNDATION MODELS", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "ASSIMAKOPOULOS \u0026 NIKOLOPOULOS (2000)", "width": 1.0}, {"description": "Foundation models are related to Croston (1972) as Croston (1972) is an academic paper mentioned in the text", "from": "FOUNDATION MODELS", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "CROSTON (1972)", "width": 1.0}, {"description": "Foundation models are related to Syntetos \u0026 Boylan (2005) as Syntetos \u0026 Boylan (2005) is an academic paper mentioned in the text", "from": "FOUNDATION MODELS", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "SYNTETOS \u0026 BOYLAN (2005)", "width": 1.0}, {"description": "Foundation models are related to Hyndman \u0026 Athanasopoulos (2018) as Hyndman \u0026 Athanasopoulos (2018) is an academic paper mentioned in the text", "from": "FOUNDATION MODELS", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "HYNDMAN \u0026 ATHANASOPOULOS (2018)", "width": 1.0}, {"description": "Foundation models are related to Benidis et al. (2022) as Benidis et al. (2022) is an academic paper mentioned in the text", "from": "FOUNDATION MODELS", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "BENIDIS ET AL. (2022)", "width": 1.0}, {"description": "Foundation models are related to Salinas et al. (2020) as Salinas et al. (2020) is an academic paper mentioned in the text", "from": "FOUNDATION MODELS", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "SALINAS ET AL. (2020)", "width": 1.0}, {"description": "Foundation models are related to Wen et al. (2018) as Wen et al. (2018) is an academic paper mentioned in the text", "from": "FOUNDATION MODELS", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "WEN ET AL. (2018)", "width": 1.0}, {"description": "Pre-trained LLM is related to foundation models as foundation models are trained from scratch on a large and diverse collection of time series datasets", "from": "FOUNDATION MODELS", "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "to": "PRETRAINED LLM", "width": 1.0}, {"description": "Foundation models are related to pre-trained foundation models as pre-trained foundation models have made impressive strides in NLP and CV", "from": "FOUNDATION MODELS", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "PRE-TRAINED FOUNDATION MODELS", "width": 1.0}, {"description": "Data sparsity is related to foundation models as foundation models are affected by data sparsity", "from": "FOUNDATION MODELS", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "DATA SPARSITY", "width": 1.0}, {"description": "Foundation language models are related to foundation models as foundation models have driven rapid progress in computer vision and natural language processing", "from": "FOUNDATION MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "FOUNDATION LANGUAGE MODELS", "width": 1.0}, {"description": "Foundation models are related to time series modeling as time series modeling has not benefited from the same significant breakthroughs as foundation models", "from": "FOUNDATION MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "TIME SERIES MODELING", "width": 1.0}, {"description": "Fine-tuning is related to foundation model as it is used to adapt the model to a specific task or dataset", "from": "FOUNDATION MODEL", "source_id": "53f80422fbf85856ecf940d5c2450665", "to": "FINE-TUNING", "width": 1.0}, {"description": "Foundation model is related to LLMs as both are types of pre-trained models", "from": "FOUNDATION MODEL", "source_id": "53f80422fbf85856ecf940d5c2450665", "to": "LLMS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe \"FOUNDATION MODEL\" and \"GPT-2\" are related entities in the context of pre-trained language models. Specifically, \"GPT-2\" is a type of foundation model that can be fine-tuned for various tasks, including general time series forecasting. This suggests that \"GPT-2\" is a specific implementation of a foundation model, which is a broader category of pre-trained models that can be adapted for different applications.\n\nIn this context, the foundation model is a general framework that can be fine-tuned for specific tasks, and \"GPT-2\" is an example of such a model that has been fine-tuned for time series forecasting. This relationship highlights the versatility of foundation models and their potential applications in various domains, including time series analysis.\n\nOverall, the summary provides a clear understanding of the relationship between the \"FOUNDATION MODEL\" and \"GPT-2\", and highlights the potential of foundation models in general time series forecasting tasks.", "from": "FOUNDATION MODEL", "source_id": "3174231a67593609c727151c9df31d0a,6f6e27166a1506482bfcaf5585322595", "to": "GPT-2", "width": 2.0}, {"description": "Foundation model is related to TimeGPT-1 as TimeGPT-1 is a type of model that is specifically designed for time series forecasting", "from": "FOUNDATION MODEL", "source_id": "6f6e27166a1506482bfcaf5585322595", "to": "TIMEGPT-1", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe Foundation Model and TimeGPT are closely related entities in the context of artificial intelligence and machine learning. Specifically, TimeGPT is a type of foundation model that has achieved state-of-the-art performance. This indicates that TimeGPT is a cutting-edge foundation model that has demonstrated exceptional capabilities in its field.\n\nThe descriptions provided suggest a high degree of consistency, with all three statements affirming the relationship between Foundation Model and TimeGPT. The use of the term \"state-of-the-art performance\" in the third description further emphasizes TimeGPT\u0027s advanced capabilities.\n\nGiven the technical nature of the descriptions, it is likely that the context is related to research papers or technical documents in the field of artificial intelligence and machine learning. The use of terms such as \"foundation model\" and \"state-of-the-art performance\" suggests a focus on advanced machine learning techniques and models.\n\nOverall, the summary can be written as follows:\n\nThe Foundation Model and TimeGPT are closely related entities in the context of artificial intelligence and machine learning. TimeGPT is a type of foundation model that has achieved state-of-the-art performance, indicating its exceptional capabilities in its field.", "from": "FOUNDATION MODEL", "source_id": "4de223d7df157faf857c3e17d9e6e5b6,b8ce119147e4d1454453c514e68dc4dc,f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "TIMEGPT", "width": 3.0}, {"description": "Standardized large-scale datasets are related to foundation models as foundation models require large-scale datasets to perform well", "from": "FOUNDATION MODEL", "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "to": "STANDARDIZED LARGE-SCALE DATASETS", "width": 1.0}, {"description": "Lag-Llama is a type of foundation model", "from": "FOUNDATION MODEL", "source_id": "c7167cf92abe7513ccb936ca79871932", "to": "LAG-LLAMA", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"FOUNDATION MODEL\" is closely related to \"TIME SERIES FORECASTING\" as it is applied to this field. Specifically, foundation models are utilized in time series forecasting, indicating their significance in this area of study. Furthermore, the mention of \"Lag-Llama\" as a strong contender to the current state-of-the-art in time series forecasting suggests that foundation models, such as Lag-Llama, are being explored for their potential in improving time series forecasting capabilities.\n\nThis summary is based on the provided descriptions, which collectively highlight the connection between foundation models and time series forecasting. The information suggests that foundation models are being applied to time series forecasting and are being researched for their potential to improve forecasting capabilities in this field.", "from": "FOUNDATION MODEL", "source_id": "c4e47033b8ecc85beacde9d560e66062,c7167cf92abe7513ccb936ca79871932", "to": "TIME SERIES FORECASTING", "width": 3.0}, {"description": "Foundation models are related to the foundation model approach as they are applied to specific domains or tasksThe foundation model approach is related to foundation models as it is a method of applying them to specific domains or tasks", "from": "FOUNDATION MODEL", "source_id": "c4e47033b8ecc85beacde9d560e66062", "to": "FOUNDATION MODEL APPROACH", "width": 3.0}, {"description": "Foundation models are related to time series datasets as they are trained on them", "from": "FOUNDATION MODEL", "source_id": "c4e47033b8ecc85beacde9d560e66062", "to": "TIME SERIES DATASETS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe \"FOUNDATION MODEL\" and \"CORPUS\" are closely related entities in the context of machine learning and time series forecasting. A \"CORPUS\" is a collection of data that is used to train a \"FOUNDATION MODEL\", which is a type of model that is trained on a large corpus of diverse time series data. In other words, the \"FOUNDATION MODEL\" is trained on the \"CORPUS\", and the \"CORPUS\" serves as the foundation for the model\u0027s development.\n\nThis relationship is further reinforced by the fact that foundation models are specifically designed to be trained on large corpora of data, and the corpus of diverse time series is used to train the foundation model. This suggests that the corpus is a critical component in the development and training of the foundation model.\n\nOverall, the summary highlights the interdependent relationship between the \"FOUNDATION MODEL\" and the \"CORPUS\", with the corpus serving as the foundation for the model\u0027s development and training.", "from": "FOUNDATION MODEL", "source_id": "00007df4774d6122e3848802a24f9536,ac37bdb991d1513e44e4c5fc7a28b187,c4e47033b8ecc85beacde9d560e66062", "to": "CORPUS", "width": 3.0}, {"description": "A foundation model is used to tokenize a time series", "from": "FOUNDATION MODEL", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "TOKENIZATION", "width": 1.0}, {"description": "Foundation model is related to pre-training data as pre-training data is a specific type of foundation model", "from": "FOUNDATION MODEL", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "PRE-TRAINING DATA", "width": 1.0}, {"description": "LLAMA is a type of foundation model that can be fine-tuned for general time series forecasting", "from": "FOUNDATION MODEL", "source_id": "3174231a67593609c727151c9df31d0a", "to": "LLAMA", "width": 1.0}, {"description": "Transformers and MLPs are both used for natural language processing tasks, but transformers are more effective", "from": "TRANSFORMERS", "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "to": "MULTI-LAYER PERCEPTRONS", "width": 1.0}, {"description": "Mean absolute error is related to transformers as transformers are used to make predictions in time series forecasting", "from": "TRANSFORMERS", "source_id": "6f6e27166a1506482bfcaf5585322595", "to": "MEAN ABSOLUTE ERROR", "width": 1.0}, {"description": "Hyper-parameter tuning is related to transformers as it involves adjusting or optimizing the hyper-parameters of a transformer model to improve its performance or accuracy", "from": "TRANSFORMERS", "source_id": "9b150491b487d5b2da482652e2fe509d", "to": "HYPER-PARAMETER TUNING", "width": 1.0}, {"description": "Transformers are used in the TFT model", "from": "TRANSFORMERS", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "TFT", "width": 1.0}, {"description": "Transformers are used in the PatchTST model", "from": "TRANSFORMERS", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "PATCHTST", "width": 1.0}, {"description": "Neural forecasting is related to transformers as transformers are a type of neural model used in time series forecasting", "from": "TRANSFORMERS", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "NEURAL FORECASTING", "width": 1.0}, {"description": "Transformers are related to Muxi Chen as Muxi Chen is an author of a paper on transformers", "from": "TRANSFORMERS", "source_id": "98c6b5003112ab7110e45414a2fa468b", "to": "MUXI CHEN", "width": 1.0}, {"description": "Finance is related to healthcare as both fields involve the analysis and management of data", "from": "FINANCE", "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "to": "HEALTHCARE", "width": 1.0}, {"description": "Energy is related to finance as energy is a key input in many financial markets and transactions", "from": "FINANCE", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "ENERGY", "width": 1.0}, {"description": "Finance is related to economics as finance is a key component of economic activity", "from": "FINANCE", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "ECONOMICS", "width": 1.0}, {"description": "Exchange rate is related to finance as exchange rates affect international trade and investment", "from": "FINANCE", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "EXCHANGE RATE", "width": 1.0}, {"description": "Domains are related to finance as finance is a specific domain or category of data or information", "from": "FINANCE", "source_id": "1b48e9ca066ac5ba037066bb762d3458", "to": "DOMAINS", "width": 1.0}, {"description": "Healthcare is a domain where METS model is used", "from": "HEALTHCARE", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "METS", "width": 1.0}, {"description": "Economics is related to healthcare as healthcare is a significant sector of the economy", "from": "HEALTHCARE", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "ECONOMICS", "width": 1.0}, {"description": "Hospital is related to healthcare as hospitals provide medical care and treatment", "from": "HEALTHCARE", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "HOSPITAL", "width": 1.0}, {"description": "Cloud computing is related to energy as both fields involve the management and analysis of data in a cloud-based environment", "from": "CLOUD COMPUTING", "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "to": "ENERGY", "width": 1.0}, {"description": "Energy is related to economics as energy is a key factor in economic growth and development", "from": "ENERGY", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "ECONOMICS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"ENERGY\" is closely related to \"LONDON SMART METERS.\" Specifically, London Smart Meters is a dataset used for pretraining and fine-tuning, and it is directly related to energy as it provides data on energy consumption. This suggests that the London Smart Meters dataset is a valuable resource for analyzing and understanding energy consumption patterns, which can be used to inform various applications and decisions related to energy management.\n\nIn the context of machine learning and time series forecasting, the London Smart Meters dataset can be leveraged to develop models that accurately predict energy consumption patterns, enabling more efficient energy management and potentially reducing energy waste. The dataset\u0027s focus on energy consumption data makes it an ideal resource for researchers and practitioners working in the field of energy management and sustainability.\n\nOverall, the relationship between ENERGY and LONDON SMART METERS is one of data collection and analysis, with the London Smart Meters dataset serving as a critical resource for understanding and predicting energy consumption patterns.", "from": "ENERGY", "source_id": "4c09f35749179fe18c7d7290eaa57955,4eb417cb4bd15ceba2949a1358623cb8", "to": "LONDON SMART METERS", "width": 2.0}, {"description": "Solar (5 Min., Hourly) is related to energy as it provides data on solar power generation", "from": "ENERGY", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "SOLAR (5 MIN., HOURLY)", "width": 1.0}, {"description": "Energy is related to Australian electricity demand as Australian electricity demand is a dataset used for pretraining and fine-tuning", "from": "ENERGY", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "AUSTRALIAN ELECTRICITY DEMAND", "width": 1.0}, {"description": "Energy is related to wind farms as wind farms is a dataset used for pretraining and fine-tuning", "from": "ENERGY", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "WIND FARMS", "width": 1.0}, {"description": "Air Quality is related to Energy as both are domains used for fine-tuning forecasting tasks", "from": "ENERGY", "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26", "to": "AIR QUALITY", "width": 1.0}, {"description": "Instances are related to energy as energy is a specific type of instance", "from": "ENERGY", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "INSTANCES", "width": 1.0}, {"description": "Urban computing is related to ACM as ACM publishes and disseminates research in computer science and related fields", "from": "URBAN COMPUTING", "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "to": "ACM", "width": 1.0}, {"description": "Spain is related to KDD \u201924 as KDD \u201924 was held in Spain", "from": "KDD \u201924", "source_id": "736a43d5fef4417bac9757301b4721c3", "to": "SPAIN", "width": 1.0}, {"description": "Yuxuan Liang et al. are related to KDD \u201924 as they are the authors of the paper presented at the conference", "from": "KDD \u201924", "source_id": "ef8fd6170b08af3bd99c9df44ffa8b57", "to": "YUXUAN LIANG ET AL.", "width": 1.0}, {"description": "Table 1 is related to survey as Table 1 is a comparison between the authors\u0027 survey and related surveys", "from": "TABLE 1", "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "to": "SURVEY", "width": 1.0}, {"description": "Table 1 depicts existing studies on time series foundation models", "from": "TABLE 1", "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "to": "TIME SERIES FOUNDATION MODELS", "width": 1.0}, {"description": "Covariates are related to the data sources used for training, summarized in Table 1", "from": "TABLE 1", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "COVARIATES", "width": 1.0}, {"description": "This survey provides a comprehensive methodological analysis of time series foundation models", "from": "SURVEY", "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "to": "TIME SERIES FOUNDATION MODELS", "width": 1.0}, {"description": "Taxonomy is related to pipeline as taxonomy is used to categorize and organize data or concepts in a pipeline", "from": "TAXONOMY", "source_id": "5bb0db923f70e5f431183c6ab7dc25dc", "to": "PIPELINE", "width": 1.0}, {"description": "Large language models are related to decoder-only foundation model as the model is based on large language models", "from": "LARGE LANGUAGE MODELS", "source_id": "323c49cf157623a685283fcfc9b05491", "to": "DECODER-ONLY FOUNDATION MODEL", "width": 1.0}, {"description": "Natural Language Processing is related to large language models as large language models are used for NLP tasks", "from": "LARGE LANGUAGE MODELS", "source_id": "323c49cf157623a685283fcfc9b05491", "to": "NATURAL LANGUAGE PROCESSING", "width": 1.0}, {"description": "Large language models are related to zero-shot learning as zero-shot learning can be used to enable large language models to perform tasks without explicit training", "from": "LARGE LANGUAGE MODELS", "source_id": "0bef137d159ccc86cdee0a8be788bd26", "to": "ZERO-SHOT LEARNING", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe relationship between LARGE LANGUAGE MODELS and TIME SERIES FORECASTING is that large language models can be effectively utilized for time series forecasting. This is because time series forecasting can be cast as a \"language\" task that can be tackled by an off-the-shelf LARGE LANGUAGE MODEL. Specifically, LARGE LANGUAGE MODELS can be used to predict future values of a time series, making them a valuable tool in the field of TIME SERIES FORECASTING.\n\nThis summary is based on the information provided in the description list, which highlights the connection between LARGE LANGUAGE MODELS and TIME SERIES FORECASTING. The use of LARGE LANGUAGE MODELS for time series forecasting is a promising area of research, and further exploration of this relationship could lead to the development of more accurate and effective forecasting models.", "from": "LARGE LANGUAGE MODELS", "source_id": "8f4724ff6541b8924f0cebe9872ed040,b27c89cb0db6646b1203b2701e017aeb", "to": "TIME SERIES FORECASTING", "width": 2.0}, {"description": "Large language models are related to TIME-LLM as TIME-LLM is a reprogramming framework to repurpose LLMs for general time series forecasting", "from": "LARGE LANGUAGE MODELS", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "TIME-LLM", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities in question are \"NATURAL LANGUAGE PROCESSING\" and \"COMPUTER VISION.\" These two fields have a significant connection, as they have both benefited from the use of the Transformer architecture. This architecture has been instrumental in advancing the capabilities of both natural language processing and computer vision.\n\nA key relationship between the two entities is that computer vision is used to interpret and understand visual information from the world, which is closely related to natural language processing. This connection highlights the interdisciplinary nature of these fields, where advancements in one area can have a positive impact on the other.\n\nIn essence, the Transformer architecture has played a crucial role in bridging the gap between natural language processing and computer vision, enabling researchers and practitioners to leverage the strengths of both fields to tackle complex problems in areas such as language understanding, image recognition, and more.\n\nThis summary is based on the information provided in the description list, which highlights the connection between natural language processing and computer vision, as well as the benefits of using the Transformer architecture in both fields.", "from": "NATURAL LANGUAGE PROCESSING", "source_id": "7dbc961fe1959f844ebac283498e7fb0,8f4724ff6541b8924f0cebe9872ed040", "to": "COMPUTER VISION", "width": 2.0}, {"description": "Time series domains are related to natural language processing as natural language processing is used to interpret and understand human language", "from": "NATURAL LANGUAGE PROCESSING", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "TIME SERIES DOMAINS", "width": 1.0}, {"description": "Computer vision and speech are two fields that have benefited from the use of the Transformer architecture.", "from": "COMPUTER VISION", "source_id": "7dbc961fe1959f844ebac283498e7fb0", "to": "SPEECH", "width": 1.0}, {"description": "Natural language is related to computer vision as computer vision is used to interpret and understand visual information from the world", "from": "COMPUTER VISION", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "NATURAL LANGUAGE", "width": 1.0}, {"description": "Time series foundation models aim to harness the power of the foundation model paradigm", "from": "TIME SERIES FOUNDATION MODELS", "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "to": "FOUNDATION MODEL PARADIGM", "width": 1.0}, {"description": "Time series foundation models are used to understand and forecast time series data", "from": "TIME SERIES FOUNDATION MODELS", "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "to": "TIME SERIES DATA", "width": 1.0}, {"description": "Time series foundation models have various model architectures", "from": "TIME SERIES FOUNDATION MODELS", "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "to": "MODEL ARCHITECTURES", "width": 1.0}, {"description": "Time series foundation models use pre-training techniques", "from": "TIME SERIES FOUNDATION MODELS", "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "to": "PRE-TRAINING TECHNIQUES", "width": 1.0}, {"description": "Time series foundation models use adaptation methods", "from": "TIME SERIES FOUNDATION MODELS", "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "to": "ADAPTATION METHODS", "width": 1.0}, {"description": "Time series foundation models use various data modalities", "from": "TIME SERIES FOUNDATION MODELS", "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "to": "DATA MODALITIES", "width": 1.0}, {"description": "Figure 1 depicts the developmental roadmap of current time series foundation models", "from": "TIME SERIES FOUNDATION MODELS", "source_id": "f92205091f8d3b7e1e60b9bc80883a62", "to": "FIGURE 1", "width": 1.0}, {"description": "Time series foundation models are related to LLMs as LLMs are pre-trained models that can be fine-tuned for specific tasks", "from": "TIME SERIES FOUNDATION MODELS", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "LLMS", "width": 1.0}, {"description": "Mean scaling quantization is related to time series data as mean scaling quantization can be used to reduce the precision of model weights and activations when working with time series data", "from": "TIME SERIES DATA", "source_id": "0bef137d159ccc86cdee0a8be788bd26", "to": "MEAN SCALING QUANTIZATION", "width": 1.0}, {"description": "Time series data is related to synthetic time series data as synthetic time series data can be used to augment time series data", "from": "TIME SERIES DATA", "source_id": "0bef137d159ccc86cdee0a8be788bd26", "to": "SYNTHETIC TIME SERIES DATA", "width": 1.0}, {"description": "Time series data is related to tokenization as tokenization is the process of mapping observations into a finite set of tokens", "from": "TIME SERIES DATA", "source_id": "6eb4c16edf2eedfd03721efb199478d8", "to": "TOKENIZATION", "width": 1.0}, {"description": "Time series data is related to local statistical models as local statistical models fit parameters for each time series", "from": "TIME SERIES DATA", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 1.0}, {"description": "Time series data is related to task-specific deep learning models as task-specific deep learning models are trained across multiple time series for a specific task", "from": "TIME SERIES DATA", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "TASK-SPECIFIC DEEP LEARNING MODELS", "width": 2.0}, {"description": "Chronos is related to time series data as it can recognize fundamental patterns present in it", "from": "TIME SERIES DATA", "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "to": "CHRONOS", "width": 1.0}, {"description": "Lags are used in the time series data used to train Lag-Llama", "from": "TIME SERIES DATA", "source_id": "c7167cf92abe7513ccb936ca79871932", "to": "LAGS", "width": 1.0}, {"description": "The time series data is part of a large corpus used to train Lag-Llama", "from": "TIME SERIES DATA", "source_id": "c7167cf92abe7513ccb936ca79871932", "to": "CORPUS", "width": 1.0}, {"description": "Value scaling is related to time series data as it is a process that is used to scale the values of a time series", "from": "TIME SERIES DATA", "source_id": "00007df4774d6122e3848802a24f9536", "to": "VALUE SCALING", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"TIME SERIES DATA\" is related to the entity \"NATURAL LANGUAGE\" in two distinct ways. Firstly, Large Language Models (LLMs) operate on discrete tokens, which contrasts with the inherently continuous nature of time series data. This suggests that there is a fundamental difference in the way LLMs process time series data compared to natural language. Secondly, natural language is used to represent time series data, indicating that there is a connection between the two entities through the use of language to describe or analyze time series data.\n\nThis summary takes into account the information provided in the description list and resolves the apparent contradiction between the two statements. The first statement highlights the difference in the nature of time series data and natural language, while the second statement emphasizes the connection between the two entities through the use of language. By combining these two perspectives, a more comprehensive understanding of the relationship between time series data and natural language can be gained.", "from": "TIME SERIES DATA", "source_id": "89b79391630ac478085efea89fad5736,8f4724ff6541b8924f0cebe9872ed040", "to": "NATURAL LANGUAGE", "width": 2.0}, {"description": "LLM architectures are related to time series data as they can be applied to forecasting tasks without learning from scratch", "from": "TIME SERIES DATA", "source_id": "89b79391630ac478085efea89fad5736", "to": "LLM ARCHITECTURES", "width": 1.0}, {"description": "Time series data can be represented as text prototypes", "from": "TIME SERIES DATA", "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8", "to": "TEXT PROTOTYPES", "width": 1.0}, {"description": "Domain-specific datasets are related to time series data as domain-specific datasets are used to train models for time series forecasting", "from": "TIME SERIES DATA", "source_id": "f7266cfccedb1d9840d10afa689a05e9", "to": "DOMAIN-SPECIFIC DATASETS", "width": 1.0}, {"description": "Model architecture is related to pre-training techniques as pre-training techniques are used to train a model before fine-tuning it for a specific task", "from": "PRE-TRAINING TECHNIQUES", "source_id": "79c0a74b91761402b67c3574db02e8e7", "to": "MODEL ARCHITECTURE", "width": 1.0}, {"description": "Pre-training techniques are related to application domain as application domain is the specific area or field where a model is applied", "from": "PRE-TRAINING TECHNIQUES", "source_id": "79c0a74b91761402b67c3574db02e8e7", "to": "APPLICATION DOMAIN", "width": 1.0}, {"description": "TSFMs are related to data modalities as they are used to handle different types of data", "from": "DATA MODALITIES", "source_id": "53f80422fbf85856ecf940d5c2450665", "to": "TSFMS", "width": 1.0}, {"description": "Spatial time series is related to spatio-temporal graph as spatio-temporal graph describes the spatial correlation of sensors in spatial time series", "from": "SPATIAL TIME SERIES", "source_id": "79c0a74b91761402b67c3574db02e8e7", "to": "SPATIO-TEMPORAL GRAPH", "width": 1.0}, {"description": "Diffusion models are related to spatial time series as they can be used to model data points with spatial coordinates", "from": "SPATIAL TIME SERIES", "source_id": "9125a7d3794226f109debe90e5db041c", "to": "DIFFUSION MODELS", "width": 1.0}, {"description": "Trajectories are related to events as events occur along trajectories", "from": "TRAJECTORIES", "source_id": "ef8fd6170b08af3bd99c9df44ffa8b57", "to": "EVENTS", "width": 1.0}, {"description": "Trajectories are related to clinical records as clinical records contain information about trajectories", "from": "TRAJECTORIES", "source_id": "ef8fd6170b08af3bd99c9df44ffa8b57", "to": "CLINICAL RECORDS", "width": 1.0}, {"description": "AnomalyBERT is inspired by BERT in the natural language processing field", "from": "BERT", "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "to": "ANOMALYBERT", "width": 1.0}, {"description": "BERT is related to MLM as it is a concept used in the BERT model", "from": "BERT", "source_id": "1303ca4c43652bb8052df34d21c78eca", "to": "MASKED LANGUAGE MODELING (MLM)", "width": 1.0}, {"description": "BERT and XLNet are both pre-trained language models that use different techniques to model language", "from": "BERT", "source_id": "a5d60c1a355b77187003182f6df57646", "to": "XLNET", "width": 1.0}, {"description": "BERT is used in BART as a component of the noising schemes", "from": "BERT", "source_id": "a5d60c1a355b77187003182f6df57646", "to": "BART", "width": 1.0}, {"description": "MLM is used in BERT as a technique to predict missing words in a sentence", "from": "BERT", "source_id": "a5d60c1a355b77187003182f6df57646", "to": "MLM", "width": 1.0}, {"description": "Jacob Devlin is related to BERT as he has published a paper on the model", "from": "BERT", "source_id": "a73df99fe49b288e1c8751be2008b191", "to": "JACOB DEVLIN", "width": 1.0}, {"description": "GPT-3 is related to RWC+19 as RWC+19 is a model mentioned in the text, specifically GPT-3", "from": "GPT-3", "source_id": "2bc70082c142ee2ef5d6a941611505b7", "to": "RWC+19", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nGPT-3 and LLAMA 2 are two large language models that share a common relationship. Both models are popular in the field of natural language processing, with GPT-3 being a well-known model developed by OpenAI and LLAMA 2 being another prominent model. The descriptions provided suggest that both models are related due to their large size and popularity, indicating that they are likely to be used for similar applications such as language translation, text generation, and conversational AI.\n\nGiven the information collected from all the descriptions, it can be inferred that GPT-3 and LLAMA 2 are both advanced language models that are widely used in the field of artificial intelligence. Their relationship is based on their shared characteristics as large language models, which makes them suitable for various applications in natural language processing.\n\nIt is worth noting that the provided descriptions are not contradictory, and they provide a consistent view of the relationship between GPT-3 and LLAMA 2. Therefore, the summary generated is a coherent and accurate representation of the information provided.", "from": "GPT-3", "source_id": "0bef137d159ccc86cdee0a8be788bd26,1f1221583d838c2407fe9864225e9eda", "to": "LLAMA 2", "width": 2.0}, {"description": "The decoder-only architecture is used in the GPT-3 model", "from": "GPT-3", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "DECODER-ONLY ARCHITECTURE", "width": 1.0}, {"description": "T5 is related to GPT-3 as both are popular language models", "from": "GPT-3", "source_id": "1f1221583d838c2407fe9864225e9eda", "to": "T5", "width": 1.0}, {"description": "LLM is related to GPT-3 as GPT-3 is a type of LLM", "from": "GPT-3", "source_id": "1f1221583d838c2407fe9864225e9eda", "to": "LLM", "width": 1.0}, {"description": "GPT-3 is a foundation language model that can perform well on a diverse range of NLP tasks in a few-shot or even zero-shot setting", "from": "GPT-3", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "FOUNDATION LANGUAGE MODELS", "width": 1.0}, {"description": "Pre-trained language models can be fine-tuned to adapt to specific tasks", "from": "PRE-TRAINED LLM", "source_id": "79b26808691b6333aafce43c5de531f7", "to": "FINE-TUNING", "width": 1.0}, {"description": "Pre-trained language models and AM are related as both are used in time series analysis", "from": "PRE-TRAINED LLM", "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "to": "AM", "width": 1.0}, {"description": "Pre-trained language models and VLM are related as both are used in time series analysis", "from": "PRE-TRAINED LLM", "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "to": "VLM", "width": 1.0}, {"description": "Decoder-only models are related to pre-trained LLMs as pre-trained LLMs are used to fine-tune decoder-only models", "from": "PRE-TRAINED LLM", "source_id": "1f1221583d838c2407fe9864225e9eda", "to": "DECODER-ONLY", "width": 1.0}, {"description": "Pre-trained LLMs are related to corpus as corpus is used to train pre-trained LLMs", "from": "PRE-TRAINED LLM", "source_id": "1f1221583d838c2407fe9864225e9eda", "to": "CORPUS", "width": 1.0}, {"description": "Patch embeddings are related to pre-trained LLM as pre-trained LLM is used to fine-tune patch embeddings", "from": "PRE-TRAINED LLM", "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "to": "PATCH EMBEDDINGS", "width": 9.0}, {"description": "Fine-tuning and few-shot learning are techniques used to adapt pre-trained models to specific tasks", "from": "FINE-TUNING", "source_id": "79b26808691b6333aafce43c5de531f7", "to": "FEW-SHOT LEARNING", "width": 1.0}, {"description": "Based on the provided information, the primary entity is \"FINE-TUNING\" and \"DIRECT USAGE\", which are both strategies for adapting pre-trained models. \n\nThe descriptions provided suggest that \"DIRECT USAGE\" and \"FINE-TUNING\" are related concepts, as they both involve adapting pre-trained models to specific tasks or datasets. \n\nMore specifically, \"FINE-TUNING\" is a method of adapting TSFM (Time Series Forecasting Models) to specific tasks or datasets, and \"DIRECT USAGE\" is also related to fine-tuning as both are strategies for adapting pre-trained models.\n\nIn the context of time series forecasting, fine-tuning and direct usage are both used to adapt pre-trained models to specific tasks or datasets, allowing for more accurate and effective forecasting.\n\nTherefore, the comprehensive summary of the data is:\n\n\"FINE-TUNING\" and \"DIRECT USAGE\" are related strategies for adapting pre-trained models, specifically in the context of time series forecasting (TSFM). Fine-tuning is a method of adapting TSFM to specific tasks or datasets, and direct usage is also related to fine-tuning, as both involve adapting pre-trained models to achieve more accurate and effective forecasting results.", "from": "FINE-TUNING", "source_id": "53f80422fbf85856ecf940d5c2450665,de8e097ce66fbc954bd529888cbc15ea", "to": "DIRECT USAGE", "width": 2.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"FINE-TUNING\" and \"PROMPT ENGINEERING\" are closely related concepts in the context of adapting and adjusting pre-trained models for specific tasks or datasets. Fine-tuning is a method used to adjust the weights of a pre-trained model to fit a specific task, which is also a key aspect of prompt engineering. Prompt engineering involves adapting TSFM (Time Series Forecasting Models) to specific tasks or datasets, and fine-tuning can be used as a technique to achieve this adaptation. In essence, fine-tuning and prompt engineering are interconnected concepts that work together to enable the effective application of pre-trained models to new and specific tasks.\n\nThis summary is based on the information provided in the description list, which highlights the relationship between fine-tuning and prompt engineering. The description list indicates that fine-tuning can be used to adjust the weights of a pre-trained model to fit a specific task, which is also a key aspect of prompt engineering. Therefore, the summary provides a coherent and concise description of the relationship between the two entities.", "from": "FINE-TUNING", "source_id": "0bef137d159ccc86cdee0a8be788bd26,de8e097ce66fbc954bd529888cbc15ea", "to": "PROMPT ENGINEERING", "width": 2.0}, {"description": "Fine-tuning can be used to adapt a pre-trained LLM to a specific task or dataset", "from": "FINE-TUNING", "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "to": "LLMS", "width": 1.0}, {"description": "Fine-tuning is related to hyper-parameter tuning as it involves adjusting or optimizing the hyper-parameters of a model to improve its performance or accuracy", "from": "FINE-TUNING", "source_id": "9b150491b487d5b2da482652e2fe509d", "to": "HYPER-PARAMETER TUNING", "width": 1.0}, {"description": "Chronos-T5 (Small) is fine-tuned on datasets from Benchmark II to improve its performance", "from": "FINE-TUNING", "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "to": "CHRONOS-T5 (SMALL)", "width": 1.0}, {"description": "Fine-tuning involves adjusting the learning rate to improve the model\u0027s performance", "from": "FINE-TUNING", "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "to": "LEARNING RATE", "width": 1.0}, {"description": "Fine-tuning involves annealing the learning rate linearly to improve the model\u0027s performance", "from": "FINE-TUNING", "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "to": "ANNEALED LINEARLY", "width": 1.0}, {"description": "Figure 6 shows that fine-tuning significantly improves the aggregate performance of the model on Benchmark II", "from": "FINE-TUNING", "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "to": "FIGURE 6", "width": 1.0}, {"description": "Zero-shot learning is related to fine-tuning as fine-tuning is a type of transfer learning", "from": "FINE-TUNING", "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "to": "ZERO-SHOT LEARNING", "width": 1.0}, {"description": "Fine-tuning is a process of adjusting model parameters on a task-specific dataset to tailor the model\u0027s pre-existing knowledge toward the requirements of the new task", "from": "FINE-TUNING", "source_id": "55eb54ef455d14cd1a11760924f99eb8", "to": "TRANSFORMER-BASED ARCHITECTURES", "width": 1.0}, {"description": "Domain-specific applications refer to tasks or domains that require specialized models or techniques, and fine-tuning is a process of adjusting model parameters on a task-specific dataset", "from": "FINE-TUNING", "source_id": "55eb54ef455d14cd1a11760924f99eb8", "to": "DOMAIN-SPECIFIC APPLICATIONS", "width": 1.0}, {"description": "Seasonal Naive is a simple model used for comparison with TimeGPT, and fine-tuning is a process of adjusting model parameters on a task-specific dataset", "from": "FINE-TUNING", "source_id": "55eb54ef455d14cd1a11760924f99eb8", "to": "SEASONAL NAIVE", "width": 1.0}, {"description": "Numba compiling is a method used to optimize code for parallel computing, and fine-tuning is a process of adjusting model parameters on a task-specific dataset", "from": "FINE-TUNING", "source_id": "55eb54ef455d14cd1a11760924f99eb8", "to": "NUMBA COMPILING", "width": 1.0}, {"description": "Zero-shot and fine-tuning are two different settings for testing models", "from": "FINE-TUNING", "source_id": "2f3b2f69f8eb4f958c3ca793cae36581", "to": "ZERO-SHOT", "width": 1.0}, {"description": "LAG-LLAMA achieves state-of-the-art performance in fine-tuning setting", "from": "FINE-TUNING", "source_id": "2f3b2f69f8eb4f958c3ca793cae36581", "to": "LAG-LLAMA", "width": 1.0}, {"description": "Lag-Llama achieves state-of-the-art performance in 3 datasets and significantly improves performance in all other datasets when fine-tuned", "from": "FINE-TUNING", "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "to": "LAG-LAG", "width": 1.0}, {"description": "Pretraining is related to fine-tuning as fine-tuning is used to adjust a pre-trained model to a specific task", "from": "FINE-TUNING", "source_id": "76cc338e586223647fd3dbe4e7a7c131", "to": "PRETRAINING", "width": 1.0}, {"description": "The foundation model approach is related to few-shot learning as it enables models to learn from small amounts of data", "from": "FEW-SHOT LEARNING", "source_id": "c4e47033b8ecc85beacde9d560e66062", "to": "FOUNDATION MODEL APPROACH", "width": 1.0}, {"description": "Few-shot learning is related to forecasting tasks as models can learn from limited training data", "from": "FEW-SHOT LEARNING", "source_id": "6d66c2ea37e25b646a13d751c05a8e4d", "to": "FORECASTING TASKS", "width": 1.0}, {"description": "Trajectory is related to event sequence as event sequence describes the progression of actions or occurrences within a specific context", "from": "TRAJECTORY", "source_id": "79c0a74b91761402b67c3574db02e8e7", "to": "EVENT SEQUENCE", "width": 1.0}, {"description": "Spatio-temporal graph is related to spatio-temporal raster as spatio-temporal raster is a grid of sensors in geographical space", "from": "SPATIO-TEMPORAL GRAPH", "source_id": "79c0a74b91761402b67c3574db02e8e7", "to": "SPATIO-TEMPORAL RASTER", "width": 1.0}, {"description": "Spatio-temporal raster is related to FourCast-Net as FourCast-Net is a model that uses spatio-temporal raster data", "from": "SPATIO-TEMPORAL RASTER", "source_id": "859c14b7112010530eddafc204b4b305", "to": "FOURCAST-NET", "width": 1.0}, {"description": "Data category is related to model architecture as model architecture is a component of data category", "from": "DATA CATEGORY", "source_id": "79c0a74b91761402b67c3574db02e8e7", "to": "MODEL ARCHITECTURE", "width": 1.0}, {"description": "Foundation models for time series are related to standard time series as they can be used to analyze standard time series", "from": "FOUNDATION MODELS FOR TIME SERIES", "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "to": "STANDARD TIME SERIES", "width": 1.0}, {"description": "Standard time series are related to Lag-Llama as Lag-Llama is a pioneering effort as a forecasting foundation model", "from": "STANDARD TIME SERIES", "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "to": "LAG-LAGGPT-1", "width": 1.0}, {"description": "Standard time series are related to TimeGPT-1 as TimeGPT-1 features an encoder-decoder structure with several transformer layers, facilitating efficient zero-shot forecasting", "from": "STANDARD TIME SERIES", "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "to": "TIMEGPT-1", "width": 1.0}, {"description": "Standard time series are related to TTMs as TTMs is a recent endeavor in creating a domain-agnostic forecasting model built upon TSMixer", "from": "STANDARD TIME SERIES", "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "to": "TTMS", "width": 1.0}, {"description": "Lag-Llama and TimeGPT-1 are related as both are pioneering efforts as forecasting foundation models", "from": "LAG-LAGGPT-1", "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "to": "TIMEGPT-1", "width": 1.0}, {"description": "Lag-Llama and TTMs are related as both are used in time series analysis", "from": "LAG-LAGGPT-1", "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "to": "TTMS", "width": 1.0}, {"description": "TimeGPT-1 and TTMs are related as both are used in time series analysis", "from": "TIMEGPT-1", "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "to": "TTMS", "width": 1.0}, {"description": "CLU DA is a model that uses TTMs methodology", "from": "TTMS", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "CLUDA", "width": 1.0}, {"description": "TSMixer and TTMs are both used for time series data tasks, but TSMixer is more effective", "from": "TTMS", "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "to": "TSMIXER", "width": 1.0}, {"description": "Generative is a methodology used in TSMixer model", "from": "TSMIXER", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "GENERATIVE", "width": 1.0}, {"description": "AM and VLM are related as both are used in time series analysis", "from": "AM", "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "to": "VLM", "width": 1.0}, {"description": "Time-LLM and OFA are related as both are pre-trained models used in time series analysis", "from": "TIME-LLM", "source_id": "bb10b1a7f98a4f869b7abbc5f9632dd1", "to": "OFA", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTIME-LLM and LLMTime are two models for time series forecasting that introduce various strategies for effectively tokenizing time series data. Both models are designed to improve the accuracy of forecasting tasks. However, TIME-LLM has shown a substantial improvement, exceeding 75%, when benchmarked against LLMTime. This suggests that TIME-LLM outperforms LLMTime in forecasting tasks, making it a more effective model for time series forecasting.\n\nIt is worth noting that the improvement of TIME-LLM over LLMTime is significant, with a benchmark of over 75%. This indicates that TIME-LLM is a more reliable and accurate model for time series forecasting, making it a valuable tool for researchers and practitioners in the field.\n\nThe use of tokenization strategies in both models is also noteworthy, as it allows for the effective processing and analysis of time series data. This is particularly important in time series forecasting, where the ability to accurately capture and analyze patterns in data is crucial for making accurate predictions.\n\nOverall, the comparison between TIME-LLM and LLMTime highlights the importance of model evaluation and benchmarking in time series forecasting. It also underscores the need for continued research and development in this area, as new models and techniques are continually being developed to improve the accuracy and reliability of time series forecasting.", "from": "TIME-LLM", "source_id": "1ea4e7d0dd5128868533b4567a0a0273,3e937ba8de0e7eca993c50506ceb8f1f,7e03744dea80e2138baff03611104fa8", "to": "LLMTIME", "width": 3.0}, {"description": "Time-LLM is related to METS as both are models for time series forecasting that integrate textual and time series information", "from": "TIME-LLM", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "METS", "width": 1.0}, {"description": "Self-supervised contrastive learning is related to Time-LLM as Time-LLM uses self-supervised contrastive learning to process time series data", "from": "TIME-LLM", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "SELF-SUPERVISED CONTRASTIVE LEARNING", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"TIME-LLM\" is related to the concept of \"REPROGRAMMING\". Specifically, TIME-LLM utilizes reprogramming to modify or update its parameters. In other words, reprogramming is used to update or modify TIME-LLM, indicating a bidirectional relationship between the two entities. This relationship suggests that TIME-LLM is a dynamic entity that can be modified or updated through the process of reprogramming.\n\nThis summary is based on the information provided in the description list, which states that TIME-LLM uses reprogramming to modify or update its parameters, and that reprogramming is used to modify or update TIME-LLM. These two statements are not contradictory, but rather complementary, highlighting the interdependence of the two entities.\n\nThe language used in the description list is formal and technical, suggesting that the context is likely an academic or research setting. The use of terms such as \"reprogramming\" and \"parameters\" implies a focus on machine learning or artificial intelligence, which is consistent with the entity names provided.\n\nOverall, the summary provides a clear and concise description of the relationship between TIME-LLM and reprogramming, highlighting the dynamic and bidirectional nature of this relationship.", "from": "TIME-LLM", "source_id": "83f62beddf5cf53ed2e2d4517569deb8,84bc2afcbbd278961c3c7a637c6a189e", "to": "REPROGRAMMING", "width": 2.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe TIME-LLM model outperforms the GPT-2 model by 2.7% in terms of average Mean Squared Error (MSE) reduction. Notably, GPT-2 serves as the backbone for the TIME-LLM model, indicating a close relationship between the two entities. This suggests that TIME-LLM builds upon the foundation established by GPT-2, leveraging its capabilities to achieve improved performance in terms of MSE reduction.\n\nThis summary is written in third person and includes the entity names, TIME-LLM and GPT-2, to provide context. The information is also enriched with relevant details from the provided descriptions, resolving any potential contradictions and presenting a coherent summary.", "from": "TIME-LLM", "source_id": "1f1221583d838c2407fe9864225e9eda,7e03744dea80e2138baff03611104fa8", "to": "GPT-2", "width": 2.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\n**Summary of TIME-LLM and PATCHTST**\n\nTIME-LLM and PATCHTST are two models that utilize patch embeddings for time series analysis tasks. Both models are compared in various forecasting scenarios, including short-term and long-term forecasting. The results indicate that TIME-LLM consistently outperforms PATCHTST in most cases, with a significant margin of over 8% reduction in Mean Squared Error (MSE) in some instances. However, PATCHTST exhibits comparable or superior performances to TIME-LLM in short-term forecasting tasks.\n\nNotably, TIME-LLM is able to outperform PATCHTST without any parameter updates on the backbone Large Language Model (LLM), suggesting its robustness and effectiveness in time series forecasting tasks. The comparison between the two models is also extended to the standard deviations of long-term forecasting, with TIME-LLM being compared to PATCHTST in this regard.\n\nOverall, the results suggest that TIME-LLM is a more effective model for time series forecasting tasks, particularly in long-term forecasting scenarios. However, PATCHTST may have its own strengths in short-term forecasting tasks, making it a viable alternative in certain situations.\n\n**Key Findings:**\n\n* TIME-LLM outperforms PATCHTST in most cases, with a significant margin of over 8% reduction in MSE.\n* PATCHTST exhibits comparable or superior performances to TIME-LLM in short-term forecasting tasks.\n* TIME-LLM is able to outperform PATCHTST without any parameter updates on the backbone LLM.\n* TIME-LLM is compared to PATCHTST in terms of standard deviations of long-term forecasting.\n\n**Entities:**\n\n* TIME-LLM: A model that utilizes patch embeddings for time series analysis tasks and exhibits superior performances in most cases.\n* PATCHTST: A model that utilizes patch embeddings for time series analysis tasks and exhibits comparable or superior performances in short-term forecasting tasks.", "from": "TIME-LLM", "source_id": "1f1221583d838c2407fe9864225e9eda,3e937ba8de0e7eca993c50506ceb8f1f,5e4d9ca02ee6a285d5223c820743eb12,5fa25d3abec59ccd2718e00c0e0eb440,7e03744dea80e2138baff03611104fa8,84bc2afcbbd278961c3c7a637c6a189e,91e161ba596a0cbbcae541ddb2106310,ab91381dd032db318e5aec1ad5b914a6,d4551c2839eaa68a7cb7324089956581,ef2ef6c95c36f51706c54ca3f2893641", "to": "PATCHTST", "width": 10.0}, {"description": "Time-LLM repurposes LLMs for time series forecasting", "from": "TIME-LLM", "source_id": "bc54a718d1886698232d578fd88c3ac7", "to": "LLMS", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe TIME-LLM framework is a proposed reprogramming framework that aims to repurpose Large Language Models (LLMs) for general time series forecasting. This framework involves reprogramming the input time series into text prototype representations, which can then be utilized by the LLM for forecasting purposes. Specifically, the LLM is used in the TIME-LLM model for time series forecasting, indicating a direct relationship between the two entities. In essence, TIME-LLM is a reprogramming framework that enables the application of LLMs to the task of time series forecasting, making it a crucial component in the field of time series forecasting.\n\nThe TIME SERIES FORECASTING entity is closely related to TIME-LLM, as the latter is specifically designed to facilitate forecasting in this domain. The use of LLMs in TIME-LLM for time series forecasting suggests that the framework has the potential to improve forecasting accuracy and efficiency in this area. Overall, the TIME-LLM framework represents a significant development in the field of time series forecasting, offering a novel approach to leveraging the capabilities of LLMs for this critical task.\n\nRelevant information from the nearby text includes the use of technical terms, mathematical equations, and references to academic papers, all of which are written in English. This suggests that the text is from a research paper or technical document, and the language used is formal and academic. The presence of English-language abbreviations and citations to English-language academic papers further supports this conclusion.", "from": "TIME-LLM", "source_id": "072b166b9a1b6afecf5874f45af61699,8f4724ff6541b8924f0cebe9872ed040,b27c89cb0db6646b1203b2701e017aeb", "to": "TIME SERIES FORECASTING", "width": 3.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTIME-LLM and LLM are two entities that are closely related. TIME-LLM is a reprogrammed Large Language Model (LLM) that has been adapted for time series forecasting. In other words, TIME-LLM is a reprogramming framework that repurposes LLMs for general time series forecasting. This suggests that LLM is used as the backbone of TIME-LLM, indicating a strong connection between the two entities.\n\nThe relationship between TIME-LLM and LLM can be summarized as follows: TIME-LLM is a specialized version of LLM, designed specifically for time series forecasting tasks. This adaptation enables TIME-LLM to leverage the strengths of LLM while addressing the unique challenges of time series forecasting.\n\nOverall, the summary highlights the close relationship between TIME-LLM and LLM, with TIME-LLM being a reprogrammed version of LLM for time series forecasting applications.", "from": "TIME-LLM", "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8,8f4724ff6541b8924f0cebe9872ed040,fececbac281c1e2b13921f378df30919", "to": "LLM", "width": 3.0}, {"description": "TIME-LLM is related to prototype representations as the core idea is to reprogram the input time series into text prototype representations", "from": "TIME-LLM", "source_id": "89b79391630ac478085efea89fad5736", "to": "PROTOTYPE REPRESENTATIONS", "width": 1.0}, {"description": "TIME-LLM encompasses reprogramming the input time series into text prototype representations", "from": "TIME-LLM", "source_id": "b27c89cb0db6646b1203b2701e017aeb", "to": "TEXT PROTOTYPE REPRESENTATIONS", "width": 1.0}, {"description": "Time series reasoning is related to TIME-LLM as TIME-LLM is optimized for time series forecasting", "from": "TIME-LLM", "source_id": "fececbac281c1e2b13921f378df30919", "to": "TIME SERIES REASONING", "width": 1.0}, {"description": "Time-LLM outperforms ESTformer in forecasting tasks", "from": "TIME-LLM", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "ESTFORMER", "width": 1.0}, {"description": "Time-LLM outperforms Non-Stationary Transformer in forecasting tasks", "from": "TIME-LLM", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "NON-STATIONARY TRANSFORMER", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe TIME-LLM and FEDFORMER are two entities related to time series forecasting. According to the descriptions, TIME-LLM outperforms FEDFORMER in forecasting tasks. Specifically, TIME-LLM is related to FEDFORMER as it surpasses FEDFORMER in terms of performance, indicating that TIME-LLM is a more effective model for time series forecasting.\n\nThis information suggests that TIME-LLM is a more advanced or improved version of FEDFORMER, with better capabilities in forecasting tasks. The exact nature of the improvements is not specified, but it is clear that TIME-LLM has a competitive advantage over FEDFORMER in this context.\n\nOverall, the summary provides a clear understanding of the relationship between TIME-LLM and FEDFORMER, highlighting the superior performance of TIME-LLM in time series forecasting tasks.", "from": "TIME-LLM", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,d4551c2839eaa68a7cb7324089956581", "to": "FEDFORMER", "width": 2.0}, {"description": "Time-LLM outperforms Autoformer in forecasting tasks", "from": "TIME-LLM", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "AUTOFORMER", "width": 1.0}, {"description": "Time-LLM outperforms Informer in forecasting tasks", "from": "TIME-LLM", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "INFORMER", "width": 1.0}, {"description": "Time-LLM outperforms Reformer in forecasting tasks", "from": "TIME-LLM", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "REFORMER", "width": 1.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\n**Summary of TIME-LLM and GPT4TS**\n\nTIME-LLM and GPT4TS are two entities related to time series forecasting models. According to the descriptions, TIME-LLM consistently outperforms GPT4TS in various scenarios, including few-shot learning and forecasting tasks. Specifically, TIME-LLM exhibits remarkable superiority in forecasting with limited data compared to GPT4TS, surpassing it by 8.7% in forecasting tasks and reducing the Mean Squared Error (MSE) by over 5% in few-shot learning scenarios.\n\nThe comparison between TIME-LLM and GPT4TS is evident in long-term forecasting tasks, as shown in Fig. 7 and Fig. 8. TIME-LLM is related to GPT4TS as both are models used for forecasting and time series forecasting. The substantial margins by which TIME-LLM outperforms GPT4TS in most cases indicate its remarkable performance in this domain.\n\n**Key Findings**\n\n* TIME-LLM consistently outperforms GPT4TS in few-shot learning and forecasting tasks.\n* TIME-LLM exhibits remarkable superiority in forecasting with limited data compared to GPT4TS.\n* TIME-LLM surpasses GPT4TS by 8.7% in forecasting tasks.\n* TIME-LLM reduces the MSE by over 5% in few-shot learning scenarios.\n* The comparison between TIME-LLM and GPT4TS is evident in long-term forecasting tasks, as shown in Fig. 7 and Fig. 8.\n\n**Conclusion**\n\nTIME-LLM and GPT4TS are two entities related to time series forecasting models. TIME-LLM consistently outperforms GPT4TS in various scenarios, indicating its remarkable performance in this domain. The substantial margins by which TIME-LLM outperforms GPT4TS in most cases suggest its potential as a superior model for time series forecasting tasks.", "from": "TIME-LLM", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,5e4d9ca02ee6a285d5223c820743eb12,5fa25d3abec59ccd2718e00c0e0eb440,6d66c2ea37e25b646a13d751c05a8e4d,7e03744dea80e2138baff03611104fa8,84bc2afcbbd278961c3c7a637c6a189e,91e161ba596a0cbbcae541ddb2106310,ab91381dd032db318e5aec1ad5b914a6,d4551c2839eaa68a7cb7324089956581,f49330b6fd81d86d14e7a9d4b8e45576,fd1092903d83bf6e90a6caa371d7c514", "to": "GPT4TS", "width": 11.0}, {"description": "Based on the provided data, a comprehensive summary of the entities \"TIME-LLM\" and \"TimesNet\" can be generated as follows:\n\nTIME-LLM and TimesNet are two entities that have been compared in terms of their performance in time series forecasting. According to the analysis, TIME-LLM consistently outperforms TimesNet by a significant margin, with a reduction of over 33% in Mean Squared Error (MSE). However, TimesNet exhibits comparable or superior performances to TIME-LLM in short-term forecasting.\n\nIt is also noted that TIME-LLM remains competitive even when compared with TimesNet, suggesting that it is a strong performer in time series forecasting. Additionally, TIME-LLM outperforms TimesNet in most cases, with significant improvements in the majority of them.\n\nOverall, the comparison between TIME-LLM and TimesNet suggests that TIME-LLM is a strong performer in time series forecasting, particularly in long-term forecasting, while TimesNet performs well in short-term forecasting.", "from": "TIME-LLM", "source_id": "5e4d9ca02ee6a285d5223c820743eb12,7e03744dea80e2138baff03611104fa8,d4551c2839eaa68a7cb7324089956581,ef2ef6c95c36f51706c54ca3f2893641", "to": "TIMESNET", "width": 4.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nThe TIME-LLM model consistently outperforms the DLinear model by a significant margin, with a notable reduction of over 12% in Mean Squared Error (MSE). This performance gap is observed in most cases, with TIME-LLM demonstrating a substantial advantage over DLinear. Both TIME-LLM and DLinear are models used for forecasting purposes, indicating a relationship between the two entities. This relationship is further reinforced by the fact that TIME-LLM outperforms DLinear, suggesting that TIME-LLM is a more effective forecasting model.\n\nThe summary is written in third person and includes the entity names, TIME-LLM and DLinear, to provide context. The information is enriched with relevant details from the descriptions, including the performance gap between the two models and their relationship as forecasting models.", "from": "TIME-LLM", "source_id": "5e4d9ca02ee6a285d5223c820743eb12,7e03744dea80e2138baff03611104fa8,91e161ba596a0cbbcae541ddb2106310,d4551c2839eaa68a7cb7324089956581", "to": "DLINER", "width": 4.0}, {"description": "Based on the provided data, a comprehensive summary of the entities \"TIME-LLM\" and \"N-HITS\" can be generated as follows:\n\nTIME-LLM and N-HITS are two entities that have been compared in terms of their performance in various forecasting tasks. According to the descriptions, TIME-LLM exhibits comparable or superior performances to N-HITS in short-term forecasting, and it outperforms N-HITS in this regard. Additionally, TIME-LLM remains competitive with N-HITS in terms of MASE (Mean Absolute Scaled Error) and OWA (Out-of-Sample Mean Absolute Error).\n\nIn contrast, N-HITS exhibits comparable or superior performances to TIME-LLM in short-term forecasting, suggesting that the performance of these two entities can vary depending on the specific task. However, TIME-LLM is shown to outperform N-HITS in some cases, indicating that it has an advantage over N-HITS in certain scenarios.\n\nIt is also mentioned that TIME-LLM is compared to N-HITS in terms of performance, and that it is compared with N-HITS on long-term forecasting tasks, as shown in Table 20. This suggests that the comparison between TIME-LLM and N-HITS is not limited to short-term forecasting, but also extends to long-term forecasting tasks.\n\nOverall, the summary suggests that TIME-LLM and N-HITS are two entities that have been compared in terms of their performance in various forecasting tasks, with TIME-LLM exhibiting superior performance in some cases and N-HITS exhibiting comparable or superior performance in other cases.\n\nRelevant information from the nearby text is not provided, but based on the descriptions, it can be inferred that the comparison between TIME-LLM and N-HITS is likely in the context of time series forecasting, given the mention of short-term and long-term forecasting tasks, as well as the use of metrics such as MASE and OWA, which are commonly used in time series forecasting.", "from": "TIME-LLM", "source_id": "5e4d9ca02ee6a285d5223c820743eb12,5fa25d3abec59ccd2718e00c0e0eb440,6d66c2ea37e25b646a13d751c05a8e4d,9ba0189af2ef0720a721c16eef0f0788,ab91381dd032db318e5aec1ad5b914a6,d4551c2839eaa68a7cb7324089956581,ef2ef6c95c36f51706c54ca3f2893641", "to": "N-HITS", "width": 7.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n**Summary of TIME-LLM and N-BEATS**\n\nTIME-LLM and N-BEATS are two models used for time series forecasting. A comparison of their performance has been conducted, with TIME-LLM emerging as the superior model. Specifically, TIME-LLM outperforms N-BEATS in short-term forecasting. This suggests that TIME-LLM is a more effective model for predicting short-term time series data, while N-BEATS may be more suitable for other applications or longer-term forecasting tasks.\n\n**Key Findings**\n\n* Both TIME-LLM and N-BEATS are used for time series forecasting.\n* TIME-LLM outperforms N-BEATS in short-term forecasting.\n* A comparison of their performance has been conducted.\n\n**Implications**\n\nThe findings suggest that TIME-LLM is a more effective model for short-term time series forecasting, while N-BEATS may be more suitable for other applications or longer-term forecasting tasks. This information can be useful for researchers and practitioners looking to select the most appropriate model for their specific time series forecasting needs.\n\n**Context**\n\nThe comparison of TIME-LLM and N-BEATS is likely related to the field of time series analysis and forecasting, where models are evaluated based on their performance in predicting future values of a time series. The use of these models in various applications, such as finance, economics, and engineering, is a common practice in the field.", "from": "TIME-LLM", "source_id": "1b48e9ca066ac5ba037066bb762d3458,5e4d9ca02ee6a285d5223c820743eb12,9ba0189af2ef0720a721c16eef0f0788,d4551c2839eaa68a7cb7324089956581", "to": "N-BEATS", "width": 4.0}, {"description": "M4 is related to TIME-LLM as TIME-LLM is used for short-term time series forecasting on M4", "from": "TIME-LLM", "source_id": "d4551c2839eaa68a7cb7324089956581", "to": "M4", "width": 1.0}, {"description": "TIME-LLM is related to ETSformer as TIME-LLM outperforms ETSformer", "from": "TIME-LLM", "source_id": "d4551c2839eaa68a7cb7324089956581", "to": "ETSFORMER", "width": 1.0}, {"description": "TIME-LLM is related to LightTS as TIME-LLM outperforms LightTS", "from": "TIME-LLM", "source_id": "d4551c2839eaa68a7cb7324089956581", "to": "LIGHTTS", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTIME-LLM and LLAMA-7B are two models used for text-based tasks. They are related in that LLAMA-7B serves as the backbone for TIME-LLM, which manifests a striking average improvement of over 20% when utilizing LLAMA-7B in this capacity. This suggests a strong synergy between the two models, with LLAMA-7B providing a robust foundation for TIME-LLM\u0027s performance in text-based tasks.", "from": "TIME-LLM", "source_id": "1b48e9ca066ac5ba037066bb762d3458,7e03744dea80e2138baff03611104fa8", "to": "LLAMA-7B", "width": 2.0}, {"description": "Based on the provided information, the primary entity of interest is TIME-LLM, and it is related to ICLR 2024. \n\nTIME-LLM was presented at ICLR 2024, indicating that the entity is associated with the conference. Additionally, the paper related to TIME-LLM was published at ICLR 2024, further solidifying the connection between the two entities.\n\nIt is worth noting that there are minor variations in the spelling of the entity name, with \"TIME-LLM\" and \"Time-LLM\" being used interchangeably. However, this does not affect the overall relationship between the entities.\n\nIn terms of the language used, the text is written in English, as indicated by the use of technical terms, mathematical equations, and references to academic papers. The language is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nOverall, the relationship between TIME-LLM and ICLR 2024 can be summarized as follows:\n\nTIME-LLM, a research entity, was presented and published at ICLR 2024, a prominent academic conference. The connection between the two entities is established through the presentation and publication of a paper related to TIME-LLM at ICLR 2024.", "from": "TIME-LLM", "source_id": "072b166b9a1b6afecf5874f45af61699,d4e3d8b5bf043b78bb9f1551080cab91,e57d44d23f5a3a82ce9a9b9532d31cbe", "to": "ICLR 2024", "width": 3.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe TIME-LLM model is related to the ETTh1 dataset, which is used for testing the model. Specifically, TIME-LLM is utilized for forecasting purposes on the ETTh1 dataset. This suggests that the ETTh1 dataset serves as a benchmark or evaluation platform for the TIME-LLM model\u0027s forecasting capabilities.\n\nIn other words, the ETTh1 dataset is used to assess the performance and accuracy of the TIME-LLM model in forecasting tasks. This relationship highlights the importance of the ETTh1 dataset in the development and evaluation of the TIME-LLM model.\n\nOverall, the TIME-LLM model and the ETTh1 dataset are interconnected, with the latter serving as a critical component in the testing and evaluation of the former\u0027s forecasting abilities.", "from": "TIME-LLM", "source_id": "50bf8b7f27ec843882dbc6eadb2bf158,84bc2afcbbd278961c3c7a637c6a189e", "to": "ETTH1", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe TIME-LLM entity is a machine learning model that has parameters that can be adjusted or updated. These parameters play a crucial role in the model\u0027s training process, as they are used to fine-tune the model\u0027s performance and accuracy. In other words, the parameters of TIME-LLM are essential for its development and optimization.\n\nThis relationship between TIME-LLM and its parameters is fundamental to the model\u0027s functionality and effectiveness. By adjusting or updating the parameters, developers can improve the model\u0027s ability to learn from data and make accurate predictions or forecasts.\n\nIn the context of time series forecasting, TIME-LLM\u0027s parameters are likely to be critical in determining the model\u0027s performance and accuracy. The ability to adjust or update these parameters can be a significant advantage in developing and refining the model for specific applications or use cases.\n\nOverall, the connection between TIME-LLM and its parameters is a key aspect of the model\u0027s development and optimization, and understanding this relationship is essential for leveraging the full potential of TIME-LLM in time series forecasting and other applications.", "from": "TIME-LLM", "source_id": "50bf8b7f27ec843882dbc6eadb2bf158,84bc2afcbbd278961c3c7a637c6a189e", "to": "PARAMETERS", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe TIME-LLM entity is closely related to the MEMORY entity. Specifically, TIME-LLM utilizes MEMORY to store its model weights. Furthermore, TIME-LLM leverages MEMORY to store its parameters and other essential data. This suggests that MEMORY plays a crucial role in the functioning and operation of TIME-LLM, enabling it to retain and access critical information necessary for its performance.\n\nIn the context of machine learning and time series forecasting, the relationship between TIME-LLM and MEMORY is significant. The use of MEMORY to store model weights and parameters allows TIME-LLM to maintain a level of continuity and consistency in its predictions, which is essential for accurate time series forecasting. This relationship highlights the importance of MEMORY in supporting the functionality of TIME-LLM, particularly in applications where long-term series forecasting and frequency analysis are critical.", "from": "TIME-LLM", "source_id": "50bf8b7f27ec843882dbc6eadb2bf158,84bc2afcbbd278961c3c7a637c6a189e", "to": "MEMORY", "width": 2.0}, {"description": "Based on the provided information, the entity \"TIME-LLM\" is related to the entity \"SPEED\". The description of \"TIME-LLM\" indicates that it has a speed that can be improved or optimized, suggesting that the speed of TIME-LLM is a critical aspect of its performance.\n\nFurthermore, the description states that speed is a measure of the model\u0027s computational efficiency, implying that TIME-LLM\u0027s speed is directly related to its ability to process and compute information efficiently. This is a key aspect of TIME-LLM\u0027s functionality, as it is likely designed to handle large amounts of data and perform complex computations quickly.\n\nIn the context of machine learning and time series forecasting, TIME-LLM\u0027s speed is likely a critical factor in its ability to accurately predict and analyze time series data. The relationship between TIME-LLM and speed suggests that optimizing TIME-LLM\u0027s speed could lead to improved performance and more accurate predictions.\n\nOverall, the entity \"TIME-LLM\" is closely tied to the entity \"SPEED\", with speed being a key aspect of TIME-LLM\u0027s performance and functionality.", "from": "TIME-LLM", "source_id": "50bf8b7f27ec843882dbc6eadb2bf158,84bc2afcbbd278961c3c7a637c6a189e", "to": "SPEED", "width": 2.0}, {"description": "TIME-LLM is related to patch reprogramming as TIME-LLM proposes reprogramming time series with the source data modality along with prompting", "from": "TIME-LLM", "source_id": "f7266cfccedb1d9840d10afa689a05e9", "to": "PATCH REPROGRAMMING", "width": 1.0}, {"description": "TIME-LLM is related to Tab. 9 as Tab. 9 details the experimental configurations for TIME-LLM", "from": "TIME-LLM", "source_id": "306c29917039736b5e882376c7647704", "to": "TAB. 9", "width": 1.0}, {"description": "TIME-LLM is related to model training as model training is used to train the TIME-LLM model", "from": "TIME-LLM", "source_id": "306c29917039736b5e882376c7647704", "to": "MODEL TRAINING", "width": 1.0}, {"description": "TIME-LLM is used for short-term forecasting, consistently outperforming baseline models in most cases", "from": "TIME-LLM", "source_id": "ef2ef6c95c36f51706c54ca3f2893641", "to": "SHORT-TERM FORECASTING", "width": 1.0}, {"description": "TIME-LLM consistently outperforms baseline models in most cases", "from": "TIME-LLM", "source_id": "ef2ef6c95c36f51706c54ca3f2893641", "to": "BASELINE MODELS", "width": 1.0}, {"description": "M3-Quarterly is a dataset used for evaluating the performance of models in short-term forecasting, including TIME-LLM", "from": "TIME-LLM", "source_id": "ef2ef6c95c36f51706c54ca3f2893641", "to": "M3-QUARTERLY", "width": 1.0}, {"description": "TIME-LLM attains on-par performance compared to TimesNet and PATCHTST on the M3-Quarterly dataset", "from": "TIME-LLM", "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "to": "M3-QUARTERLY DATASET", "width": 1.0}, {"description": "TIME-LLM exhibits comparable or superior performances on the M4-Yearly dataset", "from": "TIME-LLM", "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "to": "M4-YEARLY DATASET", "width": 1.0}, {"description": "TIME-LLM exhibits comparable or superior performances on the M4-Hourly dataset", "from": "TIME-LLM", "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "to": "M4-HOURLY DATASET", "width": 1.0}, {"description": "TIME-LLM exhibits comparable or superior performances on the M4-Daily dataset", "from": "TIME-LLM", "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "to": "M4-DAILY DATASET", "width": 1.0}, {"description": "TIME-LLM exhibits comparable or superior performances on the M4-Weekly dataset", "from": "TIME-LLM", "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "to": "M4-WEEKLY DATASET", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe TIME-LLM and ILI entities are related in the context of time series forecasting. TIME-LLM exhibits comparable or superior performances on the ILI dataset, indicating its effectiveness in handling ILI data. Furthermore, ILI is a type of data used in short-term forecasting, suggesting that TIME-LLM is capable of handling short-term forecasting tasks, particularly those involving ILI data.\n\nThis summary resolves the relationship between TIME-LLM and ILI, highlighting their connection through the ILI dataset and the forecasting tasks they are associated with.", "from": "TIME-LLM", "source_id": "27efab80b405b365d8e9dd9834dd1ca8,5fa25d3abec59ccd2718e00c0e0eb440", "to": "ILI", "width": 2.0}, {"description": "TIME-LLM is related to zero-shot forecasting as it surpasses the six most competitive time series models in zero-shot adaptation", "from": "TIME-LLM", "source_id": "41fe893a178ebc8790ef4da83da5ab6e", "to": "ZERO-SHOT FORECASTING", "width": 1.0}, {"description": "Time-LLM is related to DLinear as both are models mentioned in the text", "from": "TIME-LLM", "source_id": "fd1092903d83bf6e90a6caa371d7c514", "to": "DLINEAR", "width": 1.0}, {"description": "TIME-LLM is visualized in various scenarios, as shown in Fig. 7, Fig. 8, Fig. 9, and Fig. 10", "from": "TIME-LLM", "source_id": "ab91381dd032db318e5aec1ad5b914a6", "to": "VISUALIZATION", "width": 1.0}, {"description": "TIME-LLM is used for forecasting on the ETTh2 dataset", "from": "TIME-LLM", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "ETTH2", "width": 1.0}, {"description": "TIME-LLM is used for forecasting on the ETTm1 dataset", "from": "TIME-LLM", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "ETTM1", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe TIME-LLM and ETTm2 are two models that are related in their application for short-term forecasting. Specifically, TIME-LLM is utilized for forecasting on the ETTm2 dataset, indicating a direct connection between the two entities in terms of their functionality and data usage.\n\nThis summary is derived from the provided descriptions, which collectively convey the relationship between TIME-LLM and ETTm2 as models for short-term forecasting, with TIME-LLM being specifically applied to the ETTm2 dataset.", "from": "TIME-LLM", "source_id": "27efab80b405b365d8e9dd9834dd1ca8,84bc2afcbbd278961c3c7a637c6a189e", "to": "ETTM2", "width": 2.0}, {"description": "TIME-LLM uses parameter-efficient fine-tuning to fine-tune its parameters", "from": "TIME-LLM", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "PARAMETER-EFFICIENT FINE-TUNING", "width": 1.0}, {"description": "TIME-LLM and PatchTST (2023) are related as both are models used for short-term forecasting", "from": "TIME-LLM", "source_id": "27efab80b405b365d8e9dd9834dd1ca8", "to": "PATCHTST (2023)", "width": 1.0}, {"description": "TIME-LLM is related to weather as weather is a type of data used in short-term forecasting", "from": "TIME-LLM", "source_id": "27efab80b405b365d8e9dd9834dd1ca8", "to": "WEATHER", "width": 1.0}, {"description": "TIME-LLM is related to electricity as electricity is a type of data used in short-term forecasting", "from": "TIME-LLM", "source_id": "27efab80b405b365d8e9dd9834dd1ca8", "to": "ELECTRICITY", "width": 1.0}, {"description": "TIME-LLM is related to traffic as traffic is a type of data used in short-term forecasting", "from": "TIME-LLM", "source_id": "27efab80b405b365d8e9dd9834dd1ca8", "to": "TRAFFIC", "width": 1.0}, {"description": "Time-LLM is related to monthly as it is used for monthly time series forecasting", "from": "TIME-LLM", "source_id": "d4e3d8b5bf043b78bb9f1551080cab91", "to": "MONTHLY", "width": 1.0}, {"description": "Time-LLM is related to N-HiTS (2023a) as it is referenced in the paper", "from": "TIME-LLM", "source_id": "d4e3d8b5bf043b78bb9f1551080cab91", "to": "N-HITS (2023A)", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nN-BEATS and OFA are two models used for time series forecasting. They are mentioned in the text, as indicated by the use of their names in the table. Both models are utilized for long-term series forecasting, which involves analyzing and predicting future values in a time series. The text also suggests that these models may employ techniques such as frequency analysis and multi-head cross-attention, which are commonly used in time series forecasting and machine learning applications.\n\nThe use of N-BEATS and OFA in time series forecasting is likely related to their ability to handle complex patterns and relationships in data, which is a key challenge in this field. The text does not provide further details on the specific characteristics or advantages of these models, but it is clear that they are being used for time series forecasting and may be compared or contrasted with other models in the field.\n\nOverall, N-BEATS and OFA are two models that are being used for time series forecasting, and they may be of interest to researchers or practitioners working in this area.", "from": "OFA", "source_id": "6fa5f635ad5f7e6b67d5e467f130345c,bb87457fce8d4214bfe1f398b7ea35f2", "to": "NBEATS", "width": 2.0}, {"description": "OFA is related to CRPS as CRPS is used to evaluate the performance of OFA", "from": "OFA", "source_id": "990578022879395d00b7a5b229863c2f", "to": "CRPS", "width": 1.0}, {"description": "OFA and INFORMER are models mentioned in the text, as indicated by the use of their names in the table", "from": "OFA", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "INFORMER", "width": 1.0}, {"description": "OFA and AUTOFORMER are models mentioned in the text, as indicated by the use of their names in the table", "from": "OFA", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "AUTOFORMER", "width": 1.0}, {"description": "OFA and ETSFORMER are models mentioned in the text, as indicated by the use of their names in the table", "from": "OFA", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "ETSFORMER", "width": 1.0}, {"description": "OFA is a specific model mentioned in the text, as indicated by the use of the name \"OFA\"", "from": "OFA", "source_id": "79c41aff65d584b4c8d4c769b82756d5", "to": "MODEL", "width": 1.0}, {"description": "LLM4TS is related to TEMPO as both are models for time series forecasting that leverage pre-trained language models", "from": "LLM4TS", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "TEMPO", "width": 1.0}, {"description": "GPT-2 is related to LLM4TS as GPT-2 is a pre-trained language model used in LLM4TS", "from": "LLM4TS", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "GPT-2", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Summary of PROMPTCAST and LLMTIME**\n\nPROMPTCAST and LLMTIME are two models related to time series forecasting. Both models are designed to convert numerical inputs and outputs into prompts, which is a key aspect of their functionality. Additionally, both models utilize pre-trained Large Language Models (LLMs) for forecasting purposes. This suggests that PROMPTCAST and LLMTIME share a common approach to time series forecasting, leveraging the capabilities of LLMs to generate accurate predictions.\n\n**Key Features of PROMPTCAST and LLMTIME**\n\n* Both models are designed for time series forecasting\n* Both models convert numerical inputs and outputs into prompts\n* Both models use pre-trained LLMs for forecasting\n* Both models are related to time series forecasting, suggesting a common application domain\n\n**Implications of the Relationship between PROMPTCAST and LLMTIME**\n\nThe relationship between PROMPTCAST and LLMTIME suggests that there may be opportunities for collaboration or knowledge sharing between the two models. By leveraging the strengths of both models, researchers and practitioners may be able to develop more accurate and effective time series forecasting techniques. Additionally, the use of pre-trained LLMs in both models highlights the potential benefits of using transfer learning and multi-task learning approaches in time series forecasting.\n\nOverall, the summary provides a comprehensive understanding of the relationship between PROMPTCAST and LLMTIME, highlighting their shared features and potential applications in time series forecasting.", "from": "PROMPTCAST", "source_id": "1ea4e7d0dd5128868533b4567a0a0273,1f1221583d838c2407fe9864225e9eda", "to": "LLMTIME", "width": 2.0}, {"description": "METS is related to PromptCast as both are models for time series forecasting that engage in the synchronization of time series and acoustic data", "from": "PROMPTCAST", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "METS", "width": 1.0}, {"description": "PromptCast leverages language models directly for forecasting", "from": "PROMPTCAST", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "LLMS", "width": 1.0}, {"description": "PromptCast frames the forecasting task as a sentence-to-sentence conversion to leverage language models directly for forecasting", "from": "PROMPTCAST", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "TIME SERIES TASKS", "width": 1.0}, {"description": "Decomposition strategies are related to TEMPO as TEMPO is a model that employs decomposition strategies", "from": "TEMPO", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "DECOMPOSITION STRATEGIES", "width": 1.0}, {"description": "Based on the provided information, the primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nThe two entities, \"LLMTIME\" and \"TIMESFM\", are related to each other as both are models used for time series forecasting and analysis. This is supported by the descriptions provided, which state that \"TimesFM is related to LLMTIME as both are models used for time series forecasting and analysis\" and \"TimesFM is related to llmtime as both are models used for time-series forecasting\".\n\nGiven the consistency of the descriptions, it can be concluded that both \"LLMTIME\" and \"TIMESFM\" are models used for time series forecasting and analysis, with the primary difference being the slight variation in naming conventions (\"llmtime\" vs. \"LLMTIME\" and \"TimesFM\" vs. \"TIMESFM\").\n\nTherefore, a comprehensive summary of the data provided is:\n\n\"LLMTIME\" and \"TIMESFM\" are two models used for time series forecasting and analysis, with both entities being related to each other in their application and purpose.", "from": "LLMTIME", "source_id": "892b821ed44c13c4d09e0ceedac3051d,cc1063ba5913ea38c84ac0250c01fe84", "to": "TIMESFM", "width": 2.0}, {"description": "ARIMA is related to llmtime as both are models used for time-series forecasting", "from": "LLMTIME", "source_id": "892b821ed44c13c4d09e0ceedac3051d", "to": "ARIMA", "width": 1.0}, {"description": "llmtime is related to GFQW23 as GFQW23 is a model mentioned in the text, specifically llmtime", "from": "LLMTIME", "source_id": "2bc70082c142ee2ef5d6a941611505b7", "to": "GFQW23", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n**Summary of LLMTIME and PATCHTST**\n\nLLMTIME and PATCHTST are two entities related to time-series forecasting. Both entities are models used for this purpose, with LLMTIME being a model and PATCHTST being a type of model used for time-series forecasting. This suggests that LLMTIME is a specific model that can be used for time-series forecasting, while PATCHTST is a broader category of models that includes LLMTIME.\n\nThis relationship is further supported by the fact that both entities are mentioned together in the description list, indicating that they are closely related. The descriptions provided also suggest that both entities are used for time-series forecasting, which is a key aspect of their relationship.\n\nOverall, the summary of LLMTIME and PATCHTST can be written as follows:\n\nLLMTIME and PATCHTST are two entities related to time-series forecasting, with LLMTIME being a specific model and PATCHTST being a broader category of models that includes LLMTIME. Both entities are used for time-series forecasting, with LLMTIME being a model used for this purpose and PATCHTST being a type of model used for time-series forecasting.", "from": "LLMTIME", "source_id": "500710c8a95f1310d383fa71fd806c1c,673b0feee6eb5843954defc670e4ba29", "to": "PATCHTST", "width": 2.0}, {"description": "llmtime is related to GPT-3.5-Turbo as GPT-3.5-Turbo is a model used for time-series forecasting", "from": "LLMTIME", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "GPT-3.5-TURBO", "width": 1.0}, {"description": "Llmtime is thrown off by outliers in the context in the Air Passenger dataset", "from": "LLMTIME", "source_id": "e6ec99a117b9abd42452ff51cd6e8f0b", "to": "AIR PASSENGER DATASET", "width": 1.0}, {"description": "LLMTime is related to GPT-2 as GPT-2 is used as a backbone for LLMTime", "from": "LLMTIME", "source_id": "1f1221583d838c2407fe9864225e9eda", "to": "GPT-2", "width": 1.0}, {"description": "ForecastPFN is related to LLMTime as both are models mentioned in the text", "from": "LLMTIME", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "FORECASTPFN", "width": 1.0}, {"description": "LLMTime is related to Chronos-T5 as Chronos-T5 is a type of model that uses a transformer approach to forecasting", "from": "LLMTIME", "source_id": "116332ac4538a1430c83a34fcbec22d1", "to": "CHRONOS-T5", "width": 1.0}, {"description": "AutoARIMA is related to LLMTime as LLMTime is a type of model that uses a language model approach to forecasting", "from": "LLMTIME", "source_id": "116332ac4538a1430c83a34fcbec22d1", "to": "AUTOARIMA", "width": 1.0}, {"description": "LLMTime inference is performed on the p3dn.24xlarge AWS EC2 instance", "from": "LLMTIME", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "AWS EC2", "width": 1.0}, {"description": "Model reprogramming is related to LLMTime as LLMTime is a model that adapts LLMs for zero-shot time series forecasting", "from": "LLMTIME", "source_id": "f7266cfccedb1d9840d10afa689a05e9", "to": "MODEL REPROGRAMMING", "width": 1.0}, {"description": "Voice2Series is related to Wimmer et al. as Wimmer et al. utilize vision-language models to predict market changes", "from": "VOICE2SERIES", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "WIMMER ET AL.", "width": 1.0}, {"description": "Prompt-based time series forecasting is related to Voice2series as both are concepts mentioned in the text", "from": "VOICE2SERIES", "source_id": "98c6b5003112ab7110e45414a2fa468b", "to": "PROMPT-BASED TIME SERIES FORECASTING", "width": 1.0}, {"description": "Voice2series is related to acoustic models as acoustic models are used in Voice2series", "from": "VOICE2SERIES", "source_id": "98c6b5003112ab7110e45414a2fa468b", "to": "ACOUSTIC MODELS", "width": 1.0}, {"description": "Model reprogramming is related to Voice2Series as Voice2Series is a model that adapts an acoustic model from speech recognition for time series classification", "from": "VOICE2SERIES", "source_id": "f7266cfccedb1d9840d10afa689a05e9", "to": "MODEL REPROGRAMMING", "width": 1.0}, {"description": "Wimmer et al. are related to VLMs as VLMs are vision-language models used by Wimmer et al.", "from": "WIMMER ET AL.", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "VLMS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nPATCHTST and N-BEATS are two machine learning models that are related to each other. Both models are used for time-series forecasting and have been compared in the context of their performance. Specifically, PATCHTST and N-BEATS are models that utilize transfer learning in semi-supervised settings and between various source-target dataset pairs, respectively. This comparison highlights the similarities and differences between the two models, providing valuable insights into their strengths and weaknesses in time-series forecasting tasks.\n\nThe language used to describe these models is formal and academic, suggesting that the text is from a research paper or technical document. The presence of technical terms such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis\" further supports this conclusion. Additionally, the use of English-language abbreviations and citations of academic papers and authors indicates that the primary language of the text is English.\n\nOverall, PATCHTST and N-BEATS are two related models that have been compared and contrasted in the context of time-series forecasting, highlighting their unique strengths and weaknesses in this domain.", "from": "PATCHTST", "source_id": "39365aee753fb73130c208fdf4046bb7,af36d1634490149b96980fb1dff57cd1,ff641d7e46d16e86c55d25c86b49bd52", "to": "N-BEATS", "width": 3.0}, {"description": "Decoder-only model is related to PatchTST as PatchTST is a type of decoder-only model", "from": "PATCHTST", "source_id": "f98d2bec31738be3f7be750b5fe6180e", "to": "DECODER-ONLY MODEL", "width": 1.0}, {"description": "PatchTST is related to LLM as LLM is used to fine-tune PatchTST", "from": "PATCHTST", "source_id": "f98d2bec31738be3f7be750b5fe6180e", "to": "LLM", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nPatchTST and TimesFM are two models used for zero-shot forecasting, which are compared in the paper. Both models are related to time-series forecasting, and they are the best performing models in this case. An ablation study is conducted to compare PatchTST and TimesFM, indicating that they are being evaluated and compared in terms of their performance. The evaluation of these models suggests that TimesFM is compared to PatchTST, further emphasizing their comparison in the context of time-series forecasting.\n\nThis summary includes information from all the descriptions, resolves any potential contradictions, and provides a coherent description of the entities and their relationships. The summary is written in the third person and includes the entity names for context.", "from": "PATCHTST", "source_id": "28b097d339554431fa14e113c98ed49e,39365aee753fb73130c208fdf4046bb7,4ade7764ebe998b5f35a7fd2b58d4796,892b821ed44c13c4d09e0ceedac3051d,d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "TIMESFM", "width": 5.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nPATCHTST and INFORMER are two entities related to time-series forecasting. Specifically, INFORMER introduced transformers for long sequence forecasting through the Prob-sparse self-attention mechanism, which has since been further refined in models like PATCHTST [Nie et al., 2022]. Both PATCHTST and INFORMER are used for benchmarking various supervised long-horizon forecasting methods, indicating their relevance in the field of time-series forecasting. Furthermore, PATCHTST is a model used for time-series forecasting, which is closely related to INFORMER\u0027s capabilities. Overall, these two entities are interconnected through their applications and advancements in time-series forecasting.\n\nRelevant information from the nearby text suggests that the language of the text is English, which is consistent with the formal and academic tone of the descriptions provided. The use of technical terms, mathematical equations, and references to academic papers also supports this conclusion.", "from": "PATCHTST", "source_id": "673b0feee6eb5843954defc670e4ba29,7d5d82d600620153153772a9bc498ac0,efb7975581b22620b8277417edeee3e9", "to": "INFORMER", "width": 3.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nPATCHTST and FEDFORMER are two entities related to time-series forecasting. Both models are used for benchmarking various supervised long-horizon forecasting methods, indicating their shared purpose in evaluating the performance of different forecasting techniques. Additionally, they are both utilized for time-series forecasting, suggesting their applicability in predicting future values in a sequence of data points. \n\nThe use of these models for benchmarking purposes implies that they are being used as reference points for comparing the performance of other forecasting methods, which is a common practice in the field of time-series forecasting. This suggests that PATCHTST and FEDFORMER are being used as tools for evaluating and improving the accuracy of forecasting models.\n\nOverall, the information provided indicates that PATCHTST and FEDFORMER are two related entities that are used for time-series forecasting and benchmarking supervised long-horizon forecasting methods.", "from": "PATCHTST", "source_id": "500710c8a95f1310d383fa71fd806c1c,efb7975581b22620b8277417edeee3e9", "to": "FEDFORMER", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe PatchTST and PatchTST(ZS) models are two entities that are compared and contrasted in the evaluation. PatchTST(ZS) is a pre-trained variant of the PatchTST model, specifically designed for zero-shot forecasting. This suggests that PatchTST(ZS) has been modified or enhanced to improve its performance in forecasting tasks, particularly those that require no prior training data.\n\nIn comparison, PatchTST is likely the original or base model, which may not have the same level of performance in zero-shot forecasting tasks. The evaluation of PatchTST and PatchTST(ZS) is likely aimed at assessing the effectiveness of the modifications made to PatchTST(ZS) and its suitability for zero-shot forecasting applications.\n\nOverall, the PatchTST and PatchTST(ZS) models are related entities that are being compared and evaluated in the context of time series forecasting, with a focus on zero-shot forecasting capabilities.", "from": "PATCHTST", "source_id": "28b097d339554431fa14e113c98ed49e,39365aee753fb73130c208fdf4046bb7,4ade7764ebe998b5f35a7fd2b58d4796", "to": "PATCHTST(ZS)", "width": 3.0}, {"description": "The transformer stack is a component of the PatchTST model", "from": "PATCHTST", "source_id": "28b097d339554431fa14e113c98ed49e", "to": "TRANSFORMER STACK", "width": 1.0}, {"description": "Input patch length is a parameter used in the PatchTST model", "from": "PATCHTST", "source_id": "28b097d339554431fa14e113c98ed49e", "to": "INPUT PATCH LENGTH", "width": 1.0}, {"description": "Stride is a parameter used in the PatchTST model", "from": "PATCHTST", "source_id": "28b097d339554431fa14e113c98ed49e", "to": "STRIDE", "width": 1.0}, {"description": "PatchTST is compared to ARIMA, which is a model used for time-series forecasting", "from": "PATCHTST", "source_id": "39365aee753fb73130c208fdf4046bb7", "to": "ARIMA", "width": 1.0}, {"description": "MAE is related to PatchTST as PatchTST performs well for time-series forecasting tasks", "from": "PATCHTST", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "MAE", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nPATCHTST and PATCH EMBEDDINGS are two entities related to time series analysis tasks. PATCHTST utilizes PATCH EMBEDDINGS, which are similar to those used in GPT4TS, for its time series analysis capabilities. This suggests that PATCH EMBEDDINGS play a crucial role in PATCHTST\u0027s functionality, enabling it to perform tasks such as time series analysis and forecasting.\n\nThe use of PATCH EMBEDDINGS in PATCHTST is likely related to its ability to analyze and process time series data, which is a key aspect of time series analysis. The fact that PATCH EMBEDDINGS are similar to those used in GPT4TS implies that they may be a key component in enabling PATCHTST to perform tasks such as long-term series forecasting and frequency analysis.\n\nOverall, the relationship between PATCHTST and PATCH EMBEDDINGS is one of utilization, with PATCHTST relying on PATCH EMBEDDINGS to perform its time series analysis tasks.", "from": "PATCHTST", "source_id": "1f1221583d838c2407fe9864225e9eda,bc54a718d1886698232d578fd88c3ac7", "to": "PATCH EMBEDDINGS", "width": 2.0}, {"description": "WaveNet is related to PatchTST as both are models mentioned in the text", "from": "PATCHTST", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "WAVENET", "width": 1.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nDeepAR and PatchTST are two related deep learning models used for time series forecasting. They are both methods for probabilistic forecasting and are task-specific models that perform better than Chronos models on some datasets. The two models are compared in terms of performance, suggesting that they are being evaluated and compared in a research or technical context. The use of their abbreviations in a table and their mention in the text further supports their relationship as models.\n\nThe language used to describe these models is formal and academic, suggesting that the text is from a research paper or a technical document. The presence of mathematical equations and formulas, as well as English-language abbreviations and citations, further supports this conclusion.\n\nOverall, DeepAR and PatchTST are two related deep learning models used for time series forecasting, with a focus on probabilistic forecasting and task-specific performance. They are being compared and evaluated in a research or technical context, and their relationship is supported by their use in a table and their mention in the text.", "from": "PATCHTST", "source_id": "1dc665214307b1656d1a0094a1918ece,2565ae205d4c98342168bb67a4f7a309,2f3b2f69f8eb4f958c3ca793cae36581,376897a501ac50834f626fcc5fb13ece,56de1d5a5b467101344afa4248d829dc,af36d1634490149b96980fb1dff57cd1,f0c52387a7b3a5c3850fe6991f0a7c83", "to": "DEEPAR", "width": 7.0}, {"description": "Based on the provided information, the primary entities are \"PATCHTST\" and \"TFT\". The descriptions provided offer insights into the relationships between these entities.\n\nAfter analyzing the descriptions, it is clear that PATCHTST and TFT are related in several ways. They are both deep learning models used for forecasting, as indicated by the descriptions. Specifically, PATCHTST and TFT are compared in terms of performance, suggesting that they are being evaluated against each other. Additionally, both PATCHTST and TFT are methods for probabilistic forecasting, which further solidifies their connection.\n\nThe use of their abbreviations in the table and their mention in the text as models also supports the relationship between PATCHTST and TFT. Therefore, it can be concluded that PATCHTST and TFT are two related deep learning models used for probabilistic forecasting, with PATCHTST being compared to TFT in terms of performance.\n\nHere is a comprehensive summary of the data:\n\nPATCHTST and TFT are two related deep learning models used for probabilistic forecasting. They are compared in terms of performance, with PATCHTST being evaluated against TFT. Both models are methods for probabilistic forecasting, and their abbreviations are used in the table, indicating their connection as models. They are also mentioned in the text as models, further solidifying their relationship.", "from": "PATCHTST", "source_id": "1dc665214307b1656d1a0094a1918ece,2565ae205d4c98342168bb67a4f7a309,2f3b2f69f8eb4f958c3ca793cae36581,af36d1634490149b96980fb1dff57cd1,f0c52387a7b3a5c3850fe6991f0a7c83", "to": "TFT", "width": 5.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n**Summary of PATCHTST and DLINER**\n\nPATCHTST and DLINER are two models related to time series forecasting. Both models are mentioned in the text as being used for forecasting purposes. PATCHTST is a model that utilizes a patch-based approach to forecasting, while DLINER is related to PATCHTST as they are both models used for time series forecasting. The two models are compared with each other, suggesting that they are being evaluated and compared in terms of their performance and effectiveness in forecasting time series data.\n\n**Key Points:**\n\n* PATCHTST and DLINER are both models used for time series forecasting.\n* PATCHTST uses a patch-based approach to forecasting.\n* The two models are compared with each other.\n* Both models are mentioned in the text as being used for forecasting purposes.\n\n**Entity Information:**\n\n* PATCHTST: A model for time series forecasting that uses a patch-based approach.\n* DLINER: A model related to PATCHTST, also used for time series forecasting.\n\n**Context:**\n\nThe summary is based on the provided descriptions, which suggest that PATCHTST and DLINER are two models related to time series forecasting. The text does not provide further information about the specific characteristics or features of the models, but it does indicate that they are being compared and evaluated in terms of their performance and effectiveness in forecasting time series data.", "from": "PATCHTST", "source_id": "116332ac4538a1430c83a34fcbec22d1,5e4d9ca02ee6a285d5223c820743eb12,91e161ba596a0cbbcae541ddb2106310,af36d1634490149b96980fb1dff57cd1,f6fac8e5c6fd12724fe3aa84a2e1cfa6", "to": "DLINER", "width": 5.0}, {"description": "Based on the provided information, the primary entities are \"PATCHTST\" and \"N-HITS.\" These two entities are compared on long-term forecasting tasks, as shown in Table 20. They are also related to each other as both are models mentioned in the text and are used for time series forecasting.\n\nGiven the descriptions provided, it can be concluded that PATCHTST and N-HITS are two models that are used for time series forecasting. They are compared on long-term forecasting tasks, indicating that they are being evaluated for their performance in predicting future values in a time series.\n\nThe fact that they are both used for time series forecasting suggests that they share similar goals and applications, but the comparison in Table 20 implies that there may be some differences in their performance or approaches.\n\nOverall, the relationship between PATCHTST and N-HITS can be summarized as follows:\n\nPATCHTST and N-HITS are two models that are used for time series forecasting. They are compared on long-term forecasting tasks, as shown in Table 20, and are related to each other as both are models mentioned in the text.\n\nThis summary is written in third person and includes the entity names for context. It also resolves any potential contradictions in the descriptions provided, resulting in a coherent and concise summary.", "from": "PATCHTST", "source_id": "41fe893a178ebc8790ef4da83da5ab6e,ab91381dd032db318e5aec1ad5b914a6,af36d1634490149b96980fb1dff57cd1,f49330b6fd81d86d14e7a9d4b8e45576", "to": "N-HITS", "width": 4.0}, {"description": "Based on the provided information, the following comprehensive summary can be generated:\n\n**Summary of PATCHTST and GPT4TS**\n\nPATCHTST and GPT4TS are two models that are related to each other in the context of time series forecasting. Both models are used for long-term forecasting tasks, as demonstrated in Figures 7 and 8. They are compared in these figures, suggesting that they are being evaluated against each other in terms of their performance on forecasting tasks. The text also mentions that both models are used for time series forecasting, indicating that they share a common application domain.\n\nThe language used to describe these models is formal and academic, suggesting that the text is from a research paper or technical document. The presence of mathematical equations and formulas, as well as English-language abbreviations and citations, further supports this conclusion.\n\nOverall, PATCHTST and GPT4TS are two models that are being compared and evaluated in the context of time series forecasting, with a focus on their performance on long-term forecasting tasks.\n\n**Additional Context**\n\nThe text is written in English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers. The language is formal and academic, suggesting that the text is from a research paper or technical document. The presence of figures (Fig. 7 and Fig. 8) suggests that the text is accompanied by visual aids, such as graphs or charts, that are used to illustrate the performance of the models on forecasting tasks.\n\n**Entity Information**\n\n* PATCHTST: A model used for time series forecasting, compared to GPT4TS in Figures 7 and 8.\n* GPT4TS: A model used for time series forecasting, related to PATCHTST and compared to it in Figures 7 and 8.", "from": "PATCHTST", "source_id": "91e161ba596a0cbbcae541ddb2106310,ab91381dd032db318e5aec1ad5b914a6,af36d1634490149b96980fb1dff57cd1,d4e3d8b5bf043b78bb9f1551080cab91", "to": "GPT4TS", "width": 4.0}, {"description": "PatchTST is related to local statistical models as PatchTST performs better than local statistical models", "from": "PATCHTST", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"PATCHTST\" and \"LAG-LLAMA\" are two models that are related to each other. As indicated by the use of their abbreviations in the table, both models are being compared and analyzed in the context of their performance. This suggests that the primary focus of the comparison is to evaluate and contrast the effectiveness of these two models.\n\nGiven the context of the comparison, it is likely that the analysis involves evaluating the performance metrics of both models, such as accuracy, precision, recall, and F1-score, among others. The comparison may also involve a detailed examination of the strengths and weaknesses of each model, including their ability to handle specific tasks or datasets.\n\nThe fact that both models are being compared in terms of performance suggests that they are being evaluated for their ability to perform a specific task or set of tasks. This could be related to natural language processing, time series forecasting, or another area of machine learning.\n\nOverall, the summary of the data suggests that the comparison between \"PATCHTST\" and \"LAG-LLAMA\" is focused on evaluating their performance and effectiveness as models, with the goal of identifying the strengths and weaknesses of each and determining which one is better suited for a particular task or application.\n\nRelevant information from the nearby text that enriches this summary includes the mention of technical terms such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention.\" These terms suggest that the comparison between the two models may be related to time series forecasting or another area of machine learning that involves analyzing and predicting temporal data.\n\nAdditionally, the mention of academic conferences and journals such as \"ICLR,\" \"AAAI,\" and \"PMLR\" suggests that the comparison between the two models may be part of a research paper or technical document that is being presented at one of these conferences or published in one of these journals.", "from": "PATCHTST", "source_id": "1dc665214307b1656d1a0094a1918ece,2f3b2f69f8eb4f958c3ca793cae36581", "to": "LAG-LLAMA", "width": 2.0}, {"description": "Datasets are used to train and test PATCHTST", "from": "PATCHTST", "source_id": "2f3b2f69f8eb4f958c3ca793cae36581", "to": "DATASETS", "width": 1.0}, {"description": "NPts and PatchTST are both models used for time series forecasting", "from": "PATCHTST", "source_id": "376897a501ac50834f626fcc5fb13ece,6fa5f635ad5f7e6b67d5e467f130345c", "to": "NPTS", "width": 2.0}, {"description": "PatchTST and Temporal Fusion Transformer are both models used for time series forecasting", "from": "PATCHTST", "source_id": "376897a501ac50834f626fcc5fb13ece,6fa5f635ad5f7e6b67d5e467f130345c", "to": "TEMPORALFUSIONT", "width": 2.0}, {"description": "AutoETS and PatchTST are both models used for time series forecasting", "from": "PATCHTST", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "AUTOETS", "width": 1.0}, {"description": "Croston\u0027s seasonal batch arrival model and PatchTST are both models used for time series forecasting", "from": "PATCHTST", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "CROSTONSBA", "width": 1.0}, {"description": "Dynamic optimization and PatchTST are both models used for time series forecasting", "from": "PATCHTST", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "DYNAMICOPTIMIZE", "width": 1.0}, {"description": "AutoARIMA and PatchTST are both models used for time series forecasting", "from": "PATCHTST", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "AUTOARIMA", "width": 1.0}, {"description": "PatchTST and N-BEATS are both models used for time series forecasting", "from": "PATCHTST", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "NBEATS", "width": 1.0}, {"description": "Based on the provided descriptions, it can be concluded that the entities \"PATCHTST\" and \"TIMESNET\" are related to time series forecasting models. \n\nThe descriptions collectively suggest that both PATCHTST and TIMESNET are models used for time series forecasting, with TIMESNET being specifically designed for this purpose. They are also compared in a table, indicating that they are being evaluated and contrasted in terms of their performance or characteristics.\n\nHere is a comprehensive summary of the data:\n\nPATCHTST and TIMESNET are two time series forecasting models mentioned in the text. They are compared in a table, suggesting that they are being evaluated and contrasted in terms of their performance or characteristics. TIMESNET is specifically designed for time series forecasting, and both models are used for forecasting purposes. \n\nThis summary is written in third person and includes the entity names for context. It also resolves any potential contradictions by focusing on the common theme of time series forecasting and the comparison of the two models.", "from": "PATCHTST", "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,5e4d9ca02ee6a285d5223c820743eb12,91e161ba596a0cbbcae541ddb2106310,9ba0189af2ef0720a721c16eef0f0788,f49330b6fd81d86d14e7a9d4b8e45576,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "to": "TIMESNET", "width": 7.0}, {"description": "M4 is related to PatchTST as PatchTST is used for short-term time series forecasting on M4", "from": "PATCHTST", "source_id": "d4551c2839eaa68a7cb7324089956581", "to": "M4", "width": 1.0}, {"description": "PatchTST and DLinear are models mentioned in the text, as indicated by the use of their names", "from": "PATCHTST", "source_id": "1d6fb60c5060c25ae4791b03a0513a7f", "to": "DLINERAR", "width": 1.0}, {"description": "PatchTST is related to dataset as it is used to evaluate the performance on the dataset", "from": "PATCHTST", "source_id": "74527a4337ed6919731be520311ae774", "to": "DATASET", "width": 1.0}, {"description": "ILI is related to PatchTST as it is a forecasting horizon used in the model", "from": "PATCHTST", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "ILI", "width": 9.0}, {"description": "DLinear is related to PatchTST as both are models mentioned in the text", "from": "PATCHTST", "source_id": "fd1092903d83bf6e90a6caa371d7c514", "to": "DLINEAR", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nPATCHTST and AUTOFORMER are two models used for time series forecasting. They are compared on long-term forecasting tasks, as evident from the figures mentioned (Fig. 7 and Fig. 8). The comparison suggests that both models are utilized for predicting future values in a time series, indicating their relevance in the field of long-term series forecasting.\n\nThe relationship between PATCHTST and AUTOFORMER is established as both models are employed for time series forecasting, highlighting their shared application in this domain. This connection is further reinforced by the fact that they are compared in the context of long-term forecasting tasks, implying that they are both used to predict future values in a time series over an extended period.\n\nThe use of these models in time series forecasting is likely facilitated by their ability to handle complex patterns and trends in data, which is a key aspect of long-term series forecasting. The comparison of PATCHTST and AUTOFORMER on this task suggests that they are both capable of handling such complexities, making them suitable for applications in this domain.\n\nOverall, the summary highlights the shared application of PATCHTST and AUTOFORMER in time series forecasting, particularly in the context of long-term forecasting tasks.", "from": "PATCHTST", "source_id": "ab91381dd032db318e5aec1ad5b914a6,d4e3d8b5bf043b78bb9f1551080cab91", "to": "AUTOFORMER", "width": 2.0}, {"description": "PatchTST is visualized in various scenarios, as shown in Fig. 7, Fig. 8, Fig. 9, and Fig. 10", "from": "PATCHTST", "source_id": "ab91381dd032db318e5aec1ad5b914a6", "to": "VISUALIZATION", "width": 1.0}, {"description": "PATCHTST is used for comparison with TIME-LLM on the ETTh1 dataset", "from": "PATCHTST", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "ETTH1", "width": 1.0}, {"description": "PATCHTST is used for comparison with TIME-LLM on the ETTh2 dataset", "from": "PATCHTST", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "ETTH2", "width": 1.0}, {"description": "PATCHTST is used for comparison with TIME-LLM on the ETTm1 dataset", "from": "PATCHTST", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "ETTM1", "width": 1.0}, {"description": "PATCHTST is used for comparison with TIME-LLM on the ETTm2 dataset", "from": "PATCHTST", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "ETTM2", "width": 1.0}, {"description": "Lag-Llama is related to Moirai as both are models for time series forecasting", "from": "MOIRAI", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "LAG-LLAMA", "width": 1.0}, {"description": "Moirai is related to LOTSA as LOTSA is a pre-training dataset for time series forecasting", "from": "MOIRAI", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "LOTSA", "width": 1.0}, {"description": "Multi-resolution analysis is related to Moirai as Moirai is a model that employs multi-resolution analysis", "from": "MOIRAI", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "MULTI-RESOLUTION ANALYSIS", "width": 1.0}, {"description": "Moirai is related to decomposition strategies as decomposition strategies are specialized approaches used in time series forecasting models", "from": "MOIRAI", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "DECOMPOSITION STRATEGIES", "width": 1.0}, {"description": "Based on the provided information, the entity \"CHRONOS\" is related to \"TOKENIZATION\". \n\nThe relationship between CHRONOS and TOKENIZATION is that tokenization is a key component of the CHRONOS model, and it is used to break down text into individual tokens. This process requires minimal modifications to existing language model architectures. \n\nIn other words, CHRONOS utilizes tokenization as a fundamental aspect of its architecture, allowing it to process and analyze text data effectively. The use of tokenization in CHRONOS enables the model to handle text inputs by breaking them down into smaller, manageable units, which can then be processed and utilized for various tasks.\n\nOverall, the relationship between CHRONOS and TOKENIZATION is one of integration, where tokenization serves as a crucial component of the CHRONOS model, facilitating its ability to process and analyze text data.", "from": "CHRONOS", "source_id": "0bef137d159ccc86cdee0a8be788bd26,6c273e9f841addeed2a33240ddaa3d96,bca10b04933dc3a7f98a3f1b610e419b,f622f27b5c3f52c6b04ada48bd63b03d", "to": "TOKENIZATION", "width": 4.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Summary of CHRONOS and LANGUAGE MODEL**\n\nCHRONOS is a language modeling framework that has been minimally adapted for time series forecasting. It operates over a fixed vocabulary and is related to LANGUAGE MODEL, as it adapts existing language model architectures and training procedures. This suggests that CHRONOS leverages the strengths of language models to tackle the challenges of time series forecasting, potentially utilizing techniques such as multi-head cross-attention to analyze and predict temporal patterns.\n\nThe primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. The language is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nOverall, CHRONOS appears to be a framework that combines the capabilities of language models with the requirements of time series forecasting, offering a novel approach to this challenging problem.", "from": "CHRONOS", "source_id": "42c90ca40b234c098c55632ec70038a1,6eb4c16edf2eedfd03721efb199478d8,f7e3207706b170296cb036538a709689", "to": "LANGUAGE MODEL", "width": 3.0}, {"description": "Chronos integrates data augmentation strategies to enhance model robustness and generalization", "from": "CHRONOS", "source_id": "42c90ca40b234c098c55632ec70038a1", "to": "DATA AUGMENTATION", "width": 1.0}, {"description": "Chronos uses TSMixup as a data augmentation technique to generate new time series", "from": "CHRONOS", "source_id": "42c90ca40b234c098c55632ec70038a1", "to": "TSMIXUP", "width": 1.0}, {"description": "Chronos uses KernelSynth as a data augmentation technique to generate synthetic time series", "from": "CHRONOS", "source_id": "42c90ca40b234c098c55632ec70038a1", "to": "KERNELSYNTH", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Summary of CHRONOS and LLMs**\n\nCHRONOS and Large Language Models (LLMs) are interconnected entities that share a symbiotic relationship. CHRONOS, a system or platform, can seamlessly integrate with future advancements in LLMs, suggesting a high degree of compatibility and adaptability. In fact, LLMs are used in CHRONOS, leveraging developments in the area of LLMs and through better data strategies. This integration enables CHRONOS to tap into the capabilities of LLMs, potentially enhancing its performance and effectiveness.\n\nThe relationship between CHRONOS and LLMs is one of mutual benefit, with CHRONOS benefiting from the advancements in LLMs and LLMs being utilized to improve the performance of CHRONOS. This partnership has the potential to drive innovation and improvement in both areas, leading to better outcomes and more effective solutions.\n\nOverall, the summary highlights the close relationship between CHRONOS and LLMs, with CHRONOS leveraging the capabilities of LLMs to enhance its performance and effectiveness.", "from": "CHRONOS", "source_id": "42c90ca40b234c098c55632ec70038a1,6c273e9f841addeed2a33240ddaa3d96", "to": "LLMS", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Summary of CHRONOS and LANGUAGE MODELS**\n\nCHRONOS is a language model specifically designed for time series analysis and forecasting. It trains language models from scratch on a large collection of time series data. This suggests that CHRONOS is a type of language model that is tailored to handle the unique characteristics of time series data, such as temporal dependencies and seasonality.\n\nThe relationship between CHRONOS and LANGUAGE MODELS is that CHRONOS is a language model that is designed for time series analysis and forecasting, implying that it is a specialized variant of language models. This indicates that language models, in general, can be adapted and fine-tuned for specific tasks, such as time series analysis, to improve their performance and accuracy.\n\nOverall, CHRONOS is a language model that leverages the strengths of language models to tackle the challenges of time series analysis and forecasting, making it a valuable tool for researchers and practitioners working in this domain.\n\n**Relevant Information**\n\n* CHRONOS trains language models from scratch on a large collection of time series data, indicating its ability to learn from large datasets and adapt to complex temporal patterns.\n* Language models are related to CHRONOS as CHRONOS is a language model that is specifically designed for time series analysis and forecasting, highlighting the versatility of language models in handling different types of data and tasks.\n* The fact that CHRONOS is a language model designed for time series analysis and forecasting suggests that it may employ techniques such as multi-head cross-attention, frequency analysis, and long-term series forecasting, which are commonly used in time series analysis and forecasting.", "from": "CHRONOS", "source_id": "bc54a718d1886698232d578fd88c3ac7,c5c841baa0bc205103ac433d446da3b6", "to": "LANGUAGE MODELS", "width": 2.0}, {"description": "Chronos uses real data in combination with synthetic data to train models", "from": "CHRONOS", "source_id": "bc54a718d1886698232d578fd88c3ac7", "to": "REAL DATA", "width": 1.0}, {"description": "Chronos tokenizes time series values into a fixed vocabulary", "from": "CHRONOS", "source_id": "bc54a718d1886698232d578fd88c3ac7", "to": "TOKENIZED INPUT", "width": 1.0}, {"description": "Chronos uses a categorical distribution to model the observations", "from": "CHRONOS", "source_id": "bc54a718d1886698232d578fd88c3ac7", "to": "VOCABULARY", "width": 1.0}, {"description": "Chronos performs regression via classification", "from": "CHRONOS", "source_id": "bc54a718d1886698232d578fd88c3ac7", "to": "TOKENIZED VALUES", "width": 1.0}, {"description": "Chronos is related to the objective function as the objective function is the function that is being optimized during training, in this case the cross entropy between the distribution of the quantized ground truth label and the predicted distribution", "from": "CHRONOS", "source_id": "c5c841baa0bc205103ac433d446da3b6", "to": "OBJECTIVE FUNCTION", "width": 1.0}, {"description": "Chronos is related to probabilistic time series forecasting as Chronos is a model for forecasting", "from": "CHRONOS", "source_id": "6212b2146d9ad27124114c334a946854", "to": "PROBABILISTIC TIME SERIES FORECASTING", "width": 1.0}, {"description": "Quantile regression is related to Chronos as Chronos uses quantile regression", "from": "CHRONOS", "source_id": "6212b2146d9ad27124114c334a946854", "to": "QUANTILE REGRESSION", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Summary of Chronos and Language Model Architecture**\n\nChronos is a language modeling framework specifically designed for time series data. It is an agnostic framework, meaning it can be used with any language model architecture, and requires minimal modifications to existing models. Chronos utilizes a language model architecture, which is a crucial component in its design. The framework\u0027s compatibility with various language model architectures allows it to be adaptable and flexible in its application. Overall, Chronos is a powerful tool for time series analysis, leveraging the strengths of language model architectures to provide accurate and efficient results.\n\n**Key Points:**\n\n* Chronos is a language modeling framework for time series data.\n* It is agnostic to time and compatible with any language model architecture.\n* Chronos requires minimal modifications to existing models.\n* The framework utilizes a language model architecture.\n* Chronos is designed for time series analysis and leverages the strengths of language model architectures.\n\n**Entity Information:**\n\n* **CHRONOS**: A language modeling framework for time series data.\n* **LANGUAGE MODEL ARCHITECTURE**: A crucial component of Chronos, used for time series analysis.\n\nThis summary provides a clear and concise overview of Chronos and its relationship with language model architecture, highlighting its key features and capabilities.", "from": "CHRONOS", "source_id": "6212b2146d9ad27124114c334a946854,6c273e9f841addeed2a33240ddaa3d96", "to": "LANGUAGE MODEL ARCHITECTURE", "width": 2.0}, {"description": "Chronos is related to T5 as T5 is the main architecture used in the Chronos framework", "from": "CHRONOS", "source_id": "f72ba51a17dd00cff43bcdda22c273e8", "to": "T5", "width": 1.0}, {"description": "Chronos is related to GPT-2 as GPT-2 is used as a decoder-only model in the Chronos framework", "from": "CHRONOS", "source_id": "f72ba51a17dd00cff43bcdda22c273e8", "to": "GPT-2", "width": 1.0}, {"description": "Chronos is a model for time series forecasting, as indicated by the text", "from": "CHRONOS", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "TIME SERIES FORECASTING", "width": 1.0}, {"description": "Weighted quantile loss is related to Chronos as Chronos uses weighted quantile loss as a metric", "from": "CHRONOS", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "WEIGHTED QUANTILE LOSS", "width": 1.0}, {"description": "Benchmark II is a challenging benchmark for Chronos, consisting of diverse datasets and frequencies", "from": "CHRONOS", "source_id": "532a6d434dab611a80aef1e94dd2fb45", "to": "BENCHMARK II", "width": 1.0}, {"description": "Chronos models significantly out-perform local statistical models on probabilistic forecasting", "from": "CHRONOS", "source_id": "532a6d434dab611a80aef1e94dd2fb45", "to": "LOCAL STATISTICAL MODELS", "width": 1.0}, {"description": "Chronos models out-perform task-specific models on probabilistic forecasting", "from": "CHRONOS", "source_id": "532a6d434dab611a80aef1e94dd2fb45", "to": "TASK-SPECIFIC MODELS", "width": 1.0}, {"description": "Pretraining dataset is used to train Chronos models", "from": "CHRONOS", "source_id": "532a6d434dab611a80aef1e94dd2fb45", "to": "PRETRAINING DATASET", "width": 1.0}, {"description": "Chronos is a type of time series forecaster that performs significantly better than local models", "from": "CHRONOS", "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "to": "TIME SERIES FORECASTER", "width": 1.0}, {"description": "Chronos outperforms local models in a zero-shot setting and on Benchmark II", "from": "CHRONOS", "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "to": "LOCAL MODELS", "width": 1.0}, {"description": "Chronos performs on par with the best task-specific deep learning models", "from": "CHRONOS", "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "to": "TASK-SPECIFIC DEEP LEARNING MODELS", "width": 1.0}, {"description": "Figure 5 shows the results on Benchmark II, highlighting the promise of Chronos as a generalist time series forecaster", "from": "CHRONOS", "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "to": "FIGURE 5", "width": 1.0}, {"description": "Chronos accurately models trends in time series data", "from": "CHRONOS", "source_id": "bca10b04933dc3a7f98a3f1b610e419b", "to": "TREND", "width": 1.0}, {"description": "Chronos accurately models seasonal patterns in time series data", "from": "CHRONOS", "source_id": "bca10b04933dc3a7f98a3f1b610e419b", "to": "SEASONALITY", "width": 1.0}, {"description": "Chronos is related to predictive distributions as it can output them", "from": "CHRONOS", "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "to": "PREDICTIVE DISTRIBUTIONS", "width": 1.0}, {"description": "Chronos is related to zero-shot performance as the Chronos model performs well in a zero-shot manner", "from": "CHRONOS", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "ZERO-SHOT PERFORMANCE", "width": 1.0}, {"description": "Task-specific adaptors can be used to inject covariates into the Chronos model", "from": "CHRONOS", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "TASK-SPECIFIC ADAPTORS", "width": 1.0}, {"description": "Stacking ensembles can be used to combine the predictions of multiple models, including Chronos", "from": "CHRONOS", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "STACKING ENSEMBLES", "width": 1.0}, {"description": "Chronos and LightGBM are both models that can be used for time series forecasting", "from": "CHRONOS", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "LIGHTGBM", "width": 1.0}, {"description": "Chronos models can be slower than task-specific models", "from": "CHRONOS", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "INFERENCE SPEED", "width": 1.0}, {"description": "Pretrained models like Chronos can be deployed for datasets with diverse history lengths, frequencies, prediction horizons, and context lengths", "from": "CHRONOS", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "PRETRAINED MODELS", "width": 1.0}, {"description": "Chronos is a minimalist approach to developing generalist pretrained forecasting models, adapting existing language model architectures and training procedures for time series forecasting", "from": "CHRONOS", "source_id": "6c273e9f841addeed2a33240ddaa3d96", "to": "MINIMALIST", "width": 1.0}, {"description": "Chronos uses scaling as a process that requires minimal modifications to existing language model architectures", "from": "CHRONOS", "source_id": "6c273e9f841addeed2a33240ddaa3d96", "to": "SCALING", "width": 1.0}, {"description": "Chronos uses quantization as a process that requires minimal modifications to existing language model architectures", "from": "CHRONOS", "source_id": "6c273e9f841addeed2a33240ddaa3d96", "to": "QUANTIZATION", "width": 1.0}, {"description": "Stefano Soatto challenged the authors to think about the fundamental question regarding language models and time series modeling, ultimately leading to the creation of Chronos", "from": "CHRONOS", "source_id": "6c273e9f841addeed2a33240ddaa3d96", "to": "STEFANO SOATTO", "width": 1.0}, {"description": "Devamanyu Hazarika contributed to this work with insightful discussions and valuable feedback, including the development of Chronos", "from": "CHRONOS", "source_id": "6c273e9f841addeed2a33240ddaa3d96", "to": "DEVA MANYU HAZARICA", "width": 1.0}, {"description": "TEST is a dataset used in TimeCLR model", "from": "TEST", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "TIMECLR", "width": 1.0}, {"description": "METS employs a trainable ECG encoder to process paired ECG and clinical reports", "from": "METS", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "ECG", "width": 1.0}, {"description": "METS processes paired ECG and clinical reports", "from": "METS", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "CLINICAL REPORTS", "width": 1.0}, {"description": "Hybrid is a methodology used in SimMTM model", "from": "SIMMTM", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "HYBRID", "width": 1.0}, {"description": "General is a domain where TimeXer model is used", "from": "TIMEXER", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "GENERAL", "width": 1.0}, {"description": "TSMAE is a model that uses self-supervised methodology", "from": "SELF-SUPERVISED", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "TSMAE", "width": 1.0}, {"description": "Self-supervised is related to contrastive as both are methodologies used in various models", "from": "SELF-SUPERVISED", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "CONTRASTIVE", "width": 1.0}, {"description": "Hybrid is related to mobility as hybrid approaches often combine different methods for mobility forecasting", "from": "HYBRID", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "MOBILITY", "width": 1.0}, {"description": "Non-Transformer-based is a methodology used in MLP RNN CNN model", "from": "NON-TRANSFORMER-BASED", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "MLP RNN CNN", "width": 1.0}, {"description": "TF-C is a model that uses TS2Vec methodology", "from": "TF-C", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "TS2VEC", "width": 1.0}, {"description": "TS2Vec introduces a universal framework for representing time series via contrastive learning", "from": "TS2VEC", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "TSFMS", "width": 1.0}, {"description": "TimesNet is a model that uses RWKV-TS methodology", "from": "TIMESNET", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "RWKV-TS", "width": 1.0}, {"description": "TimesNet and parameter-efficient inception block are both used for time series data tasks, but parameter-efficient inception block is more effective", "from": "TIMESNET", "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "to": "PARAMETER-EFFICIENT INCCEPTION BLOCK", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTIMESNET and DPMN are two models that utilize Convolutional Neural Networks (CNNs) as a fundamental component. DPMN is closely related to TIMESNET, as TIMESNET is a type of neural network specifically designed for time series forecasting. This suggests that both models share a common architectural foundation, with CNNs serving as a key building block for their respective time series forecasting capabilities.\n\nThis summary is derived from the provided descriptions, which collectively convey the relationship between TIMESNET and DPMN, as well as their shared reliance on CNNs. The information is presented in a coherent and concise manner, highlighting the key similarities and connections between the two models.", "from": "TIMESNET", "source_id": "4de223d7df157faf857c3e17d9e6e5b6,7d5d82d600620153153772a9bc498ac0", "to": "DPMN", "width": 2.0}, {"description": "CNNs are used as a building block in models like TimesNet [Wu et al., 2022]", "from": "TIMESNET", "source_id": "7d5d82d600620153153772a9bc498ac0", "to": "CNN", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nGPT4TS and TIMESNET are two entities that are related to each other in the context of time series forecasting. Both models are used for this purpose, and they are compared with each other in the text. This comparison suggests that both models are used for long-term series forecasting, frequency analysis, and other related tasks. The use of multi-head cross-attention in these models is also mentioned, indicating their advanced capabilities in handling complex time series data.\n\nThe entities are mentioned in the context of academic research, with references to conferences and journals such as ICLR, AAAI, and PMLR. This suggests that the text is from a research paper or technical document, and the language used is formal and academic.\n\nOverall, GPT4TS and TIMESNET are two time series forecasting models that are compared and related to each other in the text, with a focus on their capabilities and applications in the field of time series analysis.\n\nRelevant information from the nearby text that was used to enrich the summary includes:\n\n* The use of technical terms and mathematical equations in the text, which suggests a formal and academic tone.\n* The presence of English-language abbreviations and citations, which confirms that the text is written in English.\n* The mention of time series forecasting, frequency analysis, and multi-head cross-attention, which provides context for the comparison between GPT4TS and TIMESNET.", "from": "TIMESNET", "source_id": "41fe893a178ebc8790ef4da83da5ab6e,5e4d9ca02ee6a285d5223c820743eb12,5fa25d3abec59ccd2718e00c0e0eb440,f49330b6fd81d86d14e7a9d4b8e45576", "to": "GPT4TS", "width": 4.0}, {"description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nTIMESNET and FEDFORMER are two entities that are related to each other in the context of time series forecasting. FEDFORMER is a model specifically designed for time series forecasting, and it is mentioned alongside TIMESNET in the text. Both models are used for forecasting purposes, and FEDFORMER is compared to TIMESNET, indicating that they are likely competing or complementary models in the field of time series forecasting.\n\nThe relationship between TIMESNET and FEDFORMER is further established by the fact that they are both models mentioned in the text, suggesting that they are relevant and notable in the context of time series forecasting. The use of FEDFORMER for comparison with TIMESNET implies that FEDFORMER is a model that is being evaluated or benchmarked against TIMESNET, which is likely a state-of-the-art or prominent model in the field.\n\nOverall, the summary suggests that TIMESNET and FEDFORMER are two related entities that are both involved in time series forecasting, with FEDFORMER being a model that is specifically designed for this purpose and is being compared to TIMESNET.", "from": "TIMESNET", "source_id": "7e97089185883c456c798ddc5ec86373,91e161ba596a0cbbcae541ddb2106310,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "to": "FEDFORMER", "width": 4.0}, {"description": "M4 is related to TimesNet as TimesNet is used for short-term time series forecasting on M4", "from": "TIMESNET", "source_id": "d4551c2839eaa68a7cb7324089956581", "to": "M4", "width": 1.0}, {"description": "DLinear and TimesNet are models mentioned in the text, as indicated by the use of their names", "from": "TIMESNET", "source_id": "1d6fb60c5060c25ae4791b03a0513a7f", "to": "DLINERAR", "width": 1.0}, {"description": "ILI is related to TimesNet as it is a forecasting horizon used in the model", "from": "TIMESNET", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "ILI", "width": 9.0}, {"description": "AutoETS and TimesNet are models used for comparison in the text", "from": "TIMESNET", "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "to": "AUTOETS", "width": 1.0}, {"description": "Parameter-efficient fine-tuning is related to TimesNet as TimesNet is a model that uses parameter-efficient fine-tuning", "from": "TIMESNET", "source_id": "7e97089185883c456c798ddc5ec86373", "to": "PARAMETER-EFFICIENT FINE-TUNING", "width": 1.0}, {"description": "Autoformer is related to TimesNet as both are models used for time series forecasting", "from": "TIMESNET", "source_id": "d4e3d8b5bf043b78bb9f1551080cab91", "to": "AUTOFORMER", "width": 1.0}, {"description": "RWKV-TS and RWKV are both used for time series data tasks, but RWKV-TS is more effective", "from": "RWKV-TS", "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "to": "RWKV", "width": 1.0}, {"description": "Diffusion-based is a methodology used in TimeGrad model", "from": "DIFFUSION-BASED", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "TIMEGRAD", "width": 1.0}, {"description": "TimeGrad is a model that uses D3VAE methodology", "from": "TIMEGRAD", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "D3VAE", "width": 1.0}, {"description": "TimeGrad is related to TransFusion as both are diffusion models for time series forecasting", "from": "TIMEGRAD", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "TRANSFUSION", "width": 1.0}, {"description": "RNNs are used in the TimeGrad model", "from": "TIMEGRAD", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "RNNS", "width": 1.0}, {"description": "TransFusion is a model that uses ScoreGrad methodology", "from": "TRANSFUSION", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "SCOREGRAD", "width": 1.0}, {"description": "Bilo\u0161 et al. and Crabb\u00e9 et al. are models that use similar methodologies", "from": "BILO\u0160 ET AL.", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "CRABB\u00c9 ET AL.", "width": 1.0}, {"description": "TimeDiff is a model that uses Wang et al. methodology", "from": "TIMEDIFF", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "WANG ET AL.", "width": 1.0}, {"description": "Wang et al. is a model that uses DiffTime methodology", "from": "WANG ET AL.", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "DIFFTIME", "width": 1.0}, {"description": "FTS-Diffusion is a model that uses DiffLoad methodology", "from": "FTS-DIFFUSION", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "DIFFLOAD", "width": 1.0}, {"description": "ST-LLM is a model that uses TPLLM methodology", "from": "ST-LLM", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "TPLLM", "width": 1.0}, {"description": "Attention mechanism is related to ST-LLM as ST-LLM is a model that employs a novel partially frozen attention strategy for traffic prediction", "from": "ST-LLM", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "ATTENTION MECHANISM", "width": 1.0}, {"description": "GATGPT is a model that uses STEP methodology", "from": "GATGPT", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "STEP", "width": 1.0}, {"description": "STEP is related to domain-agnostic models as STEP is an example of a domain-agnostic model", "from": "STEP", "source_id": "859c14b7112010530eddafc204b4b305", "to": "DOMAIN-AGNOSTIC MODELS", "width": 1.0}, {"description": "STEP is related to MetePFL as MetePFL is a model that uses spatial-temporal prompt learning", "from": "STEP", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "METEPFL", "width": 1.0}, {"description": "SPGCL is a model that uses STGCL methodology", "from": "SPGCL", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "STGCL", "width": 1.0}, {"description": "SPGCL is related to domain-agnostic models as SPGCL is an example of a domain-agnostic model", "from": "SPGCL", "source_id": "859c14b7112010530eddafc204b4b305", "to": "DOMAIN-AGNOSTIC MODELS", "width": 1.0}, {"description": "STGCL is related to domain-agnostic models as STGCL is an example of a domain-agnostic model", "from": "STGCL", "source_id": "859c14b7112010530eddafc204b4b305", "to": "DOMAIN-AGNOSTIC MODELS", "width": 1.0}, {"description": "DiffSTG is a model that uses DSTPP methodology", "from": "DIFFSTG", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "DSTPP", "width": 1.0}, {"description": "DYffusion is a model that uses USTD methodology", "from": "DYFFUSION", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "USTD", "width": 1.0}, {"description": "ClimaX is related to DYffusion as both models are used for climate and weather-related tasks", "from": "DYFFUSION", "source_id": "859c14b7112010530eddafc204b4b305", "to": "CLIMAX", "width": 1.0}, {"description": "USTD is related to domain-agnostic models as USTD is an example of a domain-agnostic model", "from": "USTD", "source_id": "859c14b7112010530eddafc204b4b305", "to": "DOMAIN-AGNOSTIC MODELS", "width": 1.0}, {"description": "PriSTI is a model that uses FourCastNet methodology", "from": "PRISTI", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "FOURCASTNET", "width": 1.0}, {"description": "FedWing is a model that uses Pangu-Weather methodology", "from": "FEDWING", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "PANGU-WEATHER", "width": 1.0}, {"description": "MetePFL is related to FedWing as FedWing is a model that uses spatial-temporal prompt learning", "from": "FEDWING", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "METEPFL", "width": 1.0}, {"description": "FourCast-Net is related to Pangu-Weather as both models are used for weather forecasting", "from": "PANGU-WEATHER", "source_id": "859c14b7112010530eddafc204b4b305", "to": "FOURCAST-NET", "width": 1.0}, {"description": "Pangu-Weather is related to ClimaX as both models are used for climate and weather-related tasks", "from": "PANGU-WEATHER", "source_id": "859c14b7112010530eddafc204b4b305", "to": "CLIMAX", "width": 1.0}, {"description": "ClimaX is a model that uses mobility domain", "from": "CLIMAX", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "MOBILITY", "width": 1.0}, {"description": "Mobility is related to TrajGDM as TrajGDM is a model for mobility forecasting", "from": "MOBILITY", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "TRAJGDM", "width": 1.0}, {"description": "Mobility is related to DiffTraj as DiffTraj is a model for mobility forecasting", "from": "MOBILITY", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "DIFFTRAJ", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe TREMBR model is a machine learning model that utilizes the START methodology. This model is closely related to START, as both TREMBR and START are employed for trajectory embedding learning. The relationship between TREMBR and START suggests that they share a common purpose and may have similar applications in the field of trajectory embedding learning.\n\nThis summary is based on the provided descriptions, which collectively provide a clear understanding of the relationship between TREMBR and START. The information suggests that TREMBR is a model that leverages the START methodology, and both models are used for a specific task, namely trajectory embedding learning.", "from": "TREMBR", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416,ef8fd6170b08af3bd99c9df44ffa8b57", "to": "START", "width": 2.0}, {"description": "LLM-Mob is related to Trembr as both models are used for trajectory embedding learning", "from": "TREMBR", "source_id": "ef8fd6170b08af3bd99c9df44ffa8b57", "to": "LLM-MOB", "width": 1.0}, {"description": "START is related to GTM as both models are used for trajectory embedding learning", "from": "START", "source_id": "ef8fd6170b08af3bd99c9df44ffa8b57", "to": "GTM", "width": 1.0}, {"description": "TrajGDM is a model that uses DiffTraj methodology", "from": "TRAJGDM", "source_id": "77c7ced6ab4a0e57fba604c1a3e00416", "to": "DIFFTRAJ", "width": 1.0}, {"description": "GTM is related to DiffTraj as both models are used for trajectory prediction", "from": "DIFFTRAJ", "source_id": "ef8fd6170b08af3bd99c9df44ffa8b57", "to": "GTM", "width": 1.0}, {"description": "TimesFM is related to Lag-Llama as both are models for time series forecasting", "from": "TIMESFM", "source_id": "1ea4e7d0dd5128868533b4567a0a0273", "to": "LAG-LLAMA", "width": 1.0}, {"description": "Decoder-only foundation models are related to TimesFM as TimesFM is a specific example of a decoder-only foundation model", "from": "TIMESFM", "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "to": "DECODER-ONLY FOUNDATION MODEL", "width": 1.0}, {"description": "TimesFM is related to input patching as it uses input patching to process input data", "from": "TIMESFM", "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "to": "INPUT PATCHING", "width": 1.0}, {"description": "TimesFM is a model for time-series forecasting, as described in the text", "from": "TIMESFM", "source_id": "3cf566c27c75f886658002bf9b3b0b15", "to": "TIME-SERIES", "width": 1.0}, {"description": "TimesFM is related to pretraining dataset as TimesFM is trained on the pretraining dataset", "from": "TIMESFM", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "PRETRAINING DATASET", "width": 1.0}, {"description": "TimesFM is a model described in the preprint", "from": "TIMESFM", "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "to": "PREPRINT", "width": 1.0}, {"description": "The TimesFM pretraining dataset is used to train the TimesFM model", "from": "TIMESFM", "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "to": "TIMESFM PRETRAINING DATASET", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nTIMESFM and ARIMA are two entities related to time series forecasting and analysis. Both TIMESFM and ARIMA are models used for time series forecasting, indicating a strong connection between the two entities. This connection is further reinforced by the fact that both models are used for time series forecasting and analysis, suggesting that they share similar goals and applications.\n\nGiven the information collected from all the descriptions, it is clear that TIMESFM and ARIMA are closely related models used for time series forecasting and analysis. The descriptions provided are consistent and do not contain any contradictions, allowing for a coherent summary to be generated.\n\nIn terms of the language used, the descriptions suggest that the primary language is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers. This is consistent with the formal and academic tone of the language used.\n\nOverall, the summary provides a clear understanding of the relationship between TIMESFM and ARIMA, highlighting their shared applications and goals in the context of time series forecasting and analysis.", "from": "TIMESFM", "source_id": "39365aee753fb73130c208fdf4046bb7,892b821ed44c13c4d09e0ceedac3051d,cc1063ba5913ea38c84ac0250c01fe84", "to": "ARIMA", "width": 3.0}, {"description": "TimesFM is related to zero-shot llmtime as zero-shot llmtime is a model mentioned in the text, specifically top model", "from": "TIMESFM", "source_id": "2bc70082c142ee2ef5d6a941611505b7", "to": "ZERO-SHOT LLMTIME", "width": 1.0}, {"description": "TimesFM is related to llmtime as both models perform well in time-series forecasting tasks", "from": "TIMESFM", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "LLMETIME", "width": 1.0}, {"description": "TimesFM is related to seasonal ARIMA as both models perform well in time-series forecasting tasks", "from": "TIMESFM", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "SEASONAL ARIMA", "width": 1.0}, {"description": "TimesFM is a model that is trained on time-series data", "from": "TIMESFM", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "TIMESERIES DATA", "width": 1.0}, {"description": "TimesFM is a model that is used for forecasting", "from": "TIMESFM", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "FORECASTING", "width": 1.0}, {"description": "Informer datasets are related to TimesFM as TimesFM is a machine learning model used for time-series forecasting", "from": "TIMESFM", "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "to": "INFORMER DATASETS", "width": 1.0}, {"description": "TimesFM is related to GPT4TS as GPT4TS is a machine learning model used for time-series forecasting", "from": "TIMESFM", "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "to": "GPT4TS", "width": 1.0}, {"description": "Based on the provided information, the primary entities are \"TIMESFM\" and \"PATCHTST(ZS)\". The descriptions provided suggest that there is a comparison between these two entities in the context of an ablation study and an evaluation.\n\nAfter analyzing the descriptions, it appears that there is a contradiction in the order of comparison between the two entities. However, to provide a coherent summary, we can resolve this contradiction by stating that both \"TIMESFM\" and \"PATCHTST(ZS)\" are compared to each other in the ablation study and the evaluation.\n\nHere is a comprehensive summary of the data:\n\n\"TIMESFM\" and \"PATCHTST(ZS)\" are two entities that are compared to each other in both the ablation study and the evaluation. This comparison suggests that the performance and effectiveness of these two entities are being assessed and compared in a rigorous manner. The ablation study and evaluation provide a comprehensive analysis of the strengths and weaknesses of \"TIMESFM\" and \"PATCHTST(ZS)\", allowing for a deeper understanding of their capabilities and limitations.\n\nThe comparison between \"TIMESFM\" and \"PATCHTST(ZS)\" is likely focused on their performance in time series forecasting, given the presence of technical terms such as \"time series\" and \"long-term series forecasting\" in the surrounding text. The use of mathematical equations and formulas also suggests that the comparison is based on quantitative metrics and performance indicators.\n\nOverall, the comparison between \"TIMESFM\" and \"PATCHTST(ZS)\" is a critical aspect of the research, providing valuable insights into the strengths and weaknesses of these two entities in the context of time series forecasting.", "from": "TIMESFM", "source_id": "28b097d339554431fa14e113c98ed49e,4ade7764ebe998b5f35a7fd2b58d4796", "to": "PATCHTST(ZS)", "width": 2.0}, {"description": "The transformer stack is a component of the TimesFM model", "from": "TIMESFM", "source_id": "28b097d339554431fa14e113c98ed49e", "to": "TRANSFORMER STACK", "width": 1.0}, {"description": "TimesFM(ZS) is a variant of the TimesFM model, which is used for zero-shot forecasting", "from": "TIMESFM", "source_id": "39365aee753fb73130c208fdf4046bb7", "to": "TIMESFM(ZS)", "width": 1.0}, {"description": "TimesFM is compared to N-BEATS, which is a model used for time-series forecasting", "from": "TIMESFM", "source_id": "39365aee753fb73130c208fdf4046bb7", "to": "N-BEATS", "width": 1.0}, {"description": "MAE is related to TimesFM as TimesFM performs well for time-series forecasting tasks", "from": "TIMESFM", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "MAE", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"TIMESFM\" and \"AUTOFORMER\" are both related to time-series forecasting. Specifically, \"AutoFormer\" is associated with \"TimesFM\" as they share a common application in time-series forecasting. Furthermore, \"TimesFM\" is compared to \"AutoFormer\" in an evaluation, suggesting that they are being assessed and compared in terms of their performance and effectiveness in time-series forecasting tasks.\n\nThis summary is based on the information provided in the description list, which indicates a direct relationship between \"AutoFormer\" and \"TimesFM\" in the context of time-series forecasting, as well as a comparison between the two entities in an evaluation.", "from": "TIMESFM", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796,500710c8a95f1310d383fa71fd806c1c", "to": "AUTOFORMER", "width": 2.0}, {"description": "TimesFM is compared to FEDFormer in the evaluation", "from": "TIMESFM", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796", "to": "FEDFORMER", "width": 1.0}, {"description": "TimesFM is compared to Informer in the evaluation", "from": "TIMESFM", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796", "to": "INFORMER", "width": 1.0}, {"description": "TimesFM is compared to llmtime(ZS) in the evaluation", "from": "TIMESFM", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796", "to": "LLMTIME(ZS)", "width": 1.0}, {"description": "TimesFM is related to hyper-parameters as the hyper-parameters are used to train the model", "from": "TIMESFM", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "HYPER-PARAMETERS", "width": 1.0}, {"description": "TimesFM correctly captures the amplitude increase with trend in the Air Passenger dataset", "from": "TIMESFM", "source_id": "e6ec99a117b9abd42452ff51cd6e8f0b", "to": "AIR PASSENGER DATASET", "width": 1.0}, {"description": "TimesFM can correctly identify seasonal peaks even in the presence of outliers in synthetic curves", "from": "TIMESFM", "source_id": "e6ec99a117b9abd42452ff51cd6e8f0b", "to": "SYNTHETIC CURVES", "width": 1.0}, {"description": "Lag-Llama is related to Moirai-1.0-R as both are models mentioned in the text", "from": "LAG-LLAMA", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "MOIRAI-1.0-R", "width": 1.0}, {"description": "Lag-Llama incorporates pre-normalization via the RMSNorm and Rotary Positional Encoding at each attention layer\u2019s query and key representations, as in Touvron et al. (2023)", "from": "LAG-LLAMA", "source_id": "b305a0d76a0159dc10338467e16d6761", "to": "TOUVRON ET AL. (2023)", "width": 1.0}, {"description": "Lag-Llama incorporates pre-normalization via the RMSNorm, as in Zhang \u0026 Sennrich (2019)", "from": "LAG-LLAMA", "source_id": "b305a0d76a0159dc10338467e16d6761", "to": "ZHANG \u0026 SENNRICH (2019)", "width": 1.0}, {"description": "Lag-Llama incorporates Rotary Positional Encoding, as in Su et al. (2021)", "from": "LAG-LLAMA", "source_id": "b305a0d76a0159dc10338467e16d6761", "to": "SU ET AL. (2021)", "width": 1.0}, {"description": "Lag-Llama incorporates pre-normalization via the RMSNorm and Rotary Positional Encoding at each attention layer\u2019s query and key representations, as in LLaMA", "from": "LAG-LLAMA", "source_id": "b305a0d76a0159dc10338467e16d6761", "to": "LLAMA", "width": 1.0}, {"description": "Lag-Llama is related to AutoML as AutoML is used to automate machine learning tasks", "from": "LAG-LLAMA", "source_id": "51ee17c1f0212d4c94010d8e376f649b", "to": "AUTOML", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"LAG-LLAMA\" and \"TFT\" are two models that are compared in terms of their performance. As indicated by the use of their abbreviations in the table, both models are related to each other. This suggests that the comparison between LAG-LLAMA and TFT is likely to be a technical evaluation of their capabilities, possibly in the context of time series forecasting or a related field.\n\nGiven the formal and academic language used in the descriptions, it is likely that this comparison is being made in the context of a research paper or technical document. The use of technical terms such as \"time series\" and \"long-term series forecasting\" further supports this interpretation.\n\nOverall, the summary can be written as follows:\n\n\"LAG-LLAMA and TFT are two related models that are compared in terms of their performance, likely in the context of time series forecasting or a related field. This comparison is likely to be a technical evaluation of their capabilities, as indicated by the use of their abbreviations in the table.\"\n\nThis summary takes into account all the provided descriptions and resolves any potential contradictions. It is written in third person and includes the entity names for context.", "from": "LAG-LLAMA", "source_id": "1dc665214307b1656d1a0094a1918ece,2f3b2f69f8eb4f958c3ca793cae36581", "to": "TFT", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"LAG-LLAMA\" and \"DEEPAR\" are two models that are related to each other. As indicated by the use of their abbreviations in the table, both models are being compared and analyzed together. Specifically, \"LAG-LLAMA\" is being compared to \"DEEPAR\" in terms of their performance. This suggests that the two models are being evaluated and contrasted in order to determine their relative strengths and weaknesses.\n\nGiven the context of the comparison, it is likely that both models are being used for a specific task, such as time series forecasting or long-term series forecasting, as hinted at by the presence of technical terms like \"time series\" and \"frequency analysis\" in the nearby text. The use of multi-head cross-attention, a technique commonly used in natural language processing and machine learning, further suggests that the models are being used for a task that involves complex pattern recognition and analysis.\n\nOverall, the comparison between \"LAG-LLAMA\" and \"DEEPAR\" is likely being conducted in order to determine which model is better suited for a particular task or application, and to identify areas where one model may have an advantage over the other.", "from": "LAG-LLAMA", "source_id": "1dc665214307b1656d1a0094a1918ece,2f3b2f69f8eb4f958c3ca793cae36581", "to": "DEEPAR", "width": 2.0}, {"description": "LAG-LLAMA achieves comparable performance to baselines in zero-shot setting", "from": "LAG-LLAMA", "source_id": "2f3b2f69f8eb4f958c3ca793cae36581", "to": "ZERO-SHOT", "width": 1.0}, {"description": "Based on the provided information, the primary entity is \"LAG-LLAMA\" and the related entity is \"DATASETS\". \n\nAfter analyzing the descriptions, it is clear that there is a strong relationship between LAG-LLAMA and datasets. The descriptions collectively suggest that datasets play a crucial role in the development and evaluation of LAG-LLAMA.\n\nHere is a comprehensive summary of the data:\n\nLAG-LLAMA is a machine learning model that relies heavily on datasets for its training and testing. Specifically, datasets are used for pretraining and fine-tuning LAG-LLAMA, indicating that the model\u0027s performance is closely tied to the quality and quantity of the datasets used. The results section of the text further supports this relationship, as it is mentioned that LAG-LLAMA is used on various datasets, suggesting that the model\u0027s performance is evaluated across different datasets.\n\nIn summary, LAG-LLAMA is a machine learning model that is deeply dependent on datasets for its development, evaluation, and performance. The model\u0027s relationship with datasets is multifaceted, encompassing pretraining, fine-tuning, and testing, highlighting the importance of high-quality datasets in the development and evaluation of LAG-LLAMA.\n\nThis summary is written in third person and includes the entity names, providing a clear and concise description of the relationship between LAG-LLAMA and datasets.", "from": "LAG-LLAMA", "source_id": "2f3b2f69f8eb4f958c3ca793cae36581,4c09f35749179fe18c7d7290eaa57955,e995e5477f470244a4a6afb9417f6d96", "to": "DATASETS", "width": 3.0}, {"description": "OneFitsAll model is related to Lag-Llama as Lag-Llama achieves significantly better performance in all datasets, except for the dataset beijing-pm2.5", "from": "LAG-LLAMA", "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "to": "ONEFITSALL MODEL", "width": 1.0}, {"description": "Lag-Llama is related to pre-trained LLM as Lag-Llama adapts a pre-trained LLM for forecasting", "from": "LAG-LLAMA", "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "to": "PRETRAINED LLM", "width": 1.0}, {"description": "Beijing-pm2.5 is related to Lag-Llama as Lag-Llama performs similarly to the baseline on this dataset", "from": "LAG-LLAMA", "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "to": "BEIJING-PM2.5", "width": 1.0}, {"description": "Lag-Llama is a model for time series forecasting, as indicated by the use of the phrase \"time series forecasting\"", "from": "LAG-LLAMA", "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "to": "TIME SERIES FORECASTING", "width": 1.0}, {"description": "Lag-Llama uses a decoder-only transformer architecture, as indicated by the use of the phrase \"decoder-only transformer architecture\"", "from": "LAG-LLAMA", "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "to": "DECODER-ONLY TRANSFORMER ARCHITECTURE", "width": 1.0}, {"description": "Arjun is related to Lag-Llama, as indicated by the use of the phrase \"Lag-Llama architecture\"", "from": "LAG-LLAMA", "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "to": "ARJUN", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities involved are \"LAG-LLAMA\" and \"KASHIF\". \n\nKASHIF is closely related to LAG-LLAMA, as indicated by the use of the phrase \"Lag-Llama architecture\". Furthermore, KASHIF contributed significantly to the development of LAG-LLAMA by writing the code for its architecture and training strategies. This suggests a strong connection between KASHIF and LAG-LLAMA, with KASHIF playing a key role in the development of the latter.\n\nOverall, the relationship between KASHIF and LAG-LLAMA is one of collaboration and contribution, with KASHIF being a key contributor to the development of LAG-LLAMA\u0027s architecture and training strategies.", "from": "LAG-LLAMA", "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd,1727ee77a376bbef1c7625a001e4ac3d", "to": "KASHIF", "width": 2.0}, {"description": "Hena contributed to the development of Lag-Llama and added support for time features and other features", "from": "LAG-LLAMA", "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "to": "HENA", "width": 1.0}, {"description": "Andrew contributed to the development of Lag-Llama and expanded the empirical design of the paper", "from": "LAG-LLAMA", "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "to": "ANDREW", "width": 1.0}, {"description": "Rishika contributed to the development of Lag-Llama and ran experiments and contributed to the writing of the first version of the paper", "from": "LAG-LLAMA", "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "to": "RISHIKA", "width": 1.0}, {"description": "Arian contributed to the development of Lag-Llama and ran experiments and contributed to the writing of the first version of the paper", "from": "LAG-LLAMA", "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "to": "ARIAN", "width": 1.0}, {"description": "Mohammad contributed to the development of Lag-Llama and worked with all AutoGluon models and experiments", "from": "LAG-LLAMA", "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "to": "MOHAMMAD", "width": 1.0}, {"description": "George contributed to the development of Lag-Llama and ran experiments and contributed to the writing of the first version of the paper", "from": "LAG-LLAMA", "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "to": "GEORGE", "width": 1.0}, {"description": "Roland contributed to the development of Lag-Llama and worked with the code and experiments of the One-FitsAll model", "from": "LAG-LLAMA", "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "to": "ROLAND", "width": 1.0}, {"description": "Nadhir contributed to the development of Lag-Llama and integrated the N-BEATS model", "from": "LAG-LLAMA", "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "to": "NADHIR", "width": 1.0}, {"description": "Marin contributed to the development of Lag-Llama and wrote the initial code for sampling windows for the pretraining set", "from": "LAG-LLAMA", "source_id": "07a4ccfc1f863a9f11a4c0ea65a2a6dd", "to": "MARIN", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nLag-Llama is a machine learning model that is related to validation loss, which is a key metric used to evaluate its performance. Validation loss is utilized to assess the performance of Lag-Llama, as indicated by the results presented in the text. This suggests that validation loss is a crucial component in evaluating the effectiveness of Lag-Llama, and its performance is directly tied to the model\u0027s ability to minimize or optimize this metric.\n\nIn the context of machine learning, validation loss is a common metric used to evaluate the performance of models, particularly in tasks such as time series forecasting. The use of validation loss to evaluate Lag-Llama\u0027s performance implies that the model is being trained and tested on a dataset, and its performance is being evaluated based on its ability to minimize the difference between its predictions and the actual values.\n\nOverall, the relationship between Lag-Llama and validation loss is a critical aspect of the model\u0027s performance, and understanding this relationship is essential for evaluating the effectiveness of Lag-Llama in various applications, such as time series forecasting.", "from": "LAG-LLAMA", "source_id": "4c09f35749179fe18c7d7290eaa57955,e995e5477f470244a4a6afb9417f6d96", "to": "VALIDATION LOSS", "width": 2.0}, {"description": "Lag-Llama is related to train split as train split is used to divide the dataset into training and validation sets", "from": "LAG-LLAMA", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "TRAIN SPLIT", "width": 1.0}, {"description": "Lag-Llama is related to Number of Layers as both are concepts related to hyperparameter choices", "from": "LAG-LLAMA", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "NUMBER OF LAYERS", "width": 1.0}, {"description": "Lag-Llama is related to Number of Heads as both are concepts related to hyperparameter choices", "from": "LAG-LLAMA", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "NUMBER OF HEADS", "width": 1.0}, {"description": "Hyperparameter is related to Lag-Llama as hyperparameters are used to fine-tune Lag-Llama", "from": "LAG-LLAMA", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "HYPERPARAMETER", "width": 1.0}, {"description": "Lag-Llama is compared to supervised models in the text, as indicated by the results", "from": "LAG-LLAMA", "source_id": "e995e5477f470244a4a6afb9417f6d96", "to": "SUPERVISED MODEL", "width": 1.0}, {"description": "Neural scaling laws are used to describe the relationship between model parameters and performance of Lag-Llama in the text, as indicated by the results", "from": "LAG-LLAMA", "source_id": "e995e5477f470244a4a6afb9417f6d96", "to": "NEURAL SCALING LAWS", "width": 1.0}, {"description": "Forecast visualizations are used to display the results of a forecast of Lag-Llama in the text, as indicated by the results", "from": "LAG-LLAMA", "source_id": "e995e5477f470244a4a6afb9417f6d96", "to": "FORECAST VISUALIZATIONS", "width": 1.0}, {"description": "Electricity hourly is a dataset that is used to train and evaluate Lag-Llama in the text, as indicated by the results", "from": "LAG-LLAMA", "source_id": "e995e5477f470244a4a6afb9417f6d96", "to": "ELECTRICITY HOURLY", "width": 1.0}, {"description": "ETT-H2 is a dataset that is used to train and evaluate Lag-Llama in the text, as indicated by the results", "from": "LAG-LLAMA", "source_id": "e995e5477f470244a4a6afb9417f6d96", "to": "ETT-H2", "width": 1.0}, {"description": "Traffic is a dataset that is used to train and evaluate Lag-Llama in the text, as indicated by the results", "from": "LAG-LLAMA", "source_id": "e995e5477f470244a4a6afb9417f6d96", "to": "TRAFFIC", "width": 1.0}, {"description": "ETT-M2 is a dataset that is used to train and evaluate Lag-Llama in the text, as indicated by the results", "from": "LAG-LLAMA", "source_id": "e995e5477f470244a4a6afb9417f6d96", "to": "ETT-M2", "width": 1.0}, {"description": "Pedestrian counts is a dataset that is used to train and evaluate Lag-Llama in the text, as indicated by the results", "from": "LAG-LLAMA", "source_id": "e995e5477f470244a4a6afb9417f6d96", "to": "PEDESTRIAN COUNTS", "width": 1.0}, {"description": "Requests minute is a dataset that is used to train and evaluate Lag-Llama in the text, as indicated by the results", "from": "LAG-LLAMA", "source_id": "e995e5477f470244a4a6afb9417f6d96", "to": "REQUESTS MINUTE", "width": 1.0}, {"description": "Lag-Llama is a specific model mentioned in the text, as indicated by the use of the name \"Lag-Llama\"", "from": "LAG-LLAMA", "source_id": "79c41aff65d584b4c8d4c769b82756d5", "to": "MODEL", "width": 1.0}, {"description": "GPT4TS uses a pre-trained GPT-2 model as a backbone", "from": "GPT-2", "source_id": "bc54a718d1886698232d578fd88c3ac7", "to": "GPT4TS", "width": 1.0}, {"description": "Chronos models also use GPT-2 as a decoder-only model", "from": "GPT-2", "source_id": "63e284ec7e76fa859cc46b72d3746654", "to": "CHRONOS MODELS", "width": 1.0}, {"description": "T5 and GPT-2 are related as both are pre-trained language models used in the Chronos framework", "from": "GPT-2", "source_id": "f72ba51a17dd00cff43bcdda22c273e8", "to": "T5", "width": 1.0}, {"description": "Lag-Llama is related to GPT-2 as GPT-2 is used in OneFitsAll", "from": "GPT-2", "source_id": "f0c52387a7b3a5c3850fe6991f0a7c83", "to": "LAG-LAGMA", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nGPT-2 and LLAMA are two large language models mentioned in the text. They are both used as examples of advanced language models, showcasing their capabilities in processing and generating human-like language. LLAMA is specifically related to GPT-2, as they share similarities in their architecture and functionality as language models. The text suggests that these models are used in various applications, including long-term series forecasting, frequency analysis, and multi-head cross-attention, which are all techniques commonly employed in natural language processing and time series forecasting tasks.\n\nThe text also implies that GPT-2 and LLAMA are used in academic and research settings, as evidenced by the presence of technical terms, mathematical equations, and references to academic papers. The use of English-language abbreviations such as ICLR, AAAI, and PMLR further supports this notion, suggesting that the text is from a research paper or technical document written in English.\n\nOverall, GPT-2 and LLAMA are two prominent language models that are used as examples in the text, highlighting their potential applications in various fields, including natural language processing and time series forecasting.", "from": "GPT-2", "source_id": "b27c89cb0db6646b1203b2701e017aeb,e6a6bc6fbd362394320961ac10cdd230", "to": "LLAMA", "width": 2.0}, {"description": "LLAMA-7B outperforms GPT-2 by 14.5% in terms of MSE reduction", "from": "GPT-2", "source_id": "072b166b9a1b6afecf5874f45af61699", "to": "LLAMA-7B", "width": 1.0}, {"description": "GPT-2 is related to GPT-4 as they are both language models", "from": "GPT-2", "source_id": "e6a6bc6fbd362394320961ac10cdd230", "to": "GPT-4", "width": 1.0}, {"description": "LLMs are related to VLMs as VLMs are pre-trained models that can be fine-tuned for specific tasks", "from": "VLMS", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "LLMS", "width": 1.0}, {"description": "VLMs are related to AMS as AMS are pre-trained models that can be fine-tuned for specific tasks", "from": "VLMS", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "AMS", "width": 1.0}, {"description": "ECG is related to medical text reports as medical text reports are used to process medical data", "from": "ECG", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "MEDICAL TEXT REPORTS", "width": 1.0}, {"description": "LLMs are related to embeddings as embeddings are used to represent natural language data", "from": "LLMS", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "EMBEDDINGS", "width": 1.0}, {"description": "Handcrafted dataset descriptions are related to LLMs as LLMs use handcrafted dataset descriptions to process natural language data", "from": "LLMS", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "HANDCRAFTED DATASET DESCRIPTIONS", "width": 1.0}, {"description": "Large language models are related to parameters as the number of parameters affects the performance of LLMs", "from": "LLMS", "source_id": "f7ce89346ea6715560163ef3a630a5df", "to": "PARAMETERS", "width": 1.0}, {"description": "KMH+20 is related to LLMs as it established a power law like relationship between the number of parameters in a language model and its downstream performance", "from": "LLMS", "source_id": "f7ce89346ea6715560163ef3a630a5df", "to": "KMH+20", "width": 1.0}, {"description": "LLMs are discussed in the paper by Brown et al.", "from": "LLMS", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "BROWN ET AL.", "width": 1.0}, {"description": "LLMs are discussed in the paper by Chung et al.", "from": "LLMS", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "CHUNG ET AL.", "width": 1.0}, {"description": "LLMs are discussed in the paper by Touvron et al.", "from": "LLMS", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "TOUVRON ET AL.", "width": 1.0}, {"description": "Task instructions are used to guide LLMs in time series forecasting", "from": "LLMS", "source_id": "b27c89cb0db6646b1203b2701e017aeb", "to": "TASK INSTRUCTIONS", "width": 1.0}, {"description": "LLMs are used in task-specific learning, but TIME-LLM proposes a different approach", "from": "LLMS", "source_id": "b27c89cb0db6646b1203b2701e017aeb", "to": "TASK-SPECIFIC LEARNING", "width": 1.0}, {"description": "LLMs retain few-shot learning capabilities in forecasting tasks", "from": "LLMS", "source_id": "6d66c2ea37e25b646a13d751c05a8e4d", "to": "TIME-LLM (REPROGRAMMED)", "width": 1.0}, {"description": "Large language models are related to arXiv as they are often published on the arXiv database", "from": "LLMS", "source_id": "a73df99fe49b288e1c8751be2008b191", "to": "ARXIV", "width": 1.0}, {"description": "Spatio-temporal graphs and rasters refer to data that changes over time and space", "from": "SPATIO-TEMPORAL GRAPHS", "source_id": "ad8efcfda80253036294f2eeb48926e8", "to": "SPATIO-TEMPORAL RASTERS", "width": 1.0}, {"description": "Spatio-temporal graph forecasting is related to domain-agnostic models as domain-agnostic models aim to generalize across different domains, including spatio-temporal graph forecasting", "from": "DOMAIN-AGNOSTIC MODELS", "source_id": "859c14b7112010530eddafc204b4b305", "to": "SPATIO-TEMPORAL GRAPH FORECASTING", "width": 1.0}, {"description": "FourCast-Net is related to FengWu as both models are used for weather forecasting", "from": "FOURCAST-NET", "source_id": "859c14b7112010530eddafc204b4b305", "to": "FENGWU", "width": 1.0}, {"description": "FourCast-Net is related to W-MAE as both models are used for weather forecasting", "from": "FOURCAST-NET", "source_id": "859c14b7112010530eddafc204b4b305", "to": "W-MAE", "width": 1.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nAUXMOBLCAST and LLM-MOB are two entities that are closely related to each other. Both models are utilized for human mobility forecasting and prediction. This suggests that they share a common purpose and application in the field of human mobility analysis.\n\nThe relationship between AUXMOBLCAST and LLM-MOB is further reinforced by the fact that they are both used for forecasting and predicting human mobility patterns. This implies that they are likely to be used in similar contexts, such as urban planning, transportation systems, and emergency response planning.\n\nOverall, the summary indicates that AUXMOBLCAST and LLM-MOB are two related models that are used for human mobility forecasting and prediction, suggesting a strong connection between the two entities.\n\nRelevant information from the nearby text is not provided, but based on the given information, the following can be inferred:\n\n* Both models are used for human mobility forecasting and prediction, which suggests that they are likely to be used in similar applications.\n* The relationship between the two models is likely to be collaborative, with AUXMOBLCAST and LLM-MOB being used together to improve the accuracy and effectiveness of human mobility forecasting and prediction.\n\nHowever, without more information, it is not possible to determine the specific nature of the relationship between AUXMOBLCAST and LLM-MOB, such as whether they are complementary or redundant models.", "from": "AUXMOBLCAST", "source_id": "859c14b7112010530eddafc204b4b305,ef8fd6170b08af3bd99c9df44ffa8b57", "to": "LLM-MOB", "width": 2.0}, {"description": "The attention function is a key component of the Transformer architecture, which uses self-attention mechanisms to process sequential data.", "from": "TRANSFORMER ARCHITECTURE", "source_id": "7dbc961fe1959f844ebac283498e7fb0", "to": "ATTENTION FUNCTION", "width": 1.0}, {"description": "The Transformer architecture is designed to process sequential data, such as text or time series data.", "from": "TRANSFORMER ARCHITECTURE", "source_id": "7dbc961fe1959f844ebac283498e7fb0", "to": "DATA", "width": 1.0}, {"description": "Recurrent neural networks and transformer architecture are both designed to handle sequential data", "from": "TRANSFORMER ARCHITECTURE", "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "to": "RECURRENT NEURAL NETWORKS", "width": 1.0}, {"description": "The transformer architecture is used in the BART model", "from": "TRANSFORMER ARCHITECTURE", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "BART", "width": 1.0}, {"description": "The transformer architecture is used in the T5 model", "from": "TRANSFORMER ARCHITECTURE", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "T5", "width": 1.0}, {"description": "Data is used as input to the attention function, which computes the attention weights based on the queries, keys, and values matrices.", "from": "DATA", "source_id": "7dbc961fe1959f844ebac283498e7fb0", "to": "QUERIES", "width": 1.0}, {"description": "The text contains data from sensors that collect data from various sources", "from": "DATA", "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "to": "SENSORS", "width": 1.0}, {"description": "The text contains data on pedestrians counted between 2009 and 2020", "from": "DATA", "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "to": "PEDESTRIANS", "width": 1.0}, {"description": "The text contains various data and information", "from": "DATA", "source_id": "deb67d5386710136cf24bcbf135a66c4", "to": "TEXT", "width": 1.0}, {"description": "The data presented in the text consists of various values", "from": "DATA", "source_id": "deb67d5386710136cf24bcbf135a66c4", "to": "VALUES", "width": 1.0}, {"description": "The data is used to compute the reconstruction, which is used to generate a predicted value", "from": "DATA", "source_id": "1b51ec337efd822ca3a0b3eb819c1b91", "to": "RECONSTRUCTION", "width": 1.0}, {"description": "Queries and keys are used to compute the attention weights in the attention function.", "from": "QUERIES", "source_id": "7dbc961fe1959f844ebac283498e7fb0", "to": "KEYS", "width": 1.0}, {"description": "Keys and values are used to compute the attention weights in the attention function.", "from": "KEYS", "source_id": "7dbc961fe1959f844ebac283498e7fb0", "to": "VALUES", "width": 1.0}, {"description": "Value is related to keys as keys are used to encode the value", "from": "KEYS", "source_id": "4bdf596e75e1cb06d11b25e95491037e", "to": "VALUE", "width": 1.0}, {"description": "Keys are related to the query matrix as the query matrix is used to encode the keys", "from": "KEYS", "source_id": "4bdf596e75e1cb06d11b25e95491037e", "to": "QUERY MATRIX", "width": 1.0}, {"description": "Values are used as input to the softmax function, which normalizes the attention weights.", "from": "VALUES", "source_id": "7dbc961fe1959f844ebac283498e7fb0", "to": "SOFTMAX", "width": 1.0}, {"description": "Tokenization is related to values as values are represented as tokens in the Chronos model", "from": "VALUES", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "TOKENIZATION", "width": 1.0}, {"description": "Values are related to scale as the scale of the data affects the representation of values in the Chronos model", "from": "VALUES", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "SCALE", "width": 1.0}, {"description": "Error bars are related to values as they represent the variability of the values", "from": "VALUES", "source_id": "6fa5f635ad5f7e6b67d5e467f130345c", "to": "ERROR BARS", "width": 1.0}, {"description": "The values presented in the text are measurements of various quantities", "from": "VALUES", "source_id": "deb67d5386710136cf24bcbf135a66c4", "to": "MEASUREMENTS", "width": 1.0}, {"description": "Softmax is used to normalize the attention weights, which are moderated by the scaling factor.", "from": "SOFTMAX", "source_id": "7dbc961fe1959f844ebac283498e7fb0", "to": "SCALING FACTOR", "width": 1.0}, {"description": "Softmax is used in attention head to normalize the attention weights", "from": "SOFTMAX", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "ATTENTION HEAD", "width": 1.0}, {"description": "Attention is related to softmax as softmax is used in attention", "from": "SOFTMAX", "source_id": "509b431231e669be373f593b31412eed", "to": "ATTENTION", "width": 1.0}, {"description": "Softmax is related to weights as weights are the coefficients used to compute the attention scores", "from": "SOFTMAX", "source_id": "f2e78b25a535b82e2743a0ea4052eb9a", "to": "WEIGHTS", "width": 1.0}, {"description": "Diffusion-based models and video are both used for generative tasks, but diffusion-based models are more effective", "from": "VIDEO", "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "to": "DIFFUSION-BASED MODELS", "width": 1.0}, {"description": "Patch is related to decoder-only model as decoder-only model is used to predict patch", "from": "DECODER-ONLY MODEL", "source_id": "f98d2bec31738be3f7be750b5fe6180e", "to": "PATCH", "width": 1.0}, {"description": "Time series forecasting is related to zero-shot performance as a good zero-shot performance is required for time series forecasting", "from": "TIME SERIES FORECASTING", "source_id": "6f6e27166a1506482bfcaf5585322595", "to": "ZERO-SHOT PERFORMANCE", "width": 1.0}, {"description": "Time series forecasting is related to decoder-only foundation model as it is a type of model used for time-series forecasting", "from": "TIME SERIES FORECASTING", "source_id": "f7ce89346ea6715560163ef3a630a5df", "to": "DECODER-ONLY FOUNDATION MODEL", "width": 1.0}, {"description": "Language model weights are related to time series forecasting as they are used for initialization", "from": "TIME SERIES FORECASTING", "source_id": "c7f04fa1168df64cce110e20a1b72b51", "to": "LANGUAGE MODEL WEIGHTS", "width": 1.0}, {"description": "Time series forecasting can be improved by taking into account additional information such as covariates", "from": "TIME SERIES FORECASTING", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "COVARIATES", "width": 1.0}, {"description": "Time series forecasting is related to transfer learning as transfer learning is used to improve performance on a new forecasting task", "from": "TIME SERIES FORECASTING", "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "to": "TRANSFER LEARNING", "width": 1.0}, {"description": "Probabilistic time series forecasting is a type of time series forecasting", "from": "TIME SERIES FORECASTING", "source_id": "c7167cf92abe7513ccb936ca79871932", "to": "PROBABILISTIC TIME SERIES FORECASTING", "width": 1.0}, {"description": "Prior deep learning approaches are used for time series forecasting", "from": "TIME SERIES FORECASTING", "source_id": "c7167cf92abe7513ccb936ca79871932", "to": "DEEP LEARNING APPROACHES", "width": 1.0}, {"description": "Time series forecasting is related to statistical models as statistical models are used in time series forecasting", "from": "TIME SERIES FORECASTING", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "STATISTICAL MODELS", "width": 1.0}, {"description": "Time series forecasting is related to neural forecasting as neural forecasting is a rapidly developing research area in time series forecasting", "from": "TIME SERIES FORECASTING", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "NEURAL FORECASTING", "width": 1.0}, {"description": "Time series forecasting is related to general-purpose forecasting algorithm as general-purpose forecasting algorithm is a broader concept that includes time series forecasting", "from": "TIME SERIES FORECASTING", "source_id": "d08a82328191907abfcdb72efab9a6cf", "to": "GENERAL-PURPOSE FORECASTING ALGORITHM", "width": 1.0}, {"description": "Time series forecasting is related to foundation language models as foundation language models can perform well on a diverse range of NLP tasks in a few-shot or even zero-shot setting", "from": "TIME SERIES FORECASTING", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "FOUNDATION LANGUAGE MODELS", "width": 1.0}, {"description": "Jin et al. (2023a) is a paper that discusses time series forecasting", "from": "TIME SERIES FORECASTING", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "JIN ET AL. (2023A)", "width": 1.0}, {"description": "Leonard (2001) is a paper that discusses demand planning", "from": "TIME SERIES FORECASTING", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "LEONARD (2001)", "width": 1.0}, {"description": "Li et al. (2022) is a paper that discusses inventory optimization", "from": "TIME SERIES FORECASTING", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "LI ET AL. (2022)", "width": 1.0}, {"description": "Liu et al. (2023a) is a paper that discusses energy load forecasting", "from": "TIME SERIES FORECASTING", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "LIU ET AL. (2023A)", "width": 1.0}, {"description": "Schneider \u0026 Dickinson (1974) is a paper that discusses climate modeling", "from": "TIME SERIES FORECASTING", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "SCHNEIDER \u0026 DICKINSON (1974)", "width": 1.0}, {"description": "Time series forecasting is related to multimodal knowledge as tapping into this knowledge can enable synergistic forecasting", "from": "TIME SERIES FORECASTING", "source_id": "89b79391630ac478085efea89fad5736", "to": "MULTIMODAL KNOWLEDGE", "width": 1.0}, {"description": "Time series forecasts are related to time series forecasting as the proposed framework offers an extensible paradigm for imbuing large models with new capabilities beyond their original pre-training", "from": "TIME SERIES FORECASTING", "source_id": "89b79391630ac478085efea89fad5736", "to": "TIME SERIES FORECASTS", "width": 1.0}, {"description": "Time series forecasting is related to a sequence of historical observations as it uses the data to make predictions", "from": "TIME SERIES FORECASTING", "source_id": "3174231a67593609c727151c9df31d0a", "to": "SEQUENCE OF HISTORICAL OBSERVATIONS", "width": 1.0}, {"description": "Prompt-as-prefix is used in the LLM for time series forecasting", "from": "TIME SERIES FORECASTING", "source_id": "072b166b9a1b6afecf5874f45af61699", "to": "PROMPT-AS-PREFIX", "width": 1.0}, {"description": "TIME-LLM framework can be used to achieve state-of-the-art performance in time series forecasting", "from": "TIME SERIES FORECASTING", "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8", "to": "TIME-LLM FRAMEWORK", "width": 1.0}, {"description": "Time series forecasting can be cast as a \"language\" task that can be tackled by an off-the-shelf LLM", "from": "TIME SERIES FORECASTING", "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8", "to": "PARAMETER-EFFICIENT FINE-TUNING", "width": 1.0}, {"description": "Time series forecasting is related to language task as time series forecasting can be cast as a language task", "from": "TIME SERIES FORECASTING", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "LANGUAGE TASK", "width": 1.0}, {"description": "Time series forecasting is related to short-term time series forecasting as short-term time series forecasting is a specific type of time series forecasting", "from": "TIME SERIES FORECASTING", "source_id": "f49330b6fd81d86d14e7a9d4b8e45576", "to": "SHORT-TERM TIME SERIES FORECASTING", "width": 1.0}, {"description": "Time series forecasting is related to MASE values as MASE values are used to evaluate the performance of time series forecasting models", "from": "TIME SERIES FORECASTING", "source_id": "d540ee970ce7debedc6b1b95e9c88be8", "to": "MASE VALUES", "width": 1.0}, {"description": "Time series forecasting is related to OWA values as OWA values are used to evaluate the performance of time series forecasting models", "from": "TIME SERIES FORECASTING", "source_id": "d540ee970ce7debedc6b1b95e9c88be8", "to": "OWA VALUES", "width": 1.0}, {"description": "Time series forecasting is related to SMAPE values as SMAPE values are used to evaluate the performance of time series forecasting models", "from": "TIME SERIES FORECASTING", "source_id": "d540ee970ce7debedc6b1b95e9c88be8", "to": "SMAPE VALUES", "width": 1.0}, {"description": "Time series forecasting is related to reprogramming framework as reprogramming framework is used for time series forecasting", "from": "TIME SERIES FORECASTING", "source_id": "7e97089185883c456c798ddc5ec86373", "to": "REPROGRAMMING FRAMEWORK", "width": 1.0}, {"description": "Time series forecasting models are related to patches as patches are a type of segment used in time series forecasting models to encapsulate local dynamics within input tokens", "from": "TIME SERIES FORECASTING MODELS", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "PATCHES", "width": 1.0}, {"description": "Time series forecasting models are related to normalization layer as normalization layer is a critical design in time series forecasting models", "from": "TIME SERIES FORECASTING MODELS", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "NORMALIZATION LAYER", "width": 1.0}, {"description": "Patches are related to normalization layer as normalization layer is used to standardize data through instance-specific mean and variance", "from": "PATCHES", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "NORMALIZATION LAYER", "width": 1.0}, {"description": "Patches are related to Prompt-as-Prefix as Prompt-as-Prefix enriches the input context and directs the transformation of reprogrammed input patches", "from": "PATCHES", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "PROMPT-AS-PREFIX", "width": 1.0}, {"description": "Input time series is related to patches as patches are used to represent input time series", "from": "PATCHES", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "INPUT TIME SERIES", "width": 1.0}, {"description": "Normalization layer is related to instance normalization as instance normalization is a technique used in the normalization layer to standardize data", "from": "NORMALIZATION LAYER", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "INSTANCE NORMALIZATION", "width": 1.0}, {"description": "Instance normalization is related to multi-resolution analysis as multi-resolution analysis is a specialized approach used in time series forecasting models", "from": "INSTANCE NORMALIZATION", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "MULTI-RESOLUTION ANALYSIS", "width": 1.0}, {"description": "Instance normalization is related to patching as patching uses instance normalization to normalize activations", "from": "INSTANCE NORMALIZATION", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "PATCHING", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n**Summary of MULTI-LAYER PERCEPTRONS and RECURRENT NEURAL NETWORKS**\n\nMULTI-LAYER PERCEPTRONS (MLPs) and RECURRENT NEURAL NETWORKS (RNNs) are two types of models used in time series forecasting. Both MLPs and RNNs are employed for sequential data tasks, with RNNs being more effective in this regard. These models are utilized in various applications, including long-term series forecasting, frequency analysis, and multi-head cross-attention. The use of mathematical equations and formulas, as well as English-language abbreviations such as ICLR, AAAI, and PMLR, further supports the application of these models in academic and research settings.\n\nIn terms of their relationship, MLPs and RNNs are closely related, as both are types of models used in time series forecasting. This suggests that they share commonalities in their architecture and functionality, which can be leveraged to improve their performance and effectiveness in sequential data tasks.\n\nOverall, the use of MLPs and RNNs in time series forecasting highlights their importance in the field of machine learning and artificial intelligence. Their effectiveness in sequential data tasks, combined with their application in various research settings, makes them valuable tools for researchers and practitioners alike.", "from": "MULTI-LAYER PERCEPTRONS", "source_id": "5805d8a1afe3d6bc6300ac71f7163831,b9d22fe9f2eecb1665f4bea365c7d612", "to": "RECURRENT NEURAL NETWORKS", "width": 2.0}, {"description": "Convolutional Neural Networks (CNNs) are related to Multi-Layer Perceptrons (MLPs) as both are types of models used in time series forecasting", "from": "MULTI-LAYER PERCEPTRONS", "source_id": "5805d8a1afe3d6bc6300ac71f7163831", "to": "CONVOLUTIONAL NEURAL NETWORKS", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nRecurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) are two types of models used in time series forecasting. Both RNNs and CNNs are utilized for sequential data tasks, with RNNs being particularly effective in handling temporal relationships and CNNs being more effective in certain scenarios. \n\nRNNs and CNNs are related as both are types of neural networks, with CNNs being a specific type of neural network used for image and video data. However, CNNs can also be applied to sequential data tasks, making them a versatile tool in the field of time series forecasting.\n\nThe primary language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers. The language is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nOverall, RNNs and CNNs are both powerful tools in the field of time series forecasting, with RNNs being particularly effective in handling temporal relationships and CNNs being more effective in certain scenarios.", "from": "RECURRENT NEURAL NETWORKS", "source_id": "4de223d7df157faf857c3e17d9e6e5b6,5805d8a1afe3d6bc6300ac71f7163831,b9d22fe9f2eecb1665f4bea365c7d612", "to": "CONVOLUTIONAL NEURAL NETWORKS", "width": 3.0}, {"description": "Deep learning forecasting models are related to recurrent neural networks as recurrent neural networks are a type of neural network used for time series forecasting", "from": "RECURRENT NEURAL NETWORKS", "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "to": "DEEP LEARNING FORECASTING MODELS", "width": 1.0}, {"description": "CNNs and ResNet are both used for image classification tasks, but ResNet is more effective", "from": "CONVOLUTIONAL NEURAL NETWORKS", "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "to": "RESNET", "width": 1.0}, {"description": "ResNet and dilated convolution layers are both used for image classification tasks, but dilated convolution layers are more effective", "from": "RESNET", "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "to": "DILATED CONVOLUTION LAYERS", "width": 1.0}, {"description": "Diffusion-based models and computer vision are both used for generative tasks, but diffusion-based models are more effective", "from": "DIFFUSION-BASED MODELS", "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "to": "CV", "width": 1.0}, {"description": "Diffusion-based models and traffic forecasting are both used for generative tasks, but diffusion-based models are more effective", "from": "DIFFUSION-BASED MODELS", "source_id": "b9d22fe9f2eecb1665f4bea365c7d612", "to": "TRAFFIC FORECASTING", "width": 1.0}, {"description": "Diffusion models are related to temporal dynamics as they are used to capture patterns and relationships in a time series", "from": "DIFFUSION MODELS", "source_id": "9125a7d3794226f109debe90e5db041c", "to": "TEMPORAL DYNAMICS", "width": 1.0}, {"description": "Self-supervision signals are related to pre-training as pre-training is the process of training a time series foundation model on unlabeled data", "from": "PRE-TRAINING", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "SELF-SUPERVISION SIGNALS", "width": 1.0}, {"description": "Self-supervised learning is related to Lei Zhang as Lei Zhang is an author of a paper on self-supervised learning", "from": "SELF-SUPERVISED LEARNING", "source_id": "98c6b5003112ab7110e45414a2fa468b", "to": "LEI ZHANG", "width": 1.0}, {"description": "Spatial-temporal encoders are related to contrastive learning as contrastive learning is used to enhance the robustness of pre-training time series foundation models", "from": "CONTRASTIVE LEARNING", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "SPATIAL-TEMPORAL ENCODERS", "width": 1.0}, {"description": "Contrastive learning is related to self-supervision signals as self-supervision signals are used in contrastive learning", "from": "CONTRASTIVE LEARNING", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "SELF-SUPERVISION SIGNALS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"MASKED AUTOENCODING\" is related to \"PROBABILISTIC MODELING\" in the context of time series forecasting. Probabilistic modeling is used in time series forecasting to model the uncertainty and variability in a time series, which is a key aspect of masked autoencoding. In other words, masked autoencoding is a technique that leverages probabilistic modeling to capture the underlying patterns and trends in a time series, enabling accurate long-term series forecasting.\n\nThis relationship is further supported by the fact that probabilistic modeling is a fundamental concept in time series analysis, which is closely related to masked autoencoding. The use of probabilistic modeling in masked autoencoding allows for the incorporation of uncertainty and variability in the forecasting process, making it a powerful tool for time series forecasting.\n\nOverall, the connection between masked autoencoding and probabilistic modeling is rooted in their shared goal of modeling and forecasting time series data, with probabilistic modeling providing a key framework for capturing the underlying uncertainty and variability in the data.\n\nRelevant information from the nearby text that supports this summary includes:\n\n* The use of technical terms such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis,\" which suggests a strong connection to the field of time series analysis.\n* The presence of mathematical equations and formulas, which are likely used to model and analyze time series data.\n* The citation of academic papers and authors, which suggests a strong connection to the academic literature on time series analysis and probabilistic modeling.\n\nOverall, the summary provides a clear and concise description of the relationship between masked autoencoding and probabilistic modeling, highlighting their shared goals and techniques in the context of time series forecasting.", "from": "MASKED AUTOENCODING", "source_id": "9125a7d3794226f109debe90e5db041c,de8e097ce66fbc954bd529888cbc15ea", "to": "PROBABILISTIC MODELING", "width": 2.0}, {"description": "Log-likelihood is related to probabilistic modeling as log-likelihood is used to measure the probability of a model given the data", "from": "PROBABILISTIC MODELING", "source_id": "9125a7d3794226f109debe90e5db041c", "to": "LOG-LIKELIHOOD", "width": 1.0}, {"description": "Probabilistic modeling is related to temporal encoders as temporal encoders are used in time series forecasting", "from": "PROBABILISTIC MODELING", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "TEMPORAL ENCODERS", "width": 1.0}, {"description": "Temporal encoders are related to spatial-temporal encoders as spatial-temporal encoders are used in time series forecasting", "from": "TEMPORAL ENCODERS", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "SPATIAL-TEMPORAL ENCODERS", "width": 1.0}, {"description": "AMS are related to TSFM as TSFM are pre-trained models that can be fine-tuned for specific time series tasks", "from": "AMS", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "TSFM", "width": 1.0}, {"description": "TSFM is related to direct usage as direct usage is a method of adapting TSFM to specific tasks or datasets", "from": "TSFM", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "DIRECT USAGE", "width": 1.0}, {"description": "Time series tokenization is related to TSFM as TSFM are pre-trained models that can be fine-tuned for specific time series tasks", "from": "TSFM", "source_id": "de8e097ce66fbc954bd529888cbc15ea", "to": "TIME SERIES TOKENIZATION", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n**Summary of \"PROMPT ENGINEERING\" and \"TIME SERIES TOKENIZATION\"**\n\n\"PROMPT ENGINEERING\" and \"TIME SERIES TOKENIZATION\" are two related techniques used in Large Language Model (LLM)-based Time Series Forecasting Models (TSFMs). Both techniques are employed to adapt TSFMs to specific tasks or datasets, with the primary goal of improving the accuracy and effectiveness of time series forecasting.\n\n**Key Relationship:**\n\n* Prompt engineering is closely related to time series tokenization, as both techniques are used to adapt TSFMs to specific tasks or datasets.\n* Time series tokenization is a method of adapting TSFMs to specific tasks or datasets, which is also a key aspect of prompt engineering.\n\n**Context:**\n\n* The context in which these techniques are used is LLM-based TSFMs, which suggests that they are employed in a machine learning framework.\n* The primary goal of these techniques is to improve the accuracy and effectiveness of time series forecasting, which is a critical application in various fields such as finance, economics, and engineering.\n\n**Conclusion:**\n\nIn summary, \"PROMPT ENGINEERING\" and \"TIME SERIES TOKENIZATION\" are two related techniques used in LLM-based TSFMs to adapt the models to specific tasks or datasets, with the primary goal of improving the accuracy and effectiveness of time series forecasting.", "from": "PROMPT ENGINEERING", "source_id": "53f80422fbf85856ecf940d5c2450665,de8e097ce66fbc954bd529888cbc15ea", "to": "TIME SERIES TOKENIZATION", "width": 2.0}, {"description": "Traffic and climate forecasting are related as both are real-world applications of TSFMs", "from": "TRAFFIC", "source_id": "53f80422fbf85856ecf940d5c2450665", "to": "CLIMATE FORECASTING", "width": 1.0}, {"description": "The traffic dataset is a subset of the TimesFM pretraining dataset", "from": "TRAFFIC", "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "to": "TIMESFM PRETRAINING DATASET", "width": 1.0}, {"description": "NN5 is related to traffic as it is used to evaluate the performance of the Chronos model", "from": "TRAFFIC", "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "to": "NN5", "width": 1.0}, {"description": "Traffic is related to hospital as it is used to evaluate the performance of the Chronos model", "from": "TRAFFIC", "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "to": "HOSPITAL", "width": 1.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"TRAFFIC\" and \"WEATHER\" are closely related concepts that are often mentioned together in the context of time series data. Both \"TRAFFIC\" and \"WEATHER\" are domains or categories of time series data that are frequently used in short-term forecasting. The relationship between \"TRAFFIC\" and \"WEATHER\" is bidirectional, as \"TRAFFIC\" is affected by \"WEATHER\" conditions, and \"WEATHER\" affects \"TRAFFIC\" conditions. This interdependence is a key aspect of their relationship, making them closely intertwined in the context of time series forecasting.\n\nThis summary is based on the information collected from all the descriptions, and it resolves any potential contradictions by presenting a coherent and unified view of the relationship between \"TRAFFIC\" and \"WEATHER\". The summary is written in the third person and includes the entity names for context.", "from": "TRAFFIC", "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,27efab80b405b365d8e9dd9834dd1ca8,4eb417cb4bd15ceba2949a1358623cb8,91e161ba596a0cbbcae541ddb2106310,bb03c20a6fc1d9af2f3cbfa55b50cfb8,c84edbea28fbbed451e8d0b7df4ffb7c", "to": "WEATHER", "width": 6.0}, {"description": "Traffic and Uber TLC are datasets that contain hourly data", "from": "TRAFFIC", "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "to": "UBER TLC", "width": 1.0}, {"description": "Uber TLC Hourly is related to traffic as both are concepts related to transportation", "from": "TRAFFIC", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "UBER TLC HOURLY", "width": 1.0}, {"description": "Function delay is related to traffic as traffic is a specific type of function delay", "from": "TRAFFIC", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "FUNCTION DELAY", "width": 1.0}, {"description": "Traffic is related to electricity weather CPU usage as electricity weather CPU usage is a specific type of traffic", "from": "TRAFFIC", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "ELECTRICITY WEATHER CPU USAGE", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe Traffic dataset and the ILI (Influenza-Like Illness) dataset are two related datasets. Specifically, there is a connection between traffic and ILI, as ILI is a type of illness that can be spread through traffic. This implies that the Traffic dataset and the ILI dataset are linked through their potential relationship with each other, with traffic potentially being a factor in the spread of ILI.\n\nThis summary is generated by combining the information from the provided descriptions, which are:\n\n1. The Traffic dataset and the ILI dataset are both datasets.\n2. Traffic is related to ILI as ILI is a type of illness that can be spread through traffic.\n\nBy combining these two descriptions, we can infer that the Traffic dataset and the ILI dataset are related through their potential connection with each other, specifically through the spread of ILI through traffic.\n\nThis summary is written in the third person and includes the entity names for context. It also includes relevant information from the nearby text, which is that ILI is a type of illness that can be spread through traffic.", "from": "TRAFFIC", "source_id": "50eeacd99c68b2581be90310bedcbc2c,bb03c20a6fc1d9af2f3cbfa55b50cfb8", "to": "ILI", "width": 2.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe primary language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nThe text mentions two entities: \"TRAFFIC\" and \"ECL\". These entities are concepts mentioned in the text, as indicated by the use of their names. Specifically, ECL is related to traffic as it is used to forecast traffic. This suggests that ECL is a tool or method used for predicting traffic patterns or conditions.\n\nOverall, the text provides information about the relationship between ECL and traffic, indicating that ECL is used for forecasting traffic. This summary is based on the information provided in the description list and the analysis of the language used in the text.\n\nRelevant information from the nearby text that enriches this summary includes the formal and academic language used, which suggests that the text is from a research paper or technical document. The use of technical terms such as \"time series\" and \"long-term series forecasting\" also suggests that the text is related to a specific field of study, possibly transportation or urban planning.\n\nIn terms of resolving any contradictions, there are no apparent contradictions in the provided descriptions. The descriptions are consistent with each other, and the summary generated above reflects this consistency.", "from": "TRAFFIC", "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,91e161ba596a0cbbcae541ddb2106310", "to": "ECL", "width": 2.0}, {"description": "1stCount and Traffic are concepts mentioned in the text, as indicated by the use of their names", "from": "TRAFFIC", "source_id": "1d6fb60c5060c25ae4791b03a0513a7f", "to": "1STCOUNT", "width": 1.0}, {"description": "Traffic is related to traffic flow as traffic flow is a measure of traffic", "from": "TRAFFIC", "source_id": "2dbfdb45630a023a5a6979b9573a868f", "to": "TRAFFIC FLOW", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"TRAFFIC\" and \"ELECTRICITY\" are both domains or categories of time series data. Specifically, they are related as both are types of data used in short-term forecasting. This suggests that there is a connection between the two entities in the context of time series analysis and forecasting.\n\nGiven the context of time series forecasting, it is likely that the data related to \"TRAFFIC\" and \"ELECTRICITY\" involves analyzing patterns and trends in their respective time series data to make predictions about future values. This could involve techniques such as frequency analysis, multi-head cross-attention, and long-term series forecasting, which are commonly used in time series forecasting.\n\nOverall, the summary highlights the connection between \"TRAFFIC\" and \"ELECTRICITY\" as related domains in the context of time series data and forecasting.", "from": "TRAFFIC", "source_id": "27efab80b405b365d8e9dd9834dd1ca8,c84edbea28fbbed451e8d0b7df4ffb7c", "to": "ELECTRICITY", "width": 2.0}, {"description": "PatchTST (2023) is related to traffic as traffic is a type of data used in short-term forecasting", "from": "TRAFFIC", "source_id": "27efab80b405b365d8e9dd9834dd1ca8", "to": "PATCHTST (2023)", "width": 1.0}, {"description": "ETTm2 is related to traffic as traffic is a type of data used in short-term forecasting", "from": "TRAFFIC", "source_id": "27efab80b405b365d8e9dd9834dd1ca8", "to": "ETTM2", "width": 1.0}, {"description": "Reversible instance normalization is related to input transformation as input transformation uses reversible instance normalization", "from": "REVERSIBLE INSTANCE NORMALIZATION", "source_id": "fececbac281c1e2b13921f378df30919", "to": "INPUT TRANSFORMATION", "width": 1.0}, {"description": "Time series decomposition is related to explainable components as explainable components are used to identify and interpret the underlying factors that contribute to a time series", "from": "TIME SERIES DECOMPOSITION", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "EXPLAINABLE COMPONENTS", "width": 1.0}, {"description": "Embeddings are related to transformer-based architectures as transformer-based architectures use embeddings to process sequential data", "from": "EMBEDDINGS", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "TRANSFORMER-BASED ARCHITECTURES", "width": 1.0}, {"description": "Patching is related to channel independence strategy as channel independence strategy is used to extract features from a dataset", "from": "PATCHING", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "CHANNEL INDEPENDENCE STRATEGY", "width": 1.0}, {"description": "Fine-tuning strategies are related to privacy-preserved manner as privacy-preserved manner is used to protect sensitive information while still allowing for the use of machine learning models", "from": "FINE-TUNING STRATEGIES", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "PRIVACY-PRESERVED MANNER", "width": 1.0}, {"description": "Multi-modality is related to task-oriented FMs as task-oriented FMs use multi-modality to process multiple types of data or inputs", "from": "MULTI-MODALITY", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "TASK-ORIENTED FMS", "width": 1.0}, {"description": "External ChatGPT is related to evolving graph structure as evolving graph structure is used to represent companies based on news headlines", "from": "EXTERNAL CHATGPT", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "EVOLVING GRAPH STRUCTURE", "width": 1.0}, {"description": "News headlines are related to stock prices as stock prices are influenced by news headlines", "from": "NEWS HEADLINES", "source_id": "83f62beddf5cf53ed2e2d4517569deb8", "to": "STOCK PRICES", "width": 1.0}, {"description": "Patches can be reprogrammed to modify or update a time series patch", "from": "REPROGRAMMING", "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8", "to": "PATCH", "width": 1.0}, {"description": "Reprogramming can be used to provide natural language guidance via Prompt-as-Prefix", "from": "REPROGRAMMING", "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8", "to": "PROMPT-AS-PREFIX", "width": 1.0}, {"description": "Language task is related to reprogramming as reprogramming can be used to adapt LLMs to new tasks or data", "from": "REPROGRAMMING", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "LANGUAGE TASK", "width": 1.0}, {"description": "Reprogramming is related to optimal reprogramming representations as optimal reprogramming representations are needed for effective reprogramming", "from": "REPROGRAMMING", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "OPTIMAL REPROGRAMMING REPRESENTATIONS", "width": 1.0}, {"description": "GPT4TS uses reprogramming to modify or update its parameters", "from": "REPROGRAMMING", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "GPT4TS", "width": 1.0}, {"description": "Time-series forecasting is related to decoder-only foundation model as the model is designed for time-series forecasting", "from": "DECODER-ONLY FOUNDATION MODEL", "source_id": "323c49cf157623a685283fcfc9b05491", "to": "TIME-SERIES FORECASTING", "width": 1.0}, {"description": "Time-series corpus is related to decoder-only foundation models as they are trained on time-series data", "from": "DECODER-ONLY FOUNDATION MODEL", "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "to": "TIME-SERIES CORPUS", "width": 1.0}, {"description": "Logits are related to decoder-only foundation model as logits are the output of the neural network before applying the softmax function", "from": "DECODER-ONLY FOUNDATION MODEL", "source_id": "4ab34d32601d452f14b4a1c31415292d", "to": "LOGITS", "width": 1.0}, {"description": "Decoder-only foundation model is related to Google Trends as Google Trends is a data source used for pretraining the model", "from": "DECODER-ONLY FOUNDATION MODEL", "source_id": "4ab34d32601d452f14b4a1c31415292d", "to": "GOOGLE TRENDS", "width": 1.0}, {"description": "Decoder-only foundation model is related to Wiki Pageviews as Wiki Pageviews is a data source used for pretraining the model", "from": "DECODER-ONLY FOUNDATION MODEL", "source_id": "4ab34d32601d452f14b4a1c31415292d", "to": "WIKI PAGEVIEWS", "width": 1.0}, {"description": "Decoder-only foundation model is related to synthetic time-series as synthetic time-series is a data source used for pretraining the model", "from": "DECODER-ONLY FOUNDATION MODEL", "source_id": "4ab34d32601d452f14b4a1c31415292d", "to": "SYNTHETIC TIME-SERIES", "width": 1.0}, {"description": "Decoder-only foundation model is related to encoder-decoder style training as it is a type of training method used for the model", "from": "DECODER-ONLY FOUNDATION MODEL", "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "to": "ENCODER-DECODER STYLE TRAINING", "width": 1.0}, {"description": "Context window is related to decoder-only foundation model as decoder-only foundation model uses context window to make predictions", "from": "DECODER-ONLY FOUNDATION MODEL", "source_id": "500710c8a95f1310d383fa71fd806c1c", "to": "CONTEXT WINDOW", "width": 1.0}, {"description": "Decoder-only foundation model is related to Informer as both are used for time-series forecasting", "from": "DECODER-ONLY FOUNDATION MODEL", "source_id": "500710c8a95f1310d383fa71fd806c1c", "to": "INFORMER", "width": 1.0}, {"description": "Time-series forecasting is related to time-series as time-series data is used for forecasting", "from": "TIME-SERIES", "source_id": "323c49cf157623a685283fcfc9b05491", "to": "TIME-SERIES FORECASTING", "width": 1.0}, {"description": "Time-series is broken down into patches in the input layers of TimesFM", "from": "TIME-SERIES", "source_id": "3cf566c27c75f886658002bf9b3b0b15", "to": "PATCH", "width": 1.0}, {"description": "Time series is related to wiki pageviews as wiki pageviews captures hourly views of all Wikimedia pages", "from": "TIME-SERIES", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "WIKI PAGEVIEWS", "width": 1.0}, {"description": "Time series is related to ARMA processes as ARMA processes are a type of time-series model", "from": "TIME-SERIES", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "ARMA PROCESSES", "width": 1.0}, {"description": "Time series is related to seasonal patterns as seasonal patterns refer to periodic fluctuations in time-series data", "from": "TIME-SERIES", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "SEASONAL PATTERNS", "width": 1.0}, {"description": "Time series is related to trends as trends refer to long-term patterns or directions in time-series data", "from": "TIME-SERIES", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "TRENDS", "width": 1.0}, {"description": "Time series is related to step functions as step functions are a type of time-series model", "from": "TIME-SERIES", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "STEP FUNCTIONS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n**Summary of \"DEEP LEARNING MODELS\" and \"ARIMA\"**\n\n\"DEEP LEARNING MODELS\" and \"ARIMA\" are two distinct entities in the realm of time series forecasting. According to the analysis, \"DEEP LEARNING MODELS\" have been shown to outperform \"ARIMA\" in forecasting tasks. This is further supported by the fact that deep learning models have been demonstrated to outperform traditional statistical methods, including \"ARIMA\", in various studies. The superior performance of deep learning models can be attributed to their ability to learn complex patterns and relationships in data, which is not easily captured by traditional statistical methods like \"ARIMA\". Overall, the comparison between \"DEEP LEARNING MODELS\" and \"ARIMA\" highlights the growing importance of deep learning techniques in time series forecasting and the need to re-evaluate the role of traditional statistical methods in this context.\n\n**Key Findings:**\n\n* \"DEEP LEARNING MODELS\" outperform \"ARIMA\" in forecasting tasks.\n* Deep learning models outperform traditional statistical methods, including \"ARIMA\".\n* The superior performance of deep learning models can be attributed to their ability to learn complex patterns and relationships in data.\n\n**Relevant Information:**\n\n* The comparison between \"DEEP LEARNING MODELS\" and \"ARIMA\" is relevant to the field of time series forecasting.\n* The analysis highlights the growing importance of deep learning techniques in time series forecasting.\n* The need to re-evaluate the role of traditional statistical methods, including \"ARIMA\", in the context of deep learning models is emphasized.", "from": "DEEP LEARNING MODELS", "source_id": "323c49cf157623a685283fcfc9b05491,ff641d7e46d16e86c55d25c86b49bd52", "to": "ARIMA", "width": 2.0}, {"description": "Salinas et al. is related to deep learning models as Salinas et al. is a study on deep learning models for time series applications", "from": "DEEP LEARNING MODELS", "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "to": "SALINAS ET AL.", "width": 1.0}, {"description": "Deep learning models are related to quantization as quantization is used to convert real-valued time series into discrete tokens", "from": "DEEP LEARNING MODELS", "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "to": "QUANTIZATION", "width": 1.0}, {"description": "ARIMA is related to GARCH as both are statistical models used for forecasting", "from": "ARIMA", "source_id": "323c49cf157623a685283fcfc9b05491", "to": "GARCH", "width": 1.0}, {"description": "ARIMA and exponential smoothing are traditional statistical methods used for time-series forecasting", "from": "ARIMA", "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "to": "EXPOENTIAL SMOOTHING", "width": 1.0}, {"description": "Darts and ARIMA are models that perform well for time-series forecasting tasks", "from": "ARIMA", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "DARTS", "width": 1.0}, {"description": "GP and ARIMA are models that perform well for time-series forecasting tasks", "from": "ARIMA", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "GP", "width": 1.0}, {"description": "ARIMA and TCN are models that perform well for time-series forecasting tasks", "from": "ARIMA", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "TCN", "width": 1.0}, {"description": "ARIMA and N-BEATS are models that perform well for time-series forecasting tasks", "from": "ARIMA", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "N-BEATS", "width": 1.0}, {"description": "ARIMA and N-HITS are models that perform well for time-series forecasting tasks", "from": "ARIMA", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "N-HITS", "width": 1.0}, {"description": "ARIMA and llmtime(ZS) are models that perform well for time-series forecasting tasks", "from": "ARIMA", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "LLMTIME(ZS)", "width": 1.0}, {"description": "ARIMA and NAIVE are models that perform well for time-series forecasting tasks", "from": "ARIMA", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "NAIVE", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nARIMA and ETS are statistical models used for time series forecasting. Specifically, ARIMA (AutoRegressive Integrated Moving Average) is a model used for evaluating time series data, while ETS (Exponential Smoothing) is also a statistical model used for time series forecasting. Notably, ETS is related to ARIMA, as ARIMA is often used as a model for evaluating the performance of ETS models.\n\nThis summary is written in third person and includes the entity names, providing a clear and concise description of the relationship between ARIMA and ETS. The information is also enriched with relevant details from the nearby text, highlighting the specific applications and relationships between the two models.", "from": "ARIMA", "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae,6771b6279846e780d6807b15184ae008", "to": "ETS", "width": 2.0}, {"description": "ARIMA is related to PR as PR is a model used for evaluation", "from": "ARIMA", "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "to": "PR", "width": 1.0}, {"description": "Arima is compared to TimesFM in the Air Passenger dataset", "from": "ARIMA", "source_id": "e6ec99a117b9abd42452ff51cd6e8f0b", "to": "AIR PASSENGER DATASET", "width": 1.0}, {"description": "Prophet and ARIMA are models that were excluded from the analysis due to their prohibitive computational requirements and extensive training times", "from": "ARIMA", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "PROPHET", "width": 1.0}, {"description": "Statistical models are related to ARIMA as ARIMA is a type of statistical model", "from": "ARIMA", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "STATISTICAL MODELS", "width": 1.0}, {"description": "M5 competition is related to IARAI Traffic4cast contest as both are forecasting competitions", "from": "M5 COMPETITION", "source_id": "323c49cf157623a685283fcfc9b05491", "to": "IARAI TRAFFIC4CAST CONTEST", "width": 1.0}, {"description": "Abhimanyu Das is related to Weihao Kong as they are co-authors of the paper", "from": "ABHIMANYU DAS", "source_id": "323c49cf157623a685283fcfc9b05491", "to": "WEIHAO KONG", "width": 1.0}, {"description": "Abhimanyu Das is related to Rajat Sen as they are co-authors of the paper", "from": "ABHIMANYU DAS", "source_id": "323c49cf157623a685283fcfc9b05491", "to": "RAJAT SEN", "width": 1.0}, {"description": "Abhimanyu Das is related to Yichen Zhou as they are co-authors of the paper", "from": "ABHIMANYU DAS", "source_id": "323c49cf157623a685283fcfc9b05491", "to": "YICHEN ZHOU", "width": 1.0}, {"description": "Time-series data is related to time-series forecasting as forecasting is the primary application of time-series data", "from": "TIME-SERIES FORECASTING", "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "to": "TIME-SERIES DATA", "width": 1.0}, {"description": "Pretrained models are related to time-series foundation models as foundation models are a type of pretrained model designed for time-series forecasting tasks", "from": "PRETRAINED MODELS", "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "to": "TIME-SERIES FOUNDATION MODEL", "width": 1.0}, {"description": "Chronos models outperform pretrained models on Benchmark I, which were trained on a large corpus of time series data", "from": "PRETRAINED MODELS", "source_id": "2eea45c6111e468189580a21686cb14a", "to": "BENCHMARK I", "width": 1.0}, {"description": "Chronos models outperform pretrained models on Benchmark II", "from": "PRETRAINED MODELS", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "CHRONOS MODELS", "width": 1.0}, {"description": "Task-specific models need to be trained for each task individually", "from": "PRETRAINED MODELS", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "TASK-SPECIFIC MODELS", "width": 1.0}, {"description": "Time-series foundation models are related to zero-shot performance as they are designed to perform well on time-series forecasting tasks without additional training", "from": "TIME-SERIES FOUNDATION MODEL", "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "to": "ZERO-SHOT PERFORMANCE", "width": 1.0}, {"description": "Zero-shot performance is related to test datasets as the Chronos model is evaluated on test datasets", "from": "ZERO-SHOT PERFORMANCE", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "TEST DATASETS", "width": 1.0}, {"description": "Task-specific deep learning baselines are compared to Chronos models in terms of their zero-shot performance, with Chronos models performing competitively", "from": "ZERO-SHOT PERFORMANCE", "source_id": "6c273e9f841addeed2a33240ddaa3d96", "to": "TASK-SPECIFIC DEEP LEARNING BASELINES", "width": 1.0}, {"description": "Zero-shot performance is related to Lag-Llama as Lag-Llama demonstrated strong zero-shot performance", "from": "ZERO-SHOT PERFORMANCE", "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "to": "LAG-LAGA", "width": 1.0}, {"description": "Zero-shot performance is related to Lag-Llama as Lag-Llama is a model that demonstrates zero-shot performance", "from": "ZERO-SHOT PERFORMANCE", "source_id": "d08a82328191907abfcdb72efab9a6cf", "to": "LAG-LAGGAGE", "width": 1.0}, {"description": "The time-series corpus is used for training and finetuning the model", "from": "TIME-SERIES CORPUS", "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "to": "FINE-TUNED MODEL", "width": 1.0}, {"description": "Synthetic data is related to ARMA processes as ARMA processes are used to generate synthetic data", "from": "SYNTHETIC DATA", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "ARMA PROCESSES", "width": 1.0}, {"description": "Synthetic data is related to seasonal patterns as seasonal patterns are used to generate synthetic data", "from": "SYNTHETIC DATA", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "SEASONAL PATTERNS", "width": 1.0}, {"description": "Synthetic data is related to trends as trends are used to generate synthetic data", "from": "SYNTHETIC DATA", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "TRENDS", "width": 1.0}, {"description": "Synthetic data is related to step functions as step functions are used to generate synthetic data", "from": "SYNTHETIC DATA", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "STEP FUNCTIONS", "width": 1.0}, {"description": "Dataset ablation showcases the need for synthetic data", "from": "SYNTHETIC DATA", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "DATASET ABLATION", "width": 1.0}, {"description": "ForecastPFN trains a transformer-based model purely on synthetic data", "from": "SYNTHETIC DATA", "source_id": "bc54a718d1886698232d578fd88c3ac7", "to": "FORECASTPFN", "width": 1.0}, {"description": "Synthetic data is related to KernelSynth data as they are both used for training and testing", "from": "SYNTHETIC DATA", "source_id": "c7f04fa1168df64cce110e20a1b72b51", "to": "KERNELSYNTH DATA", "width": 1.0}, {"description": "Figure 10b is related to synthetic data as it shows the performance of models trained with different proportions of synthetic data", "from": "SYNTHETIC DATA", "source_id": "c7f04fa1168df64cce110e20a1b72b51", "to": "FIGURE 10B", "width": 1.0}, {"description": "Chronos-T5 was trained on synthetic data", "from": "SYNTHETIC DATA", "source_id": "a90c6ca26542ea5935f9d8d5bf3688a9", "to": "CHRONOS-T5", "width": 1.0}, {"description": "Input patching is related to attention architecture as it is used in some models that use attention architecture", "from": "INPUT PATCHING", "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "to": "ATTENTION ARCHITECTURE", "width": 1.0}, {"description": "Attention architecture is related to parameter size as the complexity of the architecture can affect the number of parameters", "from": "ATTENTION ARCHITECTURE", "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "to": "PARAMETER SIZE", "width": 1.0}, {"description": "Parameter size is related to pretraining data size as the amount of data used to train a model can affect its complexity and performance", "from": "PARAMETER SIZE", "source_id": "dd9bcf836b1de9b8c99d5ba8188a5ed4", "to": "PRETRAINING DATA SIZE", "width": 1.0}, {"description": "Prophet is a non-autoregressive model used for time-series forecasting, which is a type of local univariate model", "from": "PROPHET", "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "to": "LOCAL UNIVARIATE MODELS", "width": 1.0}, {"description": "Local univariate models are trained individually for each time-series, while global univariate models are trained globally on many time-series", "from": "LOCAL UNIVARIATE MODELS", "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "to": "GLOBAL UNIVARIATE MODELS", "width": 1.0}, {"description": "Global univariate models are a type of model that is trained globally on many time-series, while global multivariate models take in the past of all time-series in the dataset to predict the future of all the time-series", "from": "GLOBAL UNIVARIATE MODELS", "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "to": "GLOBAL MULTIVARIATE MODELS", "width": 1.0}, {"description": "Global multivariate models are a type of model that takes in the past of all time-series in the dataset to predict the future of all the time-series, which is similar to the VAR model", "from": "GLOBAL MULTIVARIATE MODELS", "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "to": "VAR MODEL", "width": 1.0}, {"description": "N-BEATS uses transfer learning between various source-target dataset pairs", "from": "N-BEATS", "source_id": "ff641d7e46d16e86c55d25c86b49bd52", "to": "TRANSFER LEARNING", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"N-BEATS\" and \"DARTS\" are two models that have been identified as performing well for time-series forecasting tasks. Specifically, \"N-BEATS\" is mentioned in the text as a model that is related to \"DARTS\", suggesting a connection or similarity between the two models. This connection is further supported by the fact that both models are used for time-series forecasting, a task that involves predicting future values in a sequence of data points.\n\nIt is worth noting that the text does not provide further information on the specific characteristics or differences between \"N-BEATS\" and \"DARTS\", but rather highlights their shared application in time-series forecasting. However, the fact that they are both mentioned in the context of time-series forecasting suggests that they may be competing or complementary models, or that they may be used in different scenarios or with different data sets.\n\nOverall, the summary provides a concise overview of the two entities and their relationship, highlighting their shared application in time-series forecasting and their connection to each other.", "from": "N-BEATS", "source_id": "2bc70082c142ee2ef5d6a941611505b7,d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "DARTS", "width": 2.0}, {"description": "GP and N-BEATS are models that perform well for time-series forecasting tasks", "from": "N-BEATS", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "GP", "width": 1.0}, {"description": "TCN and N-BEATS are models that perform well for time-series forecasting tasks", "from": "N-BEATS", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "TCN", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nN-BEATS and N-HITS are two time series models that have been compared and evaluated in the context of time-series forecasting tasks. Both models are mentioned in the text as being used for this purpose. A comparison of the two models is presented in a table, where their performance is assessed. According to the information provided, N-HITS places 2nd in terms of point forecasting performance, while N-BEATS places 3rd. This suggests that both models are capable of performing well in time-series forecasting tasks, but N-HITS has a slight edge over N-BEATS in terms of point forecasting performance.\n\nIt is worth noting that the text does not provide any information on the specific architecture or implementation details of N-BEATS and N-HITS. However, based on the context, it appears that both models are designed for time-series forecasting tasks and have been evaluated using a common set of metrics.\n\nOverall, the summary provides a concise overview of the relationship between N-BEATS and N-HITS, highlighting their shared purpose as time series models and their relative performance in a comparison study.", "from": "N-BEATS", "source_id": "532a6d434dab611a80aef1e94dd2fb45,5e4d9ca02ee6a285d5223c820743eb12,9ba0189af2ef0720a721c16eef0f0788,af36d1634490149b96980fb1dff57cd1,d40f5dc25597e4e4d7c30b8bfd98f89a,f49330b6fd81d86d14e7a9d4b8e45576", "to": "N-HITS", "width": 6.0}, {"description": "N-BEATS and llmtime(ZS) are models that perform well for time-series forecasting tasks", "from": "N-BEATS", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "LLMTIME(ZS)", "width": 1.0}, {"description": "N-BEATS and NAIVE are models that perform well for time-series forecasting tasks", "from": "N-BEATS", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "NAIVE", "width": 1.0}, {"description": "WaveNet is related to N-BEATS as both are models mentioned in the text", "from": "N-BEATS", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "WAVENET", "width": 1.0}, {"description": "DeepAR is related to N-BEATS as both are models mentioned in the text", "from": "N-BEATS", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "DEEPAR", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"N-BEATS\" and \"TFT\" are two models/methods mentioned in the text that are related to each other. Both \"N-BEATS\" and \"TFT\" are methods for probabilistic forecasting, indicating that they share a common purpose in the context of time series analysis. The text does not provide further information on the specific differences or similarities between these two models, but it is clear that they are both relevant to the field of probabilistic forecasting.\n\nIt is worth noting that the text does not provide any information on the language of the text, as the analysis provided earlier was not part of the original data. However, based on the context and the information provided, it can be inferred that the text is likely written in English, given the use of technical terms and mathematical equations that are commonly used in English-language academic papers.\n\nIn terms of enriching the summary with relevant information from the nearby text, it is not possible to do so without access to the surrounding text. However, based on the information provided, it can be inferred that the text is likely discussing the application of \"N-BEATS\" and \"TFT\" in the context of time series forecasting, and may be comparing or contrasting the two models in some way.", "from": "N-BEATS", "source_id": "af36d1634490149b96980fb1dff57cd1,f0c52387a7b3a5c3850fe6991f0a7c83", "to": "TFT", "width": 2.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nN-BEATS and DLINER are two entities that are related to each other in the context of time series forecasting. Both models are mentioned in the text and are compared with each other. Specifically, DLINER is compared with N-BEATS, suggesting that they are competing models or have similar applications in the field of time series forecasting.\n\nGiven the context of time series forecasting, it can be inferred that both N-BEATS and DLINER are machine learning models designed to predict future values in a time series based on historical data. The fact that they are compared with each other suggests that they have similar strengths and weaknesses, and researchers are interested in evaluating their performance and comparing their results.\n\nOverall, the summary provides a clear understanding of the relationship between N-BEATS and DLINER, and highlights their relevance to the field of time series forecasting.\n\nAdditional information that can be inferred from the text is that both N-BEATS and DLINER are likely to be deep learning models, given the mention of \"multi-head cross-attention\" in the text, which is a technique commonly used in deep learning architectures. However, this information is not explicitly stated in the provided descriptions, but can be inferred based on the context and the technical terms used in the text.", "from": "N-BEATS", "source_id": "41fe893a178ebc8790ef4da83da5ab6e,5e4d9ca02ee6a285d5223c820743eb12,af36d1634490149b96980fb1dff57cd1", "to": "DLINER", "width": 3.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nN-BEATS and GPT4TS are two entities that are related to each other in terms of performance comparison. Both models are mentioned in the text, suggesting that they are being evaluated or compared in some capacity. The exact nature of their comparison is not specified, but it is mentioned that GPT4TS is compared to N-BEATS in terms of performance.\n\nGiven the context of the text, which appears to be a technical document or research paper, it is likely that N-BEATS and GPT4TS are machine learning models, possibly used for time series forecasting or a related task. The mention of \"time series\" and \"long-term series forecasting\" in the text supports this interpretation.\n\nThe use of technical terms and mathematical equations in the text also suggests that the comparison between N-BEATS and GPT4TS is likely to be a technical or academic evaluation, possibly involving frequency analysis or multi-head cross-attention mechanisms.\n\nOverall, the summary of the data suggests that N-BEATS and GPT4TS are two related machine learning models that are being compared in terms of performance, likely in the context of time series forecasting or a related task.", "from": "N-BEATS", "source_id": "9ba0189af2ef0720a721c16eef0f0788,af36d1634490149b96980fb1dff57cd1", "to": "GPT4TS", "width": 2.0}, {"description": "N-BEATS is related to local statistical models as N-BEATS performs better than local statistical models", "from": "N-BEATS", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "N-BEATS is used for inference on AWS EC2", "from": "N-BEATS", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "AWS EC2", "width": 1.0}, {"description": "N-BEATS is related to Informer as both are methods for probabilistic forecasting", "from": "N-BEATS", "source_id": "f0c52387a7b3a5c3850fe6991f0a7c83", "to": "INFORMER", "width": 1.0}, {"description": "M4 is related to N-BEATS as N-BEATS is used for short-term time series forecasting on M4", "from": "N-BEATS", "source_id": "d4551c2839eaa68a7cb7324089956581", "to": "M4", "width": 1.0}, {"description": "ILI is related to N-BEATS as it is a forecasting horizon used in the model", "from": "N-BEATS", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "ILI", "width": 1.0}, {"description": "N-BEATS and AutoARIMA are models used for comparison in the text", "from": "N-BEATS", "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "to": "AUTOARIMA", "width": 1.0}, {"description": "N-BEATS is related to ETSformer as both are models used in time series forecasting", "from": "N-BEATS", "source_id": "f49330b6fd81d86d14e7a9d4b8e45576", "to": "ETSFORMER", "width": 1.0}, {"description": "Deep forecasters are related to transfer learning as transfer learning can be used to adapt deep forecasters to new tasks", "from": "TRANSFER LEARNING", "source_id": "0bef137d159ccc86cdee0a8be788bd26", "to": "DEEP FORECASTERS", "width": 1.0}, {"description": "Transfer learning is related to domain adaptation as domain adaptation can be used to adapt models trained on one dataset to another", "from": "TRANSFER LEARNING", "source_id": "0bef137d159ccc86cdee0a8be788bd26", "to": "DOMAIN ADAPTATION", "width": 1.0}, {"description": "Transfer learning is related to zero-shot learning as zero-shot learning is a type of transfer learning", "from": "TRANSFER LEARNING", "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "to": "ZERO-SHOT LEARNING", "width": 1.0}, {"description": "Hassan Ismail Fawaz is related to transfer learning as he has published a paper on the topic", "from": "TRANSFER LEARNING", "source_id": "a73df99fe49b288e1c8751be2008b191", "to": "HASSAN ISMAIL FAWAZ", "width": 1.0}, {"description": "Context is related to horizon as horizon is the number of future time points that are predicted", "from": "CONTEXT", "source_id": "6f6e27166a1506482bfcaf5585322595", "to": "HORIZON", "width": 1.0}, {"description": "MSE and MAE are both evaluation metrics", "from": "MEAN ABSOLUTE ERROR", "source_id": "50eeacd99c68b2581be90310bedcbc2c", "to": "MEAN SQUARE ERROR", "width": 1.0}, {"description": "MAE and MASE are both evaluation metrics", "from": "MEAN ABSOLUTE ERROR", "source_id": "50eeacd99c68b2581be90310bedcbc2c", "to": "MEAN ABSOLUTE SCALDED ERROR", "width": 1.0}, {"description": "Patch based modeling is related to patch length as patch length is a key component of patch based modeling", "from": "PATCH BASED MODELING", "source_id": "6f6e27166a1506482bfcaf5585322595", "to": "PATCH LENGTH", "width": 1.0}, {"description": "Long horizon forecasting is related to patch based modeling as patch based modeling is used in long horizon forecasting", "from": "PATCH BASED MODELING", "source_id": "f98d2bec31738be3f7be750b5fe6180e", "to": "LONG HORIZON FORECASTING", "width": 1.0}, {"description": "Token is related to patch as patch is used to represent token", "from": "TOKEN", "source_id": "f98d2bec31738be3f7be750b5fe6180e", "to": "PATCH", "width": 1.0}, {"description": "Vocabulary is related to token as token refers to a single unit of data", "from": "TOKEN", "source_id": "6eb4c16edf2eedfd03721efb199478d8", "to": "VOCABULARY", "width": 1.0}, {"description": "Token is related to scale as scale refers to the range or magnitude of a time series", "from": "TOKEN", "source_id": "6eb4c16edf2eedfd03721efb199478d8", "to": "SCALE", "width": 1.0}, {"description": "Patch size is determined by the input patch length in TimesFM", "from": "PATCH", "source_id": "3cf566c27c75f886658002bf9b3b0b15", "to": "INPUT PATCH LENGTH", "width": 1.0}, {"description": "Patch is related to output patch length as output patch length is the number of time-points predicted by the model", "from": "PATCH", "source_id": "4ab34d32601d452f14b4a1c31415292d", "to": "OUTPUT PATCH LENGTH", "width": 1.0}, {"description": "Model is related to patch as patch is a specific section or segment of a time series that is often used in data analysis and forecasting", "from": "PATCH", "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "to": "MODEL", "width": 1.0}, {"description": "Patch is related to patch 5 as patch 5 is an example of patch", "from": "PATCH", "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "to": "PATCH 5", "width": 9.0}, {"description": "Patch is related to reprogramming as reprogramming is used to modify patch", "from": "PATCH", "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a014d14e003d0998b877eacffa8ebfc4,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "to": "REPROGRAM", "width": 10.0}, {"description": "ICLR 2024 is related to patch as patch is a concept mentioned in the text", "from": "PATCH", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "ICLR 2024", "width": 1.0}, {"description": "Patch is related to patch embeddings as patch embeddings represent patches", "from": "PATCH", "source_id": "fececbac281c1e2b13921f378df30919", "to": "PATCH EMBEDDINGS", "width": 1.0}, {"description": "Local semantic information is related to patch as patch represents local semantic information", "from": "PATCH", "source_id": "fececbac281c1e2b13921f378df30919", "to": "LOCAL SEMANTIC INFORMATION", "width": 1.0}, {"description": "Horizontal sliding stride is related to patch as patch is extracted using horizontal sliding stride", "from": "PATCH", "source_id": "fececbac281c1e2b13921f378df30919", "to": "HORIZONTAL SLIDING STRIDE", "width": 1.0}, {"description": "Patch is used to generate representations that refer to the output of the language model after processing the input", "from": "PATCH", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "REPRESENTATIONS", "width": 1.0}, {"description": "Text prototypes can be used to represent patches", "from": "PATCH", "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8", "to": "TEXT PROTOTYPES", "width": 1.0}, {"description": "LLM is related to context window as context window is used in LLM", "from": "LLM", "source_id": "f98d2bec31738be3f7be750b5fe6180e", "to": "CONTEXT WINDOW", "width": 1.0}, {"description": "LLM is related to BART as BART is a type of LLM", "from": "LLM", "source_id": "1f1221583d838c2407fe9864225e9eda", "to": "BART", "width": 1.0}, {"description": "LLM is related to T5 as T5 is a type of LLM", "from": "LLM", "source_id": "1f1221583d838c2407fe9864225e9eda", "to": "T5", "width": 1.0}, {"description": "LLM is related to LLAMA 2 as LLAMA 2 is a type of LLM", "from": "LLM", "source_id": "1f1221583d838c2407fe9864225e9eda", "to": "LLAMA 2", "width": 1.0}, {"description": "Patch embeddings are used in LLM to generate high-precision numerals with precision and efficiency", "from": "LLM", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "PATCH EMBEDDINGS", "width": 1.0}, {"description": "LLM is related to ICLR 2024 as it is a conference where the paper was presented", "from": "LLM", "source_id": "41fe893a178ebc8790ef4da83da5ab6e", "to": "ICLR 2024", "width": 1.0}, {"description": "Reprogramming framework is related to LLM as LLM is used in reprogramming framework", "from": "LLM", "source_id": "7e97089185883c456c798ddc5ec86373", "to": "REPROGRAMMING FRAMEWORK", "width": 1.0}, {"description": "LLM is related to QLoRA as QLoRA is used to fine-tune LLM", "from": "LLM", "source_id": "7e97089185883c456c798ddc5ec86373", "to": "QLORA", "width": 1.0}, {"description": "Context window is related to horizon length as horizon length is used in context window", "from": "CONTEXT WINDOW", "source_id": "f98d2bec31738be3f7be750b5fe6180e", "to": "HORIZON LENGTH", "width": 1.0}, {"description": "Horizon length is related to zero-shot forecasting as zero-shot forecasting is used to handle horizon length", "from": "HORIZON LENGTH", "source_id": "f98d2bec31738be3f7be750b5fe6180e", "to": "ZERO-SHOT FORECASTING", "width": 1.0}, {"description": "Horizon length is related to context length as both concepts are used in time-series forecasting", "from": "HORIZON LENGTH", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "CONTEXT LENGTH", "width": 1.0}, {"description": "Zero-shot forecasting is related to patch masking as patch masking is used in zero-shot forecasting", "from": "ZERO-SHOT FORECASTING", "source_id": "f98d2bec31738be3f7be750b5fe6180e", "to": "PATCH MASKING", "width": 1.0}, {"description": "ForecastPFN tackles the problem of zero-shot forecasting", "from": "ZERO-SHOT FORECASTING", "source_id": "bc54a718d1886698232d578fd88c3ac7", "to": "FORECASTPFN", "width": 1.0}, {"description": "Reprogrammed LLM is used for zero-shot forecasting tasks", "from": "ZERO-SHOT FORECASTING", "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "to": "REPROGRAMMED LLM", "width": 1.0}, {"description": "Patch masking is a type of masking used in TimesFM", "from": "PATCH MASKING", "source_id": "3cf566c27c75f886658002bf9b3b0b15", "to": "MASKING", "width": 1.0}, {"description": "Patch masking is used in the residual block of TimesFM", "from": "PATCH MASKING", "source_id": "3cf566c27c75f886658002bf9b3b0b15", "to": "RESIDUAL BLOCK", "width": 1.0}, {"description": "There is a trade-off between input patch length and output patch length in TimesFM", "from": "INPUT PATCH LENGTH", "source_id": "3cf566c27c75f886658002bf9b3b0b15", "to": "OUTPUT PATCH LENGTH", "width": 1.0}, {"description": "Output patch length affects the number of auto-regressive generation steps in TimesFM", "from": "OUTPUT PATCH LENGTH", "source_id": "3cf566c27c75f886658002bf9b3b0b15", "to": "AUTO-REGRESSIVE GENERATION", "width": 1.0}, {"description": "The fully connected layer is used to map the output tokens to predictions, as described in the text", "from": "OUTPUT PATCH LENGTH", "source_id": "ee8135f4497807724f665a5cdaa775fb", "to": "FULLY CONNECTED LAYER", "width": 1.0}, {"description": "Output patch length is related to model dimension as they are both hyperparameters of the TimesFM model, as described in the text", "from": "OUTPUT PATCH LENGTH", "source_id": "ee8135f4497807724f665a5cdaa775fb", "to": "MODEL DIMENSION", "width": 1.0}, {"description": "Number of heads is related to output patch length as they are both hyperparameters of the TimesFM model, as described in the text", "from": "OUTPUT PATCH LENGTH", "source_id": "ee8135f4497807724f665a5cdaa775fb", "to": "NUMBER OF HEADS", "width": 1.0}, {"description": "Model sizes are related to output patch length as output patch length affects the performance of a model", "from": "OUTPUT PATCH LENGTH", "source_id": "f7ce89346ea6715560163ef3a630a5df", "to": "MODEL SIZES", "width": 1.0}, {"description": "Auto-regressive generation is affected by masking in TimesFM", "from": "AUTO-REGRESSIVE GENERATION", "source_id": "3cf566c27c75f886658002bf9b3b0b15", "to": "MASKING", "width": 1.0}, {"description": "Residual block is a type of MLP block in TimesFM", "from": "RESIDUAL BLOCK", "source_id": "3cf566c27c75f886658002bf9b3b0b15", "to": "MULTI-LAYER PERCEPTRON", "width": 1.0}, {"description": "Residual block is related to output residual block as output residual block is a component of the neural network architecture used for time-series forecasting", "from": "RESIDUAL BLOCK", "source_id": "4ab34d32601d452f14b4a1c31415292d", "to": "OUTPUT RESIDUAL BLOCK", "width": 1.0}, {"description": "MLP block uses positional encoding in TimesFM", "from": "MULTI-LAYER PERCEPTRON", "source_id": "3cf566c27c75f886658002bf9b3b0b15", "to": "POSITIONAL ENCODING", "width": 1.0}, {"description": "Self-attention is related to positional encoding as positional encoding is used to enrich the input", "from": "POSITIONAL ENCODING", "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "to": "SELF-ATTENTION", "width": 1.0}, {"description": "Positional encoding is related to input embedding as input embedding is used to enrich the input", "from": "POSITIONAL ENCODING", "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "to": "INPUT EMBEDDING", "width": 1.0}, {"description": "Positional encoding is used to incorporate information about the position of a data point in the sequence", "from": "POSITIONAL ENCODING", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "INPUT", "width": 1.0}, {"description": "Layer norm is related to transformer layers as transformer layers use layer norm", "from": "TRANSFORMER LAYERS", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "LAYER NORM", "width": 1.0}, {"description": "Transformer layers are related to residual blocks as residual blocks are used in transformer layers", "from": "TRANSFORMER LAYERS", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "RESIDUAL BLOCKS", "width": 1.0}, {"description": "Backbone model layers are related to transformer layers as transformer layers are a type of backbone model layer", "from": "TRANSFORMER LAYERS", "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "to": "BACKBONE MODEL LAYERS", "width": 1.0}, {"description": "Transformer layers are related to forecasting accuracy as transformer layers affect the model\u0027s forecasting accuracy", "from": "TRANSFORMER LAYERS", "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "to": "FORECASTING ACCURACY", "width": 1.0}, {"description": "The stacked transformer uses self-attention as a type of attention mechanism, as described in the text", "from": "STACKED TRANSFORMER", "source_id": "ee8135f4497807724f665a5cdaa775fb", "to": "SELF-ATTENTION", "width": 1.0}, {"description": "The stacked transformer uses causal attention as a type of attention mechanism, as described in the text", "from": "STACKED TRANSFORMER", "source_id": "ee8135f4497807724f665a5cdaa775fb", "to": "CAUSAL ATTENTION", "width": 1.0}, {"description": "The stacked transformer uses a fully connected layer as a type of layer, as described in the text", "from": "STACKED TRANSFORMER", "source_id": "ee8135f4497807724f665a5cdaa775fb", "to": "FULLY CONNECTED LAYER", "width": 1.0}, {"description": "Self-attention and causal attention are both types of attention mechanisms used in the TimesFM model, as described in the text", "from": "SELF-ATTENTION", "source_id": "ee8135f4497807724f665a5cdaa775fb", "to": "CAUSAL ATTENTION", "width": 1.0}, {"description": "Relative position bias is used in self-attention to consider the relative positions between features in a window", "from": "SELF-ATTENTION", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "RELATIVE POSITION BIAS", "width": 1.0}, {"description": "Self-attention uses a relative position bias table to store relative position biases", "from": "SELF-ATTENTION", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "RELATIVE POSITION BIAS TABLE", "width": 1.0}, {"description": "TimeGPT uses self-attention mechanisms to capture the diversity of past events", "from": "SELF-ATTENTION", "source_id": "518bfcd6711530089fe3914ca16459c2", "to": "TIMEGPT", "width": 1.0}, {"description": "The window encoder uses self-attention to mask data at subsequent positions", "from": "SELF-ATTENTION", "source_id": "4bdf596e75e1cb06d11b25e95491037e", "to": "WINDOW ENCODER", "width": 1.0}, {"description": "Model dimension is related to number of heads as they are both hyperparameters of the TimesFM model, as described in the text", "from": "MODEL DIMENSION", "source_id": "ee8135f4497807724f665a5cdaa775fb", "to": "NUMBER OF HEADS", "width": 1.0}, {"description": "Number of layers is related to number of heads as both are hyperparameters that determine the architecture of the model", "from": "NUMBER OF HEADS", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "NUMBER OF LAYERS", "width": 1.0}, {"description": "Number of heads is related to embedding dimensions per head as both are hyperparameters that determine the size of the embeddings", "from": "NUMBER OF HEADS", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "EMBEDDING DIMENSIONS PER HEAD", "width": 1.0}, {"description": "Mean squared error is related to point forecasting as mean squared error is a loss function used for point forecasting", "from": "MEAN SQUARED ERROR", "source_id": "4ab34d32601d452f14b4a1c31415292d", "to": "POINT FORECASTING", "width": 1.0}, {"description": "Point forecasting is related to probabilistic forecasting as probabilistic forecasting is a more general task that includes point forecasting", "from": "POINT FORECASTING", "source_id": "4ab34d32601d452f14b4a1c31415292d", "to": "PROBABILISTIC FORECASTING", "width": 1.0}, {"description": "Probabilistic forecasting is related to quantile loss as quantile loss is a loss function used for probabilistic forecasting", "from": "PROBABILISTIC FORECASTING", "source_id": "4ab34d32601d452f14b4a1c31415292d", "to": "QUANTILE LOSS", "width": 1.0}, {"description": "Chain-of-thought can be used to improve the performance of a model in probabilistic forecasting tasks", "from": "PROBABILISTIC FORECASTING", "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "to": "CHAIN-OF-THOUGHT", "width": 1.0}, {"description": "Conformal prediction is related to probabilistic forecasting as probabilistic forecasting was used to estimate TimeGPT\u0027s uncertainty", "from": "PROBABILISTIC FORECASTING", "source_id": "42e1bb44edbe787e104f589e74b95d6c", "to": "CONFORMAL PREDICTION", "width": 1.0}, {"description": "Probabilistic forecasting is related to time series domain as time series domain is where probabilistic forecasting is applied", "from": "PROBABILISTIC FORECASTING", "source_id": "42e1bb44edbe787e104f589e74b95d6c", "to": "TIME SERIES DOMAIN", "width": 1.0}, {"description": "Weighted average is used to aggregate errors in WQL, which is related to quantile loss", "from": "QUANTILE LOSS", "source_id": "e37f4063d074e1697e1f539131b3d963", "to": "WEIGHTED AVERAGE", "width": 1.0}, {"description": "Quantile loss is used to evaluate the performance of time series forecasting models, which is related to CRPS", "from": "QUANTILE LOSS", "source_id": "e37f4063d074e1697e1f539131b3d963", "to": "CONTINUOUS RANKED PROBABILITY SCORE", "width": 1.0}, {"description": "Data privacy is related to Google Trends data source", "from": "GOOGLE TRENDS", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "DATA PRIVACY", "width": 1.0}, {"description": "Google Trends data is differentially private", "from": "GOOGLE TRENDS", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "DIFFERENTIALLY PRIVATE", "width": 1.0}, {"description": "ARMA is related to seasonal patterns as seasonal patterns are often represented using ARMA models", "from": "SEASONAL PATTERNS", "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "to": "ARMA", "width": 1.0}, {"description": "Seasonal patterns are related to sine waves as sine waves are often used to represent seasonal or periodic events", "from": "SEASONAL PATTERNS", "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "to": "SINE WAVES", "width": 1.0}, {"description": "Seasonal patterns are related to cosine waves as cosine waves are often used to represent seasonal or periodic events", "from": "SEASONAL PATTERNS", "source_id": "cc1063ba5913ea38c84ac0250c01fe84", "to": "COSINE WAVES", "width": 1.0}, {"description": "M4 dataset is related to electricity dataset as both datasets contain time-series data", "from": "M4 DATASET", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "ELECTRICITY DATASET", "width": 1.0}, {"description": "M4 dataset is related to traffic dataset as both datasets contain time-series data", "from": "M4 DATASET", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "TRAFFIC DATASET", "width": 1.0}, {"description": "M4 dataset is related to weather dataset as both datasets contain time-series data", "from": "M4 DATASET", "source_id": "ad7c537267a3aaae402b03f7150950cf", "to": "WEATHER DATASET", "width": 1.0}, {"description": "M4 dataset is related to M3 dataset as they are both used for short-term forecasting", "from": "M4 DATASET", "source_id": "74527a4337ed6919731be520311ae774", "to": "M3 DATASET", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe \"ELECTRICITY DATASET\" and the \"TRAFFIC DATASET\" are two entities that share a common relationship. Both datasets contain time-series data, which is a key characteristic that links them together. Furthermore, both datasets are utilized for long-term forecasting purposes, indicating that they are employed in similar applications. This connection between the two datasets suggests that they may be used in conjunction with each other or that they share similar characteristics that make them suitable for long-term forecasting.\n\nThis summary is based on the information provided in the description list, which states that both datasets contain time-series data and are used for long-term forecasting. The primary language of the text is English, as indicated by the presence of technical terms, mathematical equations, and references to academic papers written in English.", "from": "ELECTRICITY DATASET", "source_id": "74527a4337ed6919731be520311ae774,ad7c537267a3aaae402b03f7150950cf", "to": "TRAFFIC DATASET", "width": 2.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe \"ELECTRICITY DATASET\" and the \"WEATHER DATASET\" are two entities that are closely related to each other. Both datasets contain time-series data, which suggests that they share a common characteristic in terms of their data structure. Furthermore, both datasets are used for long-term forecasting, indicating that they are utilized for similar purposes in the context of predictive analytics.\n\nThis relationship between the two datasets is further reinforced by the fact that they are used in conjunction with each other, as implied by the descriptions provided. The \"ELECTRICITY DATASET\" is related to the \"WEATHER DATASET\" as they are both used for long-term forecasting, and vice versa. This suggests that the two datasets are complementary and are used together to inform predictive models.\n\nOverall, the \"ELECTRICITY DATASET\" and the \"WEATHER DATASET\" are two entities that are closely intertwined in terms of their data characteristics and usage. They are both used for long-term forecasting and contain time-series data, making them suitable for predictive analytics applications.", "from": "ELECTRICITY DATASET", "source_id": "74527a4337ed6919731be520311ae774,ad7c537267a3aaae402b03f7150950cf", "to": "WEATHER DATASET", "width": 2.0}, {"description": "ETT datasets are related to electricity dataset as they are both used for long-term forecasting", "from": "ELECTRICITY DATASET", "source_id": "74527a4337ed6919731be520311ae774", "to": "ETT DATASETS", "width": 1.0}, {"description": "Electricity dataset is related to ILI dataset as they are both used for long-term forecasting", "from": "ELECTRICITY DATASET", "source_id": "74527a4337ed6919731be520311ae774", "to": "ILI DATASET", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe \"TRAFFIC DATASET\" and the \"WEATHER DATASET\" are two entities that are closely related to each other. Both datasets contain time-series data, which suggests that they share a common characteristic in terms of their data structure. Furthermore, both datasets are used for long-term forecasting, indicating that they are utilized for predicting future outcomes based on historical data.\n\nThis relationship between the two datasets is further reinforced by the fact that they are used in conjunction with each other, as mentioned in the descriptions. The \"TRAFFIC DATASET\" is related to the \"WEATHER DATASET\" as they are both used for long-term forecasting, and vice versa. This suggests that the two datasets are complementary and are used together to improve the accuracy of forecasting models.\n\nOverall, the \"TRAFFIC DATASET\" and the \"WEATHER DATASET\" are two entities that are closely intertwined in terms of their data characteristics and usage. They are both used for long-term forecasting and contain time-series data, making them suitable for analysis and modeling in the context of traffic and weather prediction.", "from": "TRAFFIC DATASET", "source_id": "74527a4337ed6919731be520311ae774,ad7c537267a3aaae402b03f7150950cf", "to": "WEATHER DATASET", "width": 2.0}, {"description": "The Traffic dataset encompasses 862 hourly time series depicting road occupancy rates on the freeways in the San Francisco Bay area from 2015 to 2016", "from": "TRAFFIC DATASET", "source_id": "76cc338e586223647fd3dbe4e7a7c131", "to": "ROAD OCCUPANCY RATES", "width": 1.0}, {"description": "ETT-H2 dataset is related to Traffic dataset as both are collections of data used to train and evaluate machine learning models", "from": "TRAFFIC DATASET", "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "to": "ETT-H2 DATASET", "width": 1.0}, {"description": "Traffic dataset is related to ETT-M2 dataset as both are collections of data used to train and evaluate machine learning models", "from": "TRAFFIC DATASET", "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "to": "ETT-M2 DATASET", "width": 1.0}, {"description": "ETT datasets are related to traffic dataset as they are both used for long-term forecasting", "from": "TRAFFIC DATASET", "source_id": "74527a4337ed6919731be520311ae774", "to": "ETT DATASETS", "width": 1.0}, {"description": "Traffic dataset is related to ILI dataset as they are both used for long-term forecasting", "from": "TRAFFIC DATASET", "source_id": "74527a4337ed6919731be520311ae774", "to": "ILI DATASET", "width": 1.0}, {"description": "The Weather dataset includes time series of hourly climate data near Monash University, Clayton, Victoria, Australia, from January 2010 to May 2021", "from": "WEATHER DATASET", "source_id": "76cc338e586223647fd3dbe4e7a7c131", "to": "CLIMATE DATA", "width": 1.0}, {"description": "ETT datasets are related to weather dataset as they are both used for long-term forecasting", "from": "WEATHER DATASET", "source_id": "74527a4337ed6919731be520311ae774", "to": "ETT DATASETS", "width": 1.0}, {"description": "Weather dataset is related to ILI dataset as they are both used for long-term forecasting", "from": "WEATHER DATASET", "source_id": "74527a4337ed6919731be520311ae774", "to": "ILI DATASET", "width": 1.0}, {"description": "The synthetic dataset is a subset of the TimesFM pretraining dataset", "from": "TIMESFM PRETRAINING DATASET", "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "to": "SYNTHETIC", "width": 1.0}, {"description": "The electricity dataset is a subset of the TimesFM pretraining dataset", "from": "TIMESFM PRETRAINING DATASET", "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "to": "ELECTRICITY", "width": 1.0}, {"description": "The weather dataset is a subset of the TimesFM pretraining dataset", "from": "TIMESFM PRETRAINING DATASET", "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "to": "WEATHER ZZP+21", "width": 1.0}, {"description": "The Favorita sales dataset is a subset of the TimesFM pretraining dataset", "from": "TIMESFM PRETRAINING DATASET", "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "to": "FAVORITA SALES", "width": 1.0}, {"description": "The LibCity dataset is a subset of the TimesFM pretraining dataset", "from": "TIMESFM PRETRAINING DATASET", "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "to": "LIBCITY WJJ", "width": 1.0}, {"description": "The M4 hourly dataset is a subset of the TimesFM pretraining dataset", "from": "TIMESFM PRETRAINING DATASET", "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "to": "M4 HOURLY", "width": 1.0}, {"description": "The M4 daily dataset is a subset of the TimesFM pretraining dataset", "from": "TIMESFM PRETRAINING DATASET", "source_id": "269a6f95aee3c9e28d0290fd10ed476d", "to": "M4 DAILY", "width": 1.0}, {"description": "Australian Electricity is a subset of the Electricity dataset", "from": "ELECTRICITY", "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "to": "AUSTRALIAN ELECTRICITY", "width": 1.0}, {"description": "Electricity is related to ERCOT Load as both datasets contain energy data", "from": "ELECTRICITY", "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "to": "ERCOT LOAD", "width": 1.0}, {"description": "Electricity is related to in-domain evaluation as in-domain evaluation is used to evaluate Electricity", "from": "ELECTRICITY", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "IN-DOMAIN EVALUATION", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe Electricity dataset and the Weather dataset are both datasets that fall under the categories of time series data, specifically within the domains of electricity and weather. These two datasets are related as they are both types of data used in short-term forecasting, indicating a connection between the two entities in the context of time series analysis and forecasting.\n\nThis summary incorporates information from all the descriptions provided, resolving any potential contradictions and presenting a coherent overview of the entities and their relationships. The entities in question are \"ELECTRICITY\" and \"WEATHER\", and the summary highlights their shared characteristics as time series data used in short-term forecasting.", "from": "ELECTRICITY", "source_id": "27efab80b405b365d8e9dd9834dd1ca8,50eeacd99c68b2581be90310bedcbc2c,c84edbea28fbbed451e8d0b7df4ffb7c", "to": "WEATHER", "width": 3.0}, {"description": "PatchTST (2023) is related to electricity as electricity is a type of data used in short-term forecasting", "from": "ELECTRICITY", "source_id": "27efab80b405b365d8e9dd9834dd1ca8", "to": "PATCHTST (2023)", "width": 1.0}, {"description": "ETTm2 is related to electricity as electricity is a type of data used in short-term forecasting", "from": "ELECTRICITY", "source_id": "27efab80b405b365d8e9dd9834dd1ca8", "to": "ETTM2", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe \"WIKI DAILY\" dataset is primarily related to the \"ENGLISH WIKIPEDIA\" entity. This dataset contains daily page views on the top-100k English Wikipedia articles, providing valuable insights into the online behavior and engagement patterns of users interacting with the English Wikipedia platform. The data is likely to be useful for various applications, including time series analysis, long-term series forecasting, and frequency analysis, which are commonly used in academic and research settings. The presence of technical terms and mathematical equations in the descriptions suggests that the dataset may be used in machine learning and data science research, potentially involving techniques such as multi-head cross-attention. Overall, the \"WIKI DAILY\" dataset appears to be a valuable resource for researchers and analysts interested in understanding the dynamics of online engagement with the English Wikipedia platform.", "from": "WIKI DAILY", "source_id": "2565ae205d4c98342168bb67a4f7a309,a875a1c0bede47a1c4e3823be81c42c6", "to": "ENGLISH WIKIPEDIA", "width": 2.0}, {"description": "Geometric mean is related to MAE as MAE is used to calculate the average of a set of numbers", "from": "MAE", "source_id": "892b821ed44c13c4d09e0ceedac3051d", "to": "GEOMETRIC MEAN", "width": 1.0}, {"description": "MAE is related to scaling as scaling is a concept used to improve the performance of a model", "from": "MAE", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "SCALING", "width": 1.0}, {"description": "Teraflops are related to MAE as MAE is a measure of the difference between predicted and actual values", "from": "MAE", "source_id": "f7ce89346ea6715560163ef3a630a5df", "to": "TERAFLOPS", "width": 1.0}, {"description": "Model is related to MAE as MAE is a metric used to evaluate the model\u0027s performance", "from": "MAE", "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "to": "MODEL", "width": 1.0}, {"description": "GM is related to MAE as it is a metric used to evaluate the model\u0027s performance", "from": "MAE", "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "to": "GM", "width": 1.0}, {"description": "Metrics are related to MAE as MAE is a metric used to evaluate the performance of a machine learning model", "from": "MAE", "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "to": "METRICS", "width": 1.0}, {"description": "MAE is related to zero-shot forecasts as zero-shot forecasts are made without any prior knowledge or training data", "from": "MAE", "source_id": "500710c8a95f1310d383fa71fd806c1c", "to": "ZERO-SHOT FORECASTS", "width": 1.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entities \"MAE\" and \"MSE\" are metrics used to evaluate the performance of a model. They are related as both are metrics used to evaluate the difference between predicted and actual values. Specifically, they are used to measure the average magnitude of errors between predicted and actual values, with MSE (Mean Squared Error) being a measure of the average squared difference and MAE (Mean Absolute Error) being a measure of the average absolute difference.\n\nThis summary is based on the information collected from all the descriptions, and it resolves any potential contradictions by providing a clear and concise explanation of the relationship between MSE and MAE. The summary is written in third person and includes the entity names for context.\n\nAdditionally, based on the nearby text, it can be inferred that these metrics are commonly used in the context of time series forecasting and model evaluation, as indicated by the use of technical terms such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis.\"", "from": "MAE", "source_id": "1b48e9ca066ac5ba037066bb762d3458,1d6fb60c5060c25ae4791b03a0513a7f,41fe893a178ebc8790ef4da83da5ab6e,919ff66615400ea06113a2a59ff34ef0,e900311a447985c0967f87fff49c58b8,fd1092903d83bf6e90a6caa371d7c514", "to": "MSE", "width": 6.0}, {"description": "MAE is related to SMAPE as both are metrics used to evaluate the performance of a model", "from": "MAE", "source_id": "1b48e9ca066ac5ba037066bb762d3458", "to": "SMAPE", "width": 1.0}, {"description": "MAE is used to evaluate the performance of a model in time series forecasting", "from": "MAE", "source_id": "e900311a447985c0967f87fff49c58b8", "to": "FORECASTING", "width": 1.0}, {"description": "Darts is related to Monash as both are datasets used for time-series forecasting", "from": "DARTS", "source_id": "892b821ed44c13c4d09e0ceedac3051d", "to": "MONASH", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nDARTS and TCN are two models that have demonstrated strong performance in time-series forecasting tasks. Specifically, TCN is mentioned in the text as a model that is related to DARTS, as it is a model that is part of the collection. This suggests that both models are being considered for their potential in time-series forecasting, and that TCN is being evaluated in the context of DARTS.\n\nThe primary language of the text is English, which is evident from the use of English words and phrases, mathematical equations, and references to academic papers. The language is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nOverall, the summary highlights the relationship between DARTS and TCN, as well as their potential in time-series forecasting tasks.", "from": "DARTS", "source_id": "2bc70082c142ee2ef5d6a941611505b7,d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "TCN", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nDARTS and N-HITS are two models that have demonstrated strong performance in time-series forecasting tasks. They share a connection, as N-HITS is a model mentioned in the text, specifically in the context of a collection. This suggests that DARTS and N-HITS may have similar or complementary strengths in handling time-series data, although the exact nature of their relationship is not explicitly stated.\n\nGiven the context of time-series forecasting, it is likely that both DARTS and N-HITS employ techniques such as frequency analysis and multi-head cross-attention to capture complex patterns and relationships in the data. The use of these techniques is consistent with the goals of time-series forecasting, which involves predicting future values based on past observations.\n\nThe performance of DARTS and N-HITS in time-series forecasting tasks is likely to be evaluated using metrics such as mean absolute error (MAE) or mean squared error (MSE), which are commonly used in the field of time-series analysis. The models may also be compared to other state-of-the-art models, such as those presented in academic papers published in conferences like ICLR, AAAI, or PMLR.\n\nOverall, DARTS and N-HITS are two models that have shown promise in time-series forecasting, and their connection through N-HITS suggests that they may be worth exploring further in the context of time-series analysis and forecasting.", "from": "DARTS", "source_id": "2bc70082c142ee2ef5d6a941611505b7,d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "N-HITS", "width": 2.0}, {"description": "Darts and GP are models that perform well for time-series forecasting tasks", "from": "DARTS", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "GP", "width": 1.0}, {"description": "Darts and llmtime(ZS) are models that perform well for time-series forecasting tasks", "from": "DARTS", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "LLMTIME(ZS)", "width": 1.0}, {"description": "Darts and NAIVE are models that perform well for time-series forecasting tasks", "from": "DARTS", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "NAIVE", "width": 1.0}, {"description": "Julien Herzen is related to Darts as he has published a paper on the model", "from": "DARTS", "source_id": "a73df99fe49b288e1c8751be2008b191", "to": "JULIEN HERZEN", "width": 1.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entities \"MONASH\" and \"ETT\" are two datasets used in the context of time-series forecasting. Specifically, they are utilized in an experiment to evaluate the performance of two machine learning models, PatchTST and TimesFM. Both datasets, MONASH and ETT, are related to each other as they are used for the same purpose of time-series forecasting. This suggests that they may share similar characteristics or features, which are relevant for evaluating the performance of time-series forecasting models.\n\nIn terms of the language used to describe these entities, it is clear that the primary language is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers and conferences. This further supports the notion that the datasets are being used in a research or academic context.\n\nOverall, the summary provides a clear understanding of the entities \"MONASH\" and \"ETT\" and their relationship to each other, as well as the context in which they are being used.", "from": "MONASH", "source_id": "28b097d339554431fa14e113c98ed49e,892b821ed44c13c4d09e0ceedac3051d,dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "ETT", "width": 3.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"MONASH\" and \"TIMESFM(ZS)\" are related in the context of machine learning model development and evaluation. Specifically, the \"MONASH\" dataset is used to test and evaluate the \"TIMESFM(ZS)\" model, which is a time series forecasting model. The \"MONASH\" dataset has different scales, suggesting that it is a diverse and challenging dataset for model evaluation. The relationship between these two entities is centered around the evaluation and testing of the \"TIMESFM(ZS)\" model on the \"MONASH\" dataset, which is a common practice in machine learning research.\n\nThis summary is based on the information provided in the description list, which states that \"Monash is related to TimesFM(ZS) as it is a dataset used to test and evaluate the model\" and \"TimesFM(ZS) is tested on the Monash dataset, which has different scales\". These two statements are consistent and provide a clear understanding of the relationship between the two entities.\n\nIn terms of the language used, the description list suggests that the language is formal and academic, consistent with the use of technical terms and mathematical equations. The use of English words and phrases, such as \"time series\" and \"long-term series forecasting\", further supports the conclusion that the primary language of the text is English.", "from": "MONASH", "source_id": "39365aee753fb73130c208fdf4046bb7,cd4ff69415c7b37cec643f8289d1c98d", "to": "TIMESFM(ZS)", "width": 2.0}, {"description": "PatchTST(ZS) is tested on the Monash dataset, which has different scales", "from": "MONASH", "source_id": "39365aee753fb73130c208fdf4046bb7", "to": "PATCHTST(ZS)", "width": 1.0}, {"description": "Monash is related to size Monash as size Monash refers to the size or scope of the Monash institution", "from": "MONASH", "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "to": "SIZE MONASH", "width": 1.0}, {"description": "ETT is related to ETTH1 as ETTH1 is a dataset used to evaluate the performance of machine learning models for time-series forecasting", "from": "ETT", "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "to": "ETTH1", "width": 1.0}, {"description": "ETT is related to ETTH2 as ETTH2 is a dataset used to evaluate the performance of machine learning models for time-series forecasting", "from": "ETT", "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "to": "ETTH2", "width": 1.0}, {"description": "ETT is related to ETTM1 as ETTM1 is a dataset used to evaluate the performance of machine learning models for time-series forecasting", "from": "ETT", "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "to": "ETTM1", "width": 1.0}, {"description": "TimesFM(ZS) is tested on the ETT dataset, which has different context lengths", "from": "ETT", "source_id": "39365aee753fb73130c208fdf4046bb7", "to": "TIMESFM(ZS)", "width": 1.0}, {"description": "PatchTST(ZS) is tested on the ETT dataset, which has different context lengths", "from": "ETT", "source_id": "39365aee753fb73130c208fdf4046bb7", "to": "PATCHTST(ZS)", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"ETT\" and \"ERCOT LOAD\" are related datasets that contain energy usage data. Both datasets are associated with energy data, indicating a strong connection between them. This relationship suggests that the data in \"ETT\" and \"ERCOT LOAD\" may be complementary or overlapping, and could potentially be used together for analysis or forecasting purposes.\n\nGiven the context of energy usage data, it is likely that these datasets are used for time series analysis, long-term series forecasting, and frequency analysis. The use of multi-head cross-attention may also be relevant in this context, as it is a technique commonly used in natural language processing and time series forecasting.\n\nThe datasets \"ETT\" and \"ERCOT LOAD\" may be used in academic research or technical applications, as indicated by the presence of technical terms and mathematical equations in the descriptions. The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports this conclusion.\n\nOverall, the summary provides a clear understanding of the relationship between the entities \"ETT\" and \"ERCOT LOAD,\" as well as the potential applications and context in which they are used.", "from": "ETT", "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502,e067234b24b0625a3f95ecc18035f915", "to": "ERCOT LOAD", "width": 2.0}, {"description": "ETT is related to Exchange Rate as both are datasets of finance data", "from": "ETT", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "EXCHANGE RATE", "width": 1.0}, {"description": "The ETT benchmark is an indicator reflective of long-term electric power deployment", "from": "ETT", "source_id": "50eeacd99c68b2581be90310bedcbc2c", "to": "ELECTRICITY TRANSFORMER TEMPERATURE", "width": 1.0}, {"description": "The ETT benchmark is sourced from two counties in China", "from": "ETT", "source_id": "50eeacd99c68b2581be90310bedcbc2c", "to": "CHINA", "width": 1.0}, {"description": "Zero-shot evaluation is related to pretrained model as pretrained model can be fine-tuned for a specific task", "from": "ZERO-SHOT EVALUATION", "source_id": "892b821ed44c13c4d09e0ceedac3051d", "to": "PRETRAINED MODEL", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities in question are \"ZERO-SHOT EVALUATION\" and \"IN-DOMAIN EVALUATION.\" These two entities are related to each other in the context of evaluation processes.\n\nAccording to the descriptions, both \"ZERO-SHOT EVALUATION\" and \"IN-DOMAIN EVALUATION\" are types of evaluation processes. However, they serve different purposes. \"IN-DOMAIN EVALUATION\" is used for datasets that are similar to the training data, whereas \"ZERO-SHOT EVALUATION\" is used for datasets that the model has not seen before.\n\nIn other words, \"IN-DOMAIN EVALUATION\" is a type of evaluation that occurs within the same domain or dataset as the training data, whereas \"ZERO-SHOT EVALUATION\" is a type of evaluation that occurs outside of the training data domain, where the model is required to generalize and perform well on unseen data.\n\nTherefore, the primary relationship between \"ZERO-SHOT EVALUATION\" and \"IN-DOMAIN EVALUATION\" is that they are both evaluation processes, but they differ in their application and scope.", "from": "ZERO-SHOT EVALUATION", "source_id": "63e284ec7e76fa859cc46b72d3746654,e067234b24b0625a3f95ecc18035f915", "to": "IN-DOMAIN EVALUATION", "width": 2.0}, {"description": "M4 is related to zero-shot evaluation as zero-shot evaluation is used to evaluate M4", "from": "ZERO-SHOT EVALUATION", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "M4", "width": 1.0}, {"description": "CatBoost is related to DeepAR as both are machine learning models used for time-series forecasting", "from": "CATBOOST", "source_id": "892b821ed44c13c4d09e0ceedac3051d", "to": "DEEPAR", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nThe entities \"DEEPAR\" and \"WAVENET\" are machine learning models used for time-series forecasting. They are related to each other, as both models are mentioned in the text and are used for the same purpose. This suggests that they share similarities in their architecture or functionality, although the specific details of their relationship are not provided.\n\nGiven the context of time-series forecasting, it is likely that both models are used for predicting future values in a sequence of data points. The fact that they are mentioned together in the text suggests that they may be compared or contrasted in terms of their performance or effectiveness in this task.\n\nOverall, the relationship between DEEPAR and WaveNet is one of similarity and shared purpose, with both models being used for time-series forecasting and potentially being compared or contrasted in terms of their performance.", "from": "DEEPAR", "source_id": "892b821ed44c13c4d09e0ceedac3051d,af36d1634490149b96980fb1dff57cd1", "to": "WAVENET", "width": 2.0}, {"description": "RNNs are used in the DeepAR model", "from": "DEEPAR", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "RNNS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n**Summary of DEEPAR and TFT**\n\nDEEPAR and TFT are two types of time series forecasting models that are closely related. Both models are mentioned in the text and are indicated by their abbreviations in the table, suggesting a strong connection between them. A comparison of their performance is also mentioned, implying that they are being evaluated and compared in terms of their effectiveness in time series forecasting.\n\n**Key Characteristics of DEEPAR and TFT**\n\n* Both DEEPAR and TFT are time series forecasting models.\n* They are related models, as indicated by their abbreviations in the table.\n* Their performance is being compared, suggesting that they are being evaluated and compared in terms of their effectiveness.\n\n**Context and Relevance**\n\nThe mention of DEEPAR and TFT in the text suggests that they are being discussed in the context of time series forecasting, which is a key area of research in machine learning and data science. The comparison of their performance implies that they are being evaluated for their ability to accurately forecast time series data, which is a critical application in many fields, including finance, economics, and engineering.\n\nOverall, the summary provides a clear understanding of the relationship between DEEPAR and TFT, as well as their key characteristics and relevance in the context of time series forecasting.", "from": "DEEPAR", "source_id": "1dc665214307b1656d1a0094a1918ece,2f3b2f69f8eb4f958c3ca793cae36581,af36d1634490149b96980fb1dff57cd1,f912df936d0b735fe654d1a9c53caa3b", "to": "TFT", "width": 4.0}, {"description": "DeepAR is related to DLinear as both are models mentioned in the text", "from": "DEEPAR", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "DLINER", "width": 1.0}, {"description": "DeepAR is related to N-HiTS as both are models mentioned in the text", "from": "DEEPAR", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "N-HITS", "width": 1.0}, {"description": "DeepAR is related to GPT4TS as both are models mentioned in the text", "from": "DEEPAR", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "GPT4TS", "width": 1.0}, {"description": "DeepAR is related to local statistical models as DeepAR performs better than local statistical models", "from": "DEEPAR", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "Local forecasting method is related to DeepAR as both are methods for probabilistic forecasting", "from": "DEEPAR", "source_id": "f0c52387a7b3a5c3850fe6991f0a7c83", "to": "LOCAL FORECASTING METHOD", "width": 1.0}, {"description": "Datasets are used to train and test DEEPAR", "from": "DEEPAR", "source_id": "2f3b2f69f8eb4f958c3ca793cae36581", "to": "DATASETS", "width": 1.0}, {"description": "Croston\u0027s seasonal batch arrival model and DeepAR are both models used for time series forecasting", "from": "DEEPAR", "source_id": "376897a501ac50834f626fcc5fb13ece,6fa5f635ad5f7e6b67d5e467f130345c", "to": "CROSTONSBA", "width": 2.0}, {"description": "Based on the provided information, the primary entities are \"DEEPAR\" and \"DYNAMICOPTIMIZE.\" The descriptions provided are:\n\n1. \"DeepAR and Dynamic Optimize are both models used for time series forecasting.\"\n2. \"DeepAR and dynamic optimization are both models used for time series forecasting.\"\n\nAfter analyzing the descriptions, it appears that there is a slight variation in the naming of the second entity, with \"Dynamic Optimize\" and \"dynamic optimization\" being used interchangeably. However, this does not affect the overall meaning of the descriptions.\n\nConsidering the information collected from all the descriptions, a comprehensive summary can be generated as follows:\n\n\"DEEPAR and DYNAMICOPTIMIZE are both models used for time series forecasting. They are designed to analyze and predict future values in a series of data points over time, with a focus on optimizing the forecasting process.\"\n\nThis summary is written in third person and includes the entity names for context. It also resolves the minor variation in the naming of the second entity and provides a clear and concise description of the models\u0027 purpose.", "from": "DEEPAR", "source_id": "376897a501ac50834f626fcc5fb13ece,6fa5f635ad5f7e6b67d5e467f130345c", "to": "DYNAMICOPTIMIZE", "width": 2.0}, {"description": "AutoETS and DeepAR are both models used for time series forecasting", "from": "DEEPAR", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "AUTOETS", "width": 1.0}, {"description": "AutoARIMA and DeepAR are both models used for time series forecasting", "from": "DEEPAR", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "AUTOARIMA", "width": 1.0}, {"description": "DeepAR and NPts are both models used for time series forecasting", "from": "DEEPAR", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "NPTS", "width": 1.0}, {"description": "DeepAR and Temporal Fusion Transformer are both models used for time series forecasting", "from": "DEEPAR", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "TEMPORALFUSIONT", "width": 1.0}, {"description": "DeepAR and N-BEATS are both models used for time series forecasting", "from": "DEEPAR", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "NBEATS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nWAVENET and TFT are two entities that are related to each other. WAVENET is a type of model that uses a convolutional neural network approach to forecasting, and it is related to TFT as both models are mentioned in the text. This suggests that TFT may also be a forecasting model, possibly utilizing a different approach or architecture compared to WAVENET.\n\nGiven the context of the text, which appears to be a technical document or research paper discussing machine learning and time series forecasting, it is likely that both WAVENET and TFT are models used for forecasting purposes. The fact that they are mentioned together in the text implies that they may be compared or contrasted in terms of their performance or effectiveness in forecasting tasks.\n\nOverall, the relationship between WAVENET and TFT is that they are both forecasting models, with WAVENET using a convolutional neural network approach and TFT possibly utilizing a different approach or architecture.", "from": "WAVENET", "source_id": "116332ac4538a1430c83a34fcbec22d1,af36d1634490149b96980fb1dff57cd1", "to": "TFT", "width": 2.0}, {"description": "WaveNet is related to DLinear as both are models mentioned in the text", "from": "WAVENET", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "DLINER", "width": 1.0}, {"description": "WaveNet is related to N-HiTS as both are models mentioned in the text", "from": "WAVENET", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "N-HITS", "width": 1.0}, {"description": "WaveNet is related to GPT4TS as both are models mentioned in the text", "from": "WAVENET", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "GPT4TS", "width": 1.0}, {"description": "WaveNet is related to local statistical models as WaveNet performs better than local statistical models", "from": "WAVENET", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "WaveNet is outperformed by Chronos models on Benchmark II", "from": "WAVENET", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "CHRONOS MODELS", "width": 1.0}, {"description": "Geometric mean is related to arithmetic mean as arithmetic mean is a metric mentioned in the text, specifically Figure 4", "from": "GEOMETRIC MEAN", "source_id": "2bc70082c142ee2ef5d6a941611505b7", "to": "ARITHMETIC MEAN", "width": 1.0}, {"description": "Net is related to ODZ+16 as ODZ+16 is a model mentioned in the text, specifically Net", "from": "ODZ+16", "source_id": "2bc70082c142ee2ef5d6a941611505b7", "to": "NET", "width": 1.0}, {"description": "Monash Huggingface repository is related to datasets as datasets are mentioned in the text, specifically repository", "from": "MONASH HUGGINGFACE REPOSITORY", "source_id": "2bc70082c142ee2ef5d6a941611505b7", "to": "DATASETS", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entity \"DATASETS\" is closely related to the entity \"MACHINE LEARNING FOR IOT\". The field of \"MACHINE LEARNING FOR IOT\" involves working with \"DATASETS\" as it is a field of research that focuses on applying machine learning techniques to IoT data. This implies that datasets play a crucial role in the development and application of machine learning models for IoT.\n\nThe language used in the descriptions is formal and academic, suggesting that the text is from a research paper or a technical document. The primary language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers.\n\nIn the context of \"MACHINE LEARNING FOR IOT\", datasets are essential for training and testing machine learning models. The field of research focuses on applying machine learning techniques to IoT data, which is typically collected from various sources and stored in datasets. Therefore, the entity \"DATASETS\" is a fundamental component of \"MACHINE LEARNING FOR IOT\", enabling researchers and practitioners to develop and deploy effective machine learning models for IoT applications.", "from": "DATASETS", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1", "to": "MACHINE LEARNING FOR IOT", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe datasets are primarily related to perception, as they serve as a crucial component in training and evaluating machine learning models. These datasets are utilized in the context of perception to interpret sensory information, enabling the development of sophisticated models that can accurately process and understand various forms of data.\n\nIn the context of machine learning, datasets play a vital role in perception, as they provide the necessary data for models to learn from and make informed decisions. The datasets are used to train models to recognize patterns, classify objects, and make predictions, all of which are essential aspects of perception.\n\nThe datasets are not only used for training but also for evaluating the performance of machine learning models. By analyzing the accuracy and efficiency of models on various datasets, researchers and developers can refine their models and improve their performance in perception tasks.\n\nOverall, the datasets are a critical component in the field of perception, enabling the development of advanced machine learning models that can accurately interpret sensory information and make informed decisions.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which is consistent with the formal and academic tone of the provided descriptions.\n* The text also mentions technical terms, mathematical equations, and references to academic papers, which further supports the conclusion that the text is from a research paper or a technical document.\n* The use of English words and phrases, such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention,\" also suggests that the text is written in English.", "from": "DATASETS", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1", "to": "PERCEPTION", "width": 2.0}, {"description": "The dataset collection includes datasets from the Monash Time Series Forecasting Repository", "from": "DATASETS", "source_id": "63e284ec7e76fa859cc46b72d3746654", "to": "MONASH TIME SERIES FORECASTING REPOSITORY", "width": 1.0}, {"description": "The dataset collection includes datasets from the M-competitions", "from": "DATASETS", "source_id": "63e284ec7e76fa859cc46b72d3746654", "to": "M-COMPETITIONS", "width": 1.0}, {"description": "The dataset collection includes public domain datasets from Kaggle", "from": "DATASETS", "source_id": "63e284ec7e76fa859cc46b72d3746654", "to": "KAGGLE", "width": 1.0}, {"description": "Timestamp is used to prevent information leakage in the datasets", "from": "DATASETS", "source_id": "532a6d434dab611a80aef1e94dd2fb45", "to": "TIMESTAMP", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe datasets are directly related to Chronos models, as they serve as a crucial component for the empirical evaluation of these models. Specifically, datasets are utilized for both training and testing Chronos models, allowing for a comprehensive assessment of their performance and accuracy. This suggests that the datasets play a vital role in the development and validation of Chronos models, enabling researchers to refine and improve their models through iterative testing and evaluation.\n\nIn the context of machine learning and time series forecasting, Chronos models rely heavily on datasets to inform their predictions and decisions. By leveraging datasets for training and testing, Chronos models can learn from patterns and relationships within the data, ultimately leading to more accurate and reliable forecasting outcomes. The datasets used in conjunction with Chronos models are likely to be diverse and representative of various time series data, allowing the models to generalize and adapt to different scenarios and applications.\n\nOverall, the datasets and Chronos models are intricately linked, with datasets serving as a critical component in the development, evaluation, and refinement of Chronos models. This symbiotic relationship between datasets and Chronos models is essential for advancing the field of time series forecasting and machine learning, enabling researchers to create more accurate and effective models for real-world applications.", "from": "DATASETS", "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502,e5e4a8f03f502fada5b17cba5dc942ba", "to": "CHRONOS MODELS", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe datasets used for experiments are described in Table 2. This table provides a list of datasets used for empirical evaluation, which are essential for the experiments. The datasets are crucial for the long-term series forecasting and frequency analysis, as they serve as the foundation for the multi-head cross-attention mechanism. The use of these datasets is a key aspect of the research, and their selection is critical for the accuracy of the time series forecasting models.\n\nThe datasets are likely used in the context of academic research, as indicated by the presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals. The citation of English-language academic papers and authors also suggests that the text is written in English and is part of a research paper or technical document.\n\nOverall, Table 2 plays a vital role in the research by providing a comprehensive list of datasets used for empirical evaluation, which is essential for the development and evaluation of time series forecasting models.", "from": "DATASETS", "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502,e5e4a8f03f502fada5b17cba5dc942ba", "to": "TABLE 2", "width": 2.0}, {"description": "Evaluation settings are related to datasets as datasets are used to evaluate the performance of a model or algorithm", "from": "DATASETS", "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "to": "EVALUATION SETTINGS", "width": 1.0}, {"description": "Domain is related to datasets as datasets are grouped by domain", "from": "DATASETS", "source_id": "51ee17c1f0212d4c94010d8e376f649b", "to": "DOMAIN", "width": 1.0}, {"description": "Datasets are used to train and test TFT", "from": "DATASETS", "source_id": "2f3b2f69f8eb4f958c3ca793cae36581", "to": "TFT", "width": 1.0}, {"description": "Lag-Llama is trained and evaluated on datasets", "from": "DATASETS", "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "to": "LAG-LAMMA", "width": 1.0}, {"description": "Supervised models are used on various datasets in the text, as indicated by the results", "from": "DATASETS", "source_id": "e995e5477f470244a4a6afb9417f6d96", "to": "SUPERVISED MODEL", "width": 1.0}, {"description": "Appendix A.5.2 and Appendix A.5.3 are sections in the document that report the results on the Monash and ETT datasets", "from": "APPENDIX A.5.2", "source_id": "28b097d339554431fa14e113c98ed49e", "to": "APPENDIX A.5.3", "width": 1.0}, {"description": "Mean MAE is related to naive baseline as naive baseline is a model mentioned in the text, specifically metric", "from": "MEAN MAE", "source_id": "2bc70082c142ee2ef5d6a941611505b7", "to": "NAIVE BASELINE", "width": 1.0}, {"description": "Multivariate datasets are related to naive baseline as naive baseline is a simple machine learning model that makes a constant prediction", "from": "NAIVE BASELINE", "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "to": "MULTIVARIATE DATASETS", "width": 1.0}, {"description": "Naive baseline is related to llmtime(ZS) as llmtime(ZS) is a model used for evaluation", "from": "NAIVE BASELINE", "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "to": "LLMTIME(ZS)", "width": 1.0}, {"description": "GP and TCN are models that perform well for time-series forecasting tasks", "from": "TCN", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "GP", "width": 1.0}, {"description": "TCN and N-HITS are models that perform well for time-series forecasting tasks", "from": "TCN", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "N-HITS", "width": 1.0}, {"description": "TCN and llmtime(ZS) are models that perform well for time-series forecasting tasks", "from": "TCN", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "LLMTIME(ZS)", "width": 1.0}, {"description": "TCN and NAIVE are models that perform well for time-series forecasting tasks", "from": "TCN", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "NAIVE", "width": 1.0}, {"description": "GP and N-HITS are models that perform well for time-series forecasting tasks", "from": "N-HITS", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "GP", "width": 1.0}, {"description": "N-HITS and llmtime(ZS) are models that perform well for time-series forecasting tasks", "from": "N-HITS", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "LLMTIME(ZS)", "width": 1.0}, {"description": "N-HITS and NAIVE are models that perform well for time-series forecasting tasks", "from": "N-HITS", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "NAIVE", "width": 1.0}, {"description": "TFT is related to N-HiTS as both are models mentioned in the text", "from": "N-HITS", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "TFT", "width": 1.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entities \"N-HITS\" and \"DLINER\" are two models that are related to each other. Specifically, DLINER is compared with N-HITS, suggesting that they are being evaluated or contrasted in some way. This comparison is likely being made in the context of time series forecasting, given the mention of \"long-term series forecasting\" and \"frequency analysis\" in the text. The use of technical terms and mathematical equations in the text further supports this interpretation.\n\nIt is worth noting that the text appears to be from a research paper or technical document, given the formal and academic language used. The presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" also suggests that the text is written in English.\n\nOverall, the summary of the data is that N-HITS and DLINER are two related models that are being compared in the context of time series forecasting, likely in a research or technical setting.", "from": "N-HITS", "source_id": "5e4d9ca02ee6a285d5223c820743eb12,af36d1634490149b96980fb1dff57cd1", "to": "DLINER", "width": 2.0}, {"description": "N-HiTS is related to GPT4TS as both are models mentioned in the text", "from": "N-HITS", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "GPT4TS", "width": 1.0}, {"description": "N-HiTS is related to local statistical models as N-HiTS performs better than local statistical models", "from": "N-HITS", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "PatchTST places 1st in terms of point forecasting performance, while N-HiTS places 2nd", "from": "N-HITS", "source_id": "532a6d434dab611a80aef1e94dd2fb45", "to": "PATCH-TST", "width": 1.0}, {"description": "N-HiTS is a model for time series forecasting", "from": "N-HITS", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE", "width": 1.0}, {"description": "M4 is related to N-HiTS as N-HiTS is used for short-term time series forecasting on M4", "from": "N-HITS", "source_id": "d4551c2839eaa68a7cb7324089956581", "to": "M4", "width": 1.0}, {"description": "N-HiTS is related to Tab. 19 as Tab. 19 is a reference to the comparison between the model and the second-best method", "from": "N-HITS", "source_id": "fd1092903d83bf6e90a6caa371d7c514", "to": "TAB. 19", "width": 1.0}, {"description": "N-HiTS is related to Tab. 20 as Tab. 20 is a reference to the comparison between the model and the second-best method", "from": "N-HITS", "source_id": "fd1092903d83bf6e90a6caa371d7c514", "to": "TAB. 20", "width": 1.0}, {"description": "Llmtime is related to seasonal ARIMA as both models perform well in time-series forecasting tasks", "from": "LLMETIME", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "SEASONAL ARIMA", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"INFORMER\" and \"FEDFORMER\" are two machine learning models used for long-term series forecasting. The \"INFORMER\" model introduced transformers for long sequence forecasting through the Prob-sparse self-attention mechanism. This innovation has since been further refined in the \"FEDFORMER\" model, which is compared to the \"INFORMER\" model in the evaluation.\n\nThe \"FEDFORMER\" model builds upon the advancements made in the \"INFORMER\" model, suggesting a connection between the two entities in terms of their development and application. The use of transformers in both models highlights their effectiveness in handling long sequence forecasting tasks.\n\nOverall, the summary provides a clear understanding of the relationship between the \"INFORMER\" and \"FEDFORMER\" models, as well as their contributions to the field of long-term series forecasting.", "from": "INFORMER", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796,7d5d82d600620153153772a9bc498ac0", "to": "FEDFORMER", "width": 2.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"INFORMER\" and \"AUTOFORMER\" are both models used for time series forecasting. They are compared in the evaluation, indicating that they are related and have been used in the same context. Specifically, Informer introduced transformers for long sequence forecasting through the Prob-sparse self-attention mechanism, which has since been further refined in models like Autoformer [Wu et al., 2021]. Both Informer and AutoFormer are methods for probabilistic forecasting and use complex inductive biases to model time series. They are mentioned in the text as part of a table, suggesting that they are being evaluated or compared in some way.\n\nIn terms of their relationship, Informer is related to AutoFormer as both are methods for probabilistic forecasting, and AutoFormer is a refinement of the Prob-sparse self-attention mechanism introduced in Informer. Overall, both models are used for time series forecasting and are being compared or evaluated in the context of the text.\n\nThis summary is written in third person and includes information collected from all the descriptions. It also resolves any contradictions and provides a single, coherent summary of the entities and their relationships.", "from": "INFORMER", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796,673b0feee6eb5843954defc670e4ba29,7d5d82d600620153153772a9bc498ac0,bb87457fce8d4214bfe1f398b7ea35f2,bcf830b85e12e1ab9498e3ac3593a88e,c84edbea28fbbed451e8d0b7df4ffb7c,f0c52387a7b3a5c3850fe6991f0a7c83", "to": "AUTOFORMER", "width": 7.0}, {"description": "Lag-Llama outperforms Informer, AutoFormer, and ETSFormer models in various datasets", "from": "INFORMER", "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "to": "LAG-LAG", "width": 1.0}, {"description": "INFORMER is related to CRPS as CRPS is used to evaluate the performance of INFORMER", "from": "INFORMER", "source_id": "990578022879395d00b7a5b229863c2f", "to": "CRPS", "width": 1.0}, {"description": "NBEATS and INFORMER are models mentioned in the text, as indicated by the use of their names in the table", "from": "INFORMER", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "NBEATS", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"INFORMER\" and \"ETSFORMER\" are both types of models used for time series forecasting. These models are mentioned in the text, as indicated by their names in the table. The primary language of the text is English, which is evident from the use of English words and phrases, mathematical equations, and references to academic papers. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nThe models \"INFORMER\" and \"ETSFORMER\" are likely discussed in the text in the context of time series forecasting, which involves analyzing and predicting future values based on past data. The text may also mention their performance, applications, and comparisons with other models in the field of time series forecasting.\n\nOverall, the summary provides a clear understanding of the entities \"INFORMER\" and \"ETSFORMER\" and their relevance to time series forecasting, as well as the language and context in which they are discussed.", "from": "INFORMER", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2,c84edbea28fbbed451e8d0b7df4ffb7c", "to": "ETSFORMER", "width": 2.0}, {"description": "INFORMER and LAGLLAMA are models mentioned in the text, as indicated by the use of their names in the table", "from": "INFORMER", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "LAGLLAMA", "width": 1.0}, {"description": "Informer is related to Tian Zhang as Tian Zhang is an author of a paper on Informer", "from": "INFORMER", "source_id": "98c6b5003112ab7110e45414a2fa468b", "to": "TIAN ZHANG", "width": 1.0}, {"description": "ILI is related to Informer as it is a forecasting horizon used in the model", "from": "INFORMER", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "ILI", "width": 8.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entities \"INFORMER\" and \"LIGHTTS\" are both types of models used for time series forecasting. They are time series models that are compared in a table, indicating that they are being evaluated and contrasted in terms of their performance or characteristics. Furthermore, both models are mentioned in the text, suggesting that they are being discussed in the context of time series forecasting.\n\nGiven the context of time series forecasting, it is likely that both INFORMER and LIGHTTS are deep learning models, as they are being compared and evaluated in a table. The use of technical terms such as \"time series forecasting\" and the presence of mathematical equations and formulas in the text suggest that the discussion is focused on the technical aspects of these models.\n\nThe fact that both models are mentioned in the text and are being compared in a table suggests that they are being evaluated in terms of their performance, accuracy, or other relevant metrics. This implies that the text is discussing the strengths and weaknesses of each model, and potentially providing recommendations or insights for users who are interested in time series forecasting.\n\nOverall, the summary of the data is as follows:\n\nINFORMER and LIGHTTS are two time series models that are being compared and evaluated in the context of time series forecasting. They are both deep learning models that are being discussed in the text, and are being compared in terms of their performance or characteristics. The text likely provides insights and recommendations for users who are interested in time series forecasting, and is focused on the technical aspects of these models.", "from": "INFORMER", "source_id": "9ba0189af2ef0720a721c16eef0f0788,c84edbea28fbbed451e8d0b7df4ffb7c,fd1092903d83bf6e90a6caa371d7c514", "to": "LIGHTTS", "width": 3.0}, {"description": "Former and Informer are both types of models used for time series forecasting", "from": "INFORMER", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c", "to": "FORMER", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"INFORMER\" and \"REFORMER\" are both types of models used for time series forecasting. They are related to each other as both models are mentioned in the text, indicating a connection or similarity between them. The text does not provide further details on the specific characteristics or differences between these two models, but it is clear that they are both used for time series forecasting purposes.\n\nIt is worth noting that the text does not provide any information on the language or content of the text itself, which was analyzed separately. However, based on the provided descriptions, we can conclude that the entities \"INFORMER\" and \"REFORMER\" are related to time series forecasting and are mentioned together in the text.", "from": "INFORMER", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c,fd1092903d83bf6e90a6caa371d7c514", "to": "REFORMER", "width": 2.0}, {"description": "Based on the provided information, the primary entities are \"FEDFORMER\" and \"AUTOFORMER.\" The descriptions provided offer insights into the relationships and comparisons between these two entities.\n\nAfter analyzing the descriptions, it is clear that both \"FEDFORMER\" and \"AUTOFORMER\" are related to time-series forecasting. They are also compared to each other in the evaluation, suggesting that they are competing models or have similar applications.\n\nHere is a comprehensive summary of the data:\n\n\"FEDFORMER\" and \"AUTOFORMER\" are two time-series forecasting models that are compared in the evaluation. They are both used for time-series forecasting, with \"AUTOFORMER\" being a model specifically designed for this purpose. The comparison between \"FEDFORMER\" and \"AUTOFORMER\" is likely to provide insights into their performance and effectiveness in time-series forecasting tasks.\n\nThe information collected from all the descriptions is as follows:\n\n* Both \"FEDFORMER\" and \"AUTOFORMER\" are related to time-series forecasting.\n* They are compared to each other in the evaluation.\n* \"AUTOFORMER\" is a model specifically designed for time-series forecasting.\n* The comparison between \"FEDFORMER\" and \"AUTOFORMER\" is likely to provide insights into their performance and effectiveness in time-series forecasting tasks.\n\nThere are no contradictions in the descriptions, and the information is consistent across all the descriptions. The summary is written in third person and includes the entity names for context.", "from": "FEDFORMER", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796,500710c8a95f1310d383fa71fd806c1c,673b0feee6eb5843954defc670e4ba29,7e97089185883c456c798ddc5ec86373,9ba0189af2ef0720a721c16eef0f0788,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "to": "AUTOFORMER", "width": 7.0}, {"description": "Informer baselines are related to FEDFormer as FEDFormer is a model used for time-series forecasting", "from": "FEDFORMER", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "INFORMER BASELINES", "width": 1.0}, {"description": "M4 is related to FEDformer as FEDformer is used for short-term time series forecasting on M4", "from": "FEDFORMER", "source_id": "d4551c2839eaa68a7cb7324089956581", "to": "M4", "width": 1.0}, {"description": "Fedformer is related to Ziqing Ma as Ziqing Ma is an author of a paper on Fedformer", "from": "FEDFORMER", "source_id": "98c6b5003112ab7110e45414a2fa468b", "to": "ZIQING MA", "width": 1.0}, {"description": "Tian Zhou is related to Fedformer as he is an author of the research paper on Fedformer", "from": "FEDFORMER", "source_id": "a69a914fb6c895c7202532b69ad3e094", "to": "TIAN ZHOU", "width": 1.0}, {"description": "Fedformer is related to International Conference on Machine Learning as research papers on Fedformer are presented at the conference", "from": "FEDFORMER", "source_id": "a69a914fb6c895c7202532b69ad3e094", "to": "INTERNATIONAL CONFERENCE ON MACHINE LEARNING", "width": 1.0}, {"description": "ILI is related to FEDformer as it is a forecasting horizon used in the model", "from": "FEDFORMER", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "ILI", "width": 9.0}, {"description": "FEDformer is related to ETTh1 as both are models that are used for time series forecasting", "from": "FEDFORMER", "source_id": "41fe893a178ebc8790ef4da83da5ab6e", "to": "ETTH1", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe datasets \"ETTM1\" and \"ETTM2\" are two entities that have been sampled at a 15-minute level. Both datasets are related to each other as they are used for benchmarking various supervised long-horizon forecasting methods. This suggests that they are used in a comparative analysis to evaluate the performance of different forecasting techniques.\n\nGiven the context of time series forecasting, it is likely that the datasets contain time-stamped data points that are used to train and evaluate machine learning models. The fact that they are used for benchmarking suggests that they are publicly available and widely used in the research community.\n\nThe use of the term \"supervised long-horizon forecasting methods\" implies that the datasets are used to predict future values in a time series, and that the models being evaluated are trained on labeled data. This is consistent with the use of datasets in machine learning research, where the goal is to develop models that can accurately predict future outcomes based on past data.\n\nOverall, the datasets \"ETTM1\" and \"ETTM2\" appear to be two related datasets that are used for benchmarking supervised long-horizon forecasting methods in the context of time series analysis.", "from": "ETTM1", "source_id": "50eeacd99c68b2581be90310bedcbc2c,efb7975581b22620b8277417edeee3e9", "to": "ETTM2", "width": 2.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nETTM1 and ETTH1 are two datasets used for time series forecasting. They are related to each other, as both datasets are utilized for benchmarking various supervised long-horizon forecasting methods. Specifically, ETTH1 is related to ETTM1, and vice versa, as they are both employed for time series forecasting purposes.\n\nThis summary is derived from the provided descriptions, which collectively convey the relationship between ETTM1 and ETTH1. The descriptions are consistent in their portrayal of the two datasets, indicating that they are used for time series forecasting and benchmarking purposes.", "from": "ETTM1", "source_id": "072b166b9a1b6afecf5874f45af61699,d4e3d8b5bf043b78bb9f1551080cab91,efb7975581b22620b8277417edeee3e9", "to": "ETTH1", "width": 3.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"ETTM1\" and \"ETTH2\" are two datasets that are closely related to each other. Both datasets are used for time series forecasting, with \"ETTM1\" being a dataset for long-term series forecasting and \"ETTH2\" being a related dataset. Furthermore, both datasets are used for benchmarking various supervised long-horizon forecasting methods, indicating that they are used to evaluate and compare the performance of different forecasting techniques.\n\nThe relationship between \"ETTM1\" and \"ETTH2\" is that of mutual relevance, with each dataset serving as a benchmark for the other. This suggests that the two datasets are complementary and are used together to evaluate the performance of forecasting methods.\n\nOverall, the summary highlights the close relationship between \"ETTM1\" and \"ETTH2\" and their shared purpose in time series forecasting and benchmarking.", "from": "ETTM1", "source_id": "41fe893a178ebc8790ef4da83da5ab6e,efb7975581b22620b8277417edeee3e9", "to": "ETTH2", "width": 2.0}, {"description": "Monash datasets are related to ETTm1 as both datasets are used for benchmarking various supervised long-horizon forecasting methods", "from": "ETTM1", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "MONASH DATASETS", "width": 1.0}, {"description": "The finetuned model performs better than GPT4TS on ETTm1 dataset", "from": "ETTM1", "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "to": "FINE-TUNED MODEL", "width": 1.0}, {"description": "ETTm1 is a specific dataset used for evaluation", "from": "ETTM1", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796", "to": "ETT DATASETS", "width": 1.0}, {"description": "GPT4TS is used for forecasting on the ETTm1 dataset", "from": "ETTM1", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "GPT4TS", "width": 1.0}, {"description": "ETTm1 is related to M4 as both are datasets used for time series forecasting", "from": "ETTM1", "source_id": "d4e3d8b5bf043b78bb9f1551080cab91", "to": "M4", "width": 1.0}, {"description": "ETTm2 is related to ETTh1 as both datasets are used for benchmarking various supervised long-horizon forecasting methods", "from": "ETTM2", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "ETTH1", "width": 1.0}, {"description": "ETTm2 is related to ETTh2 as both datasets are used for benchmarking various supervised long-horizon forecasting methods", "from": "ETTM2", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "ETTH2", "width": 1.0}, {"description": "Monash datasets are related to ETTm2 as both datasets are used for benchmarking various supervised long-horizon forecasting methods", "from": "ETTM2", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "MONASH DATASETS", "width": 1.0}, {"description": "ETTm2 is a specific dataset used for evaluation", "from": "ETTM2", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796", "to": "ETT DATASETS", "width": 1.0}, {"description": "GPT4TS is used for forecasting on the ETTm2 dataset", "from": "ETTM2", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "GPT4TS", "width": 1.0}, {"description": "ETTm2 is related to weather as weather is a type of data used in short-term forecasting", "from": "ETTM2", "source_id": "27efab80b405b365d8e9dd9834dd1ca8", "to": "WEATHER", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe datasets \"ETTH1\" and \"ETTH2\" are two entities that are closely related. Both datasets are sampled at a 1-hour level, indicating that they contain time-stamped data points. Notably, \"ETTH1\" and \"ETTH2\" are used for benchmarking various supervised long-horizon forecasting methods, suggesting that they are employed in the context of time series forecasting. This implies that the data in these datasets is likely to be used for evaluating the performance of different forecasting models, particularly those that involve long-term predictions.\n\nGiven the context of time series forecasting, it is likely that the data in \"ETTH1\" and \"ETTH2\" contains information related to frequency analysis, which is a common technique used in time series analysis. Additionally, the use of multi-head cross-attention, a technique commonly used in transformer-based models, may be relevant to the forecasting methods being benchmarked.\n\nOverall, the datasets \"ETTH1\" and \"ETTH2\" appear to be closely related and are used for evaluating the performance of supervised long-horizon forecasting methods, particularly those that involve time series analysis and frequency analysis.", "from": "ETTH1", "source_id": "50eeacd99c68b2581be90310bedcbc2c,efb7975581b22620b8277417edeee3e9", "to": "ETTH2", "width": 2.0}, {"description": "Monash datasets are related to ETTh1 as both datasets are used for benchmarking various supervised long-horizon forecasting methods", "from": "ETTH1", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "MONASH DATASETS", "width": 1.0}, {"description": "The finetuned model performs better than GPT4TS on ETTh1 dataset", "from": "ETTH1", "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "to": "FINE-TUNED MODEL", "width": 1.0}, {"description": "ETTh1 is a specific dataset used for evaluation", "from": "ETTH1", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796", "to": "ETT DATASETS", "width": 1.0}, {"description": "ETTh1 is related to GPU memory as GPU memory is used for ETTh1", "from": "ETTH1", "source_id": "7e97089185883c456c798ddc5ec86373", "to": "GPU MEMORY", "width": 1.0}, {"description": "GPT4TS is used for forecasting on the ETTh1 dataset", "from": "ETTH1", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "GPT4TS", "width": 1.0}, {"description": "M4 is related to ETTh1 as both are datasets used for time series forecasting", "from": "ETTH1", "source_id": "d4e3d8b5bf043b78bb9f1551080cab91", "to": "M4", "width": 1.0}, {"description": "Monash datasets are related to ETTh2 as both datasets are used for benchmarking various supervised long-horizon forecasting methods", "from": "ETTH2", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "MONASH DATASETS", "width": 1.0}, {"description": "The finetuned model performs better than GPT4TS on ETTh2 dataset", "from": "ETTH2", "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "to": "FINE-TUNED MODEL", "width": 1.0}, {"description": "ETTh2 is a specific dataset used for evaluation", "from": "ETTH2", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796", "to": "ETT DATASETS", "width": 1.0}, {"description": "GPT4TS is used for forecasting on the ETTh2 dataset", "from": "ETTH2", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "GPT4TS", "width": 1.0}, {"description": "Monash datasets are related to FLOPS as FLOPS is a measure of computing power", "from": "MONASH DATASETS", "source_id": "f7ce89346ea6715560163ef3a630a5df", "to": "FLOPS", "width": 1.0}, {"description": "Darts datasets and Monash datasets are used to evaluate the performance of time-series forecasting models", "from": "MONASH DATASETS", "source_id": "e6ec99a117b9abd42452ff51cd6e8f0b", "to": "DARTS DATASETS", "width": 1.0}, {"description": "Mask sampling strategy is related to context length as it is a type of training method used for the model", "from": "CONTEXT LENGTH", "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "to": "MASK SAMPLING STRATEGY", "width": 1.0}, {"description": "Chronos-T5 is related to context length as context length is used to represent Chronos-T5", "from": "CONTEXT LENGTH", "source_id": "56613213ed14f292e8ff44f4f0a8bab1", "to": "CHRONOS-T5", "width": 1.0}, {"description": "Training steps are related to context length as context length is used to represent training steps", "from": "CONTEXT LENGTH", "source_id": "56613213ed14f292e8ff44f4f0a8bab1", "to": "TRAINING STEPS", "width": 1.0}, {"description": "Context length is related to vocabulary size as vocabulary size is used to represent context length", "from": "CONTEXT LENGTH", "source_id": "56613213ed14f292e8ff44f4f0a8bab1", "to": "VOCABULARY SIZE", "width": 1.0}, {"description": "Prediction length is related to context length as context length is used to set prediction length", "from": "CONTEXT LENGTH", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "PREDICTION LENGTH", "width": 1.0}, {"description": "Context length is related to multiplier as multiplier is used to set context length", "from": "CONTEXT LENGTH", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "MULTIPLIER", "width": 1.0}, {"description": "Autoregressive sampling is related to context length as autoregressive sampling and context length are used to control the length of the context used during inference", "from": "CONTEXT LENGTH", "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "to": "AUTOREGRESSIVE SAMPLING", "width": 1.0}, {"description": "Parameters are related to scaling as scaling is a concept used to improve the performance of a model", "from": "SCALING", "source_id": "efb7975581b22620b8277417edeee3e9", "to": "PARAMETERS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"SCALING\" and \"TOKENIZATION\" are related concepts in the context of time series analysis. Tokenization involves scaling a time series into a different scale or range, which is closely tied to the concept of scaling. In fact, scaling is used to normalize time series values, making tokenization and scaling interdependent processes. This relationship highlights the importance of these concepts in preparing and preprocessing time series data for analysis and forecasting.\n\nIn the context of time series analysis, scaling is a crucial step in normalizing the values of a time series to a common range or scale. This process allows for more accurate and meaningful comparisons between different time series data. Tokenization, on the other hand, involves breaking down a time series into smaller, more manageable units or tokens, which can then be scaled and analyzed separately.\n\nOverall, the relationship between scaling and tokenization is a fundamental aspect of time series analysis, and understanding these concepts is essential for effective data preprocessing and analysis.", "from": "SCALING", "source_id": "f7e3207706b170296cb036538a709689,fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "TOKENIZATION", "width": 2.0}, {"description": "Scaling involves quantizing a time series into a discrete representation", "from": "SCALING", "source_id": "f7e3207706b170296cb036538a709689", "to": "QUANTIZATION", "width": 1.0}, {"description": "Parameters are related to downstream performance as the number of parameters affects the performance of LLMs", "from": "PARAMETERS", "source_id": "f7ce89346ea6715560163ef3a630a5df", "to": "DOWNSTREAM PERFORMANCE", "width": 1.0}, {"description": "T5 family models have a large number of parameters", "from": "PARAMETERS", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "T5 FAMILY", "width": 1.0}, {"description": "Parameters are related to corpus as corpus is used to train models with parameters", "from": "PARAMETERS", "source_id": "1f1221583d838c2407fe9864225e9eda", "to": "CORPUS", "width": 1.0}, {"description": "Backbone language model is related to parameters as parameters are updated during training of the backbone language model", "from": "PARAMETERS", "source_id": "fececbac281c1e2b13921f378df30919", "to": "BACKBONE LANGUAGE MODEL", "width": 1.0}, {"description": "Parameters are related to memory as memory is used to store the model\u0027s weights", "from": "PARAMETERS", "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "to": "MEMORY", "width": 1.0}, {"description": "Parameters are related to speed as speed is a measure of the model\u0027s computational efficiency", "from": "PARAMETERS", "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "to": "SPEED", "width": 1.0}, {"description": "GPT4TS has parameters that can be adjusted or updated", "from": "PARAMETERS", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "GPT4TS", "width": 1.0}, {"description": "FLOPS is related to TPUv5e6 as TPUv5e6 is a device that can perform FLOPS", "from": "FLOPS", "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "to": "TPUV5E6", "width": 1.0}, {"description": "ETT datasets are related to tokens as tokens refer to the individual units of text or data in a model", "from": "ETT DATASETS", "source_id": "f7ce89346ea6715560163ef3a630a5df", "to": "TOKENS", "width": 1.0}, {"description": "Model is related to ETT datasets as the model is trained and tested on ETT datasets", "from": "ETT DATASETS", "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "to": "MODEL", "width": 1.0}, {"description": "ETT datasets are related to rolling validation task as the task is used to evaluate the model\u0027s performance on ETT datasets", "from": "ETT DATASETS", "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "to": "ROLLING VALIDATION TASK", "width": 1.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe ETT datasets are closely related to prediction horizons, as prediction horizons serve as a crucial metric for evaluating the performance of models on these datasets. Specifically, the ETT datasets are utilized for evaluation purposes at various prediction horizons, indicating that the datasets are designed to assess the accuracy and effectiveness of models in forecasting future values over different time intervals.\n\nThis summary integrates the information from both descriptions, resolving any potential contradictions and providing a coherent understanding of the relationship between ETT datasets and prediction horizons. The summary is written in the third person and includes the entity names for context.", "from": "ETT DATASETS", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796,500710c8a95f1310d383fa71fd806c1c", "to": "PREDICTION HORIZONS", "width": 2.0}, {"description": "ETT datasets are related to ILI dataset as they are both used for long-term forecasting", "from": "ETT DATASETS", "source_id": "74527a4337ed6919731be520311ae774", "to": "ILI DATASET", "width": 1.0}, {"description": "Vocabulary is related to tokens as tokens are the individual units of text used in a language model", "from": "TOKENS", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "VOCABULARY", "width": 1.0}, {"description": "Bins are related to tokens as tokens are used to represent the bins in the Chronos model", "from": "TOKENS", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "BINS", "width": 1.0}, {"description": "Tokens are related to precision as the precision of the tokens affects the accuracy of the Chronos model", "from": "TOKENS", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "PRECISION", "width": 1.0}, {"description": "Training dataset is related to global batch-size as global batch-size affects the training of a model", "from": "TRAINING DATASET", "source_id": "f7ce89346ea6715560163ef3a630a5df", "to": "GLOBAL BATCH-SIZE", "width": 1.0}, {"description": "Checkpoints are related to TPUv5e6 as TPUv5e6 is a type of setup used for training models", "from": "CHECKPOINTS", "source_id": "f7ce89346ea6715560163ef3a630a5df", "to": "TPUV5E6", "width": 1.0}, {"description": "Recent work is related to GD23 as GD23 is a paper that discusses scaling studies in LLMs", "from": "RECENT WORK", "source_id": "f7ce89346ea6715560163ef3a630a5df", "to": "GD23", "width": 1.0}, {"description": "Autoregressive decoding is related to ZCZX23 as ZCZX23 is a paper that discusses autoregressive decoding in time-series forecasting", "from": "AUTOREGRESSIVE DECODING", "source_id": "f7ce89346ea6715560163ef3a630a5df", "to": "ZCZX23", "width": 1.0}, {"description": "Model is related to future as the model is used to predict future time-steps", "from": "MODEL", "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "to": "FUTURE", "width": 1.0}, {"description": "Output patch length is related to model as it is a parameter of the model", "from": "MODEL", "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "to": "OUTPUT_PATCH_LEN", "width": 1.0}, {"description": "Input patch length is related to model as it is a parameter of the model", "from": "MODEL", "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "to": "INPUT_PATCH_LEN", "width": 1.0}, {"description": "A model card is related to a model as it provides information about the model\u0027s architecture, training data, and performance metrics", "from": "MODEL", "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "to": "MODEL CARD", "width": 1.0}, {"description": "A model is related to metrics as metrics are used to evaluate the performance of the model", "from": "MODEL", "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "to": "METRICS", "width": 1.0}, {"description": "Model is related to larger dataset regimes as larger datasets are used to train and evaluate the model", "from": "MODEL", "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "to": "LARGER DATASET REGIMES", "width": 1.0}, {"description": "Etsformer is a specific model mentioned in the text, as indicated by the use of the name \"Etsformer\"", "from": "MODEL", "source_id": "79c41aff65d584b4c8d4c769b82756d5", "to": "ETSFORMER", "width": 1.0}, {"description": "Autoets is a specific model mentioned in the text, as indicated by the use of the name \"Autoets\"", "from": "MODEL", "source_id": "79c41aff65d584b4c8d4c769b82756d5", "to": "AUTOETS", "width": 1.0}, {"description": "Croston\u0027s SBA is a specific model mentioned in the text, as indicated by the use of the name \"Croston\u0027s SBA\"", "from": "MODEL", "source_id": "79c41aff65d584b4c8d4c769b82756d5", "to": "CROSTONSBA", "width": 1.0}, {"description": "Autoformer is a specific model mentioned in the text, as indicated by the use of the name \"Autoformer\"", "from": "MODEL", "source_id": "79c41aff65d584b4c8d4c769b82756d5", "to": "AUTOFORMER", "width": 1.0}, {"description": "NPTS is a specific model mentioned in the text, as indicated by the use of the name \"NPTS\"", "from": "MODEL", "source_id": "79c41aff65d584b4c8d4c769b82756d5", "to": "NPTS", "width": 1.0}, {"description": "Autoarima is a specific model mentioned in the text, as indicated by the use of the name \"Autoarima\"", "from": "MODEL", "source_id": "79c41aff65d584b4c8d4c769b82756d5", "to": "AUTOARIMA", "width": 1.0}, {"description": "Dynamic Optimize is a specific model mentioned in the text, as indicated by the use of the name \"Dynamic Optimize\"", "from": "MODEL", "source_id": "79c41aff65d584b4c8d4c769b82756d5", "to": "DYNAMICOPTIMIZE", "width": 1.0}, {"description": "The training process is related to the model as the model is trained using the training process", "from": "MODEL", "source_id": "6ea15432a841705c2e74cbc01f6004e9", "to": "TRAINING PROCESS", "width": 1.0}, {"description": "The model is related to masked multi-head attention as it is used to run the training process in parallel across several batches", "from": "MODEL", "source_id": "6ea15432a841705c2e74cbc01f6004e9", "to": "MASKED MULTI-HEAD ATTENTION", "width": 1.0}, {"description": "F1 score is related to the model as it is used to evaluate the performance of the model", "from": "MODEL", "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "to": "F1 SCORE", "width": 1.0}, {"description": "Model is related to the transformer-based encoder-decoder as it is a type of model architecture used in the paper", "from": "MODEL", "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "to": "TRANSFORMER-BASED ENCODER-DECODER", "width": 1.0}, {"description": "Model is related to the feed-forward network as it is a type of neural network architecture used in the paper", "from": "MODEL", "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "to": "FEED-FORWARD NETWORK", "width": 1.0}, {"description": "Model is related to self-conditioning as it is a technique used in the paper to improve the performance of the model", "from": "MODEL", "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "to": "SELF-CONDITIONING", "width": 1.0}, {"description": "Model is related to adversarial loss as it is a type of loss function used in the paper to improve the robustness of the model", "from": "MODEL", "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "to": "ADVERSARIAL LOSS", "width": 1.0}, {"description": "Model is related to reconstruction loss as it is a type of loss function used in the paper to evaluate the performance of the model", "from": "MODEL", "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "to": "RECONSTRUCTION LOSS", "width": 1.0}, {"description": "Model is related to meta-learning as it is a type of machine learning used in the paper to train the model", "from": "MODEL", "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "to": "META-LEARNING", "width": 1.0}, {"description": "Future is related to time-step as time-step is a unit of time in the future", "from": "FUTURE", "source_id": "cd4ff69415c7b37cec643f8289d1c98d", "to": "TIME-STEP", "width": 1.0}, {"description": "PatchTST(ZS) and TimesFM(ZS) are models used for zero-shot forecasting, which are compared in the paper", "from": "TIMESFM(ZS)", "source_id": "39365aee753fb73130c208fdf4046bb7", "to": "PATCHTST(ZS)", "width": 1.0}, {"description": "Forecasting is related to pretraining as pretraining is the process of training a model on a large corpus of data before fine-tuning it on a specific task", "from": "FORECASTING", "source_id": "63e284ec7e76fa859cc46b72d3746654", "to": "PRETRAINING", "width": 1.0}, {"description": "Forecasting is related to frequency analysis as frequency analysis can be used to identify patterns and trends in a time series, which can inform forecasting", "from": "FORECASTING", "source_id": "171fb6df1bf9905b151dfb846d75d0f8", "to": "FREQUENCY ANALYSIS", "width": 1.0}, {"description": "AutoTheta is used for time series forecasting", "from": "FORECASTING", "source_id": "e900311a447985c0967f87fff49c58b8", "to": "AUTOTHETA", "width": 1.0}, {"description": "AutoETS is used for time series forecasting", "from": "FORECASTING", "source_id": "e900311a447985c0967f87fff49c58b8", "to": "AUTOETS", "width": 1.0}, {"description": "Forecasting is related to metric as metric is used to evaluate the performance of a model", "from": "FORECASTING", "source_id": "e900311a447985c0967f87fff49c58b8", "to": "METRIC", "width": 1.0}, {"description": "MSE is used to evaluate the performance of a model in time series forecasting", "from": "FORECASTING", "source_id": "e900311a447985c0967f87fff49c58b8", "to": "MSE", "width": 1.0}, {"description": "AutoARIMA is used for time series forecasting", "from": "FORECASTING", "source_id": "e900311a447985c0967f87fff49c58b8", "to": "AUTOARIMA", "width": 1.0}, {"description": "Average is related to forecasting as forecasting often involves calculating averages or other statistical measures to make predictions", "from": "FORECASTING", "source_id": "cf0d73cbe44e03ef7300f5c53b72090a", "to": "AVERAGE", "width": 1.0}, {"description": "Based on the provided information, the primary entities are \"HOURLY\" and \"DAILY\", which are related to frequencies or granularities of time-series data. \n\nAfter analyzing the descriptions, it can be concluded that both \"HOURLY\" and \"DAILY\" refer to different time periods or frequencies of data. The descriptions \"Daily and hourly are frequencies of data or time periods\" and \"Hourly and daily are granularities of time-series data\" are consistent with each other, suggesting that both entities are used to describe the frequency or granularity of time-series data.\n\nTherefore, a comprehensive summary of the data can be written as follows:\n\nThe entities \"HOURLY\" and \"DAILY\" are related to the frequencies or granularities of time-series data, representing different time periods or frequencies of data. They are used to describe the frequency or granularity of time-series data, which can be useful in various applications such as time-series forecasting, frequency analysis, and long-term series forecasting.\n\nThis summary is written in third person and includes information collected from all the descriptions, resolving any potential contradictions and providing a single, coherent description of the entities.", "from": "HOURLY", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a,f8afc5a341360ab82dfbe9ebbb7f8e84", "to": "DAILY", "width": 2.0}, {"description": "Lag indices include hourly frequency", "from": "HOURLY", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "LAG INDICES", "width": 1.0}, {"description": "Daily and quarterly are granularities of time-series data", "from": "DAILY", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "QUARTERLY", "width": 1.0}, {"description": "Weather is related to daily as weather is typically measured and reported on a daily basis", "from": "DAILY", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "WEATHER", "width": 1.0}, {"description": "Tourism is related to daily as tourism activities and attractions are often measured and reported on a daily basis", "from": "DAILY", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "TOURISM", "width": 1.0}, {"description": "Weekly and daily are frequencies of data or time periods", "from": "DAILY", "source_id": "f8afc5a341360ab82dfbe9ebbb7f8e84", "to": "WEEKLY", "width": 1.0}, {"description": "Lag indices include daily frequency", "from": "DAILY", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "LAG INDICES", "width": 1.0}, {"description": "Quarterly and yearly are granularities of time-series data", "from": "QUARTERLY", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "YEARLY", "width": 1.0}, {"description": "Weather is related to quarterly as weather patterns can be observed and analyzed over a period of quarters", "from": "QUARTERLY", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "WEATHER", "width": 1.0}, {"description": "Tourism is related to quarterly as tourism activities and attractions are often measured and reported on a quarterly basis", "from": "QUARTERLY", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "TOURISM", "width": 1.0}, {"description": "Lag indices include quarterly frequency", "from": "QUARTERLY", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "LAG INDICES", "width": 1.0}, {"description": "Yearly and 10 minutes are granularities of time-series data", "from": "YEARLY", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "10 MINUTES", "width": 1.0}, {"description": "Weather is related to yearly as weather patterns can be observed and analyzed over a period of years", "from": "YEARLY", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "WEATHER", "width": 1.0}, {"description": "Tourism is related to yearly as tourism activities and attractions are often measured and reported on a yearly basis", "from": "YEARLY", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "TOURISM", "width": 1.0}, {"description": "10 minutes and 15 minutes are granularities of time-series data", "from": "10 MINUTES", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "15 MINUTES", "width": 1.0}, {"description": "200M model is used in the finetuning study in Appendix A.3", "from": "200M", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "APPENDIX A.3", "width": 1.0}, {"description": "Appendix A.3 is a section that provides a finetuning study that compares to ZNW+23", "from": "APPENDIX A.3", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "ZNW+23", "width": 1.0}, {"description": "Downstream tasks are related to data privacy concerns", "from": "DOWNSTREAM TASKS", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "DATA PRIVACY", "width": 1.0}, {"description": "Bias is related to covariates used to train the model", "from": "BIAS", "source_id": "dc2c05938eb6dbe0217f4c9e6b111e2a", "to": "COVARIATES", "width": 1.0}, {"description": "Bias table is used to store learnable biases, which are used to adjust the attention matrix", "from": "BIAS", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "BIAS TABLE", "width": 1.0}, {"description": "Bias is used to adjust the attention matrix", "from": "BIAS", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "ATTENTION MATRIX", "width": 1.0}, {"description": "Covariates should be considered when creating a model card to provide information about a machine learning model\u0027s limitations and potential biases", "from": "COVARIATES", "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "to": "MODEL CARD", "width": 1.0}, {"description": "Covariate handling is related to covariates as it involves incorporating additional variables or features into a model", "from": "COVARIATES", "source_id": "9b150491b487d5b2da482652e2fe509d", "to": "COVARIATE HANDLING", "width": 1.0}, {"description": "Covariates are related to date features as date features are a type of covariate", "from": "COVARIATES", "source_id": "9b150491b487d5b2da482652e2fe509d", "to": "DATE FEATURES", "width": 1.0}, {"description": "Model color is an example of a time-independent covariate", "from": "COVARIATES", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "MODEL COLOR", "width": 1.0}, {"description": "Sale days are an example of a time-varying covariate", "from": "COVARIATES", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "SALE DAYS", "width": 1.0}, {"description": "Biased forecasts can lead to unfair outcomes, such as increased police presence in certain communities", "from": "CRIME RATES", "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "to": "BIASED FORECASTS", "width": 1.0}, {"description": "Biased forecasts can have a disproportionate impact on certain communities", "from": "DISPROPORTIONATE IMPACT", "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "to": "BIASED FORECASTS", "width": 1.0}, {"description": "Releasing the exact details of the datasets used for training can aid in open weights release and model transparency", "from": "DATA SOURCES", "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "to": "OPEN WEIGHTS RELEASE", "width": 1.0}, {"description": "SOTA LLMs can be trained on a TPUv5e, which is a specific type of computing device", "from": "SOTA LLMS", "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "to": "TPUV5E", "width": 1.0}, {"description": "Human-in-the-loop can be used to address limitations of a machine learning model or approach", "from": "HUMAN-IN-THE-LOOP", "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "to": "LIMITATIONS", "width": 1.0}, {"description": "Future work can be used to address limitations of a machine learning model or approach", "from": "LIMITATIONS", "source_id": "a2f45b87ea2a0aa02b6e62e9700f20f1", "to": "FUTURE WORK", "width": 1.0}, {"description": "Future work would profit from expanding and varying this testing set", "from": "FUTURE WORK", "source_id": "f8afc5a341360ab82dfbe9ebbb7f8e84", "to": "TESTING SET", "width": 1.0}, {"description": "Loss function is related to framework as it is used within the framework to measure the difference between predicted and actual values", "from": "LOSS FUNCTION", "source_id": "9b150491b487d5b2da482652e2fe509d", "to": "FRAMEWORK", "width": 1.0}, {"description": "Framework is related to covariate handling as it provides a structure for incorporating additional variables or features into a model", "from": "COVARIATE HANDLING", "source_id": "9b150491b487d5b2da482652e2fe509d", "to": "FRAMEWORK", "width": 1.0}, {"description": "Date features are related to joint universal representation as they are used to create a single representation that combines multiple variables or features", "from": "DATE FEATURES", "source_id": "9b150491b487d5b2da482652e2fe509d", "to": "JOINT UNIVERSAL REPRESENTATION", "width": 1.0}, {"description": "Zero-shot setting is related to in-context as it involves training a model on a specific task or dataset within a specific context or environment", "from": "ZERO-SHOT SETTING", "source_id": "9b150491b487d5b2da482652e2fe509d", "to": "IN-CONTEXT", "width": 1.0}, {"description": "In-context is related to linear regression as it involves using linear regression to model the relationship between a dependent variable and one or more independent variables", "from": "IN-CONTEXT", "source_id": "9b150491b487d5b2da482652e2fe509d", "to": "LINEAR REGRESSION", "width": 1.0}, {"description": "Linear regression is related to residual model as it involves using a residual model to predict the residual or difference between the actual and predicted values", "from": "LINEAR REGRESSION", "source_id": "9b150491b487d5b2da482652e2fe509d", "to": "RESIDUAL MODEL", "width": 1.0}, {"description": "Metrics are related to msMAPE as msMAPE is a metric used to evaluate the performance of a machine learning model", "from": "METRICS", "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "to": "MSMAPE", "width": 1.0}, {"description": "Monash benchmarks are related to multivariate datasets as multivariate datasets are used to evaluate the performance of machine learning models for time-series forecasting", "from": "MONASH BENCHMARKS", "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "to": "MULTIVARIATE DATASETS", "width": 1.0}, {"description": "GPT4TS is related to GPT2 as GPT2 is a machine learning model used for natural language processing", "from": "GPT4TS", "source_id": "7cb1ca3f60421fbd20d6ae39bbf2b296", "to": "GPT2", "width": 1.0}, {"description": "The finetuned model performs better than GPT4TS on ETTh1, ETTh2, and ETTm1 datasets", "from": "GPT4TS", "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "to": "FINE-TUNED MODEL", "width": 1.0}, {"description": "GPT4TS uses patch embeddings to represent time series data", "from": "GPT4TS", "source_id": "bc54a718d1886698232d578fd88c3ac7", "to": "PATCH EMBEDDINGS", "width": 1.0}, {"description": "TFT is related to GPT4TS as both are models mentioned in the text", "from": "GPT4TS", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "TFT", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data is as follows:\n\nGPT4TS and DLINER are two entities that are closely related to each other. Both models are mentioned in the text as being used for forecasting purposes. Specifically, DLINER is a model for time series forecasting, and GPT4TS is related to DLINER in this context. This suggests that GPT4TS may also be used for time series forecasting, or that the two models are used together for forecasting tasks.\n\nThe text also implies that both models are used for long-term series forecasting, as well as frequency analysis, which are common techniques used in time series forecasting. The use of multi-head cross-attention in the text suggests that the models may be using advanced techniques to analyze and forecast time series data.\n\nOverall, the relationship between GPT4TS and DLINER is one of collaboration and shared purpose, with both models being used for time series forecasting and related tasks.\n\nRelevant information from the nearby text includes:\n\n* The text is written in English, suggesting that the models are being discussed in an academic or technical context.\n* The text mentions technical terms and mathematical equations, which are commonly used in time series forecasting and related fields.\n* The text references academic papers and authors, which suggests that the discussion is based on established research and knowledge in the field.\n\nThis information provides context for the relationship between GPT4TS and DLINER, and suggests that the two models are being used in a collaborative and interdisciplinary context.", "from": "GPT4TS", "source_id": "91e161ba596a0cbbcae541ddb2106310,af36d1634490149b96980fb1dff57cd1,f6fac8e5c6fd12724fe3aa84a2e1cfa6", "to": "DLINER", "width": 3.0}, {"description": "Chronos forecasting is related to GPT4TS as GPT4TS is a model that generates point forecasts for time series data", "from": "GPT4TS", "source_id": "56de1d5a5b467101344afa4248d829dc", "to": "CHRONOS FORECASTING", "width": 1.0}, {"description": "GPT4TS is related to local statistical models as GPT4TS performs better than local statistical models", "from": "GPT4TS", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "GPT4TS is compared to other models based on the MASE metric, as it produces point forecasts", "from": "GPT4TS", "source_id": "2eea45c6111e468189580a21686cb14a", "to": "MASE", "width": 1.0}, {"description": "GPT4TS is outperformed by Chronos models on Benchmark II", "from": "GPT4TS", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "CHRONOS MODELS", "width": 1.0}, {"description": "GPT4TS is used for inference on AWS EC2", "from": "GPT4TS", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "AWS EC2", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nGPT4TS and ILI are two entities that are related to each other in the context of time series forecasting. Specifically, ILI (Influenza-Like Illness) is related to GPT4TS as it is a forecasting horizon used in the model. This suggests that GPT4TS is a model designed to forecast time series data, including ILI, which is a critical metric for monitoring and predicting influenza-like illnesses.\n\nIn other words, GPT4TS is a model that utilizes ILI as one of its forecasting horizons, indicating that the model is capable of predicting ILI-related time series data. This relationship highlights the importance of ILI in the context of time series forecasting and the role of GPT4TS in predicting this critical metric.\n\nOverall, the summary provides a clear understanding of the relationship between GPT4TS and ILI, highlighting the model\u0027s ability to forecast ILI-related time series data and the significance of ILI in this context.", "from": "GPT4TS", "source_id": "9ba0189af2ef0720a721c16eef0f0788,f6fac8e5c6fd12724fe3aa84a2e1cfa6", "to": "ILI", "width": 2.0}, {"description": "M4 is related to GPT4TS as GPT4TS is used for short-term time series forecasting on M4", "from": "GPT4TS", "source_id": "d4551c2839eaa68a7cb7324089956581", "to": "M4", "width": 1.0}, {"description": "TIME-LLM reprogrammed excels over GPT4TS in few-shot learning tasks", "from": "GPT4TS", "source_id": "6d66c2ea37e25b646a13d751c05a8e4d", "to": "TIME-LLM (REPROGRAMMED)", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nGPT4TS and DLINEAR are two time series models that are compared in a table. They are related to each other as both models are mentioned in the text, indicating that they are being evaluated or compared in some capacity. The comparison between GPT4TS and DLINEAR is likely focused on their performance, accuracy, or other relevant metrics in the context of time series forecasting.\n\nGiven the context of the text, it appears that the comparison between GPT4TS and DLINEAR is likely being conducted in a research or academic setting, as indicated by the presence of technical terms, mathematical equations, and references to academic papers. The use of English-language abbreviations and citations also suggests that the text is written in English and is likely from a research paper or technical document.\n\nOverall, the summary provides a clear understanding of the relationship between GPT4TS and DLINEAR, as well as the context in which they are being compared.", "from": "GPT4TS", "source_id": "9ba0189af2ef0720a721c16eef0f0788,fd1092903d83bf6e90a6caa371d7c514", "to": "DLINEAR", "width": 2.0}, {"description": "Reformer is compared to GPT4TS in terms of performance", "from": "GPT4TS", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "REFORMER", "width": 1.0}, {"description": "GPT4TS and Autoformer are compared on long-term forecasting tasks, as shown in Fig. 7 and Fig. 8", "from": "GPT4TS", "source_id": "ab91381dd032db318e5aec1ad5b914a6", "to": "AUTOFORMER", "width": 1.0}, {"description": "GPT4TS uses parameter-efficient fine-tuning to fine-tune its parameters", "from": "GPT4TS", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "PARAMETER-EFFICIENT FINE-TUNING", "width": 1.0}, {"description": "GPT4TS uses memory to store its parameters and other data", "from": "GPT4TS", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "MEMORY", "width": 1.0}, {"description": "GPT4TS has a speed that can be improved or optimized", "from": "GPT4TS", "source_id": "84bc2afcbbd278961c3c7a637c6a189e", "to": "SPEED", "width": 1.0}, {"description": "The finetuned model performs better than GPT2 on the downstream forecasting task", "from": "GPT2", "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "to": "FINE-TUNED MODEL", "width": 1.0}, {"description": "The finetuned model weights are the weights of the model that have been updated during the finetuning process", "from": "FINE-TUNED MODEL WEIGHTS", "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "to": "FINE-TUNED MODEL", "width": 1.0}, {"description": "Table 2 presents the results of the experiment using the finetuned model", "from": "TABLE 2", "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "to": "FINE-TUNED MODEL", "width": 1.0}, {"description": "Section 5 is related to Table 2 as it provides a brief description of each dataset used for empirical evaluation", "from": "TABLE 2", "source_id": "e5e4a8f03f502fada5b17cba5dc942ba", "to": "SECTION 5", "width": 1.0}, {"description": "Table 13 presents the baseline numbers used in the experiment", "from": "TABLE 13", "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "to": "FINE-TUNED MODEL", "width": 1.0}, {"description": "Table 14 presents the results of another experiment used as a comparison", "from": "TABLE 14", "source_id": "ad421ec9ba83531336aaf3bec414b3d5", "to": "FINE-TUNED MODEL", "width": 1.0}, {"description": "PatchTST(ZS) is compared to llmtime(ZS) in the evaluation", "from": "PATCHTST(ZS)", "source_id": "4ade7764ebe998b5f35a7fd2b58d4796", "to": "LLMTIME(ZS)", "width": 1.0}, {"description": "GP and llmtime(ZS) are models that perform well for time-series forecasting tasks", "from": "GP", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "LLMTIME(ZS)", "width": 1.0}, {"description": "GP and NAIVE are models that perform well for time-series forecasting tasks", "from": "GP", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "NAIVE", "width": 1.0}, {"description": "llmtime(ZS) and NAIVE are models that perform well for time-series forecasting tasks", "from": "LLMTIME(ZS)", "source_id": "d40f5dc25597e4e4d7c30b8bfd98f89a", "to": "NAIVE", "width": 1.0}, {"description": "llmtime(ZS) is related to SES as SES is a model used for evaluation", "from": "LLMTIME(ZS)", "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "to": "SES", "width": 1.0}, {"description": "Hyndman \u0026 Athanasopoulos (2018) is related to Naive as Naive is a model mentioned in the text", "from": "NAIVE", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "HYNDMAN \u0026 ATHANASOPOULOS (2018)", "width": 1.0}, {"description": "Naive is outperformed by Chronos models on Benchmark II", "from": "NAIVE", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "CHRONOS MODELS", "width": 1.0}, {"description": "Based on the provided information, the expert Data Scientist has analyzed the data and generated a comprehensive summary of the entities and their relationships.\n\nThe entities in question are \"NAIVE\" and \"SEASONAL NAIVE,\" which are both statistical models used for forecasting. \n\nA closer examination of the descriptions reveals that \"Naive\" is related to \"Seasonal Naive\" as \"Seasonal Naive\" is a type of \"Naive\" model. This indicates a hierarchical relationship between the two entities, with \"Naive\" being the parent model and \"Seasonal Naive\" being a specialized variant of it.\n\nIn the context of time series forecasting, both \"Naive\" and \"Seasonal Naive\" models are used to make predictions based on historical data. However, the \"Seasonal Naive\" model takes into account seasonal patterns and trends, which are not considered in the basic \"Naive\" model.\n\nOverall, the summary of the entities and their relationships is as follows:\n\n\"NAIVE\" and \"SEASONAL NAIVE\" are two related statistical models used for time series forecasting. \"NAIVE\" is a basic model that makes predictions based on historical data, while \"SEASONAL NAIVE\" is a specialized variant of \"NAIVE\" that also considers seasonal patterns and trends.", "from": "NAIVE", "source_id": "116332ac4538a1430c83a34fcbec22d1,2565ae205d4c98342168bb67a4f7a309", "to": "SEASONAL NAIVE", "width": 2.0}, {"description": "AutoARIMA and Naive are related as they are both statistical models used for forecasting", "from": "NAIVE", "source_id": "2565ae205d4c98342168bb67a4f7a309", "to": "AUTOARIMA", "width": 1.0}, {"description": "The Heart Rate Dataset is related to heart rate as it contains heart rate data", "from": "HEART RATE", "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "to": "HEART RATE DATASET", "width": 1.0}, {"description": "MAE (Arithmetic Mean) is related to MAE (Geometric Mean) as both are metrics used to evaluate the performance of a model", "from": "MAE (ARITHMETIC MEAN)", "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "to": "MAE (GEOMETRIC MEAN)", "width": 1.0}, {"description": "SES is related to Theta as Theta is a model used for evaluation", "from": "SES", "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "to": "THETA", "width": 1.0}, {"description": "Theta is related to TBATS as TBATS is a model used for evaluation", "from": "THETA", "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "to": "TBATS", "width": 1.0}, {"description": "MSTL and Theta are statistical models used for time series forecasting", "from": "THETA", "source_id": "6771b6279846e780d6807b15184ae008", "to": "MSTL", "width": 1.0}, {"description": "Theta and CES are statistical models used for time series forecasting", "from": "THETA", "source_id": "6771b6279846e780d6807b15184ae008", "to": "CES", "width": 1.0}, {"description": "TBATS is related to ETS as ETS is a model used for evaluation", "from": "TBATS", "source_id": "41b0bd14ce7ff7419b7ee1f78b4701ae", "to": "ETS", "width": 1.0}, {"description": "ETS and MSTL are statistical models used for time series forecasting", "from": "ETS", "source_id": "6771b6279846e780d6807b15184ae008", "to": "MSTL", "width": 1.0}, {"description": "Statistical models are related to ETS as ETS is a type of statistical model", "from": "ETS", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "STATISTICAL MODELS", "width": 1.0}, {"description": "Weather is related to weekly as weather patterns can be observed and analyzed over a period of weeks", "from": "WEEKLY", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "WEATHER", "width": 1.0}, {"description": "Tourism is related to weekly as tourism activities and attractions are often measured and reported on a weekly basis", "from": "WEEKLY", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "TOURISM", "width": 1.0}, {"description": "Monthly and weekly are frequencies of data or time periods", "from": "WEEKLY", "source_id": "f8afc5a341360ab82dfbe9ebbb7f8e84", "to": "MONTHLY", "width": 1.0}, {"description": "Lag indices include weekly frequency", "from": "WEEKLY", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "LAG INDICES", "width": 1.0}, {"description": "Weather is related to monthly as weather patterns can be observed and analyzed over a period of months", "from": "MONTHLY", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "WEATHER", "width": 1.0}, {"description": "Tourism is related to monthly as tourism activities and attractions are often measured and reported on a monthly basis", "from": "MONTHLY", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "TOURISM", "width": 1.0}, {"description": "Lag indices include monthly frequency", "from": "MONTHLY", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "LAG INDICES", "width": 1.0}, {"description": "Tourism is related to CIF 2016 as CIF 2016 may have involved tourism-related activities or attractions", "from": "TOURISM", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "CIF 2016", "width": 2.0}, {"description": "NN5 weekly is related to tourism as NN5 weekly may be used to analyze or predict tourism-related data", "from": "TOURISM", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "NN5 WEEKLY", "width": 1.0}, {"description": "Kaggle hosts the Tourism Forecasting competition that uses the Tourism dataset", "from": "TOURISM", "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "to": "KAGGLE", "width": 1.0}, {"description": "CIF 2016 is related to COVID deaths as the conference may have discussed or presented data on COVID-19", "from": "CIF 2016", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "COVID DEATHS", "width": 1.0}, {"description": "CIF 2016 is related to FRED MD as the conference may have discussed or presented data on economic metrics", "from": "CIF 2016", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "FRED MD", "width": 1.0}, {"description": "CIF 2016 is related to traffic hourly as the conference may have discussed or presented data on traffic patterns", "from": "CIF 2016", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "TRAFFIC HOURLY", "width": 1.0}, {"description": "CIF 2016 is related to traffic weekly as the conference may have discussed or presented data on traffic patterns", "from": "CIF 2016", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "TRAFFIC WEEKLY", "width": 1.0}, {"description": "CIF 2016 is related to saugen day as the conference may have discussed or presented data on a specific day or event", "from": "CIF 2016", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "SAUGEN DAY", "width": 1.0}, {"description": "CIF 2016 is related to US births as the conference may have discussed or presented data on birth rates", "from": "CIF 2016", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "US BIRTHS", "width": 1.0}, {"description": "CIF 2016 is related to hospital as the conference may have discussed or presented data on hospital metrics", "from": "CIF 2016", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "HOSPITAL", "width": 1.0}, {"description": "CIF 2016 is related to Car Parts as both are datasets of retail data", "from": "CIF 2016", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "CAR PARTS", "width": 1.0}, {"description": "CIF 2016 dataset is sourced from banking data", "from": "CIF 2016", "source_id": "d4bc1397526f236029876d4fbc22a721", "to": "BANKING DATA", "width": 1.0}, {"description": "NN5 daily is related to NN5 weekly as both are models or algorithms used for specific tasks or applications", "from": "NN5 DAILY", "source_id": "edab6fd342bc0a563fd98a50eb8b3462", "to": "NN5 WEEKLY", "width": 1.0}, {"description": "Lag-Llama achieves strong performance in the platform-delay and weather datasets", "from": "WEATHER", "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "to": "PLATFORM-DELAY", "width": 1.0}, {"description": "Lag-Llama achieves strong performance in the weather and ETT-M2 datasets", "from": "WEATHER", "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "to": "ETT-M2", "width": 1.0}, {"description": "Nature is related to weather as weather is a dataset used for pretraining and fine-tuning", "from": "WEATHER", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "NATURE", "width": 1.0}, {"description": "KDD Cup 2018 is related to weather as both are datasets used for pretraining and fine-tuning", "from": "WEATHER", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "KDD CUP 2018", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"WEATHER\" and \"SUNSPOT\" are closely related concepts in the context of nature. Both are datasets used for pretraining and fine-tuning, suggesting a connection between the two in terms of their application and usage. Furthermore, the relationship between weather and sunspot is rooted in their shared association with natural phenomena, indicating a deeper connection between the two entities.\n\nThis summary is based on the information provided in the description list, which highlights the shared nature of the two entities. The fact that both are related to nature and used for pretraining and fine-tuning datasets further reinforces their connection.\n\nIt is worth noting that the summary does not imply a direct causal relationship between weather and sunspot, but rather a connection based on their shared characteristics and applications. This summary provides a concise and coherent description of the relationship between the two entities, taking into account the information provided in the description list.", "from": "WEATHER", "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "to": "SUNSPOT", "width": 2.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"ET T M2\" is closely related to the entity \"WEATHER\". This relationship is established through the use of weather-related metrics to evaluate the performance of ET T M2, as well as the fact that ET T M2 is a model used to make predictions or forecasts that take into account weather conditions. In other words, ET T M2 is a model that utilizes weather data to make informed predictions, and its performance is assessed using weather-related metrics.\n\nThis summary is based on the information provided in the description list, which indicates a clear connection between ET T M2 and weather. The use of weather-related metrics to evaluate ET T M2\u0027s performance suggests that the model is designed to handle weather data and make predictions based on this information. The fact that ET T M2 is used to make predictions or forecasts that take into account weather further reinforces this connection.\n\nOverall, the relationship between ET T M2 and weather is one of dependency, with ET T M2 relying on weather data to make informed predictions, and its performance being evaluated using weather-related metrics.", "from": "WEATHER", "source_id": "919ff66615400ea06113a2a59ff34ef0,bb03c20a6fc1d9af2f3cbfa55b50cfb8", "to": "ET T M2", "width": 2.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"ET T M1\" is closely related to the entity \"WEATHER\". This relationship is established through the use of weather-related metrics to evaluate the performance of ET T M1, indicating that ET T M1 is a model that relies on weather data to make predictions or forecasts. In other words, ET T M1 is a weather forecasting model that utilizes weather-related metrics to assess its accuracy and effectiveness.\n\nThis relationship is further reinforced by the fact that ET T M1 is used to make predictions or forecasts that take into account weather, suggesting that the model is designed to capture the complex interactions between weather patterns and other factors that influence weather outcomes.\n\nOverall, the summary highlights the strong connection between ET T M1 and weather, with ET T M1 being a weather forecasting model that relies on weather-related metrics to evaluate its performance and make accurate predictions.\n\nRelevant information from the nearby text is not provided, but based on the given information, the following inferences can be made:\n\n* ET T M1 is a type of machine learning model that is used for weather forecasting.\n* Weather-related metrics are used to evaluate the performance of ET T M1.\n* ET T M1 is designed to capture the complex interactions between weather patterns and other factors that influence weather outcomes.\n\nThese inferences are based on the provided descriptions and are consistent with the information provided.", "from": "WEATHER", "source_id": "919ff66615400ea06113a2a59ff34ef0,bb03c20a6fc1d9af2f3cbfa55b50cfb8", "to": "ET T M1", "width": 2.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nThe text mentions two entities: \"WEATHER\" and \"ECL\". These entities are related to each other, as ECL is used to forecast and predict weather-related metrics. Specifically, ECL is used to forecast weather, indicating a strong connection between the two entities.\n\nIn the context of machine learning and time series forecasting, ECL is utilized to analyze and predict weather-related metrics, which are essential components of weather forecasting. This suggests that ECL is a crucial tool for understanding and predicting weather patterns.\n\nOverall, the summary highlights the relationship between WEATHER and ECL, with ECL playing a key role in forecasting and predicting weather-related metrics. This information is relevant to the field of machine learning and time series forecasting, where understanding the relationships between entities is crucial for developing accurate models and predictions.", "from": "WEATHER", "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,91e161ba596a0cbbcae541ddb2106310,bb03c20a6fc1d9af2f3cbfa55b50cfb8", "to": "ECL", "width": 3.0}, {"description": "Weather is related to ET T h1 as ET T h1 is a model used to make predictions or forecasts that take into account weather", "from": "WEATHER", "source_id": "919ff66615400ea06113a2a59ff34ef0", "to": "ET T H1", "width": 1.0}, {"description": "Weather is related to ET T h2 as ET T h2 is a model used to make predictions or forecasts that take into account weather", "from": "WEATHER", "source_id": "919ff66615400ea06113a2a59ff34ef0", "to": "ET T H2", "width": 1.0}, {"description": "1stCount and Weather are concepts mentioned in the text, as indicated by the use of their names", "from": "WEATHER", "source_id": "1d6fb60c5060c25ae4791b03a0513a7f", "to": "1STCOUNT", "width": 1.0}, {"description": "PatchTST (2023) is related to weather as weather is a type of data used in short-term forecasting", "from": "WEATHER", "source_id": "27efab80b405b365d8e9dd9834dd1ca8", "to": "PATCHTST (2023)", "width": 1.0}, {"description": "COVID deaths are related to FRED MD as both datasets may be related to economic metrics", "from": "COVID DEATHS", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "FRED MD", "width": 1.0}, {"description": "COVID deaths are related to traffic hourly as both datasets may be related to mobility metrics", "from": "COVID DEATHS", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "TRAFFIC HOURLY", "width": 1.0}, {"description": "COVID deaths are related to traffic weekly as both datasets may be related to mobility metrics", "from": "COVID DEATHS", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "TRAFFIC WEEKLY", "width": 1.0}, {"description": "COVID deaths are related to saugen day as both datasets may be related to specific events or days", "from": "COVID DEATHS", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "SAUGEN DAY", "width": 1.0}, {"description": "COVID deaths are related to US births as both datasets may be related to demographic metrics", "from": "COVID DEATHS", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "US BIRTHS", "width": 1.0}, {"description": "COVID deaths are related to hospital as both datasets may be related to healthcare metrics", "from": "COVID DEATHS", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "HOSPITAL", "width": 1.0}, {"description": "Car Parts is related to COVID Deaths as both are datasets of healthcare and retail data", "from": "COVID DEATHS", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "CAR PARTS", "width": 1.0}, {"description": "COVID Deaths is related to Dominick as both are datasets of retail data", "from": "COVID DEATHS", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "DOMINICK", "width": 1.0}, {"description": "Covid Deaths dataset is sourced from COVID-19 deaths data", "from": "COVID DEATHS", "source_id": "d4bc1397526f236029876d4fbc22a721", "to": "COVID-19 DEATHS DATA", "width": 1.0}, {"description": "FRED MD is related to traffic hourly as both datasets may be related to economic metrics", "from": "FRED MD", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "TRAFFIC HOURLY", "width": 1.0}, {"description": "FRED MD is related to traffic weekly as both datasets may be related to economic metrics", "from": "FRED MD", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "TRAFFIC WEEKLY", "width": 1.0}, {"description": "FRED MD is related to saugen day as both datasets may be related to specific events or days", "from": "FRED MD", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "SAUGEN DAY", "width": 1.0}, {"description": "FRED MD is related to US births as both datasets may be related to demographic metrics", "from": "FRED MD", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "US BIRTHS", "width": 1.0}, {"description": "FRED MD is related to hospital as both datasets may be related to healthcare metrics", "from": "FRED MD", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "HOSPITAL", "width": 1.0}, {"description": "Traffic hourly is related to traffic weekly as both datasets may be related to traffic patterns", "from": "TRAFFIC HOURLY", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "TRAFFIC WEEKLY", "width": 1.0}, {"description": "Traffic hourly is related to saugen day as both datasets may be related to specific events or days", "from": "TRAFFIC HOURLY", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "SAUGEN DAY", "width": 1.0}, {"description": "Traffic hourly is related to US births as both datasets may be related to demographic metrics", "from": "TRAFFIC HOURLY", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "US BIRTHS", "width": 1.0}, {"description": "Traffic hourly is related to hospital as both datasets may be related to healthcare metrics", "from": "TRAFFIC HOURLY", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "HOSPITAL", "width": 1.0}, {"description": "Traffic weekly is related to saugen day as both datasets may be related to specific events or days", "from": "TRAFFIC WEEKLY", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "SAUGEN DAY", "width": 1.0}, {"description": "Traffic weekly is related to US births as both datasets may be related to demographic metrics", "from": "TRAFFIC WEEKLY", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "US BIRTHS", "width": 1.0}, {"description": "Traffic weekly is related to hospital as both datasets may be related to healthcare metrics", "from": "TRAFFIC WEEKLY", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "HOSPITAL", "width": 1.0}, {"description": "Saugen day is related to US births as both datasets may be related to demographic metrics", "from": "SAUGEN DAY", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "US BIRTHS", "width": 1.0}, {"description": "Saugen day is related to hospital as both datasets may be related to healthcare metrics", "from": "SAUGEN DAY", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "HOSPITAL", "width": 1.0}, {"description": "US births are related to hospital as both datasets may be related to healthcare metrics", "from": "US BIRTHS", "source_id": "57a3473e2c7c112cf81d520ea1ba95fa", "to": "HOSPITAL", "width": 1.0}, {"description": "Hospital dataset is sourced from medical product data", "from": "HOSPITAL", "source_id": "d4bc1397526f236029876d4fbc22a721", "to": "MEDICAL PRODUCT DATA", "width": 1.0}, {"description": "Table 4 presents the actual MAE numbers behind the main Figure 2a", "from": "FIGURE 2A", "source_id": "500710c8a95f1310d383fa71fd806c1c", "to": "TABLE 4", "width": 1.0}, {"description": "Figure 9 presents some examples of zero-shot forecasts", "from": "FIGURE 9", "source_id": "500710c8a95f1310d383fa71fd806c1c", "to": "ZERO-SHOT FORECASTS", "width": 1.0}, {"description": "Figure 9 is related to language model weights as it shows the performance of models initialized with language model weights", "from": "FIGURE 9", "source_id": "c7f04fa1168df64cce110e20a1b72b51", "to": "LANGUAGE MODEL WEIGHTS", "width": 1.0}, {"description": "Prediction length is related to prediction horizons as prediction horizons are used to evaluate models", "from": "PREDICTION HORIZONS", "source_id": "51ee17c1f0212d4c94010d8e376f649b", "to": "PREDICTION LENGTH", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"AUTOFORMER\" and \"ETSFORMER\" are two models mentioned in the text, as indicated by their names in the table. Both models are related to each other as they are methods for probabilistic forecasting. Specifically, they are both types of models used for time series forecasting. \n\nFurther analysis reveals that AutoFormer and ETSFormer are closely related, as they share a common application in time series forecasting. The text suggests that these models are designed to handle complex time series data, and their probabilistic forecasting capabilities make them suitable for a wide range of applications.\n\nIn terms of their relationship, it appears that AutoFormer and ETSFormer are both variants of transformer-based models, which are a type of neural network architecture commonly used in natural language processing and time series forecasting tasks. The use of transformer-based models in time series forecasting is a relatively recent development, and AutoFormer and ETSFormer are two of the key models in this area.\n\nOverall, the summary suggests that AutoFormer and ETSFormer are two closely related models that are designed for time series forecasting tasks. They share a common application and are both variants of transformer-based models, making them suitable for a wide range of time series forecasting tasks.", "from": "AUTOFORMER", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2,c84edbea28fbbed451e8d0b7df4ffb7c,f0c52387a7b3a5c3850fe6991f0a7c83", "to": "ETSFORMER", "width": 3.0}, {"description": "AUTOFORMER is related to CRPS as CRPS is used to evaluate the performance of AUTOFORMER", "from": "AUTOFORMER", "source_id": "990578022879395d00b7a5b229863c2f", "to": "CRPS", "width": 1.0}, {"description": "NBEATS and AUTOFORMER are models mentioned in the text, as indicated by the use of their names in the table", "from": "AUTOFORMER", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "NBEATS", "width": 1.0}, {"description": "AUTOFORMER and LAGLLAMA are models mentioned in the text, as indicated by the use of their names in the table", "from": "AUTOFORMER", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "LAGLLAMA", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe \"AUTOFORMER\" and \"STATIONARY\" entities are related to each other in the context of time series forecasting. Specifically, \"STATIONARY\" is a model for time series forecasting that is compared to \"AUTOFORMER\". Both models are mentioned in the text, indicating that they are relevant to the topic of time series forecasting.\n\nThis summary is based on the information provided in the description list, which states that \"Autoformer is related to Stationary as Stationary is a model for time series forecasting\" and \"Autoformer is related to Stationary as both are models mentioned in the text\". The third description, \"Autoformer is related to Stationary as Stationary is a model that is compared to Autoformer\", is consistent with the first two descriptions and provides additional context.\n\nOverall, the summary provides a clear understanding of the relationship between the \"AUTOFORMER\" and \"STATIONARY\" entities in the context of time series forecasting.", "from": "AUTOFORMER", "source_id": "7e97089185883c456c798ddc5ec86373,f6fac8e5c6fd12724fe3aa84a2e1cfa6,fd1092903d83bf6e90a6caa371d7c514", "to": "STATIONARY", "width": 3.0}, {"description": "ILI is related to Autoformer as it is a forecasting horizon used in the model", "from": "AUTOFORMER", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "ILI", "width": 8.0}, {"description": "Former and autoformer are both types of models used for time series forecasting", "from": "AUTOFORMER", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c", "to": "FORMER", "width": 1.0}, {"description": "Autoformer and LightTS are both types of models used for time series forecasting", "from": "AUTOFORMER", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c", "to": "LIGHTTS", "width": 1.0}, {"description": "Autoformer and Reformer are both types of models used for time series forecasting", "from": "AUTOFORMER", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c", "to": "REFORMER", "width": 1.0}, {"description": "Hyper-parameters are related to learning rate as learning rate is a hyper-parameter", "from": "HYPER-PARAMETERS", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "LEARNING RATE", "width": 1.0}, {"description": "Learning rate is related to layer norm as layer norm is used to normalize the input data", "from": "LEARNING RATE", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "LAYER NORM", "width": 1.0}, {"description": "The AdamW optimizer is used with a learning rate of 1 \u00d7 10\u22124", "from": "LEARNING RATE", "source_id": "5809618fe3a2014c2140601fcf103022", "to": "ADAMW OPTIMIZER", "width": 1.0}, {"description": "The learning rate is 1 \u00d7 10\u22124 and gradient clipping is applied at a norm of 1.0", "from": "LEARNING RATE", "source_id": "5809618fe3a2014c2140601fcf103022", "to": "GRADIENT CLIPPING", "width": 1.0}, {"description": "Batch size is related to learning rate as batch size and learning rate are used to control the step size of the model\u0027s updates during training", "from": "LEARNING RATE", "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "to": "BATCH SIZE", "width": 1.0}, {"description": "LayerNorm is used in the Transformer body to normalize the input features", "from": "LAYER NORM", "source_id": "a5d60c1a355b77187003182f6df57646", "to": "TRANSFORMER BODY", "width": 1.0}, {"description": "The decoder uses layer norm to normalize the input", "from": "LAYER NORM", "source_id": "4bdf596e75e1cb06d11b25e95491037e", "to": "DECODER", "width": 1.0}, {"description": "Residual blocks are related to FFN as FFN is used in residual blocks", "from": "RESIDUAL BLOCKS", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "FFN", "width": 1.0}, {"description": "FFN is related to model dimensions as model dimensions are used to represent the input data", "from": "FFN", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "MODEL DIMENSIONS", "width": 1.0}, {"description": "Model dimensions are related to output patch length as output patch length is a hyper-parameter", "from": "MODEL DIMENSIONS", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "OUTPUT PATCH LEN", "width": 1.0}, {"description": "Output patch length is related to input patch length as input patch length is a hyper-parameter", "from": "OUTPUT PATCH LEN", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "INPUT PATCH LEN", "width": 1.0}, {"description": "Input patch length is related to number of heads as number of heads is a hyper-parameter", "from": "INPUT PATCH LEN", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "NUM HEADS", "width": 1.0}, {"description": "Monash baselines are related to Darts baselines as Darts baselines are used for comparison with Monash baselines", "from": "MONASH BASELINES", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "DARTS BASELINES", "width": 1.0}, {"description": "Darts baselines are related to Informer baselines as Informer baselines are used for comparison with Darts baselines", "from": "DARTS BASELINES", "source_id": "673b0feee6eb5843954defc670e4ba29", "to": "INFORMER BASELINES", "width": 1.0}, {"description": "Synthetic curves are used to evaluate the performance of time-series forecasting models against ground truth", "from": "SYNTHETIC CURVES", "source_id": "e6ec99a117b9abd42452ff51cd6e8f0b", "to": "GROUND TRUTH", "width": 1.0}, {"description": "Forecasts are generated by time-series forecasting models and compared to ground truth", "from": "GROUND TRUTH", "source_id": "e6ec99a117b9abd42452ff51cd6e8f0b", "to": "FORECASTS", "width": 1.0}, {"description": "Token ID is related to ground truth as it is used to represent the actual or true value of a variable or outcome", "from": "GROUND TRUTH", "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "to": "TOKEN ID", "width": 1.0}, {"description": "Ground truth is related to median forecast as it is used to estimate the middle value of a set of predicted values", "from": "GROUND TRUTH", "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "to": "MEDIAN FORECAST", "width": 1.0}, {"description": "App is related to ground truth as it contains the visualization of the forecasts produced by Lag-Llama", "from": "GROUND TRUTH", "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "to": "APP", "width": 1.0}, {"description": "Dimensions are related to ground truth as the ground truth is used to determine the dimensions", "from": "GROUND TRUTH", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "DIMENSIONS", "width": 1.0}, {"description": "Ground truth is related to HitRate as HitRate is used to evaluate the performance of the anomaly detection model", "from": "GROUND TRUTH", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "HITRATE", "width": 1.0}, {"description": "Forecasts are produced by TimeGPT", "from": "FORECASTS", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "TIMEGPT", "width": 1.0}, {"description": "Output is used to produce forecasts", "from": "FORECASTS", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "OUTPUT", "width": 1.0}, {"description": "Representations are used to generate forecasts that refer to the predicted values or outcomes based on historical data", "from": "FORECASTS", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "REPRESENTATIONS", "width": 1.0}, {"description": "AnomalyBERT is a self-supervised transformer model for time series anomaly detection", "from": "SELF-SUPERVISED TRANSFORMER", "source_id": "3e0985c172a09bb6c99e305b9f40a513", "to": "ANOMALYBERT", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe \"DATA DEGRADATION SCHEME\" is a method used by the \"ANOMALYBERT\" model to simulate real-world anomalies and train the model. Specifically, AnomalyBERT utilizes a data degradation scheme to degrade the quality of the input data, thereby creating a more realistic representation of anomalies that the model can learn from. This approach enables AnomalyBERT to develop robust anomaly detection capabilities, making it a valuable tool for various applications.\n\nThe data degradation scheme employed by AnomalyBERT is designed to mimic the types of anomalies that occur in real-world data, allowing the model to learn from a wide range of scenarios. By using this scheme, AnomalyBERT can effectively train on degraded data, which in turn enhances its ability to detect anomalies in real-world data.\n\nOverall, the \"DATA DEGRADATION SCHEME\" plays a crucial role in the development and training of the \"ANOMALYBERT\" model, enabling it to achieve high levels of accuracy in anomaly detection tasks.", "from": "DATA DEGRADATION SCHEME", "source_id": "3e0985c172a09bb6c99e305b9f40a513,587ca8c6cc86a93299e9540153babbc6,71f873c12230e6ede47583d81eb85e23", "to": "ANOMALYBERT", "width": 3.0}, {"description": "Existing method is related to data degradation scheme as data degradation scheme is used to modify data", "from": "DATA DEGRADATION SCHEME", "source_id": "6917a14af275e30abe2d30a8f8a9a4e6", "to": "EXISTING METHOD", "width": 1.0}, {"description": "Dataset is related to data degradation scheme as it is used to train and test the model", "from": "DATA DEGRADATION SCHEME", "source_id": "71f873c12230e6ede47583d81eb85e23", "to": "DATASET", "width": 1.0}, {"description": "Numerical Computing \u0026 Image Analysis Lab is the research lab where AnomalyBERT was developed", "from": "NUMERICAL COMPUTING \u0026 IMAGE ANALYSIS LAB", "source_id": "3e0985c172a09bb6c99e305b9f40a513", "to": "ANOMALYBERT", "width": 1.0}, {"description": "Seoul National University is the university where Numerical Computing \u0026 Image Analysis Lab is located", "from": "NUMERICAL COMPUTING \u0026 IMAGE ANALYSIS LAB", "source_id": "3e0985c172a09bb6c99e305b9f40a513", "to": "SEOUL NATIONAL UNIVERSITY", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nAnomalyBERT, a machine learning model, was presented at ICLR 2023, a prominent academic conference. Specifically, the model was presented at the ICLR 2023 workshop, which is a subset of the larger conference. This relationship between AnomalyBERT and ICLR 2023 is further reinforced by the fact that the paper presenting AnomalyBERT was presented at the conference.\n\nThe presentation of AnomalyBERT at ICLR 2023 suggests that the model is a significant contribution to the field of machine learning, particularly in the area of anomaly detection. The use of the term \"AnomalyBERT\" implies that the model is a variant of the popular BERT (Bidirectional Encoder Representations from Transformers) architecture, which is widely used in natural language processing tasks.\n\nOverall, the presentation of AnomalyBERT at ICLR 2023 highlights the model\u0027s potential for detecting anomalies in various domains, and its relevance to the broader field of machine learning research.\n\nRelevant information from the nearby text:\n\n* The text mentions that the primary language of the text is English, which is consistent with the formal and academic tone of the descriptions provided.\n* The use of technical terms such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis\" suggests that the text is related to machine learning and data analysis.\n* The presence of mathematical equations and formulas, as well as references to academic papers and authors, further reinforces the idea that the text is from a research paper or technical document.", "from": "ICLR 2023", "source_id": "19a1a12db412295d0ddd19ffffb78332,587ca8c6cc86a93299e9540153babbc6,8ff636d53a50d7a3bad17443bf104ba2", "to": "ANOMALYBERT", "width": 3.0}, {"description": "ICLR 2023 is a conference where the paper by Hundman et al. (2018) was presented", "from": "ICLR 2023", "source_id": "fbc83a616e9fa96c245138bca69c177f", "to": "HUNDMAN ET AL. (2018)", "width": 1.0}, {"description": "ICLR 2023 is a conference where the paper by Su et al. (2019) was presented", "from": "ICLR 2023", "source_id": "fbc83a616e9fa96c245138bca69c177f", "to": "SU ET AL. (2019)", "width": 1.0}, {"description": "ICLR 2023 is related to SWaT as the dataset was used in the paper presented at the conference", "from": "ICLR 2023", "source_id": "8ff636d53a50d7a3bad17443bf104ba2", "to": "SWAT", "width": 1.0}, {"description": "ICLR 2023 is related to WADI as the dataset was used in the paper presented at the conference", "from": "ICLR 2023", "source_id": "8ff636d53a50d7a3bad17443bf104ba2", "to": "WADI", "width": 1.0}, {"description": "ICLR 2023 is related to SMAP as the dataset was used in the paper presented at the conference", "from": "ICLR 2023", "source_id": "8ff636d53a50d7a3bad17443bf104ba2", "to": "SMAP", "width": 1.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe \"ICLR 2023 WORKSHOP\" is a significant event related to \"MACHINE LEARNING FOR IOT\". This workshop is directly associated with the field of machine learning for IoT, as it is an event where research papers were presented. The workshop\u0027s focus on machine learning for IoT suggests that it is a platform for discussing and showcasing advancements in this area. The event likely featured presentations, discussions, and networking opportunities for researchers and experts in the field of machine learning for IoT.\n\nThe summary is written in third person and includes the entity names for context. It also resolves any potential contradictions by providing a single, coherent description of the relationship between the ICLR 2023 workshop and machine learning for IoT.", "from": "MACHINE LEARNING FOR IOT", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1", "to": "ICLR 2023 WORKSHOP", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nAnomalyBERT is a model that has a significant relationship with the SWaT dataset. Specifically, AnomalyBERT is used to evaluate the performance of anomaly detection models on the SWaT dataset, indicating that it is a crucial tool for assessing the effectiveness of anomaly detection techniques on this particular dataset. Furthermore, AnomalyBERT is also related to SWaT as both are used in anomaly detection, suggesting that they share a common application domain.\n\nMoreover, AnomalyBERT has been used to test and evaluate its performance on the SWaT dataset, which implies that the model has been extensively validated on this dataset. Additionally, AnomalyBERT has been shown to outperform previous methods on the SWaT dataset, highlighting its superior performance in anomaly detection tasks.\n\nOverall, the relationship between AnomalyBERT and SWaT is multifaceted, with AnomalyBERT being used to evaluate, test, and improve anomaly detection models on the SWaT dataset, while also demonstrating its superior performance compared to previous methods.\n\nRelevant information from the nearby text is not provided, but based on the given descriptions, the following information can be inferred:\n\n* AnomalyBERT is a model used for anomaly detection.\n* SWaT is a dataset used for evaluating anomaly detection models.\n* AnomalyBERT has been used to evaluate and test its performance on the SWaT dataset.\n* AnomalyBERT has outperformed previous methods on the SWaT dataset.\n\nNote that the descriptions provided are not contradictory, and the summary above is a coherent and comprehensive representation of the relationship between AnomalyBERT and SWaT.", "from": "ANOMALYBERT", "source_id": "19a1a12db412295d0ddd19ffffb78332,8ff636d53a50d7a3bad17443bf104ba2,9f30a997c7ea3ed8fe02f631a3bd9649,ee651b9bedb0cbcb6272a67deda44bb0", "to": "SWAT", "width": 4.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Summary of AnomalyBERT and WADI**\n\nAnomalyBERT is a model that is closely related to WADI, a dataset used in anomaly detection. Specifically, AnomalyBERT is used to evaluate the performance of anomaly detection models on the WADI dataset. Furthermore, AnomalyBERT is also used to test and evaluate its own performance on the WADI dataset. Notably, AnomalyBERT has been shown to outperform previous methods on the WADI dataset, indicating its effectiveness in anomaly detection tasks.\n\n**Key Findings**\n\n* AnomalyBERT is related to WADI in the context of anomaly detection.\n* AnomalyBERT is used to evaluate the performance of anomaly detection models on the WADI dataset.\n* AnomalyBERT is used to test and evaluate its own performance on the WADI dataset.\n* AnomalyBERT outperforms previous methods on the WADI dataset.\n\n**Entity Relationships**\n\n* AnomalyBERT is related to WADI as a model used to evaluate the performance of anomaly detection models on the WADI dataset.\n* AnomalyBERT is related to WADI as it is used to test and evaluate AnomalyBERT on the WADI dataset.\n\nOverall, the summary highlights the close relationship between AnomalyBERT and WADI, with AnomalyBERT being used to evaluate and improve its performance on the WADI dataset, ultimately outperforming previous methods.", "from": "ANOMALYBERT", "source_id": "19a1a12db412295d0ddd19ffffb78332,8ff636d53a50d7a3bad17443bf104ba2,9f30a997c7ea3ed8fe02f631a3bd9649,ee651b9bedb0cbcb6272a67deda44bb0", "to": "WADI", "width": 4.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n**Summary of ANOMALYBERT and SMAP Relationship**\n\nANOMALYBERT and SMAP are two entities that are closely related in the context of anomaly detection. Specifically, ANOMALYBERT is a model that is used to evaluate the performance of anomaly detection models on the SMAP dataset. Furthermore, ANOMALYBERT is also used to test and evaluate its own performance on the SMAP dataset, which suggests that SMAP is a benchmark dataset for evaluating anomaly detection models, including ANOMALYBERT. Additionally, the results indicate that ANOMALYBERT outperforms previous methods on the SMAP dataset, highlighting its effectiveness in anomaly detection.\n\n**Key Points:**\n\n* ANOMALYBERT is a model used for anomaly detection.\n* SMAP is a dataset used for evaluating anomaly detection models, including ANOMALYBERT.\n* ANOMALYBERT is used to test and evaluate its own performance on the SMAP dataset.\n* ANOMALYBERT outperforms previous methods on the SMAP dataset.\n\n**Context:**\n\nThe relationship between ANOMALYBERT and SMAP is in the context of anomaly detection, where ANOMALYBERT is a model used to evaluate the performance of other anomaly detection models on the SMAP dataset. The SMAP dataset is a benchmark dataset for evaluating anomaly detection models, and ANOMALYBERT is used to test and evaluate its own performance on this dataset. The results indicate that ANOMALYBERT is effective in anomaly detection and outperforms previous methods on the SMAP dataset.", "from": "ANOMALYBERT", "source_id": "19a1a12db412295d0ddd19ffffb78332,8ff636d53a50d7a3bad17443bf104ba2,9f30a997c7ea3ed8fe02f631a3bd9649,ee651b9bedb0cbcb6272a67deda44bb0", "to": "SMAP", "width": 4.0}, {"description": "LSTM-VAE and AnomalyBERT are both models mentioned in the text, as indicated by the use of their names", "from": "ANOMALYBERT", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "LSTM-VAE", "width": 1.0}, {"description": "MSCRED and AnomalyBERT are both models mentioned in the text, as indicated by the use of their names", "from": "ANOMALYBERT", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "MSCRED", "width": 1.0}, {"description": "THOC and AnomalyBERT are both models mentioned in the text, as indicated by the use of their names", "from": "ANOMALYBERT", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "THOC", "width": 1.0}, {"description": "GDN and AnomalyBERT are both models mentioned in the text, as indicated by the use of their names", "from": "ANOMALYBERT", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "GDN", "width": 1.0}, {"description": "AnomalyBERT is a Transformer-based model", "from": "ANOMALYBERT", "source_id": "587ca8c6cc86a93299e9540153babbc6", "to": "TRANSFORMER-BASED MODEL", "width": 1.0}, {"description": "AnomalyBERT is related to dataset as it is used to train and test the model", "from": "ANOMALYBERT", "source_id": "71f873c12230e6ede47583d81eb85e23", "to": "DATASET", "width": 1.0}, {"description": "AnomalyBERT is related to anomaly as it is used to detect anomalies in time series data", "from": "ANOMALYBERT", "source_id": "71f873c12230e6ede47583d81eb85e23", "to": "ANOMALY", "width": 1.0}, {"description": "Autoencoder and adversarial network are both types of neural networks that can be used for anomaly detection", "from": "AUTOENCODER", "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "to": "ADVERSARIAL NETWORK", "width": 1.0}, {"description": "Masked language modeling and relative position bias are both tasks that can be used to improve the performance of a model", "from": "MASKED LANGUAGE MODELING", "source_id": "83b49bf68dab6c36bdc09f02a63803fe", "to": "RELATIVE POSITION BIAS", "width": 1.0}, {"description": "Relative position bias is used in the Transformer body to consider the relative positions between features in a window", "from": "RELATIVE POSITION BIAS", "source_id": "a5d60c1a355b77187003182f6df57646", "to": "TRANSFORMER BODY", "width": 1.0}, {"description": "LOF is related to density-based outlier detection as it is a method for identifying outliers based on the density of the data", "from": "LOCAL OUTLIER FACTOR (LOF)", "source_id": "1303ca4c43652bb8052df34d21c78eca", "to": "DENSITY-BASED OUTLIER DETECTION", "width": 1.0}, {"description": "Statistical methods are related to ML-based methods as both are used for anomaly detection", "from": "STATISTICAL METHODS", "source_id": "1303ca4c43652bb8052df34d21c78eca", "to": "MACHINE LEARNING-BASED (ML-BASED) METHODS", "width": 1.0}, {"description": "GMM is related to deep neural networks as it is a statistical algorithm used for anomaly detection", "from": "GAUSSIAN MIXTURE MODEL (GMM)", "source_id": "1303ca4c43652bb8052df34d21c78eca", "to": "DEEP NEURAL NETWORK", "width": 1.0}, {"description": "RNNs are related to LSTM as LSTM is a type of RNN used for anomaly detection", "from": "RECURRENT NEURAL NETWORKS (RNNS)", "source_id": "1303ca4c43652bb8052df34d21c78eca", "to": "LONG SHORT-TERM MEMORY (LSTM)", "width": 1.0}, {"description": "Span masking is related to XLNet as it is a concept used in the XLNet model", "from": "SPAN MASKING", "source_id": "1303ca4c43652bb8052df34d21c78eca", "to": "XLNET", "width": 1.0}, {"description": "XLNet is used in BART as a component of the noising schemes", "from": "XLNET", "source_id": "a5d60c1a355b77187003182f6df57646", "to": "BART", "width": 1.0}, {"description": "BART is related to T5 as both are popular language models", "from": "BART", "source_id": "1f1221583d838c2407fe9864225e9eda", "to": "T5", "width": 1.0}, {"description": "CNN is not used in ViT, which instead uses the Transformer encoder", "from": "VIT", "source_id": "a5d60c1a355b77187003182f6df57646", "to": "CNN", "width": 1.0}, {"description": "MLM is related to NLP as MLM is used in NLP", "from": "MLM", "source_id": "6917a14af275e30abe2d30a8f8a9a4e6", "to": "NLP", "width": 1.0}, {"description": "CNNs demonstrated superior performance than RNNs in multiple tasks on sequential data, as shown in [Bai et al., 2018]", "from": "CNN", "source_id": "7d5d82d600620153153772a9bc498ac0", "to": "M4 COMPETITION", "width": 1.0}, {"description": "CNNs are used as a building block in models like DPMN [Olivares et al., 2023b]", "from": "CNN", "source_id": "7d5d82d600620153153772a9bc498ac0", "to": "DPMN", "width": 1.0}, {"description": "RNNs were outperformed by CNNs in multiple tasks on sequential data, as shown in [Bai et al., 2018]", "from": "CNN", "source_id": "7d5d82d600620153153772a9bc498ac0", "to": "RNN", "width": 1.0}, {"description": "Sequential data is related to CNNs as CNNs demonstrated superior performance on sequential data", "from": "CNN", "source_id": "7d5d82d600620153153772a9bc498ac0", "to": "SEQUENTIAL DATA", "width": 1.0}, {"description": "CNN is related to bidirectional LSTMs as both are used for processing time-series data", "from": "CNN", "source_id": "0259287b914980606371cd1161c6a420", "to": "BIDIRECTIONAL LSTMS", "width": 1.0}, {"description": "Self-supervised training is used in the Transformer body to train the model", "from": "SELF-SUPERVISED TRAINING", "source_id": "a5d60c1a355b77187003182f6df57646", "to": "TRANSFORMER BODY", "width": 1.0}, {"description": "A window is used in the Transformer body to represent a sequence of data points", "from": "WINDOW", "source_id": "a5d60c1a355b77187003182f6df57646", "to": "TRANSFORMER BODY", "width": 1.0}, {"description": "Window is related to objective function as objective function uses window", "from": "WINDOW", "source_id": "6917a14af275e30abe2d30a8f8a9a4e6", "to": "OBJECTIVE FUNCTION", "width": 1.0}, {"description": "Window is enriched by local positional encoding", "from": "WINDOW", "source_id": "518bfcd6711530089fe3914ca16459c2", "to": "LOCAL POSITIONAL ENCODING", "width": 1.0}, {"description": "Window is related to recurrent models as recurrent models use feedback connections to process sequential data", "from": "WINDOW", "source_id": "1902e651467179a9a1d4c4df0035e980", "to": "RECURRENT MODELS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe Transformer body is a crucial component in the prediction block, consisting of six layers with an embedding dimension of 512 and eight attention heads. This architecture is utilized within the prediction block to generate anomaly scores, indicating its significance in the overall prediction process.\n\nThis summary incorporates information from both descriptions, providing a clear and concise understanding of the Transformer body\u0027s role and characteristics within the prediction block.", "from": "TRANSFORMER BODY", "source_id": "5809618fe3a2014c2140601fcf103022,a5d60c1a355b77187003182f6df57646", "to": "PREDICTION BLOCK", "width": 2.0}, {"description": "GELU is used in the Transformer body as an activation function", "from": "TRANSFORMER BODY", "source_id": "a5d60c1a355b77187003182f6df57646", "to": "GELU", "width": 1.0}, {"description": "2-layer MLPs are used as the prediction block in the Transformer body", "from": "TRANSFORMER BODY", "source_id": "5809618fe3a2014c2140601fcf103022", "to": "MLPS", "width": 1.0}, {"description": "The prediction block contains one hidden layer of 2,048 neurons with GELU activation in between", "from": "PREDICTION BLOCK", "source_id": "5809618fe3a2014c2140601fcf103022", "to": "EMBEDDING DIMENSION", "width": 1.0}, {"description": "Relative position bias table is a type of bias table", "from": "RELATIVE POSITION BIAS TABLE", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "BIAS TABLE", "width": 1.0}, {"description": "Attention matrix is used to compute the attention weights from input features", "from": "ATTENTION MATRIX", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "INPUT FEATURES", "width": 1.0}, {"description": "Input features are used as query to compute the attention weights", "from": "INPUT FEATURES", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "QUERY", "width": 1.0}, {"description": "Input features are used as key to compute the attention weights", "from": "INPUT FEATURES", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "KEY", "width": 1.0}, {"description": "Input features are used as value to compute the attention weights", "from": "INPUT FEATURES", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "VALUE", "width": 1.0}, {"description": "Query and key are used to compute the attention weights", "from": "QUERY", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "KEY", "width": 1.0}, {"description": "Query and value are used to compute the attention weights", "from": "QUERY", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "VALUE", "width": 1.0}, {"description": "Key and value are used to compute the attention weights", "from": "KEY", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "VALUE", "width": 1.0}, {"description": "PCA is used to select features for classification ability", "from": "FEATURES", "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "to": "PCA", "width": 1.0}, {"description": "Features are selected from the HCTSA library for classification ability", "from": "FEATURES", "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "to": "HCTSA LIBRARY", "width": 1.0}, {"description": "Catch22 is related to features as features are a specific type of Catch22", "from": "FEATURES", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "CATCH22", "width": 1.0}, {"description": "Synthetic outliers include soft replacement as a type", "from": "SYNTHETIC OUTLIERS", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "SOFT REPLACEMENT", "width": 1.0}, {"description": "Synthetic outliers include uniform replacement as a type", "from": "SYNTHETIC OUTLIERS", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "UNIFORM REPLACEMENT", "width": 1.0}, {"description": "Synthetic outliers include peak noise as a type", "from": "SYNTHETIC OUTLIERS", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "PEAK NOISE", "width": 1.0}, {"description": "Synthetic outliers include length adjustment as a type", "from": "SYNTHETIC OUTLIERS", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "LENGTH ADJUSTMENT", "width": 1.0}, {"description": "Data degradation involves adding synthetic outliers to the input data", "from": "SYNTHETIC OUTLIERS", "source_id": "a7604286fb84b26c53950861f9aca4b1", "to": "DATA DEGRADATION", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities in question are \"SYNTHETIC OUTLIERS\" and \"ABALATION STUDIES.\" \n\nABALATION STUDIES are a type of analysis that involves the use of SYNTHETIC OUTLIERS to understand their impact on a particular system or process. SYNTHETIC OUTLIERS are artificially created outliers that are used to test the robustness and reliability of a model or system.\n\nIn the context of ABALATION STUDIES, SYNTHETIC OUTLIERS are used to analyze their effect on the performance of a model or system. This is typically done to identify areas where the model or system may be vulnerable to outliers and to develop strategies for mitigating their impact.\n\nThe use of SYNTHETIC OUTLIERS in ABALATION STUDIES is a common practice in machine learning and data analysis, particularly in the development and evaluation of time series forecasting models. By analyzing the impact of SYNTHETIC OUTLIERS on these models, researchers and practitioners can gain a better understanding of their strengths and weaknesses and develop more robust and reliable models.\n\nOverall, the relationship between ABALATION STUDIES and SYNTHETIC OUTLIERS is one of analysis and evaluation, with SYNTHETIC OUTLIERS being used to test and improve the performance of models and systems in ABALATION STUDIES.", "from": "SYNTHETIC OUTLIERS", "source_id": "15c3350fad556f666d93817c5036109c,deb67d5386710136cf24bcbf135a66c4", "to": "ABALATION STUDIES", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe primary focus of the analysis is on two entities: \"SYNTHETIC OUTLIERS\" and \"F1-SCORES\". The descriptions provided offer insight into the relationship between these entities.\n\nAccording to the descriptions, \"SYNTHETIC OUTLIERS\" are used to measure \"F1-SCORES\". This suggests that synthetic outliers are employed as a metric or evaluation tool to assess the performance of models in terms of F1-scores.\n\nFurthermore, it is mentioned that synthetic outliers were used to evaluate the performance of the models using F1-scores. This implies that the F1-scores are a key metric for assessing the accuracy or effectiveness of the models, and synthetic outliers are a means of testing or validating this metric.\n\nOverall, the analysis indicates that synthetic outliers and F1-scores are closely related, with synthetic outliers serving as a tool for evaluating model performance in terms of F1-scores.\n\nIn terms of the language used, the descriptions suggest that the primary language is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. This is consistent with the formal and academic tone of the language used.\n\nTherefore, the comprehensive summary of the data is as follows:\n\nThe entities \"SYNTHETIC OUTLIERS\" and \"F1-SCORES\" are closely related, with synthetic outliers being used to measure and evaluate model performance in terms of F1-scores. The primary language of the text is English, and the descriptions suggest a formal and academic tone, consistent with technical or research-oriented content.", "from": "SYNTHETIC OUTLIERS", "source_id": "15c3350fad556f666d93817c5036109c,deb67d5386710136cf24bcbf135a66c4", "to": "F1-SCORES", "width": 2.0}, {"description": "Soft replacement is related to uniform replacement as they are both techniques used to synthesize outliers", "from": "SOFT REPLACEMENT", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "UNIFORM REPLACEMENT", "width": 1.0}, {"description": "Synthesis technique covers soft replacement as it covers all typical types of outliers", "from": "SOFT REPLACEMENT", "source_id": "19a1a12db412295d0ddd19ffffb78332", "to": "SYNTHESIS TECHNIQUE", "width": 1.0}, {"description": "Uniform replacement is related to length adjustment as they are both techniques used to synthesize outliers", "from": "UNIFORM REPLACEMENT", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "LENGTH ADJUSTMENT", "width": 1.0}, {"description": "Synthesis technique covers uniform replacement as it partially covers typical types of outliers", "from": "UNIFORM REPLACEMENT", "source_id": "19a1a12db412295d0ddd19ffffb78332", "to": "SYNTHESIS TECHNIQUE", "width": 1.0}, {"description": "Length adjustment is related to peak noise as they are both techniques used to synthesize outliers", "from": "PEAK NOISE", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "LENGTH ADJUSTMENT", "width": 1.0}, {"description": "Synthesis technique covers peak noise as it partially covers typical types of outliers", "from": "PEAK NOISE", "source_id": "19a1a12db412295d0ddd19ffffb78332", "to": "SYNTHESIS TECHNIQUE", "width": 1.0}, {"description": "Synthesis technique covers length adjustment as it only covers the seasonal outlier", "from": "LENGTH ADJUSTMENT", "source_id": "19a1a12db412295d0ddd19ffffb78332", "to": "SYNTHESIS TECHNIQUE", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"PERCEPTION\" and \"UNDERSTANDING\" are closely related concepts in the context of data analysis. They both serve as essential components in gaining insight and knowledge from data. Specifically, perception is utilized in understanding to interpret and make sense of information, thereby facilitating a deeper comprehension of the data. This interplay between perception and understanding enables individuals to extract valuable insights and knowledge from complex data sets.\n\nIn this context, perception can be seen as the initial step in the process, where raw data is processed and interpreted to create a meaningful representation. Understanding, on the other hand, is the subsequent step, where the interpreted information is analyzed and comprehended to gain a deeper understanding of the data. This iterative process between perception and understanding is crucial in unlocking the full potential of data analysis and gaining valuable insights.\n\nOverall, the relationship between perception and understanding is one of interdependence, where each concept relies on the other to function effectively. By leveraging the strengths of both perception and understanding, individuals can develop a more comprehensive understanding of complex data sets and make informed decisions based on the insights gained.", "from": "PERCEPTION", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649,a7604286fb84b26c53950861f9aca4b1", "to": "UNDERSTANDING", "width": 2.0}, {"description": "Binary cross entropy loss is related to objective function as objective function uses binary cross entropy loss", "from": "BINARY CROSS ENTROPY LOSS", "source_id": "6917a14af275e30abe2d30a8f8a9a4e6", "to": "OBJECTIVE FUNCTION", "width": 1.0}, {"description": "Artificial labels are related to objective function as objective function uses artificial labels", "from": "OBJECTIVE FUNCTION", "source_id": "6917a14af275e30abe2d30a8f8a9a4e6", "to": "ARTIFICIAL LABELS", "width": 1.0}, {"description": "Based on the provided descriptions, it can be concluded that the entities \"SWAT\" and \"WADI\" are related as they are both real-world benchmark datasets used for anomaly detection. They are utilized in experiments and tables for testing and evaluation of anomaly detection models.\n\nThe descriptions collectively indicate that both \"SWAT\" and \"WADI\" are datasets, which are used for anomaly detection purposes. They are also used in experiments and tables for testing and evaluation of anomaly detection models. This information is consistent across all the descriptions provided.\n\nThe use of their abbreviations in tables and their presence in the table further supports the fact that they are datasets. The fact that they are real-world benchmark datasets used for anomaly detection suggests that they are reliable and widely used in the field of anomaly detection.\n\nTherefore, a comprehensive summary of the data provided is:\n\n\"SWAT and WADI are related as they are both real-world benchmark datasets used for anomaly detection. They are utilized in experiments and tables for testing and evaluation of anomaly detection models, and are widely used in the field of anomaly detection.\"\n\nThis summary is written in third person and includes the entity names for full context. It also resolves any potential contradictions and provides a single, coherent description of the relationship between the entities \"SWAT\" and \"WADI\".", "from": "SWAT", "source_id": "6917a14af275e30abe2d30a8f8a9a4e6,69afb4bcb8c1e03975c837102e1d0b32,8ff636d53a50d7a3bad17443bf104ba2,9c6e74299923071f9fc2b7cce49efc90,ee651b9bedb0cbcb6272a67deda44bb0,f6e57fa18831bcc732a631240536777a,fbc83a616e9fa96c245138bca69c177f", "to": "WADI", "width": 7.0}, {"description": "Goh et al. (2017) is a research paper that describes the SWaT dataset", "from": "SWAT", "source_id": "fbc83a616e9fa96c245138bca69c177f", "to": "GOH ET AL. (2017)", "width": 1.0}, {"description": "Based on the provided information, the primary entities are \"SWAT\" and \"SMAP\". After analyzing the descriptions, it is clear that both entities are related to anomaly detection and are used as datasets.\n\nHere is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\n\"SWAT and SMAP are two datasets used for anomaly detection. They are both concepts mentioned in the text and are utilized in the context of anomaly detection. Specifically, they are used to identify and analyze anomalies, which is a critical aspect of anomaly detection. The presence of both datasets in the table further emphasizes their relevance to this field. Overall, SWAT and SMAP are two key datasets in the domain of anomaly detection, and their relationship is rooted in their shared application in this area.\"\n\nNote that the descriptions provided are consistent and do not contain any contradictions. The summary is therefore a straightforward compilation of the information provided.", "from": "SWAT", "source_id": "69afb4bcb8c1e03975c837102e1d0b32,8ff636d53a50d7a3bad17443bf104ba2,d16a81565fcd654ab21768510dcc5d7f", "to": "SMAP", "width": 3.0}, {"description": "NAB and SWaT are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "SWAT", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "NAB", "width": 1.0}, {"description": "UCR and SWaT are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "SWAT", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "UCR", "width": 1.0}, {"description": "MBA and SWaT are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "SWAT", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "MBA", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nSWAT and MSDS are two entities that are datasets used for anomaly detection. They are also referred to as systems, indicating that they are likely complex systems or platforms designed for specific purposes. The presence of these entities in a table suggests that they are being compared or analyzed in some way, possibly in the context of anomaly detection or system performance evaluation.\n\nGiven the context of machine learning and time series forecasting, it is likely that SWAT and MSDS are being used as benchmarks or test cases for developing and evaluating anomaly detection models. The fact that they are mentioned in the same table as other technical terms and mathematical equations suggests that they are being used in a technical or research context.\n\nOverall, SWAT and MSDS appear to be two entities that are being used in a technical or research context for anomaly detection and system evaluation purposes.", "from": "SWAT", "source_id": "69afb4bcb8c1e03975c837102e1d0b32,f6e57fa18831bcc732a631240536777a", "to": "MSDS", "width": 2.0}, {"description": "SWaT is related to P as P is a metric used to evaluate the models on the SWaT dataset", "from": "SWAT", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "P", "width": 1.0}, {"description": "SWaT is related to R as R is a metric used to evaluate the models on the SWaT dataset", "from": "SWAT", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "R", "width": 1.0}, {"description": "SWaT is related to AUC as AUC is a metric used to evaluate the models on the SWaT dataset", "from": "SWAT", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "AUC", "width": 1.0}, {"description": "SWaT is related to F1 as F1 is a metric used to evaluate the models on the SWaT dataset", "from": "SWAT", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "F1", "width": 1.0}, {"description": "SWaT is related to Time as Time is a metric used to evaluate the models on the SWaT dataset", "from": "SWAT", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "TIME", "width": 1.0}, {"description": "MERLIN is related to SWaT as MERLIN is used to find anomalies in SWaT", "from": "SWAT", "source_id": "f6e57fa18831bcc732a631240536777a", "to": "MERLIN", "width": 1.0}, {"description": "Based on the provided information, the primary entities are \"WADI\" and \"SMAP\", which are both datasets used for anomaly detection. \n\nThe descriptions provided indicate that WADI and SMAP are related in several ways:\n\n1. They are both used for anomaly detection, as indicated by their presence in a table.\n2. They are used for testing and evaluation of anomaly detection models.\n3. They are used in the same experiment.\n4. They are both datasets used in anomaly detection.\n\nConsidering the information collected from all the descriptions, a comprehensive summary can be formed:\n\n\"WADI and SMAP are two datasets used for anomaly detection, which are related in that they are both used for testing and evaluation of anomaly detection models, and are utilized in the same experiment. Their presence in a table and use in anomaly detection further solidifies their connection as datasets used for this purpose.\"\n\nThis summary is written in third person and includes the entity names for context. It also resolves any potential contradictions by highlighting the commonalities between WADI and SMAP.", "from": "WADI", "source_id": "6917a14af275e30abe2d30a8f8a9a4e6,69afb4bcb8c1e03975c837102e1d0b32,8ff636d53a50d7a3bad17443bf104ba2,ee651b9bedb0cbcb6272a67deda44bb0", "to": "SMAP", "width": 4.0}, {"description": "Ahmed et al. (2017) is a research paper that describes the WADI dataset", "from": "WADI", "source_id": "fbc83a616e9fa96c245138bca69c177f", "to": "AHMED ET AL. (2017)", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"WADI\" and \"MERLIN\" are related in a context of model evaluation and anomaly detection. Specifically, \"MERLIN\" is used to find anomalies in \"WADI\", suggesting that \"WADI\" is a dataset or a metric that can be analyzed for anomalies. Additionally, \"WADI\" is also used as a metric to evaluate the performance of models, implying that it is a key indicator of model quality. This relationship between \"MERLIN\" and \"WADI\" highlights the importance of anomaly detection and model evaluation in the context of these entities.\n\nThis summary is based on the information provided in the description list, which indicates a clear relationship between \"MERLIN\" and \"WADI\" in the context of model evaluation and anomaly detection. The summary is written in third person and includes the entity names for context.", "from": "WADI", "source_id": "00973c1c3c962d9234a38037709824b1,f6e57fa18831bcc732a631240536777a", "to": "MERLIN", "width": 2.0}, {"description": "LSTM-NDT is related to WADI as WADI is a metric used to evaluate model performance", "from": "WADI", "source_id": "00973c1c3c962d9234a38037709824b1", "to": "LSTM-NDT", "width": 1.0}, {"description": "TranAD achieves the best rank across all models with a significant statistical difference when given the complete WADI dataset", "from": "WADI", "source_id": "70a97858b727e5af1466ae5eb9183921", "to": "TRANAD", "width": 1.0}, {"description": "NAB and WADI are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "WADI", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "NAB", "width": 1.0}, {"description": "UCR and WADI are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "WADI", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "UCR", "width": 1.0}, {"description": "MBA and WADI are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "WADI", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "MBA", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nWADI and MSDS are entities that are both datasets and systems used for anomaly detection. They are listed in a table, indicating their relevance to the field of anomaly detection. The use of their abbreviations in the table further supports their classification as systems. \n\nGiven the context of the text, it appears that WADI and MSDS are likely datasets or systems used in machine learning or time series analysis, as they are mentioned alongside other technical terms and mathematical equations. However, without further information, it is difficult to determine the specific nature of WADI and MSDS or their exact role in anomaly detection.\n\nIt is worth noting that the text does not provide any information about the specific characteristics or features of WADI and MSDS, such as their size, structure, or any notable attributes. Therefore, the summary is limited to the information provided, which is that WADI and MSDS are both datasets and systems used for anomaly detection.", "from": "WADI", "source_id": "69afb4bcb8c1e03975c837102e1d0b32,f6e57fa18831bcc732a631240536777a", "to": "MSDS", "width": 2.0}, {"description": "Hundman et al. (2018) is a research paper that describes the SMAP dataset", "from": "SMAP", "source_id": "fbc83a616e9fa96c245138bca69c177f", "to": "HUNDMAN ET AL. (2018)", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe datasets \"MBA\" and \"SMAP\" are two entities that are closely related. Both datasets are used for anomaly detection, as indicated by their presence in a table. Furthermore, they are related as they are both datasets used in experiments. This suggests that the data from \"MBA\" and \"SMAP\" are being utilized to test and evaluate anomaly detection models or techniques.\n\nGiven the context of anomaly detection, it is likely that the data from these datasets is being analyzed to identify patterns or irregularities that do not conform to expected norms. The use of these datasets in experiments implies that researchers are actively working to develop and improve anomaly detection methods, and that \"MBA\" and \"SMAP\" are playing a crucial role in this process.\n\nOverall, the summary highlights the connection between \"MBA\" and \"SMAP\" as datasets used for anomaly detection and in experiments, providing a clear understanding of their relationship and purpose.", "from": "SMAP", "source_id": "69afb4bcb8c1e03975c837102e1d0b32,9c6e74299923071f9fc2b7cce49efc90", "to": "MBA", "width": 2.0}, {"description": "NAB and SMAP are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "SMAP", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "NAB", "width": 1.0}, {"description": "UCR and SMAP are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "SMAP", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "UCR", "width": 1.0}, {"description": "SMAP and MSDS are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "SMAP", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "MSDS", "width": 1.0}, {"description": "SMAP is related to P as P is a metric used to evaluate the models on the SMAP dataset", "from": "SMAP", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "P", "width": 1.0}, {"description": "SMAP is related to R as R is a metric used to evaluate the models on the SMAP dataset", "from": "SMAP", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "R", "width": 1.0}, {"description": "SMAP is related to AUC as AUC is a metric used to evaluate the models on the SMAP dataset", "from": "SMAP", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "AUC", "width": 1.0}, {"description": "SMAP is related to F1 as F1 is a metric used to evaluate the models on the SMAP dataset", "from": "SMAP", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "F1", "width": 1.0}, {"description": "SMAP is related to Time as Time is a metric used to evaluate the models on the SMAP dataset", "from": "SMAP", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "TIME", "width": 1.0}, {"description": "Data points are related to the sliding window strategy as it is used to process data points in a time series", "from": "SLIDING WINDOW STRATEGY", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "DATA POINTS", "width": 1.0}, {"description": "Sliding window strategy is related to score prediction as it is used to predict the anomaly score of a data point", "from": "SLIDING WINDOW STRATEGY", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "SCORE PREDICTION", "width": 1.0}, {"description": "Score prediction is related to anomaly score as it is used to predict the anomaly score of a data point", "from": "SCORE PREDICTION", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "ANOMALY SCORE", "width": 1.0}, {"description": "Replication padding is related to anomaly score as anomaly score is calculated using replication padding", "from": "ANOMALY SCORE", "source_id": "477c5b7f23f4e00030ab389788a4c88d", "to": "REPPLICATION PADDING", "width": 1.0}, {"description": "Based on the provided information, the primary entities are \"ANOMALY SCORE\" and \"THRESHOLD VALUE.\" The descriptions provided offer insight into the relationship between these two entities.\n\nThe anomaly score is related to the threshold value in two distinct ways. Firstly, the threshold value is used to determine the anomaly label, suggesting that the threshold value serves as a decision boundary for categorizing data points as anomalous or not. Secondly, the threshold value is used to determine whether an input window is anomalous or not, indicating that it plays a crucial role in identifying anomalous patterns within a given time series or dataset.\n\nIn essence, the threshold value acts as a control parameter that influences the anomaly score, which in turn determines the anomaly label or the anomalous status of an input window. This relationship highlights the importance of the threshold value in anomaly detection and scoring, as it directly impacts the outcome of the anomaly detection process.\n\nGiven the context of time series analysis and anomaly detection, it is likely that the anomaly score is calculated based on statistical or machine learning-based methods, such as frequency analysis or multi-head cross-attention, which are commonly used in time series forecasting and anomaly detection. The threshold value, in turn, is used to filter or classify the anomaly scores, determining which data points or input windows are considered anomalous.\n\nOverall, the relationship between the anomaly score and the threshold value is critical in anomaly detection and scoring, and understanding this relationship is essential for developing effective anomaly detection models and techniques.", "from": "ANOMALY SCORE", "source_id": "477c5b7f23f4e00030ab389788a4c88d,f2e78b25a535b82e2743a0ea4052eb9a", "to": "THRESHOLD VALUE", "width": 2.0}, {"description": "Model weights are related to anomaly score as they are used to calculate the anomaly score", "from": "ANOMALY SCORE", "source_id": "6ea15432a841705c2e74cbc01f6004e9", "to": "MODEL WEIGHTS", "width": 1.0}, {"description": "The anomaly score is used to label the timestamp as anomalous or not", "from": "ANOMALY SCORE", "source_id": "1b51ec337efd822ca3a0b3eb819c1b91", "to": "TIMESTAMP", "width": 1.0}, {"description": "The reconstruction is used to compute the anomaly score, which is a measure of how anomalous a data point is", "from": "ANOMALY SCORE", "source_id": "1b51ec337efd822ca3a0b3eb819c1b91", "to": "RECONSTRUCTION", "width": 1.0}, {"description": "True positives are related to false positives as they are both used to evaluate the performance of an anomaly detection model", "from": "TRUE POSITIVES", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "FALSE POSITIVES", "width": 1.0}, {"description": "False positives are related to false negatives as they are both used to evaluate the performance of an anomaly detection model", "from": "FALSE POSITIVES", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "FALSE NEGATIVES", "width": 1.0}, {"description": "Global outliers are related to contextual outliers as they are both types of anomalies", "from": "GLOBAL OUTLIERS", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "CONTEXTUAL OUTLIERS", "width": 1.0}, {"description": "Contextual outliers are related to shapelet outliers as they are both types of anomalies", "from": "CONTEXTUAL OUTLIERS", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "SHAPELET OUTLIERS", "width": 1.0}, {"description": "Shapelet outliers are related to seasonal outliers as they are both types of anomalies", "from": "SHAPELET OUTLIERS", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "SEASONAL OUTLIERS", "width": 1.0}, {"description": "Seasonal outliers are related to trend outliers as they are both types of anomalies", "from": "SEASONAL OUTLIERS", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "TREND OUTLIERS", "width": 1.0}, {"description": "Point adjustment is related to Xu et al. as it is a technique discussed in the paper", "from": "POINT ADJUSTMENT", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "XU ET AL.", "width": 1.0}, {"description": "Point adjustment is related to Kim et al. as it is a technique discussed in the paper", "from": "POINT ADJUSTMENT", "source_id": "9f30a997c7ea3ed8fe02f631a3bd9649", "to": "KIM ET AL.", "width": 1.0}, {"description": "Synthesis technique covers shapelet outlier as it does not require analysis of the original data", "from": "SHAPELET OUTLIER", "source_id": "19a1a12db412295d0ddd19ffffb78332", "to": "SYNTHESIS TECHNIQUE", "width": 1.0}, {"description": "Synthesis technique covers trend outlier as it does not require analysis of the original data", "from": "TREND OUTLIER", "source_id": "19a1a12db412295d0ddd19ffffb78332", "to": "SYNTHESIS TECHNIQUE", "width": 1.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe primary language of the text document is English, as indicated by various indicators. The text contains English words and phrases, such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention,\" which are characteristic of the English language. Additionally, the presence of mathematical equations and formulas, written in a standard mathematical notation used in English-language academic papers, further supports the conclusion that the primary language of the text is English.\n\nThe use of English-language abbreviations, such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals, also suggests that the text is written in English. Furthermore, the citation of English-language academic papers and authors in the text reinforces this conclusion.\n\nIn summary, the primary language of the text document is English, as evidenced by the use of English words and phrases, mathematical equations, English-language abbreviations, and references to English-language academic papers.\n\nEntities: [\"ENGLISH\", \"LANGUAGE\"]\nDescription List: [\"Language is related to English as the text document is written in English\", \"The primary language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers\"]\n\nThis summary is written in third person and includes information collected from all the descriptions, resolving any potential contradictions.", "from": "ENGLISH", "source_id": "0654926be53a9cf18e4def1c94371576,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,69457f873272a693c1f813c75ecf030a,9f70fa3e2d0ba714627b9ce1a442fd21,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c80929fa1aa2a6f9652319844c4ca742,c9efd571b05c136c0bf9d7e89194ec88,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "to": "LANGUAGE", "width": 12.0}, {"description": "The text contains references to academic papers and authors, suggesting that it is written in English", "from": "ENGLISH", "source_id": "56613213ed14f292e8ff44f4f0a8bab1", "to": "ACADEMIC PAPERS", "width": 1.0}, {"description": "English is related to ICLR as ICLR is an academic conference where researchers present their work in English", "from": "ENGLISH", "source_id": "0654926be53a9cf18e4def1c94371576,9f70fa3e2d0ba714627b9ce1a442fd21,c80929fa1aa2a6f9652319844c4ca742", "to": "ICLR", "width": 3.0}, {"description": "English is related to AAAI as AAAI is an academic conference where researchers present their work in English", "from": "ENGLISH", "source_id": "0654926be53a9cf18e4def1c94371576,9f70fa3e2d0ba714627b9ce1a442fd21,c80929fa1aa2a6f9652319844c4ca742", "to": "AAAI", "width": 3.0}, {"description": "English is related to PMLR as PMLR is an academic conference where researchers present their work in English", "from": "ENGLISH", "source_id": "0654926be53a9cf18e4def1c94371576,9f70fa3e2d0ba714627b9ce1a442fd21,c80929fa1aa2a6f9652319844c4ca742", "to": "PMLR", "width": 3.0}, {"description": "Based on the provided information, the primary entity is \"ACADEMIC PAPERS\" and \"RESEARCH PAPER\". The descriptions provided suggest that the text is from a research paper or a technical document, written in a formal and academic language.\n\nAfter analyzing the descriptions, it appears that they are consistent and provide a clear indication of the nature of the text. Therefore, a single, concise description can be generated.\n\nThe text is a research paper or a technical document that contains references to academic papers and authors, written in a formal and academic language. It is likely from a research paper or a technical document, as indicated by the presence of technical terms, mathematical equations, and references to academic conferences and journals.\n\nRelevant information from the nearby text suggests that the language used is English, with the presence of English words and phrases, mathematical equations, and English-language abbreviations. The citation of English-language academic papers and authors further supports this conclusion.\n\nIn summary, the text is a research paper or a technical document written in English, containing references to academic papers and authors, and using formal and academic language.", "from": "ACADEMIC PAPERS", "source_id": "4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "to": "RESEARCH PAPER", "width": 13.0}, {"description": "The text document contains references to academic papers, which are used to support or validate information", "from": "ACADEMIC PAPERS", "source_id": "0654926be53a9cf18e4def1c94371576,9f70fa3e2d0ba714627b9ce1a442fd21,c80929fa1aa2a6f9652319844c4ca742", "to": "TEXT DOCUMENT", "width": 3.0}, {"description": "The primary language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers", "from": "ACADEMIC PAPERS", "source_id": "53ae42e8cf874f9df3817b3e2589d60c", "to": "LANGUAGE", "width": 1.0}, {"description": "Based on the provided information, the primary entity is a document that contains technical and academic content. The document is likely to be either a research paper or a technical document, as indicated by the formal and academic language used.\n\nUpon analyzing the descriptions, there appears to be a slight contradiction between the two statements. However, upon closer examination, it can be inferred that the document is likely to be a research paper that is also a technical document. This is because research papers often contain technical terms and mathematical equations, which are also present in the document.\n\nTherefore, a comprehensive summary of the data provided can be written as follows:\n\nThe document, which is likely to be a research paper or a technical document, contains formal and academic language, indicating its technical nature. The presence of technical terms and mathematical equations further supports this inference. Specifically, the document is likely to be a research paper that is also a technical document, as it contains a combination of academic and technical content.\n\nRelevant information from the nearby text includes the use of English words and phrases, mathematical equations and formulas, English-language abbreviations, and citations of English-language academic papers and authors. These indicators collectively suggest that the primary language of the text is English.\n\nIn terms of the entities, the document is referred to as a \"RESEARCH PAPER\" and a \"TECHNICAL DOCUMENT\", which are two related but distinct concepts. The document is likely to be a research paper that is also a technical document, as it contains a combination of academic and technical content.\n\nOverall, the document is a research paper that is also a technical document, containing formal and academic language, technical terms, and mathematical equations. The primary language of the text is English, as indicated by various linguistic and content-based indicators.", "from": "RESEARCH PAPER", "source_id": "4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "to": "TECHNICAL DOCUMENT", "width": 13.0}, {"description": "Long-term series forecasting is a concept mentioned in the text, as indicated by the use of the phrase \"long-term series forecasting\"", "from": "LONG-TERM SERIES FORECASTING", "source_id": "4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "to": "FREQUENCY ANALYSIS", "width": 13.0}, {"description": "Based on the provided data, the expert Data Scientist has generated a comprehensive summary of the entities and their relationships.\n\n**Summary:**\n\nThe entities \"FREQUENCY ANALYSIS\" and \"MULTI-HEAD CROSS-ATTENTION\" are closely related concepts in the context of time series analysis and machine learning. Frequency analysis is a technique used to analyze the frequency components of a signal or time series, which is a key aspect of understanding the underlying patterns and trends in the data. Multi-head cross-attention, on the other hand, is a mechanism used in some models to analyze the frequency of occurrence of different values or patterns in a dataset.\n\nIn particular, multi-head cross-attention can be used to analyze the frequency components of a signal or time series, making it a useful tool for frequency analysis. This relationship is supported by the fact that frequency analysis is mentioned in the text as a concept related to multi-head cross-attention.\n\nFurthermore, frequency analysis has been used in some models to analyze the data, suggesting that it is a valuable technique for understanding the underlying patterns and trends in the data.\n\n**Key Takeaways:**\n\n* Frequency analysis is a technique used to analyze the frequency components of a signal or time series.\n* Multi-head cross-attention is a mechanism used in some models to analyze the frequency of occurrence of different values or patterns in a dataset.\n* Frequency analysis and multi-head cross-attention are closely related concepts in the context of time series analysis and machine learning.\n* Frequency analysis has been used in some models to analyze the data.\n\n**Entity Relationships:**\n\n* FREQUENCY ANALYSIS is related to MULTI-HEAD CROSS-ATTENTION as multi-head cross-attention can be used to analyze the frequency components of a signal or time series.\n* FREQUENCY ANALYSIS is related to MULTI-HEAD CROSS-ATTENTION as multi-head cross-attention can be used to analyze the frequency of occurrence of different values or patterns in a dataset.\n\n**Context:**\n\nThe summary is based on the analysis of the provided text document, which suggests that the primary language of the text is English. The text contains technical terms, mathematical equations, and references to academic papers, all written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.", "from": "FREQUENCY ANALYSIS", "source_id": "15c3350fad556f666d93817c5036109c,171fb6df1bf9905b151dfb846d75d0f8,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,cf0d73cbe44e03ef7300f5c53b72090a,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "to": "MULTI-HEAD CROSS-ATTENTION", "width": 16.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe primary language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nThe text mentions \"MULTI-HEAD CROSS-ATTENTION\" as a concept, which is a technique used in machine learning. Specifically, it is a type of attention mechanism that allows a model to jointly attend to information from different representation subspaces at different positions. This technique is relevant to the field of artificial intelligence and machine learning, which is also the focus of the ICLR (International Conference on Learning Representations) conference.\n\nICLR is an academic conference where research on various topics in machine learning and artificial intelligence is presented. Given its focus on machine learning, it is likely that research on multi-head cross-attention may be presented at this conference. Therefore, there is a relationship between ICLR and multi-head cross-attention, as the conference provides a platform for researchers to share their work on this technique.\n\nIn summary, the text is written in English, and it discusses the concept of multi-head cross-attention, which is a technique used in machine learning. This technique is related to ICLR, an academic conference focused on machine learning and artificial intelligence, where research on multi-head cross-attention may be presented.", "from": "MULTI-HEAD CROSS-ATTENTION", "source_id": "171fb6df1bf9905b151dfb846d75d0f8,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,cf0d73cbe44e03ef7300f5c53b72090a,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "to": "ICLR", "width": 15.0}, {"description": "Patch as prefix is related to multi-head cross-attention as multi-head cross-attention is used in patch as prefix", "from": "MULTI-HEAD CROSS-ATTENTION", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "PATCH AS PREFIX", "width": 1.0}, {"description": "Prompt as prefix is related to multi-head cross-attention as multi-head cross-attention is used in prompt as prefix", "from": "MULTI-HEAD CROSS-ATTENTION", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "PROMPT AS PREFIX", "width": 1.0}, {"description": "Multi-head cross-attention is related to language model as language model is used to fine-tune multi-head cross-attention", "from": "MULTI-HEAD CROSS-ATTENTION", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "LANGUAGE MODEL", "width": 1.0}, {"description": "Multi-head cross-attention is related to text embeddings as text embeddings are used in multi-head cross-attention", "from": "MULTI-HEAD CROSS-ATTENTION", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "TEXT EMBEDDINGS", "width": 1.0}, {"description": "Multi-head cross-attention is related to query matrices as query matrices are used in multi-head cross-attention", "from": "MULTI-HEAD CROSS-ATTENTION", "source_id": "509b431231e669be373f593b31412eed", "to": "QUERY MATRICES", "width": 1.0}, {"description": "Multi-head cross-attention is related to key matrices as key matrices are used in multi-head cross-attention", "from": "MULTI-HEAD CROSS-ATTENTION", "source_id": "509b431231e669be373f593b31412eed", "to": "KEY MATRICES", "width": 1.0}, {"description": "Multi-head cross-attention is related to value matrices as value matrices are used in multi-head cross-attention", "from": "MULTI-HEAD CROSS-ATTENTION", "source_id": "509b431231e669be373f593b31412eed", "to": "VALUE MATRICES", "width": 1.0}, {"description": "Heads are related to multi-head cross-attention as both are used in the TIME-LLM model", "from": "MULTI-HEAD CROSS-ATTENTION", "source_id": "1b48e9ca066ac5ba037066bb762d3458", "to": "HEADS", "width": 1.0}, {"description": "Based on the provided information, the primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nThe two entities mentioned in the text are \"ICLR\" and \"AAAI\", which are both academic conferences. \n\nICLR and AAAI are academic conferences that are related to each other, as they both focus on artificial intelligence and machine learning. They may present research on related topics, and are likely to be of interest to researchers and professionals in the field of artificial intelligence and machine learning.\n\nIn particular, ICLR (International Conference on Learning Representations) and AAAI (Association for the Advancement of Artificial Intelligence) are two prominent conferences in the field of artificial intelligence and machine learning. They provide a platform for researchers to present and discuss their work, and are widely recognized as leading events in the field.\n\nOverall, the text suggests that ICLR and AAAI are two important conferences in the field of artificial intelligence and machine learning, and are likely to be of interest to researchers and professionals in this field.", "from": "ICLR", "source_id": "15c3350fad556f666d93817c5036109c,171fb6df1bf9905b151dfb846d75d0f8,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,cf0d73cbe44e03ef7300f5c53b72090a,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "to": "AAAI", "width": 16.0}, {"description": "PMLR and ICLR are academic conferences mentioned in the text", "from": "ICLR", "source_id": "15c3350fad556f666d93817c5036109c", "to": "PMLR", "width": 1.0}, {"description": "Based on the provided information, the primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nThe two entities mentioned are \"AAAI\" and \"PMLR\", which are academic conferences. According to the descriptions, both \"AAAI\" and \"PMLR\" are academic conferences mentioned in the text, with \"AAAI\" being specifically mentioned as an academic conference indicated by the use of the abbreviation \"AAAI\".\n\nFurther analysis reveals that \"AAAI\" and \"PMLR\" are related as both are academic conferences focused on machine learning and artificial intelligence. They may also present research on related topics, suggesting a connection between the two conferences.\n\nIn summary, \"AAAI\" and \"PMLR\" are two academic conferences that are related to each other in terms of their focus on machine learning and artificial intelligence, and may present research on related topics. The primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nHere is a more detailed and enriched summary:\n\n\"AAAI\" and \"PMLR\" are two prominent academic conferences in the field of machine learning and artificial intelligence. Both conferences are mentioned in the text as being related to each other, with a possible overlap in the topics they present. The use of the abbreviation \"AAAI\" in the text confirms its status as an academic conference. The mention of \"PMLR\" alongside \"AAAI\" suggests that it is also an academic conference, possibly with a similar focus on machine learning and artificial intelligence. The primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. This suggests that the text is likely from a research paper or a technical document written in English.", "from": "AAAI", "source_id": "15c3350fad556f666d93817c5036109c,171fb6df1bf9905b151dfb846d75d0f8,4742f536818b2fce762157bdb2cb1a3c,53ae42e8cf874f9df3817b3e2589d60c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,b13b2cc422483985c354844b166a0151,bb87457fce8d4214bfe1f398b7ea35f2,c9efd571b05c136c0bf9d7e89194ec88,cf0d73cbe44e03ef7300f5c53b72090a,d16a81565fcd654ab21768510dcc5d7f,e8151dde9661b4cce6a6b8e5a8371c96,ee42bd06cc3770a3384df85cc16fef54", "to": "PMLR", "width": 16.0}, {"description": "International Conference on Machine Learning is related to PMLR as research papers presented at the conference are published by PMLR", "from": "PMLR", "source_id": "a69a914fb6c895c7202532b69ad3e094", "to": "INTERNATIONAL CONFERENCE ON MACHINE LEARNING", "width": 1.0}, {"description": "LSTM-VAE and OmniAnomaly are both models mentioned in the text, as indicated by the use of their names", "from": "LSTM-VAE", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "OMNIANOMALY", "width": 1.0}, {"description": "LSTM-VAE and MSCRED are both models mentioned in the text, as indicated by the use of their names", "from": "LSTM-VAE", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "MSCRED", "width": 1.0}, {"description": "LSTM-VAE and THOC are both models mentioned in the text, as indicated by the use of their names", "from": "LSTM-VAE", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "THOC", "width": 1.0}, {"description": "LSTM-VAE and GDN are both models mentioned in the text, as indicated by the use of their names", "from": "LSTM-VAE", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "GDN", "width": 1.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\n**Summary of OmniAnomaly and MSCRED**\n\nOmniAnomaly and MSCRED are two deep learning models for anomaly detection, which have been evaluated and compared in the text. Both models are state-of-the-art for multivariate time-series anomaly detection. However, MSCRED has a higher F1 score than OmniAnomaly, indicating its superior performance in anomaly detection. Notably, OmniAnomaly outperforms MSCRED when compared across all datasets and also when given the complete WADI dataset. These models are mentioned in the text and have been used for anomaly detection, highlighting their significance in the field of time-series analysis.\n\n**Key Points:**\n\n* Both OmniAnomaly and MSCRED are deep learning models for anomaly detection.\n* MSCRED has a higher F1 score than OmniAnomaly.\n* OmniAnomaly outperforms MSCRED across all datasets and the complete WADI dataset.\n* Both models are state-of-the-art for multivariate time-series anomaly detection.\n* They are mentioned in the text and have been used for anomaly detection.\n\n**Entity Names:**\n\n* OmniAnomaly\n* MSCRED\n\n**Context:**\n\nThe summary is generated based on the descriptions provided, which indicate that OmniAnomaly and MSCRED are two deep learning models for anomaly detection. The text suggests that these models have been evaluated and compared, and their performance metrics are mentioned. The summary highlights the key points of the models, including their performance differences and their significance in the field of time-series analysis.", "from": "OMNIANOMALY", "source_id": "1b51ec337efd822ca3a0b3eb819c1b91,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,70a97858b727e5af1466ae5eb9183921,971e73b638469366cfdd3af2e7c7a824,d5c8b72da09cebefa0c26285ad5272eb,ee42bd06cc3770a3384df85cc16fef54", "to": "MSCRED", "width": 7.0}, {"description": "OmniAnomaly and THOC are both models mentioned in the text, as indicated by the use of their names", "from": "OMNIANOMALY", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "THOC", "width": 1.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\n**Entities:** \"OMNIANOMALY\" and \"DAGMM\"\n\n**Summary:** OMNIANOMALY and DAGMM are both deep learning models for anomaly detection in multivariate time series data. They are state-of-the-art models for multivariate time-series anomaly detection and have been evaluated in the text, with performance metrics indicating their effectiveness. Specifically, OMNIANOMALY uses a stochastic recurrent neural network and a planar normalizing flow to generate reconstruction probabilities, while DAGMM is related to OMNIANOMALY through their shared application in anomaly detection and diagnosis. Furthermore, both models have been compared in terms of their performance, with OMNIANOMALY having a higher F1 score than DAGMM.\n\n**Key Features:**\n\n* Both OMNIANOMALY and DAGMM are deep learning models for anomaly detection in multivariate time series data.\n* They are state-of-the-art models for multivariate time-series anomaly detection.\n* OMNIANOMALY uses a stochastic recurrent neural network and a planar normalizing flow to generate reconstruction probabilities.\n* DAGMM is related to OMNIANOMALY through their shared application in anomaly detection and diagnosis.\n* OMNIANOMALY has a higher F1 score than DAGMM.\n\n**Relationship:** OMNIANOMALY and DAGMM are related as both are models mentioned in the text and are used for anomaly detection in multivariate time series data.", "from": "OMNIANOMALY", "source_id": "1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,971e73b638469366cfdd3af2e7c7a824,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb,ffbbbf29ffb8d038e241f023079cb0a2", "to": "DAGMM", "width": 9.0}, {"description": "Omnianomaly is related to POT as POT is an adjusted Peak Over Threshold method for automated anomaly threshold selection", "from": "OMNIANOMALY", "source_id": "ffbbbf29ffb8d038e241f023079cb0a2", "to": "POT", "width": 1.0}, {"description": "The POT technique is used in OmniAnomaly to set more accurate threshold values", "from": "OMNIANOMALY", "source_id": "8e075f1de7293e0cd1724bd167c263be", "to": "POT TECHNIQUE", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"OMNIANOMALY\" and \"LSTM-NDT\" are two models that have been evaluated in the text. Both models are used for anomaly detection, with a notable difference in their performance metrics. Specifically, OmniAnomaly has been found to have a higher F1 score compared to LSTM-NDT. This suggests that OmniAnomaly is more effective in detecting anomalies than LSTM-NDT.\n\nThe relationship between the two models is that they are both mentioned in the text, indicating that they have been compared or evaluated in some capacity. This comparison has led to the conclusion that OmniAnomaly outperforms LSTM-NDT in terms of anomaly detection.\n\nOverall, the summary highlights the key characteristics and performance differences between the two models, providing valuable insights for those interested in anomaly detection and machine learning.", "from": "OMNIANOMALY", "source_id": "2fc273b26b3ec71da711daaa40c0355d,971e73b638469366cfdd3af2e7c7a824,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb", "to": "LSTM-NDT", "width": 4.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Summary of OmniAnomaly and TranAD**\n\nOmniAnomaly and TranAD are two deep learning models designed for anomaly detection. Both models are mentioned in the text, indicating their relevance to the topic of anomaly detection. A comparison of the two models reveals that TranAD outperforms OmniAnomaly across all datasets, including the complete WADI dataset. This suggests that TranAD is a more effective model for anomaly detection, at least in the context of the provided data.\n\n**Key Findings**\n\n* Both OmniAnomaly and TranAD are deep learning models for anomaly detection.\n* TranAD outperforms OmniAnomaly across all datasets, including the complete WADI dataset.\n* The comparison of the two models suggests that TranAD is a more effective model for anomaly detection.\n\n**Entity Information**\n\n* **OMNIANOMALY**: A deep learning model for anomaly detection.\n* **TRANAD**: A deep learning model for anomaly detection that outperforms OmniAnomaly across all datasets.\n\n**Context**\n\nThe text suggests that the models are being compared in the context of anomaly detection, with a focus on their performance across different datasets. The WADI dataset is mentioned as a specific dataset used in the comparison. The text also implies that the models are being evaluated using some form of evaluation metric, although the specific metric is not mentioned.", "from": "OMNIANOMALY", "source_id": "2b44aebc638544dcf835db30c4270d09,70a97858b727e5af1466ae5eb9183921,971e73b638469366cfdd3af2e7c7a824", "to": "TRANAD", "width": 3.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"OMNIANOMALY\" and \"MERLIN\" are both models used for anomaly detection. According to the description, OmniAnomaly has a higher F1 score than MERLIN, indicating its superior performance in detecting anomalies. Both models are mentioned in the text, suggesting that they are related and share similarities in their approach to anomaly detection.\n\nFurther analysis reveals that the text is written in English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers. This suggests that the models, including OMNIANOMALY and MERLIN, are likely developed and discussed within an English-language academic or research context.\n\nOverall, the summary highlights the key characteristics of the two models, their performance comparison, and the language context in which they are discussed.", "from": "OMNIANOMALY", "source_id": "971e73b638469366cfdd3af2e7c7a824,d5c8b72da09cebefa0c26285ad5272eb", "to": "MERLIN", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"OMNIANOMALY\" and \"GDN\" are both deep learning models for anomaly detection. They are mentioned in the text as being related to each other, suggesting a connection or similarity between the two models. Specifically, both OmniAnomaly and GDN are models that utilize deep learning techniques for detecting anomalies, as indicated by their names being used in the table.\n\nThis information is supported by the text, which mentions the use of technical terms and mathematical equations related to time series analysis and frequency analysis, further indicating that the models are designed for anomaly detection in time series data. The presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" also suggests that the text is written in English and is likely from a research paper or technical document.\n\nOverall, the summary provides a clear understanding of the relationship between OmniAnomaly and GDN, highlighting their shared purpose as deep learning models for anomaly detection, and their connection to time series analysis and frequency analysis.", "from": "OMNIANOMALY", "source_id": "2b44aebc638544dcf835db30c4270d09,971e73b638469366cfdd3af2e7c7a824", "to": "GDN", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe OmniAnomaly and MAD-GAN are two deep learning models specifically designed for anomaly detection. Both models are mentioned in the text, indicating their relevance to the field of anomaly detection. \n\nA comparison between the two models suggests that MAD-GAN has a higher F1 score than OmniAnomaly, implying that MAD-GAN may be more effective in detecting anomalies. However, it is essential to note that the text does not provide a comprehensive evaluation of both models, and further analysis is required to determine their relative strengths and weaknesses.\n\nThe OmniAnomaly and MAD-GAN models are likely used in various applications, including time series analysis and long-term series forecasting, given the context of the text. The use of frequency analysis and multi-head cross-attention in these models further supports their potential for anomaly detection in complex data sets.\n\nOverall, the OmniAnomaly and MAD-GAN models are two notable deep learning models for anomaly detection, with MAD-GAN showing a higher F1 score than OmniAnomaly. Further research is necessary to fully understand their capabilities and limitations in real-world applications.", "from": "OMNIANOMALY", "source_id": "2b44aebc638544dcf835db30c4270d09,971e73b638469366cfdd3af2e7c7a824,d5c8b72da09cebefa0c26285ad5272eb", "to": "MAD-GAN", "width": 3.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe OmniAnomaly and MTAD-GAT are two deep learning models specifically designed for anomaly detection. Both models are utilized for identifying anomalies, with MTAD-GAT demonstrating a higher F1 score compared to OmniAnomaly. This suggests that MTAD-GAT may be more effective in detecting anomalies, although further analysis is required to confirm this conclusion.\n\nThe use of these models in anomaly detection is likely facilitated by their ability to process complex data patterns, which is a key aspect of deep learning models. The fact that they are both used for anomaly detection implies that they share some common characteristics or features that make them suitable for this task.\n\nIt is worth noting that the comparison between OmniAnomaly and MTAD-GAT is based on their performance metrics, specifically the F1 score. This suggests that the evaluation of these models is focused on their ability to accurately detect anomalies, rather than other aspects such as computational efficiency or interpretability.\n\nOverall, the OmniAnomaly and MTAD-GAT models represent two approaches to anomaly detection, with MTAD-GAT appearing to have a slight edge in terms of performance. Further research and analysis are needed to fully understand the strengths and limitations of each model.", "from": "OMNIANOMALY", "source_id": "2b44aebc638544dcf835db30c4270d09,d5c8b72da09cebefa0c26285ad5272eb", "to": "MTAD-GAT", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe OmniAnomaly and CAE-M are two deep learning models specifically designed for anomaly detection. Both models are utilized for identifying anomalies, with CAE-M demonstrating a higher F1 score compared to OmniAnomaly. This suggests that CAE-M is more effective in detecting anomalies, as indicated by its higher performance metric. The use of these models in anomaly detection implies that they are capable of identifying unusual patterns or outliers in data, which is a critical aspect of various applications, including quality control, fraud detection, and predictive maintenance.\n\nThe summary is based on the information provided in the description list, which includes the names of the entities, their purpose, and a comparison of their performance. The summary is written in third person and includes relevant details to provide a clear understanding of the OmniAnomaly and CAE-M models.", "from": "OMNIANOMALY", "source_id": "2b44aebc638544dcf835db30c4270d09,d5c8b72da09cebefa0c26285ad5272eb", "to": "CAE-M", "width": 2.0}, {"description": "MSCRED and THOC are both models mentioned in the text, as indicated by the use of their names", "from": "MSCRED", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "THOC", "width": 1.0}, {"description": "Based on the provided information, the primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nThe text describes two entities, \"MSCRED\" and \"GDN\", which are both deep learning models for anomaly detection. This is inferred from the fact that their names are mentioned in a table, suggesting a connection to a specific application or context.\n\nA comprehensive summary of the data provided is as follows:\n\nMSCRED and GDN are two deep learning models that are related to each other, as they are both mentioned in the text. Specifically, they are both models for anomaly detection, which is a key application area for deep learning techniques. The fact that their names are mentioned in a table suggests that they are being compared or evaluated in some way, possibly as part of a research study or technical document.\n\nThis summary is based on the information provided in the description list, which is consistent across all three points. The use of technical terms and references to academic papers in the text further supports the conclusion that MSCRED and GDN are deep learning models for anomaly detection.", "from": "MSCRED", "source_id": "2b44aebc638544dcf835db30c4270d09,971e73b638469366cfdd3af2e7c7a824,ee42bd06cc3770a3384df85cc16fef54", "to": "GDN", "width": 3.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nMSCRED and MAD-GAN are two state-of-the-art deep learning models for anomaly detection in multivariate time series data. Both models have been evaluated in the text and have been used for anomaly detection, with MSCRED outperforming MAD-GAN across all datasets, including the complete WADI dataset. Specifically, MSCRED has been shown to have a higher performance metric than MAD-GAN, with the exact metric not specified in the provided descriptions. However, it is mentioned that MAD-GAN has a higher F1 score than MSCRED, which suggests that MSCRED may have a higher accuracy or precision than MAD-GAN. Overall, both MSCRED and MAD-GAN are considered to be effective models for anomaly detection in multivariate time series data, with MSCRED being the more superior model based on the provided information.\n\nIt is worth noting that the descriptions provided are consistent in their portrayal of MSCRED and MAD-GAN as models for anomaly detection, with the main point of contention being the relative performance of the two models. However, based on the information provided, it appears that MSCRED is the more effective model, with a higher performance metric than MAD-GAN across all datasets.", "from": "MSCRED", "source_id": "1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,70a97858b727e5af1466ae5eb9183921,971e73b638469366cfdd3af2e7c7a824,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb", "to": "MAD-GAN", "width": 9.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMSCRED and TRANAD are two deep learning models for anomaly detection. Both models are mentioned in the text, indicating their relevance to the topic of anomaly detection. A comparison of the two models reveals that TRANAD outperforms MSCRED across all datasets, including the complete WADI dataset. This suggests that TRANAD is a more effective model for anomaly detection, at least in the context of the provided data.", "from": "MSCRED", "source_id": "2b44aebc638544dcf835db30c4270d09,70a97858b727e5af1466ae5eb9183921,971e73b638469366cfdd3af2e7c7a824", "to": "TRANAD", "width": 3.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMSCRED and MERLIN are two models used for anomaly detection. Both models are mentioned in the text, indicating their relevance to the topic. Notably, MSCRED has been found to have a higher F1 score compared to MERLIN, suggesting its superior performance in anomaly detection tasks. This information is crucial for understanding the capabilities and limitations of each model, allowing for informed decisions in their application.\n\nThe text also implies that both MSCRED and MERLIN are used in a technical context, possibly within a research paper or a technical document, as indicated by the presence of technical terms, mathematical equations, and references to academic papers. This further emphasizes the importance of these models in the field of anomaly detection and their potential applications in various domains.\n\nOverall, the summary highlights the key characteristics and relationships between MSCRED and MERLIN, providing a clear understanding of their roles in anomaly detection and their relative performance.", "from": "MSCRED", "source_id": "971e73b638469366cfdd3af2e7c7a824,d5c8b72da09cebefa0c26285ad5272eb", "to": "MERLIN", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMSCRED and LSTM-NDT are two models used for anomaly detection. Both models are mentioned in the text, indicating their relevance to the topic. MSCRED has been found to have a higher F1 score compared to LSTM-NDT, suggesting its superior performance in anomaly detection tasks. \n\nThis summary is based on the information provided in the description list, which includes the relationship between the two models and their performance metrics. The summary is written in a coherent and concise manner, resolving any potential contradictions and providing a clear understanding of the entities and their characteristics.", "from": "MSCRED", "source_id": "971e73b638469366cfdd3af2e7c7a824,d5c8b72da09cebefa0c26285ad5272eb", "to": "LSTM-NDT", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMSCRED and DAGMM are two deep learning models used for anomaly detection. Both models are mentioned in the text, indicating their relevance to the topic of anomaly detection. Specifically, MSCRED has been shown to have a higher F1 score than DAGMM, suggesting its superior performance in this task. The use of these models in anomaly detection is further supported by their inclusion in a table, where their names are listed alongside other relevant information. Overall, MSCRED and DAGMM are two notable models in the field of anomaly detection, with MSCRED demonstrating a higher level of accuracy than DAGMM.\n\nThis summary is based on the information provided in the description list, which includes the following key points:\n\n* Both MSCRED and DAGMM are deep learning models for anomaly detection.\n* MSCRED has a higher F1 score than DAGMM.\n* Both models are mentioned in the text and included in a table.\n\nBy combining these points, we can create a comprehensive summary that highlights the key characteristics and relationships between MSCRED and DAGMM.", "from": "MSCRED", "source_id": "2b44aebc638544dcf835db30c4270d09,971e73b638469366cfdd3af2e7c7a824,d5c8b72da09cebefa0c26285ad5272eb", "to": "DAGMM", "width": 3.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nMSCRED and MTAD-GAT are two deep learning models designed for anomaly detection. Both models are utilized for identifying anomalies, with MTAD-GAT demonstrating a higher F1 score compared to MSCRED. This suggests that MTAD-GAT may be more effective in detecting anomalies than MSCRED.\n\nThe summary is written in third person and includes the entity names, providing the full context. Relevant information from the nearby text has been incorporated to enrich the summary.", "from": "MSCRED", "source_id": "2b44aebc638544dcf835db30c4270d09,d5c8b72da09cebefa0c26285ad5272eb", "to": "MTAD-GAT", "width": 2.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nMSCRED and CAE-M are two deep learning models used for anomaly detection. They are both utilized for identifying anomalies, with CAE-M demonstrating a higher F1 score compared to MSCRED. This suggests that CAE-M may be more effective in detecting anomalies than MSCRED.\n\nIt is worth noting that the primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nIn the context of anomaly detection, MSCRED and CAE-M are two models that can be used to identify unusual patterns or outliers in data. CAE-M\u0027s higher F1 score indicates its potential for better performance in detecting anomalies, making it a more suitable option for applications where anomaly detection is critical.", "from": "MSCRED", "source_id": "2b44aebc638544dcf835db30c4270d09,d5c8b72da09cebefa0c26285ad5272eb", "to": "CAE-M", "width": 2.0}, {"description": "THOC and GDN are both models mentioned in the text, as indicated by the use of their names", "from": "THOC", "source_id": "ee42bd06cc3770a3384df85cc16fef54", "to": "GDN", "width": 1.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"GDN\" and \"TRANAD\" are both deep learning models used for anomaly detection. They have been evaluated in the text, as indicated by the presence of their names and performance metrics. Both models have been compared in the text, with GDN outperforming TRANAD in terms of overall performance across all datasets, including the complete WADI dataset. However, TRANAD has been shown to outperform GDN in detection and diagnosis performance.\n\nThis summary resolves the contradictions between the descriptions by acknowledging that both models have their strengths and weaknesses, with GDN excelling in overall performance but TRANAD performing better in specific aspects of anomaly detection and diagnosis.\n\nThe summary is written in third person and includes the entity names for context. Relevant information from the nearby text has been incorporated to provide a comprehensive understanding of the relationship between GDN and TRANAD.", "from": "GDN", "source_id": "00973c1c3c962d9234a38037709824b1,0c4c072869e10b0b4bd4bc19a60a23a3,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,971e73b638469366cfdd3af2e7c7a824,a4a241e471ad258932241bc441b96155,d16a81565fcd654ab21768510dcc5d7f", "to": "TRANAD", "width": 9.0}, {"description": "LSTMs are related to GDN as GDN uses deep neural networks, which are similar to LSTMs", "from": "GDN", "source_id": "bd73ee0439609823e12a877840c6ebae", "to": "LSTMS", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMTAD-GAT and GDN are two deep learning models specifically designed for anomaly detection. Both models utilize attention mechanisms to focus on specific modes of the data, allowing them to effectively identify anomalies. As state-of-the-art methods for anomaly detection using deep neural networks, MTAD-GAT and GDN have been recognized for their ability to accurately detect anomalies in various datasets. Their use in anomaly detection is evident from their inclusion in the table, highlighting their significance in this area of research.\n\nThe use of attention mechanisms in both models enables them to selectively focus on relevant features of the data, improving their performance in anomaly detection tasks. This is particularly important in anomaly detection, where the ability to identify unusual patterns or outliers is crucial. By leveraging the strengths of deep learning and attention mechanisms, MTAD-GAT and GDN have established themselves as leading methods for anomaly detection in the field of deep neural networks.\n\nIn terms of their relationship, MTAD-GAT and GDN are closely related as both are state-of-the-art methods for anomaly detection using deep neural networks. Their shared goal of identifying anomalies and their use of attention mechanisms to achieve this goal highlight their similarities and complementarity. Overall, MTAD-GAT and GDN are two powerful models that have made significant contributions to the field of anomaly detection, and their continued development and refinement are likely to have a lasting impact on this area of research.", "from": "GDN", "source_id": "00973c1c3c962d9234a38037709824b1,2b44aebc638544dcf835db30c4270d09,8e075f1de7293e0cd1724bd167c263be,bd73ee0439609823e12a877840c6ebae", "to": "MTAD-GAT", "width": 4.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\n**Summary of GDN and CAE-M**\n\nGDN and CAE-M are two deep learning models used for anomaly detection in multivariate time series data. Both models have been evaluated in the text, with performance metrics indicating their effectiveness in identifying anomalies. They have been compared in the text, with CAE-M outperforming GDN across all datasets, including the complete WADI dataset. As both models are mentioned in the text, they are related and have been used for anomaly detection purposes.\n\n**Key Points**\n\n* Both GDN and CAE-M are deep learning models for anomaly detection.\n* They are used for anomaly detection in multivariate time series data.\n* Both models have been evaluated in the text with performance metrics.\n* CAE-M outperforms GDN across all datasets, including the complete WADI dataset.\n* They have been compared in the text, indicating their relationship and use for anomaly detection.\n\n**Entity Information**\n\n* GDN: A deep learning model for anomaly detection.\n* CAE-M: A deep learning model for anomaly detection that outperforms GDN across all datasets.\n\n**Context**\n\nThe text discusses the use of GDN and CAE-M for anomaly detection in multivariate time series data. The comparison of these models indicates their effectiveness in identifying anomalies and highlights the superiority of CAE-M over GDN. The text likely provides further details on the performance metrics and evaluation of these models, which are not included in this summary.", "from": "GDN", "source_id": "00973c1c3c962d9234a38037709824b1,1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,69afb4bcb8c1e03975c837102e1d0b32,70a97858b727e5af1466ae5eb9183921,a4a241e471ad258932241bc441b96155,d16a81565fcd654ab21768510dcc5d7f", "to": "CAE-M", "width": 9.0}, {"description": "GDN is related to MERLIN as both are models mentioned in the text", "from": "GDN", "source_id": "971e73b638469366cfdd3af2e7c7a824", "to": "MERLIN", "width": 1.0}, {"description": "GDN is related to LSTM-NDT as both are models mentioned in the text", "from": "GDN", "source_id": "971e73b638469366cfdd3af2e7c7a824", "to": "LSTM-NDT", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nGDN and DAGMM are two deep learning models for anomaly detection. They are both mentioned in the text as being related to each other, indicating a connection between the two models. Specifically, DAGMM and GDN are both models that have been used for anomaly detection, as indicated by their presence in the table.\n\nFurther analysis of the text reveals that both models are likely used for time series analysis, given the mention of \"time series\" and \"long-term series forecasting\" in the text. Additionally, the use of technical terms such as \"frequency analysis\" and \"multi-head cross-attention\" suggests that the models may be used for more complex tasks such as forecasting and anomaly detection in time series data.\n\nThe text also mentions academic conferences and journals, such as ICLR, AAAI, and PMLR, which suggests that the models may have been presented or published in these venues. However, this information is not directly related to the models themselves, but rather to the broader context in which they are being used.\n\nOverall, GDN and DAGMM are two deep learning models for anomaly detection that are related to each other and are likely used for time series analysis and forecasting.", "from": "GDN", "source_id": "2b44aebc638544dcf835db30c4270d09,971e73b638469366cfdd3af2e7c7a824", "to": "DAGMM", "width": 2.0}, {"description": "Based on the provided information, the primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nThe text describes two entities, \"GDN\" and \"MAD-GAN\", which are both deep learning models used for anomaly detection. This is supported by the fact that their names are mentioned in a table, and they are described as models used for anomaly detection.\n\nGiven the information collected from all the descriptions, a comprehensive summary of the data is as follows:\n\n\"GDN and MAD-GAN are two deep learning models for anomaly detection, as indicated by their mention in the table and their description as models used for anomaly detection. Both models are related to each other, as they are mentioned together in the text.\"\n\nThis summary is written in third person and includes the entity names, providing the full context. Relevant information from the nearby text has been incorporated to enrich the summary, resolving any potential contradictions and providing a single, coherent description.", "from": "GDN", "source_id": "00973c1c3c962d9234a38037709824b1,2b44aebc638544dcf835db30c4270d09,971e73b638469366cfdd3af2e7c7a824", "to": "MAD-GAN", "width": 3.0}, {"description": "SWAT and WADI datasets are used to evaluate AnomalyBERT", "from": "SWAT DATASET", "source_id": "587ca8c6cc86a93299e9540153babbc6", "to": "WADI DATASET", "width": 1.0}, {"description": "WADI dataset is related to Tracer dataset as both are types of datasets used to evaluate the performance of the model", "from": "WADI DATASET", "source_id": "5277ea101e78f34ea2c62fa10007b2ac", "to": "TRACER DATASET", "width": 1.0}, {"description": "Dataset is related to anomaly as it is used to train and test the model", "from": "DATASET", "source_id": "71f873c12230e6ede47583d81eb85e23", "to": "ANOMALY", "width": 1.0}, {"description": "A dataset is used to train a predictive model that can accurately predict the values at the future time points", "from": "DATASET", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "PREDICTIVE MODEL", "width": 1.0}, {"description": "Dataset is related to Tab 8 as it is used to summarize the dataset statistics", "from": "DATASET", "source_id": "74527a4337ed6919731be520311ae774", "to": "TAB 8", "width": 1.0}, {"description": "Input window is related to anomaly as anomaly is a data point or sequence that is significantly different from the rest of the data", "from": "ANOMALY", "source_id": "f2e78b25a535b82e2743a0ea4052eb9a", "to": "INPUT WINDOW", "width": 1.0}, {"description": "Sub-datasets are a collection of data from 28 different machines", "from": "SUB-DATASETS", "source_id": "5809618fe3a2014c2140601fcf103022", "to": "MACHINES", "width": 1.0}, {"description": "Machines are the sources of the sub-datasets provided by the Internet company", "from": "MACHINES", "source_id": "5809618fe3a2014c2140601fcf103022", "to": "INTERNET COMPANY", "width": 1.0}, {"description": "A linear layer is used as the embedding layer in the model", "from": "LINEAR LAYER", "source_id": "5809618fe3a2014c2140601fcf103022", "to": "MLPS", "width": 1.0}, {"description": "Linear layer maps the decoder\u0027s output to the forecasting window dimension", "from": "LINEAR LAYER", "source_id": "518bfcd6711530089fe3914ca16459c2", "to": "FORECASTING WINDOW DIMENSION", "width": 1.0}, {"description": "The embedding dimension is 512 and there are eight attention heads", "from": "EMBEDDING DIMENSION", "source_id": "5809618fe3a2014c2140601fcf103022", "to": "ATTENTION HEADS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"WINDOW SIZE\" and \"PATCH SIZE\" are related concepts in the context of computer vision models. Both are used in these models, and their values can vary depending on the specific dataset being used. The patch size is related to the window size, suggesting a connection between these two parameters in the context of image or video processing.\n\nThis summary is based on the information provided in the description list, which states that the patch size is related to the window size and that both parameters vary with datasets. This suggests that the window size and patch size are not fixed values, but rather they can be adjusted based on the characteristics of the dataset being used.\n\nIn the context of computer vision models, the window size and patch size are likely used to define the size of the regions of interest (ROIs) or patches that are being processed. The window size may refer to the size of the window or ROI that is being used to extract features from an image or video, while the patch size may refer to the size of the patch or ROI that is being used to process the image or video.\n\nOverall, the summary provides a clear understanding of the relationship between the window size and patch size in the context of computer vision models, and highlights the importance of adjusting these parameters based on the characteristics of the dataset being used.", "from": "WINDOW SIZE", "source_id": "5809618fe3a2014c2140601fcf103022,8ff636d53a50d7a3bad17443bf104ba2", "to": "PATCH SIZE", "width": 2.0}, {"description": "Window size is related to max length % of outlier as both are used in anomaly detection", "from": "WINDOW SIZE", "source_id": "8ff636d53a50d7a3bad17443bf104ba2", "to": "MAX LENGTH % OF OUTLIER", "width": 1.0}, {"description": "Dataset size is related to window size as window size can affect the calculation of statistics", "from": "WINDOW SIZE", "source_id": "9b35a2c607e0f4b8cbc9179f424f180a", "to": "DATASET SIZE", "width": 1.0}, {"description": "Window size affects the training time of TranAD, with smaller windows resulting in faster inference times", "from": "WINDOW SIZE", "source_id": "78ee4a3d7a2bffd4405a03d94a4f6cb1", "to": "TRAINING TIME", "width": 1.0}, {"description": "Window size affects the F1 score of TranAD, with a window size of 10 resulting in a reasonable balance between F1 score and training times", "from": "WINDOW SIZE", "source_id": "78ee4a3d7a2bffd4405a03d94a4f6cb1", "to": "F1 SCORE", "width": 1.0}, {"description": "Max length % of outlier is related to use of length adjustment as both are used in anomaly detection", "from": "MAX LENGTH % OF OUTLIER", "source_id": "8ff636d53a50d7a3bad17443bf104ba2", "to": "USE OF LENGTH ADJUSTMENT", "width": 1.0}, {"description": "t-SNE is related to 2D planes as it is used to visualize high-dimensional data in 2D planes", "from": "TSNE", "source_id": "ee651b9bedb0cbcb6272a67deda44bb0", "to": "2D PLANES", "width": 1.0}, {"description": "Abnormal windows are related to anomaly prediction process as they are used to predict anomalies", "from": "ABNORMAL WINDOWS", "source_id": "ee651b9bedb0cbcb6272a67deda44bb0", "to": "ANOMALY PREDICTION PROCESS", "width": 1.0}, {"description": "Tokenization is related to quantization as quantization is used to discretize time series values", "from": "TOKENIZATION", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "QUANTIZATION", "width": 1.0}, {"description": "Tokenization is related to vocabulary as vocabulary refers to the set of words or terms used in a language or model", "from": "TOKENIZATION", "source_id": "6eb4c16edf2eedfd03721efb199478d8", "to": "VOCABULARY", "width": 1.0}, {"description": "Tokenization is used to construct lagged features", "from": "TOKENIZATION", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "LAGGED FEATURES", "width": 1.0}, {"description": "Lagged features are used in the tokenization scheme of Lag-Llama", "from": "TOKENIZATION", "source_id": "55099ca8e21d1db3bdaf7bd04e590b72", "to": "LAG-LAGGED FEATURES", "width": 1.0}, {"description": "Lag indices are used to construct lagged features in the tokenization scheme of Lag-Llama", "from": "TOKENIZATION", "source_id": "55099ca8e21d1db3bdaf7bd04e590b72", "to": "LAG INDICES", "width": 1.0}, {"description": "Date-time features are used in the tokenization scheme of Lag-Llama", "from": "TOKENIZATION", "source_id": "55099ca8e21d1db3bdaf7bd04e590b72", "to": "DATE-TIME FEATURES", "width": 1.0}, {"description": "The Lag-Llama architecture is based on the decoder-only transformer-based architecture LLaMA", "from": "TOKENIZATION", "source_id": "55099ca8e21d1db3bdaf7bd04e590b72", "to": "LLAMA", "width": 1.0}, {"description": "Tokenization is related to computational burdens as tokenization reduces computational burdens", "from": "TOKENIZATION", "source_id": "fececbac281c1e2b13921f378df30919", "to": "COMPUTATIONAL BURDENS", "width": 1.0}, {"description": "Language models use tokenization to break down text into individual words or tokens", "from": "TOKENIZATION", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "LANGUAGE MODELS", "width": 1.0}, {"description": "Quantization is used to represent historical time series", "from": "QUANTIZATION", "source_id": "f7e3207706b170296cb036538a709689", "to": "HISTORICAL TIME SERIES", "width": 1.0}, {"description": "Quantization is related to bin centers as bin centers are used to define bins in quantization", "from": "QUANTIZATION", "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "to": "BIN CENTERS", "width": 1.0}, {"description": "Quantization is related to bin edges as bin edges are used to separate bins in quantization", "from": "QUANTIZATION", "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "to": "BIN EDGES", "width": 1.0}, {"description": "Rabanser et al. is related to quantization as Rabanser et al. is a study on quantization schemes for time series", "from": "QUANTIZATION", "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "to": "RABANSER ET AL.", "width": 1.0}, {"description": "Quantization is related to binning as binning is a type of quantization that divides a continuous range of values into discrete bins or intervals", "from": "QUANTIZATION", "source_id": "c5c841baa0bc205103ac433d446da3b6", "to": "BINNING", "width": 1.0}, {"description": "Rabanser et al. (2020) discusses quantization schemes for time series, including binning", "from": "QUANTIZATION", "source_id": "c5c841baa0bc205103ac433d446da3b6", "to": "RABANSER ET AL. (2020)", "width": 1.0}, {"description": "Memory footprints are related to quantization as quantization reduces memory footprints", "from": "QUANTIZATION", "source_id": "fececbac281c1e2b13921f378df30919", "to": "MEMORY FOOTPRINTS", "width": 1.0}, {"description": "Vocabulary is related to transformer-based language model architectures as they are used to process sequential data", "from": "VOCABULARY", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "TRANSFORMER-BASED LANGUAGE MODEL ARCHITECTURES", "width": 1.0}, {"description": "Tokenized values refer to the individual tokens or patches used in GPT4TS", "from": "VOCABULARY", "source_id": "bc54a718d1886698232d578fd88c3ac7", "to": "TOKENIZED VALUES", "width": 2.0}, {"description": "Vocabulary is related to prototypes as prototypes are examples of vocabulary", "from": "VOCABULARY", "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "to": "PROTOTYPES", "width": 9.0}, {"description": "Vocabulary is related to patch embeddings as patch embeddings are used to represent vocabulary", "from": "VOCABULARY", "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "to": "PATCH EMBEDDINGS", "width": 9.0}, {"description": "ICLR 2024 is related to vocabulary as vocabulary is a concept mentioned in the text", "from": "VOCABULARY", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "ICLR 2024", "width": 1.0}, {"description": "Transformer-based language model architectures are trained using cross-entropy loss", "from": "TRANSFORMER-BASED LANGUAGE MODEL ARCHITECTURES", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "CROSS-ENTROPY LOSS", "width": 1.0}, {"description": "AWS AI Labs is affiliated with Abdul Fatir Ansari", "from": "AWS AI LABS", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "ABDUL FATIR ANSARI", "width": 1.0}, {"description": "Amazon Supply Chain Optimization Technologies is affiliated with Kari Torkkola", "from": "AMAZON SUPPLY CHAIN OPTIMIZATION TECHNOLOGIES", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "KARI TORKKOLA", "width": 1.0}, {"description": "UC San Diego is affiliated with Caner Turkmen", "from": "UC SAN DIEGO", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "CANER TURKMEN", "width": 1.0}, {"description": "UC Berkeley is affiliated with Xiyuan Zhang", "from": "UC BERKELEY", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "XIYUAN ZHANG", "width": 1.0}, {"description": "New York University is affiliated with Andrew Gordon Wilson", "from": "NEW YORK UNIVERSITY", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "ANDREW GORDON WILSON", "width": 1.0}, {"description": "Abdul Fatir Ansari and Lorenzo Stella are co-authors of the paper", "from": "ABDUL FATIR ANSARI", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "LORENZO STELLA", "width": 1.0}, {"description": "Michael W. Ma-Wang and Abdul Fatir Ansari are co-authors of the paper", "from": "ABDUL FATIR ANSARI", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "MICHAEL W. MA-WANG", "width": 1.0}, {"description": "Abdul Fatir Ansari is an author of the paper \"Deep Explicit Duration Switching Models for Time Series\", which presents the Deep Explicit Duration Switching Models model", "from": "ABDUL FATIR ANSARI", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "DEEP EXPLICIT DURATION SWITCHING MODELS", "width": 1.0}, {"description": "Lorenzo Stella and Caner Turkmen are co-authors of the paper", "from": "LORENZO STELLA", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "CANER TURKMEN", "width": 1.0}, {"description": "Caner Turkmen and Xiyuan Zhang are co-authors of the paper", "from": "CANER TURKMEN", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "XIYUAN ZHANG", "width": 1.0}, {"description": "Xiyuan Zhang and Pedro Mercado are co-authors of the paper", "from": "XIYUAN ZHANG", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "PEDRO MERCADO", "width": 1.0}, {"description": "Pedro Mercado and Huibin Shen are co-authors of the paper", "from": "PEDRO MERCADO", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "HUIBIN SHEN", "width": 1.0}, {"description": "Huibin Shen and Oleksandr Shchur are co-authors of the paper", "from": "HUIBIN SHEN", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "OLEKSANDR SHCHUR", "width": 1.0}, {"description": "Oleksandr Shchur and Syama Sundar Rangapuram are co-authors of the paper", "from": "OLEKSANDR SHCHUR", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "SYAMA SUNDAR RANGAPURAM", "width": 1.0}, {"description": "Syama Sundar Rangapuram and Sebastian Pineda Arango are co-authors of the paper", "from": "SYAMA SUNDAR RANGAPURAM", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "SEBASTIAN PINEDA ARANGO", "width": 1.0}, {"description": "Sebastian Pineda Arango and Shubham Kapoor are co-authors of the paper", "from": "SEBASTIAN PINEDA ARANGO", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "SHUBHAM KAPOOR", "width": 1.0}, {"description": "Shubham Kapoor and Jasper Zschiegner are co-authors of the paper", "from": "SHUBHAM KAPOOR", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "JASPER ZSCHIEGNER", "width": 1.0}, {"description": "Jasper Zschiegner and Danielle C. Maddix are co-authors of the paper", "from": "JASPER ZSCHIEGNER", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "DANIELLE C. MADDIX", "width": 1.0}, {"description": "Danielle C. Maddix and Hao Wang are co-authors of the paper", "from": "DANIELLE C. MADDIX", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "HAO WANG", "width": 1.0}, {"description": "Hao Wang and Honey are co-authors of the paper", "from": "HAO WANG", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "HONEY", "width": 1.0}, {"description": "Honey and Kari Torkkola are co-authors of the paper", "from": "HONEY", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "KARI TORKKOLA", "width": 1.0}, {"description": "Kari Torkkola and Andrew Gordon Wilson are co-authors of the paper", "from": "KARI TORKKOLA", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "ANDREW GORDON WILSON", "width": 1.0}, {"description": "Andrew Gordon Wilson and Michael Bohlke-Schneider are co-authors of the paper", "from": "ANDREW GORDON WILSON", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "MICHAEL BOHLKE-SCHNEIDER", "width": 1.0}, {"description": "Michael Bohlke-Schneider and Yuyang are co-authors of the paper", "from": "MICHAEL BOHLKE-SCHNEIDER", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "YUYANG", "width": 1.0}, {"description": "Yuyang and Michael W. Ma-Wang are co-authors of the paper", "from": "YUYANG", "source_id": "fc51ca9f56bcadd343d50d9cb5cf9adb", "to": "MICHAEL W. MA-WANG", "width": 1.0}, {"description": "Patch reprogramming is related to zero-shot learning as it enables a model to perform well on a task without any prior training", "from": "ZERO-SHOT LEARNING", "source_id": "7e03744dea80e2138baff03611104fa8", "to": "PATCH REPROGRAMMING", "width": 1.0}, {"description": "Zero-shot learning and cross-domain adaptation are related concepts that refer to the ability of a model to perform well on a task without any prior training", "from": "ZERO-SHOT LEARNING", "source_id": "7e03744dea80e2138baff03611104fa8", "to": "CROSS-DOMAIN ADAPTATION", "width": 1.0}, {"description": "The decoder-only architecture is used in the LLAMA 2 model", "from": "LLAMA 2", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "DECODER-ONLY ARCHITECTURE", "width": 1.0}, {"description": "Chronos-T5 is a model that uses a language model as its base, as shown in Figure 8", "from": "LANGUAGE MODEL", "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6", "to": "CHRONOS-T5", "width": 1.0}, {"description": "Language model is related to text embeddings as text embeddings are used in language model", "from": "LANGUAGE MODEL", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "TEXT EMBEDDINGS", "width": 1.0}, {"description": "Historical time series are represented using context tokens", "from": "HISTORICAL TIME SERIES", "source_id": "f7e3207706b170296cb036538a709689", "to": "CONTEXT TOKENS", "width": 1.0}, {"description": "Context tokens are used to represent the entropy of a time series", "from": "CONTEXT TOKENS", "source_id": "f7e3207706b170296cb036538a709689", "to": "ENTROPY", "width": 1.0}, {"description": "Entropy is used in TSMixup to generate new time series", "from": "ENTROPY", "source_id": "f7e3207706b170296cb036538a709689", "to": "TSMIXUP", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTSMIXUP and KERNELSYNTH are two data augmentation strategies used to generate synthetic time series. Specifically, TSMixup is related to KernelSynth as KernelSynth is used to supplement the training dataset, thereby enhancing the capabilities of TSMixup in generating high-quality synthetic time series.\n\nThis summary is based on the provided descriptions, which collectively provide a clear understanding of the relationship between TSMixup and KernelSynth. The use of these data augmentation strategies is aimed at generating synthetic time series, which can be useful in various applications such as time series forecasting and analysis.\n\nIt is worth noting that the primary language of the text is English, as indicated by the presence of technical terms, mathematical equations, and references to academic papers written in English. This suggests that the text is from a research paper or a technical document, and the information provided is likely to be accurate and reliable.", "from": "TSMIXUP", "source_id": "0e3c8905bd533021b4f9bf9875ea66e0,f7e3207706b170296cb036538a709689", "to": "KERNELSYNTH", "width": 2.0}, {"description": "Data augmentation is related to TSMixup as TSMixup is a data augmentation technique", "from": "TSMIXUP", "source_id": "42c90ca40b234c098c55632ec70038a1", "to": "DATA AUGMENTATION", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nTSMIXUP and MIXUP are two related entities in the context of machine learning and time series forecasting. TSMIXUP is a variant of MIXUP, which is a technique used to improve the robustness and generalizability of machine learning models. Specifically, TSMIXUP generalizes the idea of MIXUP to more than two datapoints, allowing for a more comprehensive and robust approach to time series forecasting.\n\nThis relationship between TSMIXUP and MIXUP is supported by the fact that TSMIXUP is a variant of MIXUP, indicating that TSMIXUP builds upon the principles and concepts of MIXUP. The use of MIXUP in TSMIXUP suggests that the technique has been adapted and extended to address the specific challenges and requirements of time series forecasting.\n\nOverall, the summary highlights the connection between TSMIXUP and MIXUP, and provides insight into the relationship between these two entities in the context of machine learning and time series forecasting.", "from": "TSMIXUP", "source_id": "0e3c8905bd533021b4f9bf9875ea66e0,6212b2146d9ad27124114c334a946854", "to": "MIXUP", "width": 2.0}, {"description": "TSMixup is related to Gaussian processes as both are algorithms used in the Chronos framework", "from": "TSMIXUP", "source_id": "f72ba51a17dd00cff43bcdda22c273e8", "to": "GAUSSIAN PROCESSES", "width": 1.0}, {"description": "T5 is related to TSMixup as it is a type of data augmentation technique used to generate synthetic time series data", "from": "TSMIXUP", "source_id": "c522ea3766ae5129c11b833252340695", "to": "T5", "width": 1.0}, {"description": "Chronos-T5 is related to TSMixup as it is a type of data augmentation technique used to generate synthetic time series data", "from": "TSMIXUP", "source_id": "c522ea3766ae5129c11b833252340695", "to": "CHRONOS-T5", "width": 1.0}, {"description": "Data augmentation is related to KernelSynth as KernelSynth is a data augmentation technique", "from": "KERNELSYNTH", "source_id": "42c90ca40b234c098c55632ec70038a1", "to": "DATA AUGMENTATION", "width": 1.0}, {"description": "KernelSynth is related to Gaussian processes as Gaussian processes are used to generate synthetic time series", "from": "KERNELSYNTH", "source_id": "0e3c8905bd533021b4f9bf9875ea66e0", "to": "GAUSSIAN PROCESSES", "width": 1.0}, {"description": "T5 is related to KernelSynth as it is a type of data generation technique used to generate synthetic time series data", "from": "KERNELSYNTH", "source_id": "c522ea3766ae5129c11b833252340695", "to": "T5", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\n**Summary of KERNELSYNTH and CHRONOS-T5**\n\nKERNELSYNTH and CHRONOS-T5 are two entities related to time series data generation and forecasting. Specifically, CHRONOS-T5 is a type of data generation technique used to generate synthetic time series data, which is closely related to KERNELSYNTH. In fact, CHRONOS-T5 is trained on synthetic data generated using KERNELSYNTH, indicating a strong connection between the two entities.\n\nThis relationship suggests that KERNELSYNTH is a precursor or a component of CHRONOS-T5, and that the synthetic data generated by KERNELSYNTH is used to train CHRONOS-T5. This is a key insight into the structure and relationship between these two entities, and highlights the importance of KERNELSYNTH in the development and training of CHRONOS-T5.\n\nOverall, the summary provides a clear understanding of the relationship between KERNELSYNTH and CHRONOS-T5, and highlights the critical role that KERNELSYNTH plays in the generation and training of CHRONOS-T5.", "from": "KERNELSYNTH", "source_id": "c522ea3766ae5129c11b833252340695,e37f4063d074e1697e1f539131b3d963", "to": "CHRONOS-T5", "width": 2.0}, {"description": "KernelSynth and TSMix are models for time series data augmentation", "from": "KERNELSYNTH", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "TSMIX", "width": 1.0}, {"description": "KernelSynth is used to generate synthetic data for training time series forecasting models, which is related to Benchmark I", "from": "KERNELSYNTH", "source_id": "e37f4063d074e1697e1f539131b3d963", "to": "BENCHMARK I", "width": 1.0}, {"description": "KernelSynth is used to generate synthetic data for training time series forecasting models, which is related to Benchmark II", "from": "KERNELSYNTH", "source_id": "e37f4063d074e1697e1f539131b3d963", "to": "BENCHMARK II", "width": 1.0}, {"description": "Gaussian processes are related to kernel bank as kernel bank is used to define fundamental time series patterns", "from": "GAUSSIAN PROCESSES", "source_id": "0e3c8905bd533021b4f9bf9875ea66e0", "to": "KERNEL BANK", "width": 1.0}, {"description": "ForecastPFN is related to Gaussian processes as it is a type of data generation technique used to generate synthetic time series data", "from": "GAUSSIAN PROCESSES", "source_id": "c522ea3766ae5129c11b833252340695", "to": "FORECASTPFN", "width": 1.0}, {"description": "Classical forecasting methods differ from deep learning forecasting models in that they fit a separate model to each time series independently, while deep learning forecasting models learn across time series in a given dataset", "from": "CLASSICAL FORECASTING METHODS", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "DEEP LEARNING FORECASTING MODELS", "width": 1.0}, {"description": "Deep learning methods are related to suboptimal models as suboptimal models are a limitation of deep learning methods", "from": "DEEP LEARNING METHODS", "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "to": "SUBOPTIMAL MODELS", "width": 1.0}, {"description": "RNNs are used in the DeepState model", "from": "RNNS", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "DEEPSTATE", "width": 1.0}, {"description": "RNNs are used in the DeepFactor model", "from": "RNNS", "source_id": "7e69d9444a2084b6452e291735bf5a49", "to": "DEEPFACTOR", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"TFT\" and \"DLINER\" are related deep learning models used for forecasting. They are both mentioned in the text as models, indicating a connection between them. This relationship is further supported by the fact that they are both used for forecasting, suggesting that they share a common application or purpose.\n\nGiven the context of the text, which appears to be a technical document or research paper, it is likely that TFT and DLINER are being compared or contrasted in terms of their performance, capabilities, or limitations. The text may also discuss their implementation, evaluation, or potential applications in time series forecasting.\n\nOverall, the summary provides a concise description of the relationship between TFT and DLINER, highlighting their shared characteristics and potential areas of comparison or discussion in the text.", "from": "TFT", "source_id": "2565ae205d4c98342168bb67a4f7a309,af36d1634490149b96980fb1dff57cd1", "to": "DLINER", "width": 2.0}, {"description": "TFT is related to local statistical models as TFT performs better than local statistical models", "from": "TFT", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "Transformer-based models are used in models like TFT [Lim et al., 2021]", "from": "TFT", "source_id": "7d5d82d600620153153772a9bc498ac0", "to": "TRANSFORMER-BASED MODELS", "width": 1.0}, {"description": "TFT and NHITS are both types of time series forecasting models", "from": "TFT", "source_id": "f912df936d0b735fe654d1a9c53caa3b", "to": "NHITS", "width": 1.0}, {"description": "Chronos models use T5 as the main architecture", "from": "T5", "source_id": "63e284ec7e76fa859cc46b72d3746654", "to": "CHRONOS MODELS", "width": 1.0}, {"description": "The corpus is composed of time series data from several domains", "from": "CORPUS", "source_id": "c7167cf92abe7513ccb936ca79871932", "to": "DOMAINS", "width": 1.0}, {"description": "Lag-Llama is trained on a corpus of diverse time series", "from": "CORPUS", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "LAG-LAGGAMA", "width": 1.0}, {"description": "Stratified sampling is related to a corpus as it is a process that is used to sample a dataset in a way that ensures that each subgroup is represented proportionally", "from": "CORPUS", "source_id": "00007df4774d6122e3848802a24f9536", "to": "STRATIFIED SAMPLING", "width": 1.0}, {"description": "Pattern recognition is related to reasoning abilities as reasoning abilities involve drawing conclusions or making inferences based on data", "from": "PATTERN RECOGNITION", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "REASONING ABILITIES", "width": 1.0}, {"description": "Prototypes are related to patch embeddings as patch embeddings are used to represent prototypes", "from": "PATCH EMBEDDINGS", "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "to": "PROTOTYPES", "width": 9.0}, {"description": "ICLR 2024 is related to patch embeddings as patch embeddings are used to represent vocabulary", "from": "PATCH EMBEDDINGS", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "ICLR 2024", "width": 1.0}, {"description": "Output projection is related to patch embeddings as it uses the patch embeddings to project the output", "from": "PATCH EMBEDDINGS", "source_id": "3174231a67593609c727151c9df31d0a", "to": "OUTPUT PROJECTION", "width": 1.0}, {"description": "Patch embeddings are related to patch reprogramming as patch reprogramming uses patch embeddings", "from": "PATCH EMBEDDINGS", "source_id": "fececbac281c1e2b13921f378df30919", "to": "PATCH REPROGRAMMING", "width": 1.0}, {"description": "Patch embedder is related to patch embeddings as patch embedder creates dimensions for patch embeddings", "from": "PATCH EMBEDDINGS", "source_id": "fececbac281c1e2b13921f378df30919", "to": "PATCH EMBEDDER", "width": 1.0}, {"description": "Chronos forecasting is related to ForecastPFN as ForecastPFN is a model that generates point forecasts for time series data", "from": "FORECASTPFN", "source_id": "56de1d5a5b467101344afa4248d829dc", "to": "CHRONOS FORECASTING", "width": 1.0}, {"description": "ForecastPFN is outperformed by Chronos models on Benchmark II", "from": "FORECASTPFN", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "CHRONOS MODELS", "width": 1.0}, {"description": "FORECASTPFN is used for inference on AWS EC2", "from": "FORECASTPFN", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "AWS EC2", "width": 1.0}, {"description": "Tokenized time series refers to the process of breaking down time series data into individual tokens or patches", "from": "TOKENIZED VALUES", "source_id": "bc54a718d1886698232d578fd88c3ac7", "to": "TOKENIZED TIME SERIES", "width": 2.0}, {"description": "Language models are few-shot learners", "from": "LANGUAGE MODELS", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "FEW-SHOT LEARNERS", "width": 1.0}, {"description": "Numerals are used in language models to generate high-precision numerals with precision and efficiency", "from": "LANGUAGE MODELS", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "NUMERALS", "width": 1.0}, {"description": "Language models are used for forecasting tasks to predict future values or outcomes based on historical data", "from": "LANGUAGE MODELS", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "FORECASTING TASKS", "width": 1.0}, {"description": "Language models are proficient in time series tasks", "from": "LANGUAGE MODELS", "source_id": "6d66c2ea37e25b646a13d751c05a8e4d", "to": "TIME SERIES MACHINES", "width": 1.0}, {"description": "Scale is related to observations as the scale of the data affects the representation of observations in the Chronos model", "from": "SCALE", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "OBSERVATIONS", "width": 1.0}, {"description": "Based on the provided information, the primary entities are \"MEAN SCALING\" and \"STANDARD SCALING.\" The descriptions provided offer insights into the relationship between these two entities.\n\nAfter analyzing the descriptions, it is clear that both \"MEAN SCALING\" and \"STANDARD SCALING\" are normalization techniques. The descriptions also highlight that \"STANDARD SCALING\" is a specific type of normalization technique that involves applying an affine transformation to the time series.\n\nTo provide a comprehensive summary, we can conclude that \"MEAN SCALING\" and \"STANDARD SCALING\" are related normalization techniques, with \"STANDARD SCALING\" being a more specific type of normalization that involves an affine transformation to the time series.\n\nHere is a concise description that incorporates the provided information:\n\n\"MEAN SCALING\" and \"STANDARD SCALING\" are two related normalization techniques used in time series analysis. While both are normalization techniques, \"STANDARD SCALING\" is a more specific type of normalization that involves applying an affine transformation to the time series, making it a more comprehensive technique compared to \"MEAN SCALING.\"", "from": "MEAN SCALING", "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8,6eb4c16edf2eedfd03721efb199478d8", "to": "STANDARD SCALING", "width": 2.0}, {"description": "Mean scaling is related to min-max scaling as both are normalization techniques", "from": "MEAN SCALING", "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "to": "MIN-MAX SCALING", "width": 1.0}, {"description": "Standard scaling is related to min-max scaling as min-max scaling is a normalization technique that involves applying an affine transformation to the time series", "from": "STANDARD SCALING", "source_id": "6eb4c16edf2eedfd03721efb199478d8", "to": "MIN-MAX SCALING", "width": 1.0}, {"description": "Lags are used as covariates in the decoder-only transformer architecture", "from": "LAGS", "source_id": "c7167cf92abe7513ccb936ca79871932", "to": "DECODER-ONLY TRANSFORMER ARCHITECTURE", "width": 1.0}, {"description": "The categorical distribution is related to cross entropy as cross entropy is a measure of the difference between two probability distributions, often used as a loss function in machine learning", "from": "CATEGORICAL DISTRIBUTION", "source_id": "c5c841baa0bc205103ac433d446da3b6", "to": "CROSS ENTROPY", "width": 1.0}, {"description": "Classification and clustering are both time series analysis tasks", "from": "CLASSIFICATION", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "CLUSTERING", "width": 1.0}, {"description": "Quantile binning is related to uniform binning as both are types of binning", "from": "QUANTILE BINNING", "source_id": "6a976b57f1171abc9ea2ba6bb5b638f8", "to": "UNIFORM BINNING", "width": 1.0}, {"description": "Time series tokens are related to the PAD token as the PAD token is used to pad time series of different lengths to a fixed length for batch construction and to replace missing values", "from": "TIME SERIES TOKENS", "source_id": "c5c841baa0bc205103ac433d446da3b6", "to": "PAD TOKEN", "width": 1.0}, {"description": "Time series tokens are related to the EOS token as the EOS token is appended to the quantized and padded time series to denote the end of the sequence", "from": "TIME SERIES TOKENS", "source_id": "c5c841baa0bc205103ac433d446da3b6", "to": "EOS TOKEN", "width": 1.0}, {"description": "Categorical cross entropy loss is related to bin as bin is a category in the text", "from": "BIN", "source_id": "6212b2146d9ad27124114c334a946854", "to": "CATEGORICAL CROSS ENTROPY LOSS", "width": 1.0}, {"description": "Probabilistic time series forecasting is achieved using a decoder-only transformer architecture", "from": "PROBABILISTIC TIME SERIES FORECASTING", "source_id": "c7167cf92abe7513ccb936ca79871932", "to": "DECODER-ONLY TRANSFORMER ARCHITECTURE", "width": 1.0}, {"description": "Probabilistic time series forecasting is related to Lag-Llama as Lag-Llama is a model for this task", "from": "PROBABILISTIC TIME SERIES FORECASTING", "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "to": "LAG-LAGA", "width": 1.0}, {"description": "Gaussian is related to Student\u0027s-t as both are concepts in the text", "from": "GAUSSIAN", "source_id": "6212b2146d9ad27124114c334a946854", "to": "STUDENT\u0027S-T", "width": 1.0}, {"description": "Time series datasets are related to WikiText-103 as WikiText-103 is a dataset for time series data", "from": "TIME SERIES DATASETS", "source_id": "6212b2146d9ad27124114c334a946854", "to": "WIKITEXT-103", "width": 1.0}, {"description": "Time series datasets are related to C4 as C4 is a dataset for time series data", "from": "TIME SERIES DATASETS", "source_id": "6212b2146d9ad27124114c334a946854", "to": "C4", "width": 1.0}, {"description": "Time series datasets are related to The Pile as The Pile is a dataset for time series data", "from": "TIME SERIES DATASETS", "source_id": "6212b2146d9ad27124114c334a946854", "to": "THE PILE", "width": 1.0}, {"description": "Diversity is related to Mixup as Mixup improves pattern diversity", "from": "MIXUP", "source_id": "0e3c8905bd533021b4f9bf9875ea66e0", "to": "DIVERSITY", "width": 1.0}, {"description": "TimeGPT is related to training data as it was trained on a wide range of scenarios and time series", "from": "TRAINING DATA", "source_id": "42e1bb44edbe787e104f589e74b95d6c", "to": "TIMEGPT", "width": 1.0}, {"description": "Training data is related to Tab. 1 as Tab. 1 is a reference to the protocol used in the experiment", "from": "TRAINING DATA", "source_id": "fd1092903d83bf6e90a6caa371d7c514", "to": "TAB. 1", "width": 1.0}, {"description": "Kernel bank is related to linear kernel as linear kernel is used to define trend in time series", "from": "KERNEL BANK", "source_id": "0e3c8905bd533021b4f9bf9875ea66e0", "to": "LINEAR KERNEL", "width": 1.0}, {"description": "Kernel bank is related to RBF kernel as RBF kernel is used to define smooth local variation in time series", "from": "KERNEL BANK", "source_id": "0e3c8905bd533021b4f9bf9875ea66e0", "to": "RBF KERNEL", "width": 1.0}, {"description": "Kernel bank is related to periodic kernel as periodic kernel is used to define seasonalities in time series", "from": "KERNEL BANK", "source_id": "0e3c8905bd533021b4f9bf9875ea66e0", "to": "PERIODIC KERNEL", "width": 1.0}, {"description": "The kernel is related to the kernel bank as the kernel bank is a collection of basis kernels defining fundamental time series patterns", "from": "KERNEL BANK", "source_id": "53dbeea6d05c3695460c71f89f468def", "to": "KERNEL", "width": 1.0}, {"description": "The kernel bank is related to basis kernels as basis kernels are fundamental time series patterns used to construct the kernel bank", "from": "KERNEL BANK", "source_id": "53dbeea6d05c3695460c71f89f468def", "to": "BASIS KERNELS", "width": 1.0}, {"description": "The domain is related to the kernel as the kernel specifies a covariance function which defines the joint variability of the function values at an arbitrary pair of points", "from": "KERNEL", "source_id": "53dbeea6d05c3695460c71f89f468def", "to": "DOMAIN", "width": 1.0}, {"description": "The kernel is related to synthetic time series as synthetic time series is generated by drawing a sample of length lsyn from the GP prior", "from": "KERNEL", "source_id": "53dbeea6d05c3695460c71f89f468def", "to": "SYNTHETIC TIME SERIES", "width": 1.0}, {"description": "Basis kernels are related to linear kernels as linear kernels are used to represent trend in time series", "from": "BASIS KERNELS", "source_id": "53dbeea6d05c3695460c71f89f468def", "to": "LINEAR KERNELS", "width": 1.0}, {"description": "Basis kernels are related to RBF kernels as RBF kernels are used to represent smooth local variation in time series", "from": "BASIS KERNELS", "source_id": "53dbeea6d05c3695460c71f89f468def", "to": "RBF KERNELS", "width": 1.0}, {"description": "Basis kernels are related to periodic kernels as periodic kernels are used to represent seasonalities found in typical time series frequencies", "from": "BASIS KERNELS", "source_id": "53dbeea6d05c3695460c71f89f468def", "to": "PERIODIC KERNELS", "width": 1.0}, {"description": "Synthetic time series is related to GP prior as GP prior is a probability distribution used to generate synthetic time series", "from": "SYNTHETIC TIME SERIES", "source_id": "53dbeea6d05c3695460c71f89f468def", "to": "GP PRIOR", "width": 1.0}, {"description": "Chronos models are related to local models as both are used for time series forecasting", "from": "CHRONOS MODELS", "source_id": "53dbeea6d05c3695460c71f89f468def", "to": "LOCAL MODELS", "width": 1.0}, {"description": "Chronos models are related to task-specific deep learning models as both are used for time series forecasting", "from": "CHRONOS MODELS", "source_id": "53dbeea6d05c3695460c71f89f468def", "to": "TASK-SPECIFIC DEEP LEARNING MODELS", "width": 1.0}, {"description": "Chronos models significantly outperform local statistical models on Benchmark II, despite never having seen these datasets during training", "from": "CHRONOS MODELS", "source_id": "2eea45c6111e468189580a21686cb14a", "to": "BENCHMARK II", "width": 1.0}, {"description": "Chronos models outperform local statistical models on Benchmark I, suggesting that the datasets in this benchmark exhibit strong seasonal patterns", "from": "CHRONOS MODELS", "source_id": "2eea45c6111e468189580a21686cb14a", "to": "BENCHMARK I", "width": 1.0}, {"description": "Chronos models use WQL as a probabilistic forecasting metric to evaluate their performance", "from": "CHRONOS MODELS", "source_id": "2eea45c6111e468189580a21686cb14a", "to": "WQL", "width": 1.0}, {"description": "Chronos models use MASE as a point forecasting metric to evaluate their performance", "from": "CHRONOS MODELS", "source_id": "2eea45c6111e468189580a21686cb14a", "to": "MASE", "width": 1.0}, {"description": "Chronos models outperform local statistical models on Benchmark II", "from": "CHRONOS MODELS", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "LOCAL STATISTICAL MODELS", "width": 1.0}, {"description": "Chronos models perform on par with task-specific models on Benchmark II", "from": "CHRONOS MODELS", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "TASK-SPECIFIC MODELS", "width": 1.0}, {"description": "Moirai-1.0-R obtains the best performance after Chronos on Benchmark II", "from": "CHRONOS MODELS", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "MOIRAI-1.0-R", "width": 1.0}, {"description": "Lag-Lagga is outperformed by Chronos models on Benchmark II", "from": "CHRONOS MODELS", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "LAG-LAGGA", "width": 1.0}, {"description": "LLM Time is outperformed by Chronos models on Benchmark II", "from": "CHRONOS MODELS", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "LLM TIME", "width": 1.0}, {"description": "AutoARIMA is outperformed by Chronos models on Benchmark II", "from": "CHRONOS MODELS", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "AUTOARIMA", "width": 1.0}, {"description": "AutoETS is outperformed by Chronos models on Benchmark II", "from": "CHRONOS MODELS", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "AUTOETS", "width": 1.0}, {"description": "Seasonal Naive is outperformed by Chronos models on Benchmark II", "from": "CHRONOS MODELS", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "SEASONAL NAIVE", "width": 1.0}, {"description": "Chronos models outperform the Benchmark II baseline on Benchmark II", "from": "CHRONOS MODELS", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "BENCHMARK II BASELINE", "width": 1.0}, {"description": "Fine-tuning Chronos models individually on datasets is investigated", "from": "CHRONOS MODELS", "source_id": "5b1135d9e53e53428a042e8bbc89eaa7", "to": "FINE TUNING", "width": 1.0}, {"description": "TSMixup augmentations are related to Chronos models as they are used for training", "from": "CHRONOS MODELS", "source_id": "c7f04fa1168df64cce110e20a1b72b51", "to": "TSMIXUP AUGMENTATIONS", "width": 1.0}, {"description": "Chronos models are related to T5 model as they are initialized with pre-trained language model weights", "from": "CHRONOS MODELS", "source_id": "c7f04fa1168df64cce110e20a1b72b51", "to": "T5 MODEL", "width": 1.0}, {"description": "Pretraining-only data is used only for training Chronos models", "from": "CHRONOS MODELS", "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "to": "PRETRAINING-ONLY DATA", "width": 1.0}, {"description": "In-domain evaluation data is used for training and testing Chronos models", "from": "CHRONOS MODELS", "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "to": "IN-DOMAIN EVALUATION DATA", "width": 1.0}, {"description": "Zero-shot evaluation data is used only for evaluation of Chronos models", "from": "CHRONOS MODELS", "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "to": "ZERO-SHOT EVALUATION DATA", "width": 1.0}, {"description": "Local models can be faster than task-specific models", "from": "LOCAL MODELS", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "INFERENCE SPEED", "width": 1.0}, {"description": "Local models are compared to task-specific deep learning baselines in terms of their in-domain performance, with Chronos models outperforming existing local models", "from": "LOCAL MODELS", "source_id": "6c273e9f841addeed2a33240ddaa3d96", "to": "TASK-SPECIFIC DEEP LEARNING BASELINES", "width": 1.0}, {"description": "Local statistical models are related to task-specific deep learning models as task-specific deep learning models perform better than local statistical models", "from": "TASK-SPECIFIC DEEP LEARNING MODELS", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n\"Benchmark I and Benchmark II are two datasets used to evaluate the performance of time series forecasting models. Both benchmarks are related, as they are utilized for evaluation purposes, but they serve distinct functions. Benchmark I is employed for in-domain evaluation, whereas Benchmark II is used for zero-shot evaluation. Notably, Benchmark I is considered less challenging than Benchmark II, suggesting that Benchmark II presents a more demanding scenario for time series forecasting models.\"\n\nThis summary incorporates information from all the descriptions, resolves any potential contradictions, and provides a coherent overview of the two entities, \"BENCHMARK II\" and \"BENCHMARK I\". The summary is written in the third person and includes the entity names for context.", "from": "BENCHMARK II", "source_id": "532a6d434dab611a80aef1e94dd2fb45,63e284ec7e76fa859cc46b72d3746654,e37f4063d074e1697e1f539131b3d963", "to": "BENCHMARK I", "width": 3.0}, {"description": "Benchmark II is related to Chronos-T5 (Small) as Chronos-T5 (Small) achieves the top spot on Benchmark II with an inexpensive fine-tuning regimen", "from": "BENCHMARK II", "source_id": "56de1d5a5b467101344afa4248d829dc", "to": "CHRONOS-T5 (SMALL)", "width": 1.0}, {"description": "Validation set is related to pretraining as pretraining is used to evaluate a model\u0027s performance", "from": "PRETRAINING", "source_id": "76cc338e586223647fd3dbe4e7a7c131", "to": "VALIDATION SET", "width": 1.0}, {"description": "Weatherbench is related to in-domain evaluation as in-domain evaluation is used to evaluate Weatherbench", "from": "IN-DOMAIN EVALUATION", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "WEATHERBENCH", "width": 1.0}, {"description": "Benchmark I is related to Chronos-T5 (Large) as Chronos-T5 (Large) significantly outperforms baseline models on Benchmark I", "from": "BENCHMARK I", "source_id": "56de1d5a5b467101344afa4248d829dc", "to": "CHRONOS-T5 (LARGE)", "width": 1.0}, {"description": "Chronos models outperform task-specific models on Benchmark I, demonstrating the benefit of using models that are trained only once across multiple datasets", "from": "BENCHMARK I", "source_id": "2eea45c6111e468189580a21686cb14a", "to": "TASK-SPECIFIC MODELS", "width": 1.0}, {"description": "Training corpus is related to AWS EC2 as AWS EC2 is used to train the models on the training corpus", "from": "TRAINING CORPUS", "source_id": "f72ba51a17dd00cff43bcdda22c273e8", "to": "AWS EC2", "width": 1.0}, {"description": "AWS EC2 is related to transformers library as the transformers library is used to train the models on AWS EC2", "from": "AWS EC2", "source_id": "f72ba51a17dd00cff43bcdda22c273e8", "to": "TRANSFORMERS LIBRARY", "width": 1.0}, {"description": "LAG-LAGMA is used for inference on AWS EC2", "from": "AWS EC2", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "LAG-LAGMA", "width": 1.0}, {"description": "MOIRAI-1.0-R is used for inference on AWS EC2", "from": "AWS EC2", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "MOIRAI-1.0-R", "width": 1.0}, {"description": "Hyndman \u0026 Athanasopoulos (2018) is related to Seasonal Naive as Seasonal Naive is a model mentioned in the text", "from": "HYNDMAN \u0026 ATHANASOPOULOS (2018)", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "SEASONAL NAIVE", "width": 1.0}, {"description": "Hyndman \u0026 Athanasopoulos (2018) is related to AutoETS as AutoETS is a model mentioned in the text", "from": "HYNDMAN \u0026 ATHANASOPOULOS (2018)", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "AUTOETS", "width": 1.0}, {"description": "Hyndman \u0026 Athanasopoulos (2018) is related to AutoARIMA as AutoARIMA is a model mentioned in the text", "from": "HYNDMAN \u0026 ATHANASOPOULOS (2018)", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "AUTOARIMA", "width": 1.0}, {"description": "Hyndman \u0026 Athanasopoulos (2018) is related to AutoTheta as AutoTheta is a model mentioned in the text", "from": "HYNDMAN \u0026 ATHANASOPOULOS (2018)", "source_id": "af36d1634490149b96980fb1dff57cd1", "to": "AUTOTHETA", "width": 1.0}, {"description": "Seasonal Naive is related to Chronos-T5 (Large) as Chronos-T5 (Large) significantly outperforms baseline models on Benchmark I", "from": "SEASONAL NAIVE", "source_id": "56de1d5a5b467101344afa4248d829dc", "to": "CHRONOS-T5 (LARGE)", "width": 1.0}, {"description": "Seasonal Naive is related to local statistical models as Seasonal Naive performs competitively", "from": "SEASONAL NAIVE", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "Seasonal Naive is used to normalize the probabilistic forecasting metric WQL", "from": "SEASONAL NAIVE", "source_id": "2eea45c6111e468189580a21686cb14a", "to": "WQL", "width": 1.0}, {"description": "Seasonal Naive is used to normalize the point forecasting metric MASE", "from": "SEASONAL NAIVE", "source_id": "2eea45c6111e468189580a21686cb14a", "to": "MASE", "width": 1.0}, {"description": "Seasonal Naive and AutoTheta are related as they are both statistical models used for forecasting", "from": "SEASONAL NAIVE", "source_id": "2565ae205d4c98342168bb67a4f7a309", "to": "AUTOTHETA", "width": 1.0}, {"description": "GPU inference speed refers to the speed at which a model can perform inference on a GPU, and Seasonal Naive is a simple model used for comparison with TimeGPT", "from": "SEASONAL NAIVE", "source_id": "55eb54ef455d14cd1a11760924f99eb8", "to": "GPU INFERENCE SPEED", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n**Summary of AutoETS and AutoARIMA**\n\nAutoETS and AutoARIMA are two statistical models used for time series forecasting. They are related in that they both perform well in forecasting tasks, with AutoETS and AutoARIMA being local models that outperform Chronos models on certain datasets. Specifically, both models are used for long-term series forecasting and involve frequency analysis, indicating their applicability in time series forecasting tasks. The models are likely to be used in academic and research settings, given the presence of technical terms and references to academic papers, such as ICLR, AAAI, and PMLR. Overall, AutoETS and AutoARIMA are two related models that are used for time series forecasting and have been shown to be effective in certain scenarios.\n\n**Key Points:**\n\n* Both AutoETS and AutoARIMA are statistical models used for time series forecasting.\n* They are local models that outperform Chronos models on certain datasets.\n* Both models are used for long-term series forecasting and involve frequency analysis.\n* They are likely to be used in academic and research settings.\n* The models are related and have been shown to be effective in certain scenarios.\n\n**Entity Names:** AUTOETS, AUTOARIMA\n\n**Primary Language:** English\n\n**Context:** Time series forecasting, statistical models, long-term series forecasting, frequency analysis, academic and research settings.", "from": "AUTOETS", "source_id": "2565ae205d4c98342168bb67a4f7a309,376897a501ac50834f626fcc5fb13ece,56de1d5a5b467101344afa4248d829dc", "to": "AUTOARIMA", "width": 3.0}, {"description": "AutoETS is related to local statistical models as AutoETS performs better than local statistical models", "from": "AUTOETS", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "Chronos-T5 is related to AutoETS as AutoETS is a type of model that uses an exponential smoothing approach to forecasting", "from": "AUTOETS", "source_id": "116332ac4538a1430c83a34fcbec22d1", "to": "CHRONOS-T5", "width": 1.0}, {"description": "STATSFORCAST is used with AUTOETS as a statistical baseline", "from": "AUTOETS", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "STATSFORCAST", "width": 1.0}, {"description": "GLUONTS is used with AUTOETS as a platform for time series forecasting", "from": "AUTOETS", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "GLUONTS", "width": 1.0}, {"description": "AutoETS and Croston\u0027s seasonal batch arrival model are both models used for time series forecasting", "from": "AUTOETS", "source_id": "376897a501ac50834f626fcc5fb13ece,6fa5f635ad5f7e6b67d5e467f130345c", "to": "CROSTONSBA", "width": 2.0}, {"description": "Autoets is related to Uber as Autoets is trained on the Uber dataset", "from": "AUTOETS", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "UBER", "width": 1.0}, {"description": "Autoets is related to Windfarms as Autoets is trained on the Windfarms dataset", "from": "AUTOETS", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "WINDFARMS", "width": 1.0}, {"description": "Autoets is related to ETT H1 as Autoets is trained on the ETT H1 dataset", "from": "AUTOETS", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "ETT H1", "width": 1.0}, {"description": "Autoets is related to ETT H2 as Autoets is trained on the ETT H2 dataset", "from": "AUTOETS", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "ETT H2", "width": 1.0}, {"description": "Autoets is related to ETT M1 as Autoets is trained on the ETT M1 dataset", "from": "AUTOETS", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "ETT M1", "width": 1.0}, {"description": "Autoets is related to Air Quality UCI as Autoets is trained on the Air Quality UCI dataset", "from": "AUTOETS", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "AIR QUALITY UCI", "width": 1.0}, {"description": "Autoets is related to Beijing Multisite as Autoets is trained on the Beijing Multisite dataset", "from": "AUTOETS", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "BEIJING MULTISITE", "width": 1.0}, {"description": "AUTOETS is related to CRPS as CRPS is used to evaluate the performance of AUTOETS", "from": "AUTOETS", "source_id": "990578022879395d00b7a5b229863c2f", "to": "CRPS", "width": 1.0}, {"description": "AutoETS and dynamic optimization are both models used for time series forecasting", "from": "AUTOETS", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "DYNAMICOPTIMIZE", "width": 1.0}, {"description": "AutoETS and NPts are both models used for time series forecasting", "from": "AUTOETS", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "NPTS", "width": 1.0}, {"description": "AutoETS and Temporal Fusion Transformer are both models used for time series forecasting", "from": "AUTOETS", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "TEMPORALFUSIONT", "width": 1.0}, {"description": "AutoETS and N-BEATS are both models used for time series forecasting", "from": "AUTOETS", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "NBEATS", "width": 1.0}, {"description": "AutoARIMA is related to local statistical models as AutoARIMA performs better than local statistical models", "from": "AUTOARIMA", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"AUTOARIMA\" and \"CHRONOS-T5 (BASE)\" are related in the context of time series forecasting. Specifically, Chronos-T5 (Base) is compared to AutoARIMA in the text, suggesting a direct comparison or evaluation of their performance in generating forecasts. Chronos-T5 (Base) is capable of generating plausible forecasts across all four AR processes, which implies that it is a robust and effective model for time series forecasting. The comparison between Chronos-T5 (Base) and AutoARIMA in the text suggests that both models are being evaluated for their ability to accurately forecast time series data.\n\nThis summary is based on the information provided in the description list, which indicates a direct relationship between the two entities and a comparison of their performance in time series forecasting. The summary is written in third person and includes the entity names for context.", "from": "AUTOARIMA", "source_id": "9c155897f3e35902f57696e0abaeb161,bca10b04933dc3a7f98a3f1b610e419b", "to": "CHRONOS-T5 (BASE)", "width": 2.0}, {"description": "AR (with correct order) is compared to AutoARIMA in the text", "from": "AUTOARIMA", "source_id": "bca10b04933dc3a7f98a3f1b610e419b", "to": "AR (WITH CORRECT ORDER)", "width": 1.0}, {"description": "AR process is related to AutoARIMA as AutoARIMA is a model that combines the strengths of ARIMA and auto-regressive models", "from": "AUTOARIMA", "source_id": "9c155897f3e35902f57696e0abaeb161", "to": "AR PROCESS", "width": 1.0}, {"description": "STATSFORCAST is used with AUTOARIMA as a statistical baseline", "from": "AUTOARIMA", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "STATSFORCAST", "width": 1.0}, {"description": "GLUONTS is used with AUTOARIMA as a platform for time series forecasting", "from": "AUTOARIMA", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "GLUONTS", "width": 1.0}, {"description": "Autoarima is related to Uber as Autoarima is trained on the Uber dataset", "from": "AUTOARIMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "UBER", "width": 1.0}, {"description": "Autoarima is related to Windfarms as Autoarima is trained on the Windfarms dataset", "from": "AUTOARIMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "WINDFARMS", "width": 1.0}, {"description": "Autoarima is related to ETT H1 as Autoarima is trained on the ETT H1 dataset", "from": "AUTOARIMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "ETT H1", "width": 1.0}, {"description": "Autoarima is related to ETT H2 as Autoarima is trained on the ETT H2 dataset", "from": "AUTOARIMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "ETT H2", "width": 1.0}, {"description": "Autoarima is related to ETT M1 as Autoarima is trained on the ETT M1 dataset", "from": "AUTOARIMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "ETT M1", "width": 1.0}, {"description": "Autoarima is related to Air Quality UCI as Autoarima is trained on the Air Quality UCI dataset", "from": "AUTOARIMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "AIR QUALITY UCI", "width": 1.0}, {"description": "Autoarima is related to Beijing Multisite as Autoarima is trained on the Beijing Multisite dataset", "from": "AUTOARIMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "BEIJING MULTISITE", "width": 1.0}, {"description": "AUTOARIMA is related to CRPS as CRPS is used to evaluate the performance of AUTOARIMA", "from": "AUTOARIMA", "source_id": "990578022879395d00b7a5b229863c2f", "to": "CRPS", "width": 1.0}, {"description": "AutoARIMA and Croston\u0027s seasonal batch arrival model are both models used for time series forecasting", "from": "AUTOARIMA", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "CROSTONSBA", "width": 1.0}, {"description": "AutoARIMA and dynamic optimization are both models used for time series forecasting", "from": "AUTOARIMA", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "DYNAMICOPTIMIZE", "width": 1.0}, {"description": "AutoARIMA and NPts are both models used for time series forecasting", "from": "AUTOARIMA", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "NPTS", "width": 1.0}, {"description": "AutoARIMA and Temporal Fusion Transformer are both models used for time series forecasting", "from": "AUTOARIMA", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "TEMPORALFUSIONT", "width": 1.0}, {"description": "AutoARIMA and N-BEATS are both models used for time series forecasting", "from": "AUTOARIMA", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "NBEATS", "width": 1.0}, {"description": "AutoARIMA and AutoTheta are models used for comparison in the text", "from": "AUTOARIMA", "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "to": "AUTOTHETA", "width": 1.0}, {"description": "AutoTheta is related to StatsForecast library as it is a software library used for statistical forecasting", "from": "AUTOTHETA", "source_id": "2565ae205d4c98342168bb67a4f7a309", "to": "STATSFORECAST LIBRARY", "width": 1.0}, {"description": "STATSFORCAST is used with AUTOTHETA as a statistical baseline", "from": "AUTOTHETA", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "STATSFORCAST", "width": 1.0}, {"description": "GLUONTS is used with AUTOTHETA as a platform for time series forecasting", "from": "AUTOTHETA", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "GLUONTS", "width": 1.0}, {"description": "M4 is related to DLinear as DLinear is used for short-term time series forecasting on M4", "from": "DLINER", "source_id": "d4551c2839eaa68a7cb7324089956581", "to": "M4", "width": 1.0}, {"description": "Lag-Llama is related to Moirai-1.0-R as Moirai-1.0-R performs better than Lag-Llama on some datasets", "from": "MOIRAI-1.0-R", "source_id": "56de1d5a5b467101344afa4248d829dc", "to": "LAG-LAMMA", "width": 1.0}, {"description": "N-BEATSN-HITS is related to Moirai-1.0-R as Moirai-1.0-R is a type of model that uses a recurrent neural network approach to forecasting", "from": "MOIRAI-1.0-R", "source_id": "116332ac4538a1430c83a34fcbec22d1", "to": "N-BEATSN-HITS", "width": 1.0}, {"description": "WQL is related to probabilistic forecasts as WQL is used to evaluate probabilistic forecasts", "from": "WEIGHTED QUANTILE LOSS", "source_id": "f0808ad97bc4d26391284145427770e5", "to": "PROBABILISTIC FORECASTS", "width": 1.0}, {"description": "Quantiles are related to WQL as quantiles are used to compute WQL", "from": "WEIGHTED QUANTILE LOSS", "source_id": "f0808ad97bc4d26391284145427770e5", "to": "QUANTILES", "width": 1.0}, {"description": "CRPS is related to WQL as CRPS is a metric used to evaluate probabilistic forecasts", "from": "WEIGHTED QUANTILE LOSS", "source_id": "f0808ad97bc4d26391284145427770e5", "to": "CONTINUOUS RANKED PROBABILITY SCORE", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe two entities in question are \"CHRONOS-T5 (SMALL)\" and \"CHRONOS-T5 (BASE)\". These models are fine-tuned on datasets from Benchmark II. Notably, CHRONOS-T5 (Small) is related to CHRONOS-T5 (Base) in that CHRONOS-T5 (Base) significantly outperforms baseline models on Benchmark I. This suggests that CHRONOS-T5 (Base) is a more advanced or refined version of CHRONOS-T5 (Small), with improved performance on certain benchmarks.\n\nIt is worth noting that the primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English. The language is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nOverall, the summary provides a clear understanding of the relationship between CHRONOS-T5 (Small) and CHRONOS-T5 (Base), as well as their performance on various benchmarks.", "from": "CHRONOS-T5 (SMALL)", "source_id": "56de1d5a5b467101344afa4248d829dc,baa887ae552c6b3dac10f74896d8c4a2", "to": "CHRONOS-T5 (BASE)", "width": 2.0}, {"description": "Chronos-T5 (Small) is related to local statistical models as Chronos-T5 (Small) performs better than local statistical models", "from": "CHRONOS-T5 (SMALL)", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "Chronos-T5 (Mini) and Chronos-T5 (Small) are models that are fine-tuned on datasets from Benchmark II", "from": "CHRONOS-T5 (SMALL)", "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "to": "CHRONOS-T5 (MINI)", "width": 1.0}, {"description": "Chronos-T5 (Large) and Chronos-T5 (Small) are models that are fine-tuned on datasets from Benchmark II", "from": "CHRONOS-T5 (SMALL)", "source_id": "baa887ae552c6b3dac10f74896d8c4a2", "to": "CHRONOS-T5 (LARGE)", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"CHRONOS-T5 (BASE)\" and \"CHRONOS-T5 (LARGE)\" are related models, with \"CHRONOS-T5 (LARGE)\" significantly outperforming baseline models on Benchmark I. In terms of point forecasting performance, \"CHRONOS-T5 (LARGE)\" places 3rd, while \"CHRONOS-T5 (BASE)\" places 2nd. This suggests that \"CHRONOS-T5 (LARGE)\" is a more advanced model, but \"CHRONOS-T5 (BASE)\" is still a strong performer. The performance of these models on Benchmark I indicates their effectiveness in time series forecasting tasks.\n\nIt is worth noting that the performance of these models is likely related to their architecture and training data, as \"CHRONOS-T5 (LARGE)\" is a larger model than \"CHRONOS-T5 (BASE)\". The use of the term \"Chronos-T5\" suggests that these models are based on the T5 architecture, which is a popular choice for natural language processing and time series forecasting tasks. The fact that they are able to outperform baseline models on Benchmark I suggests that they are well-suited for time series forecasting tasks.\n\nOverall, the summary of the data suggests that \"CHRONOS-T5 (LARGE)\" is a more advanced model than \"CHRONOS-T5 (BASE)\", but both models are strong performers in time series forecasting tasks.", "from": "CHRONOS-T5 (BASE)", "source_id": "532a6d434dab611a80aef1e94dd2fb45,56de1d5a5b467101344afa4248d829dc", "to": "CHRONOS-T5 (LARGE)", "width": 2.0}, {"description": "Chronos-T5 (Base) is related to local statistical models as Chronos-T5 (Base) performs better than local statistical models", "from": "CHRONOS-T5 (BASE)", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "Chronos-T5 (Base) places 2nd in terms of point forecasting performance, while PatchTST places 1st", "from": "CHRONOS-T5 (BASE)", "source_id": "532a6d434dab611a80aef1e94dd2fb45", "to": "PATCH-TST", "width": 1.0}, {"description": "Gaussian observations are used in the Chronos-T5 (Base) model to generate forecasts", "from": "CHRONOS-T5 (BASE)", "source_id": "bca10b04933dc3a7f98a3f1b610e419b", "to": "GAUSSIAN OBSERVATIONS", "width": 1.0}, {"description": "Chronos-T5 (Base) is compared to AR (with correct order) in the text", "from": "CHRONOS-T5 (BASE)", "source_id": "bca10b04933dc3a7f98a3f1b610e419b", "to": "AR (WITH CORRECT ORDER)", "width": 1.0}, {"description": "Chronos-T5 (Large) is related to local statistical models as Chronos-T5 (Large) performs better than local statistical models", "from": "CHRONOS-T5 (LARGE)", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "Lag-Llama is related to local statistical models as Lag-Llama performs better than local statistical models", "from": "LAG-LAMMA", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "LOCAL STATISTICAL MODELS", "width": 2.0}, {"description": "Lag-Llama is trained from scratch on a large corpus of datasets", "from": "LAG-LAMMA", "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "to": "PRETRAINING CORPUS", "width": 1.0}, {"description": "Lag-Llama is related to supervised models as Lag-Llama is used as a baseline for supervised models", "from": "LAG-LAMMA", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "SUPERVISED MODELS", "width": 1.0}, {"description": "Forecast is related to Lag-Llama as Lag-Llama is a type of machine learning model used for forecasting", "from": "LAG-LAMMA", "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "to": "FORECAST", "width": 1.0}, {"description": "Based on the provided information, the primary entities are \"LAG-LAMMA\" and \"ETT-M2 DATASET\". After analyzing the descriptions, it is clear that there is a strong relationship between these two entities.\n\nHere is a comprehensive summary of the data:\n\n\"LAG-LAMMA\" is a type of machine learning model used for forecasting, and it is specifically related to the \"ETT-M2 DATASET\". The \"ETT-M2 DATASET\" is utilized for fine-tuning forecasting tasks using the \"LAG-LAMMA\" model. This suggests that the \"LAG-LAMMA\" model is designed to work effectively with the \"ETT-M2 DATASET\" for forecasting purposes.\n\nIn other words, the \"LAG-LAMMA\" model is tailored to perform well on the \"ETT-M2 DATASET\", and the dataset is used to fine-tune the model\u0027s forecasting capabilities. This relationship highlights the importance of the \"ETT-M2 DATASET\" in the development and evaluation of the \"LAG-LAMMA\" model.\n\nOverall, the connection between \"LAG-LAMMA\" and \"ETT-M2 DATASET\" is one of mutual dependence, with the model relying on the dataset for its forecasting tasks and the dataset being used to optimize the model\u0027s performance.", "from": "LAG-LAMMA", "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26,c60a8238db3d56eff9cc9692e7ac5b1c", "to": "ETT-M2 DATASET", "width": 2.0}, {"description": "Lag-Llama is related to Pedestrian Counts dataset as Lag-Llama is used for fine-tuning forecasting tasks on Pedestrian Counts dataset", "from": "LAG-LAMMA", "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26", "to": "PEDESTRIAN COUNTS DATASET", "width": 1.0}, {"description": "Lag-Llama is related to Requests Minute dataset as Lag-Llama is used for fine-tuning forecasting tasks on Requests Minute dataset", "from": "LAG-LAMMA", "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26", "to": "REQUESTS MINUTE DATASET", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"LAG-LAMMA\" and \"CRPS\" are closely related. Specifically, \"LAG-LAMMA\" is a model that utilizes the \"CRPS\" (Continuous Ranked Probability Score) metric to evaluate its performance. In other words, the \"CRPS\" is used to assess the accuracy and reliability of the predictions made by the \"LAG-LAMMA\" model. This suggests a symbiotic relationship between the two entities, where the \"CRPS\" serves as a key evaluation metric for the \"LAG-LAMMA\" model, and the \"LAG-LAMMA\" model relies on the \"CRPS\" to gauge its performance.\n\nThis relationship is further reinforced by the fact that the \"CRPS\" is a widely used metric in the field of time series forecasting, which is likely the context in which the \"LAG-LAMMA\" model operates. The use of the \"CRPS\" to evaluate the performance of the \"LAG-LAMMA\" model suggests that the model is designed to tackle complex time series forecasting tasks, where accurate predictions are critical.\n\nOverall, the summary highlights the interdependence of the \"LAG-LAMMA\" model and the \"CRPS\" metric, underscoring their close relationship in the context of time series forecasting.", "from": "LAG-LAMMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c,990578022879395d00b7a5b229863c2f", "to": "CRPS", "width": 2.0}, {"description": "Lag-Llama is related to Uber as Lag-Llama is trained on the Uber dataset", "from": "LAG-LAMMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "UBER", "width": 1.0}, {"description": "Lag-Llama is related to Windfarms as Lag-Llama is trained on the Windfarms dataset", "from": "LAG-LAMMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "WINDFARMS", "width": 1.0}, {"description": "Lag-Llama is related to ETT H1 as Lag-Llama is trained on the ETT H1 dataset", "from": "LAG-LAMMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "ETT H1", "width": 1.0}, {"description": "Lag-Llama is related to ETT H2 as Lag-Llama is trained on the ETT H2 dataset", "from": "LAG-LAMMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "ETT H2", "width": 1.0}, {"description": "Lag-Llama is related to ETT M1 as Lag-Llama is trained on the ETT M1 dataset", "from": "LAG-LAMMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "ETT M1", "width": 1.0}, {"description": "Lag-Llama is related to Air Quality UCI as Lag-Llama is trained on the Air Quality UCI dataset", "from": "LAG-LAMMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "AIR QUALITY UCI", "width": 1.0}, {"description": "Lag-Llama is related to Beijing Multisite as Lag-Llama is trained on the Beijing Multisite dataset", "from": "LAG-LAMMA", "source_id": "4bd5a8e9285aae7ca7363c8e61ba361c", "to": "BEIJING MULTISITE", "width": 1.0}, {"description": "Chronos-T5 (Mini) is related to local statistical models as Chronos-T5 (Mini) performs better than local statistical models", "from": "LOCAL STATISTICAL MODELS", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "CHRONOS-T5 (MINI)", "width": 2.0}, {"description": "Chronos-GPT2 is related to local statistical models as Chronos-GPT2 performs better than local statistical models", "from": "LOCAL STATISTICAL MODELS", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "CHRONOS-GPT2", "width": 2.0}, {"description": "Moirai-1.0-R (Large) is related to local statistical models as Moirai-1.0-R (Large) performs better than local statistical models", "from": "LOCAL STATISTICAL MODELS", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "MOIRAI-1.0-R (LARGE)", "width": 2.0}, {"description": "DLinear is related to local statistical models as DLinear performs better than local statistical models", "from": "LOCAL STATISTICAL MODELS", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "DLINEAR", "width": 2.0}, {"description": "AutoTheta is related to local statistical models as AutoTheta performs better than local statistical models", "from": "LOCAL STATISTICAL MODELS", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "AUTO-THETA", "width": 2.0}, {"description": "ILI is related to DLinear as it is a forecasting horizon used in the model", "from": "DLINEAR", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "ILI", "width": 8.0}, {"description": "Relative WQL is related to Relative MASE as both are metrics used to evaluate the performance of models", "from": "RELATIVE WQL", "source_id": "12395cf4e8efa64a847ede9775ecdf3f", "to": "RELATIVE MASE", "width": 1.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nWQL and MASE are two distinct metrics used to evaluate the performance of time series forecasting models. They are both used to measure the performance of models on Benchmark II, with WQL being a probabilistic metric and MASE being a point metric. The key difference between the two lies in their approach to aggregating errors: WQL aggregates absolute errors, whereas MASE scales errors by a term proportional to the scale of each series.\n\nThis summary is derived from the following information collected from all the descriptions:\n\n- Both WQL and MASE are metrics used to evaluate the performance of time series forecasting models.\n- They are both used to measure the performance of models on Benchmark II.\n- WQL aggregates absolute errors, whereas MASE scales errors by a term proportional to the scale of each series.\n- WQL is a probabilistic metric, whereas MASE is a point metric.\n\nThe contradictions in the descriptions have been resolved by identifying the commonalities and differences between WQL and MASE, resulting in a single, coherent summary.", "from": "WQL", "source_id": "2eea45c6111e468189580a21686cb14a,a90c6ca26542ea5935f9d8d5bf3688a9,baa887ae552c6b3dac10f74896d8c4a2,c522ea3766ae5129c11b833252340695,e37f4063d074e1697e1f539131b3d963", "to": "MASE", "width": 6.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"WQL\" is a metric that measures the performance of the model on Benchmark II, while the entity \"TRAINING LOSS\" is a metric that measures how well the model is learning from the data. However, the two metrics do not correlate closely with each other, with WQL behaving less predictably as precision increases. This suggests that WQL and TRAINING LOSS are distinct metrics that capture different aspects of the model\u0027s performance, and their relationship is not straightforward.\n\nIn the context of machine learning model development, these metrics are likely used to evaluate the performance of the model on a specific task or benchmark. The fact that WQL and TRAINING LOSS are not closely correlated suggests that the model\u0027s performance on Benchmark II may not be directly related to its ability to learn from the data, and that other factors may be at play.\n\nOverall, the summary provides a nuanced understanding of the relationship between WQL and TRAINING LOSS, highlighting their distinct roles in evaluating the performance of the model and the potential complexities of their relationship.", "from": "WQL", "source_id": "a90c6ca26542ea5935f9d8d5bf3688a9,baa887ae552c6b3dac10f74896d8c4a2", "to": "TRAINING LOSS", "width": 2.0}, {"description": "WQL approximates CRPS, a commonly used metric for evaluating probabilistic predictions", "from": "WQL", "source_id": "e37f4063d074e1697e1f539131b3d963", "to": "CRPS", "width": 1.0}, {"description": "MASE exhibits an improvement with increased precision, just as one expects for the training loss", "from": "MASE", "source_id": "a90c6ca26542ea5935f9d8d5bf3688a9", "to": "TRAINING LOSS", "width": 1.0}, {"description": "MASE is a metric for evaluating time series forecasting models, while CRPS is a metric for evaluating probabilistic predictions", "from": "MASE", "source_id": "e37f4063d074e1697e1f539131b3d963", "to": "CRPS", "width": 1.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nMASE and OWA are metrics used to evaluate the performance of models in forecasting tasks, specifically in the context of time series forecasting. They are related as both are used to assess the accuracy and effectiveness of models in predicting future values in a time series. In other words, MASE and OWA are metrics that help evaluate the performance of a model in forecasting tasks, and they share a common purpose in this regard.\n\nThis summary is based on the information provided in the description list, which states that MASE and OWA are metrics used to evaluate the performance of models in forecasting tasks, and that they are related as both are metrics used to evaluate the performance of time series forecasting models. The summary is written in third person and includes the entity names, MASE and OWA, to provide context.\n\nIt is worth noting that the summary does not provide any additional information beyond what is provided in the description list. However, it does provide a concise and coherent summary of the relationship between MASE and OWA, which is the primary purpose of the summary.", "from": "MASE", "source_id": "1b48e9ca066ac5ba037066bb762d3458,6d66c2ea37e25b646a13d751c05a8e4d,d540ee970ce7debedc6b1b95e9c88be8", "to": "OWA", "width": 3.0}, {"description": "Based on the provided information, the primary language of the text is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nThe text describes two entities, \"MASE\" and \"SMAPE,\" which are related as both are metrics used to evaluate the performance of time series forecasting models. This relationship is further emphasized by the fact that SMAPE is also related to MASE, as both are metrics used to evaluate the performance of a model.\n\nIn the context of time series forecasting, MASE (Mean Absolute Scaled Error) and SMAPE (Symmetric Mean Absolute Percentage Error) are two popular metrics used to evaluate the performance of forecasting models. Both metrics provide a measure of the accuracy of the forecasted values compared to the actual values.\n\nGiven the information provided, it can be concluded that MASE and SMAPE are two related metrics used to evaluate the performance of time series forecasting models, with SMAPE being specifically related to MASE in terms of their shared purpose of evaluating model performance.\n\nHere is a concise description of the entities and their relationship:\n\n\"MASE and SMAPE are two related metrics used to evaluate the performance of time series forecasting models. Both metrics provide a measure of the accuracy of the forecasted values compared to the actual values, with SMAPE being specifically related to MASE in terms of their shared purpose of evaluating model performance.\"", "from": "MASE", "source_id": "1b48e9ca066ac5ba037066bb762d3458,d540ee970ce7debedc6b1b95e9c88be8", "to": "SMAPE", "width": 2.0}, {"description": "Timestamp is related to the input window as the input window is used to encode the timestamp", "from": "TIMESTAMP", "source_id": "4bdf596e75e1cb06d11b25e95491037e", "to": "INPUT WINDOW", "width": 1.0}, {"description": "Predicted candidates are related to timestamp as the timestamp is used to determine the predicted candidates", "from": "TIMESTAMP", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "PREDICTED CANDIDATES", "width": 1.0}, {"description": "Timestamp is related to dimensions as the dimensions are used to determine the timestamp", "from": "TIMESTAMP", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "DIMENSIONS", "width": 1.0}, {"description": "Chronos-T5\u0027s training loss improves with its model size, as shown in Figure 7a", "from": "TRAINING LOSS", "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6", "to": "CHRONOS-T5", "width": 1.0}, {"description": "Chronos-T5\u0027s performance improves with its model size for both in-domain and zero-shot benchmarks, as shown in Figure 7b", "from": "MODEL SIZE", "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6", "to": "CHRONOS-T5", "width": 1.0}, {"description": "Chronos-T5 is related to vocabulary size as vocabulary size is used to represent Chronos-T5", "from": "VOCABULARY SIZE", "source_id": "56613213ed14f292e8ff44f4f0a8bab1", "to": "CHRONOS-T5", "width": 1.0}, {"description": "Training steps are related to vocabulary size as vocabulary size is used to represent training steps", "from": "VOCABULARY SIZE", "source_id": "56613213ed14f292e8ff44f4f0a8bab1", "to": "TRAINING STEPS", "width": 1.0}, {"description": "Tay et al. (2021) used the C4 dataset to train their T5 language model, as described in Raffel et al. (2020)", "from": "TAY ET AL. (2021)", "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6", "to": "RAFFEL ET AL. (2020)", "width": 1.0}, {"description": "C4 dataset is used to train the T5 language model described in Tay et al. (2021)", "from": "TAY ET AL. (2021)", "source_id": "973ea1d37e8f6bb0ab004f360c04ddc6", "to": "C4 DATASET", "width": 1.0}, {"description": "Chronos-T5 is related to training steps as training steps are used to train Chronos-T5", "from": "CHRONOS-T5", "source_id": "56613213ed14f292e8ff44f4f0a8bab1", "to": "TRAINING STEPS", "width": 1.0}, {"description": "Chronos-T5 is a zero-shot model", "from": "CHRONOS-T5", "source_id": "a90c6ca26542ea5935f9d8d5bf3688a9", "to": "ZERO-SHOT", "width": 1.0}, {"description": "Probabilistic predictions are used to evaluate the performance of time series forecasting models, which is related to Chronos-T5", "from": "CHRONOS-T5", "source_id": "e37f4063d074e1697e1f539131b3d963", "to": "PROBABILISTIC PREDICTIONS", "width": 1.0}, {"description": "EC2 instance is used to train Chronos-T5, a time series forecasting model", "from": "CHRONOS-T5", "source_id": "e37f4063d074e1697e1f539131b3d963", "to": "EC2 INSTANCE", "width": 1.0}, {"description": "LLM initialization is related to random initialization as they are two different approaches to initializing a model", "from": "LLM INITIALIZATION", "source_id": "c7f04fa1168df64cce110e20a1b72b51", "to": "RANDOM INITIALIZATION", "width": 1.0}, {"description": "Figure 10a is related to TSMixup augmentations as it shows the performance of models trained with and without TSMixup augmentations", "from": "TSMIXUP AUGMENTATIONS", "source_id": "c7f04fa1168df64cce110e20a1b72b51", "to": "FIGURE 10A", "width": 1.0}, {"description": "ICLR 2024 is related to prototypes as prototypes are examples of vocabulary", "from": "ICLR 2024", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "PROTOTYPES", "width": 1.0}, {"description": "ICLR 2024 is related to time as time is a concept mentioned in the text", "from": "ICLR 2024", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "TIME", "width": 1.0}, {"description": "ICLR 2024 is related to patch 1 as patch 1 is an example of time", "from": "ICLR 2024", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "PATCH 1", "width": 1.0}, {"description": "ICLR 2024 is related to patch 2 as patch 2 is an example of time", "from": "ICLR 2024", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "PATCH 2", "width": 1.0}, {"description": "ICLR 2024 is related to reprogramming as reprogramming is used to modify patch", "from": "ICLR 2024", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "REPROGRAM", "width": 1.0}, {"description": "ICLR 2024 is related to patch 5 as patch 5 is an example of patch", "from": "ICLR 2024", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "PATCH 5", "width": 1.0}, {"description": "ICLR 2024 is related to patch as prefix as both are techniques in computer vision", "from": "ICLR 2024", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "PATCH AS PREFIX", "width": 1.0}, {"description": "ICLR 2024 is related to prompt as prefix as both are techniques in computer vision", "from": "ICLR 2024", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "PROMPT AS PREFIX", "width": 1.0}, {"description": "ICLR 2024 is a conference where the paper was presented", "from": "ICLR 2024", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "FOUNDATION LANGUAGE MODELS", "width": 1.0}, {"description": "Model fine-tuning is compared to TIME-LLM in the text, which was presented at ICLR 2024", "from": "ICLR 2024", "source_id": "b27c89cb0db6646b1203b2701e017aeb", "to": "MODEL FINE-TUNING", "width": 1.0}, {"description": "ICLR 2024 is related to Time Series Library as the paper was presented at the conference", "from": "ICLR 2024", "source_id": "f6fac8e5c6fd12724fe3aa84a2e1cfa6", "to": "TIME SERIES LIBRARY", "width": 1.0}, {"description": "ILI is related to ICLR 2024 as it is a forecasting horizon used in the conference paper", "from": "ICLR 2024", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "ILI", "width": 1.0}, {"description": "ICLR 2024 is a conference where the paper was presented", "from": "ICLR 2024", "source_id": "69457f873272a693c1f813c75ecf030a,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "to": "EVENT", "width": 3.0}, {"description": "ICLR 2024 is related to few-shot forecasting as the paper presented at ICLR 2024 discusses few-shot forecasting", "from": "ICLR 2024", "source_id": "e6a6bc6fbd362394320961ac10cdd230", "to": "FEW-SHOT FORECASTING", "width": 1.0}, {"description": "Prototypes are related to Prompt-as-Prefix as Prompt-as-Prefix enriches the input context and directs the transformation of reprogrammed input patches", "from": "PROTOTYPES", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "PROMPT-AS-PREFIX", "width": 1.0}, {"description": "Time is related to patch 1 as patch 1 is an example of time", "from": "TIME", "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "to": "PATCH 1", "width": 9.0}, {"description": "Time is related to patch 2 as patch 2 is an example of time", "from": "TIME", "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "to": "PATCH 2", "width": 9.0}, {"description": "Acceleration is related to time as time is a measure of duration or sequence", "from": "TIME", "source_id": "2dbfdb45630a023a5a6979b9573a868f", "to": "ACCELERATION", "width": 1.0}, {"description": "Time is related to average as average is calculated over time", "from": "TIME", "source_id": "f05d25f1f55ce768a5d240373d5283a6", "to": "AVERAGE", "width": 1.0}, {"description": "NAB is related to Time as Time is a metric used to evaluate the models on the NAB dataset", "from": "TIME", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "NAB", "width": 1.0}, {"description": "UCR is related to Time as Time is a metric used to evaluate the models on the UCR dataset", "from": "TIME", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "UCR", "width": 1.0}, {"description": "MBA is related to Time as Time is a metric used to evaluate the models on the MBA dataset", "from": "TIME", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "MBA", "width": 1.0}, {"description": "Patch 1 is related to patch 2 as patch 2 is an example of patch 1", "from": "PATCH 1", "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "to": "PATCH 2", "width": 9.0}, {"description": "Patch 5 is related to reprogramming as reprogramming is used to modify patch 5", "from": "REPROGRAM", "source_id": "4742f536818b2fce762157bdb2cb1a3c,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "to": "PATCH 5", "width": 9.0}, {"description": "Patch as prefix is related to prompt as prefix as both are techniques in computer vision", "from": "PATCH AS PREFIX", "source_id": "4742f536818b2fce762157bdb2cb1a3c,509b431231e669be373f593b31412eed,56613213ed14f292e8ff44f4f0a8bab1,69457f873272a693c1f813c75ecf030a,a014d14e003d0998b877eacffa8ebfc4,a8638786e37b5d4f9d005cc1dbf2b8cb,b13b2cc422483985c354844b166a0151,c9efd571b05c136c0bf9d7e89194ec88,e8151dde9661b4cce6a6b8e5a8371c96", "to": "PROMPT AS PREFIX", "width": 11.0}, {"description": "Patch reprogramming and prompt as prefix are related concepts that are used to modify or update a model or system", "from": "PROMPT AS PREFIX", "source_id": "7e03744dea80e2138baff03611104fa8", "to": "PATCH REPROGRAMMING", "width": 1.0}, {"description": "Prompt as prefix is related to cross-domain adaptation as it enables a model to adapt to a new task or domain without any prior training", "from": "PROMPT AS PREFIX", "source_id": "7e03744dea80e2138baff03611104fa8", "to": "CROSS-DOMAIN ADAPTATION", "width": 1.0}, {"description": "Precision is related to loss of precision as loss of precision can occur when the precision of the tokens is low", "from": "PRECISION", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "LOSS OF PRECISION", "width": 1.0}, {"description": "Baseline models are related to precision as precision is a metric used to evaluate the performance of the baseline models", "from": "PRECISION", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "BASELINE MODELS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"PRECISION\" and \"RECALL\" are metrics used to evaluate the performance of an anomaly detection model. Precision and recall are related, as indicated by their use in the table, suggesting a connection between the two metrics. Specifically, precision is related to recall, implying that the performance of the anomaly detection model is being evaluated using both metrics. This relationship is further supported by the fact that recall is used to evaluate the performance of the anomaly detection model, indicating that it is a key metric in this context.\n\nIn the context of anomaly detection, precision and recall are likely being used to assess the model\u0027s ability to correctly identify anomalies and avoid false positives. The use of these metrics in the table suggests that the model\u0027s performance is being evaluated in terms of its precision and recall, providing a comprehensive understanding of its strengths and weaknesses.\n\nOverall, the summary highlights the relationship between precision and recall as metrics used to evaluate the performance of an anomaly detection model, and provides context on their use in the table.", "from": "PRECISION", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,f6e57fa18831bcc732a631240536777a", "to": "RECALL", "width": 2.0}, {"description": "Lag-Llama achieves comparable performance to all baselines in the zero-shot setting", "from": "ZERO-SHOT", "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "to": "LAG-LAG", "width": 1.0}, {"description": "Zero-shot is related to fine-tuned as the quality of forecasts increase significantly when the model is fine-tuned", "from": "ZERO-SHOT", "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "to": "FINE-TUNED", "width": 1.0}, {"description": "Ground truth is related to median forecast as median forecast is an estimate of ground truth", "from": "GROUNDTUTH", "source_id": "a90c6ca26542ea5935f9d8d5bf3688a9", "to": "MEDIAN FORECAST", "width": 1.0}, {"description": "80% interval is related to ground truth as 80% interval is a range of values that contains ground truth", "from": "GROUNDTUTH", "source_id": "a90c6ca26542ea5935f9d8d5bf3688a9", "to": "80% INTERVAL", "width": 1.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nThe \"MEDIAN FORECAST\" and \"80% INTERVAL\" are related entities in the context of time series forecasting. The \"MEDIAN FORECAST\" is a statistical estimate that represents the middle value of a dataset, while the \"80% INTERVAL\" is a range of values that contains the \"MEDIAN FORECAST\". This interval is used to estimate the range of values within which 80% of the predicted values are expected to fall. In other words, the \"80% INTERVAL\" is a confidence interval that provides a measure of uncertainty around the \"MEDIAN FORECAST\", indicating that 80% of the predicted values are likely to lie within this range.\n\nThis relationship between the \"MEDIAN FORECAST\" and \"80% INTERVAL\" is crucial in time series forecasting, as it allows for the estimation of the uncertainty associated with the predicted values. By providing a range of values within which the predicted values are expected to fall, the \"80% INTERVAL\" offers a more comprehensive understanding of the forecasting results, enabling users to make more informed decisions.\n\nOverall, the \"MEDIAN FORECAST\" and \"80% INTERVAL\" are interconnected concepts that work together to provide a more accurate and reliable forecasting outcome.", "from": "MEDIAN FORECAST", "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7,a90c6ca26542ea5935f9d8d5bf3688a9", "to": "80% INTERVAL", "width": 2.0}, {"description": "80% interval is related to NN5 as it is used to evaluate the performance of the Chronos model", "from": "80% INTERVAL", "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "to": "NN5", "width": 1.0}, {"description": "Frequency and trend are characteristics of time series that TimeGPT can handle", "from": "TREND", "source_id": "518bfcd6711530089fe3914ca16459c2", "to": "FREQUENCY", "width": 1.0}, {"description": "Trend and seasonality are characteristics of time series that TimeGPT can handle", "from": "TREND", "source_id": "518bfcd6711530089fe3914ca16459c2", "to": "SEASONALITY", "width": 1.0}, {"description": "Ground truth AR model is related to AR(1) as AR(1) is a specific type of AR process", "from": "GROUND TRUTH AR MODEL", "source_id": "9c155897f3e35902f57696e0abaeb161", "to": "AR(1)", "width": 1.0}, {"description": "Ground truth AR model is related to AR(2) as AR(2) is a specific type of AR process", "from": "GROUND TRUTH AR MODEL", "source_id": "9c155897f3e35902f57696e0abaeb161", "to": "AR(2)", "width": 1.0}, {"description": "Ground truth AR model is related to AR(3) as AR(3) is a specific type of AR process", "from": "GROUND TRUTH AR MODEL", "source_id": "9c155897f3e35902f57696e0abaeb161", "to": "AR(3)", "width": 1.0}, {"description": "Ground truth AR model is related to AR(4) as AR(4) is a specific type of AR process", "from": "GROUND TRUTH AR MODEL", "source_id": "9c155897f3e35902f57696e0abaeb161", "to": "AR(4)", "width": 1.0}, {"description": "AR(1) is related to AR(2) as AR(2) is a specific type of AR process", "from": "AR(1)", "source_id": "9c155897f3e35902f57696e0abaeb161", "to": "AR(2)", "width": 1.0}, {"description": "AR(2) is related to AR(3) as AR(3) is a specific type of AR process", "from": "AR(2)", "source_id": "9c155897f3e35902f57696e0abaeb161", "to": "AR(3)", "width": 1.0}, {"description": "AR(3) is related to AR(4) as AR(4) is a specific type of AR process", "from": "AR(3)", "source_id": "9c155897f3e35902f57696e0abaeb161", "to": "AR(4)", "width": 1.0}, {"description": "MSE is related to kernel density estimation as kernel density estimation is used to estimate the density of a distribution", "from": "MSE", "source_id": "9c155897f3e35902f57696e0abaeb161", "to": "KERNEL DENSITY ESTIMATION", "width": 1.0}, {"description": "Predictive distribution is related to kernel density estimation as kernel density estimation is used to estimate the density of a distribution", "from": "KERNEL DENSITY ESTIMATION", "source_id": "9c155897f3e35902f57696e0abaeb161", "to": "PREDICTIVE DISTRIBUTION", "width": 1.0}, {"description": "Predictive distributions are related to kernel density estimation as it is used to estimate them", "from": "KERNEL DENSITY ESTIMATION", "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "to": "PREDICTIVE DISTRIBUTIONS", "width": 1.0}, {"description": "Kernel density estimation is related to token ID as it is used to estimate the underlying probability distribution of a set of data", "from": "KERNEL DENSITY ESTIMATION", "source_id": "85e4fbea9a08a0a663f3c5dc3f3a95a7", "to": "TOKEN ID", "width": 1.0}, {"description": "NN5 dataset is sourced from ATM withdrawal data", "from": "NN5", "source_id": "d4bc1397526f236029876d4fbc22a721", "to": "ATM WITHDRAWAL DATA", "width": 1.0}, {"description": "The mean value is related to the variance as it is a statistic that is used to calculate the average value of a time series", "from": "VARIANCE", "source_id": "00007df4774d6122e3848802a24f9536", "to": "MEAN VALUE", "width": 1.0}, {"description": "The variance is related to summary statistics as it is a statistic that is used to calculate the spread of a time series", "from": "VARIANCE", "source_id": "00007df4774d6122e3848802a24f9536", "to": "SUMMARY STATISTICS", "width": 1.0}, {"description": "Observations are related to series as observations are part of a series in the Chronos model", "from": "OBSERVATIONS", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "SERIES", "width": 1.0}, {"description": "Series are related to bins as bins are used to represent the series in the Chronos model", "from": "SERIES", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "BINS", "width": 1.0}, {"description": "Loss of precision is related to edge cases as edge cases can cause loss of precision in the Chronos model", "from": "LOSS OF PRECISION", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "EDGE CASES", "width": 1.0}, {"description": "Test datasets are related to research avenues as the Chronos model opens up new research avenues", "from": "TEST DATASETS", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "RESEARCH AVEUNUES", "width": 1.0}, {"description": "Research avenues are related to conformal methods as conformal methods can be used to calibrate the Chronos model", "from": "RESEARCH AVEUNUES", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "CONFIRMAL METHODS", "width": 1.0}, {"description": "Conformal methods are related to parameter-efficient fine-tuning as parameter-efficient fine-tuning can be used to fine-tune the Chronos model", "from": "CONFIRMAL METHODS", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "PARAMETER-EFFICIENT FINE-TUNING", "width": 1.0}, {"description": "Parameter-efficient fine-tuning is related to low-rank adapters as low-rank adapters can be used to fine-tune the Chronos model", "from": "PARAMETER-EFFICIENT FINE-TUNING", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "LOW-RANK ADAPTERS", "width": 1.0}, {"description": "Parameter-efficient fine-tuning methods like QLoRA can be used to balance task performance and efficiency", "from": "PARAMETER-EFFICIENT FINE-TUNING", "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8", "to": "QLORA", "width": 1.0}, {"description": "Low-rank adapters are related to conformal prediction as conformal prediction can be used to calibrate the Chronos model", "from": "LOW-RANK ADAPTERS", "source_id": "f622f27b5c3f52c6b04ada48bd63b03d", "to": "CONFORMAL PREDICTION", "width": 1.0}, {"description": "Confidence interval is related to conformal prediction as conformal prediction was used to generate prediction intervals", "from": "CONFORMAL PREDICTION", "source_id": "42e1bb44edbe787e104f589e74b95d6c", "to": "CONFIDENCE INTERVAL", "width": 1.0}, {"description": "Interest rates can influence the forecast for another time series", "from": "MULTIVARIATE FORECASTING", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "INTEREST RATES", "width": 1.0}, {"description": "Housing prices can be influenced by interest rates", "from": "MULTIVARIATE FORECASTING", "source_id": "b4d5306b46bbfa4564727fe5ac6630e0", "to": "HOUSING PRICES", "width": 1.0}, {"description": "XGBoost and LightGBM are machine learning models used for time series forecasting", "from": "LIGHTGBM", "source_id": "6771b6279846e780d6807b15184ae008", "to": "XGBOOST", "width": 1.0}, {"description": "GPU is related to V100 as V100 is a specific type of graphics processing unit", "from": "GPU", "source_id": "116332ac4538a1430c83a34fcbec22d1", "to": "V100", "width": 1.0}, {"description": "GPU is related to CPU cores as GPU and CPU cores are devices used to accelerate the training of a model", "from": "GPU", "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "to": "CPU CORES", "width": 1.0}, {"description": "GPU is related to RAM as GPU and RAM are devices used to store the data used to train a model", "from": "GPU", "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "to": "RAM", "width": 1.0}, {"description": "Transformer models are related to GPU as they allow for parallelization of inference on GPUs", "from": "GPU", "source_id": "bd73ee0439609823e12a877840c6ebae", "to": "TRANSFORMER MODELS", "width": 1.0}, {"description": "GPU is related to anomaly detector as it is used for parallel processing in anomaly detection", "from": "GPU", "source_id": "bd73ee0439609823e12a877840c6ebae", "to": "ANOMALY DETECTOR", "width": 1.0}, {"description": "Salinas is an author of the paper \"GluonTS: Probabilistic and Neural Time Series Modeling in Python\", which presents the GluonTS model", "from": "GLUONTS", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "SALINAS", "width": 1.0}, {"description": "GluonTS is a probabilistic and neural time series modeling framework in Python, which was published in The Journal of Machine Learning Research", "from": "GLUONTS", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "THE JOURNAL OF MACHINE LEARNING RESEARCH", "width": 1.0}, {"description": "GLUONTS is used with STATSFORCAST as a platform for time series forecasting", "from": "GLUONTS", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "STATSFORCAST", "width": 1.0}, {"description": "GLUONTS is used with SEASONALNAIVE as a platform for time series forecasting", "from": "GLUONTS", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "SEASONALNAIVE", "width": 1.0}, {"description": "Arts is related to The Journal of Machine Learning Research as Arts was published in the journal", "from": "THE JOURNAL OF MACHINE LEARNING RESEARCH", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "ARTS", "width": 1.0}, {"description": "Konstantinos Benidis is an author of the paper \"Deep learning for time series forecasting: Tutorial and literature survey\", which presents the deep learning for time series forecasting model", "from": "KONSTANTINOS BENIDIS", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "DEEP LEARNING FOR TIME SERIES FORECASTING", "width": 1.0}, {"description": "Deep Explicit Duration Switching Models is a type of time series model, which was published in Advances in Neural Information Processing Systems", "from": "DEEP EXPLICIT DURATION SWITCHING MODELS", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS", "width": 1.0}, {"description": "Language models are few-shot learners is a type of time series forecasting model, which was published in Advances in Neural Information Processing Systems", "from": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "LANGUAGE MODELS ARE FEW-SHOT LEARNERS", "width": 1.0}, {"description": "Advances in Neural Information Processing Systems is a conference", "from": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "CONFERENCE", "width": 1.0}, {"description": "V. Assimakopoulos is an author of the paper \"The theta model: a decomposition approach to forecasting\", which presents the theta model", "from": "V ASSIMAKOPOULOS", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "THE THETA MODEL", "width": 1.0}, {"description": "The theta model is a type of time series forecasting model, which was published in International Journal of Forecasting", "from": "THE THETA MODEL", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "INTERNATIONAL JOURNAL OF FORECASTING", "width": 1.0}, {"description": "The tourism forecasting competition is an event where the paper \"The tourism forecasting competition\" was presented, which was published in International Journal of Forecasting", "from": "INTERNATIONAL JOURNAL OF FORECASTING", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "THE TOURISM FORECASTING COMPETITION", "width": 1.0}, {"description": "George Athanasopoulos is an author of the paper \"The tourism forecasting competition\", which presents the tourism forecasting competition", "from": "GEORGE ATHANASOPOULOS", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "THE TOURISM FORECASTING COMPETITION", "width": 1.0}, {"description": "Deep learning for time series forecasting is a type of time series forecasting model, which was published in ACM Comput. Surv.", "from": "DEEP LEARNING FOR TIME SERIES FORECASTING", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "ACM COMPUT SURV", "width": 1.0}, {"description": "Oliver Borchert is an author of the paper \"Multi-objective model selection for time series forecasting\", which presents the multi-objective model selection model", "from": "OLIVER BORCHERT", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "MULTI-OBJECTIVE MODEL SELECTION", "width": 1.0}, {"description": "Multi-objective model selection is a type of time series forecasting model, which was published in arXiv preprint", "from": "MULTI-OBJECTIVE MODEL SELECTION", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "ARXIV PREPRINT", "width": 1.0}, {"description": "Tom B. Brown is an author of the paper \"Language models are few-shot learners\", which presents the language models are few-shot learners model", "from": "TOM B BROWN", "source_id": "35e5d369f2d5fe8c06e11244cf7c9ba1", "to": "LANGUAGE MODELS ARE FEW-SHOT LEARNERS", "width": 1.0}, {"description": "Litwin and Gray are authors of a research paper", "from": "GRAY", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "LITWIN", "width": 1.0}, {"description": "Gray and Chess are authors of a research paper", "from": "GRAY", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "CHESS", "width": 1.0}, {"description": "Gray and Clark are authors of a research paper", "from": "GRAY", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "CLARK", "width": 1.0}, {"description": "Gray and Berner are authors of a research paper", "from": "GRAY", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "BERNER", "width": 1.0}, {"description": "Gray and McCandlish are authors of a research paper", "from": "GRAY", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "MCCANDLISH", "width": 1.0}, {"description": "Gray and Radford are authors of a research paper", "from": "GRAY", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "RADFORD", "width": 1.0}, {"description": "Gray and Sutskever are authors of a research paper", "from": "GRAY", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "SUTSKEVER", "width": 1.0}, {"description": "Gray and Amodei are authors of a research paper", "from": "GRAY", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "AMODEI", "width": 1.0}, {"description": "Litwin and Chess are authors of a research paper", "from": "CHESS", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "LITWIN", "width": 1.0}, {"description": "Chess and Clark are authors of a research paper", "from": "CHESS", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "CLARK", "width": 1.0}, {"description": "Chess and Berner are authors of a research paper", "from": "CHESS", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "BERNER", "width": 1.0}, {"description": "Chess and McCandlish are authors of a research paper", "from": "CHESS", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "MCCANDLISH", "width": 1.0}, {"description": "Chess and Radford are authors of a research paper", "from": "CHESS", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "RADFORD", "width": 1.0}, {"description": "Chess and Sutskever are authors of a research paper", "from": "CHESS", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "SUTSKEVER", "width": 1.0}, {"description": "Chess and Amodei are authors of a research paper", "from": "CHESS", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "AMODEI", "width": 1.0}, {"description": "Litwin and Clark are authors of a research paper", "from": "CLARK", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "LITWIN", "width": 1.0}, {"description": "Clark and Berner are authors of a research paper", "from": "CLARK", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "BERNER", "width": 1.0}, {"description": "Clark and McCandlish are authors of a research paper", "from": "CLARK", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "MCCANDLISH", "width": 1.0}, {"description": "Clark and Radford are authors of a research paper", "from": "CLARK", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "RADFORD", "width": 1.0}, {"description": "Clark and Sutskever are authors of a research paper", "from": "CLARK", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "SUTSKEVER", "width": 1.0}, {"description": "Clark and Amodei are authors of a research paper", "from": "CLARK", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "AMODEI", "width": 1.0}, {"description": "Litwin and Berner are authors of a research paper", "from": "BERNER", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "LITWIN", "width": 1.0}, {"description": "Berner and McCandlish are authors of a research paper", "from": "BERNER", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "MCCANDLISH", "width": 1.0}, {"description": "Berner and Radford are authors of a research paper", "from": "BERNER", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "RADFORD", "width": 1.0}, {"description": "Berner and Sutskever are authors of a research paper", "from": "BERNER", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "SUTSKEVER", "width": 1.0}, {"description": "Berner and Amodei are authors of a research paper", "from": "BERNER", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "AMODEI", "width": 1.0}, {"description": "Litwin and McCandlish are authors of a research paper", "from": "MCCANDLISH", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "LITWIN", "width": 1.0}, {"description": "McCandlish and Radford are authors of a research paper", "from": "MCCANDLISH", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "RADFORD", "width": 1.0}, {"description": "McCandlish and Sutskever are authors of a research paper", "from": "MCCANDLISH", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "SUTSKEVER", "width": 1.0}, {"description": "McCandlish and Amodei are authors of a research paper", "from": "MCCANDLISH", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "AMODEI", "width": 1.0}, {"description": "Litwin and Radford are authors of a research paper", "from": "RADFORD", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "LITWIN", "width": 1.0}, {"description": "Radford and Sutskever are authors of a research paper", "from": "RADFORD", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "SUTSKEVER", "width": 1.0}, {"description": "Radford and Amodei are authors of a research paper", "from": "RADFORD", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "AMODEI", "width": 1.0}, {"description": "Litwin and Sutskever are authors of a research paper", "from": "SUTSKEVER", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "LITWIN", "width": 1.0}, {"description": "Sutskever and Amodei are authors of a research paper", "from": "SUTSKEVER", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "AMODEI", "width": 1.0}, {"description": "Litwin and Amodei are authors of a research paper", "from": "AMODEI", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "LITWIN", "width": 1.0}, {"description": "Carmona and Aubet are authors of a research paper", "from": "CARMONA", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "AUBET", "width": 1.0}, {"description": "Carmona and Flunkert are authors of a research paper", "from": "CARMONA", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "FLUNKERT", "width": 1.0}, {"description": "Carmona and Gasthaus are authors of a research paper", "from": "CARMONA", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "GASTHAUS", "width": 1.0}, {"description": "Aubet and Flunkert are authors of a research paper", "from": "AUBET", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "FLUNKERT", "width": 1.0}, {"description": "Aubet and Gasthaus are authors of a research paper", "from": "AUBET", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "GASTHAUS", "width": 1.0}, {"description": "Flunkert and Gasthaus are authors of a research paper", "from": "FLUNKERT", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "GASTHAUS", "width": 1.0}, {"description": "Neural Contextual Anomaly Detection for Time Series is a research paper", "from": "NEURAL CONTEXTUAL ANOMALY DETECTION FOR TIME SERIES", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "ARXIV:2107.07702", "width": 1.0}, {"description": "Challu and Olivares are authors of a research paper", "from": "CHALLU", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "OLIVARES", "width": 1.0}, {"description": "Challu and Oreshkin are authors of a research paper", "from": "CHALLU", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "ORESHKIN", "width": 1.0}, {"description": "Challu and Garza Ramirez are authors of a research paper", "from": "CHALLU", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "GARZA RAMIREZ", "width": 1.0}, {"description": "Challu and Canseco are authors of a research paper", "from": "CHALLU", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "CANSECO", "width": 1.0}, {"description": "Challu and Dubrawski are authors of a research paper", "from": "CHALLU", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "DUBRAWSKI", "width": 1.0}, {"description": "Olivares and Oreshkin are authors of a research paper", "from": "OLIVARES", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "ORESHKIN", "width": 1.0}, {"description": "Olivares and Garza Ramirez are authors of a research paper", "from": "OLIVARES", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "GARZA RAMIREZ", "width": 1.0}, {"description": "Olivares and Canseco are authors of a research paper", "from": "OLIVARES", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "CANSECO", "width": 1.0}, {"description": "Olivares and Dubrawski are authors of a research paper", "from": "OLIVARES", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "DUBRAWSKI", "width": 1.0}, {"description": "Oreshkin and Garza Ramirez are authors of a research paper", "from": "ORESHKIN", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "GARZA RAMIREZ", "width": 1.0}, {"description": "Oreshkin and Canseco are authors of a research paper", "from": "ORESHKIN", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "CANSECO", "width": 1.0}, {"description": "Oreshkin and Dubrawski are authors of a research paper", "from": "ORESHKIN", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "DUBRAWSKI", "width": 1.0}, {"description": "Garza Ramirez and Canseco are authors of a research paper", "from": "GARZA RAMIREZ", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "CANSECO", "width": 1.0}, {"description": "Garza Ramirez and Dubrawski are authors of a research paper", "from": "GARZA RAMIREZ", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "DUBRAWSKI", "width": 1.0}, {"description": "Canseco and Dubrawski are authors of a research paper", "from": "CANSECO", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "DUBRAWSKI", "width": 1.0}, {"description": "Chowdhery and Narang are authors of a research paper", "from": "CHOWDHERY", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "NARANG", "width": 1.0}, {"description": "Chowdhery and Devlin are authors of a research paper", "from": "CHOWDHERY", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "DEVLIN", "width": 1.0}, {"description": "Chowdhery and PaLM are authors of a research paper", "from": "CHOWDHERY", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "PALM", "width": 1.0}, {"description": "Narang and Devlin are authors of a research paper", "from": "NARANG", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "DEVLIN", "width": 1.0}, {"description": "Narang and PaLM are authors of a research paper", "from": "NARANG", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "PALM", "width": 1.0}, {"description": "Devlin and PaLM are authors of a research paper", "from": "DEVLIN", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "PALM", "width": 1.0}, {"description": "Journal of Machine Learning Research is a journal", "from": "PALM", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "JOURNAL OF MACHINE LEARNING RESEARCH", "width": 1.0}, {"description": "Dao is an author of a research paper", "from": "DAO", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "FLASHATTENTION-2", "width": 1.0}, {"description": "Darlow and Joosen are authors of a research paper", "from": "DARLOW", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "JOOSSEN", "width": 1.0}, {"description": "Darlow and Asenov are authors of a research paper", "from": "DARLOW", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "ASENOV", "width": 1.0}, {"description": "Darlow is an author of a research paper", "from": "DARLOW", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "TSMIX", "width": 1.0}, {"description": "Joosen and Asenov are authors of a research paper", "from": "JOOSSEN", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "ASENOV", "width": 1.0}, {"description": "Joosen is an author of a research paper", "from": "JOOSSEN", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "TSMIX", "width": 1.0}, {"description": "Asenov is an author of a research paper", "from": "ASENOV", "source_id": "66b7675d8c62321e1cc8916401159787", "to": "TSMIX", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\n**Summary of ALGORITHM 1 and ALGORITHM 2**\n\nALGORITHM 1 and ALGORITHM 2 are two related entities that are used for time series data generation. Specifically, ALGORITHM 1 is used in ALGORITHM 2, representing the TranAD testing algorithm. Furthermore, ALGORITHM 2 is related to ALGORITHM 1 as it is a description of the inference procedure. This suggests that ALGORITHM 2 builds upon or utilizes the functionality of ALGORITHM 1, highlighting their interconnectedness in the context of time series data generation.\n\n**Key Takeaways:**\n\n* Both ALGORITHM 1 and ALGORITHM 2 are used for time series data generation.\n* ALGORITHM 1 is used in ALGORITHM 2, specifically representing the TranAD testing algorithm.\n* ALGORITHM 2 is a description of the inference procedure related to ALGORITHM 1.\n\n**Contextual Information:**\n\nThe provided descriptions suggest that ALGORITHM 1 and ALGORITHM 2 are closely related and are used in conjunction with each other for time series data generation. The use of ALGORITHM 1 in ALGORITHM 2 and the description of ALGORITHM 2 as an inference procedure related to ALGORITHM 1 indicate a high degree of interdependence between the two entities. This summary provides a concise and coherent overview of the relationships between ALGORITHM 1 and ALGORITHM 2.", "from": "ALGORITHM 2", "source_id": "6ea15432a841705c2e74cbc01f6004e9,a65c0f1eae6e779357739df141f75d36,e5e4a8f03f502fada5b17cba5dc942ba", "to": "ALGORITHM 1", "width": 3.0}, {"description": "Australian Electricity is related to Godahewa et al. as they are the authors of a paper that mentions the Australian Electricity dataset", "from": "AUSTRALIAN ELECTRICITY", "source_id": "e5e4a8f03f502fada5b17cba5dc942ba", "to": "GODAHEWA ET AL.", "width": 1.0}, {"description": "KDD Cup 2018 is related to Godahewa et al. as Godahewa et al. participated in the competition", "from": "GODAHEWA ET AL.", "source_id": "6820cea275275e87630a6876e890c525", "to": "KDD CUP 2018", "width": 1.0}, {"description": "Open dataset is related to the academic paper Godahewa et al., as indicated by the use of the phrase \"Godahewa et al.\"", "from": "GODAHEWA ET AL.", "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "to": "OPEN DATASET", "width": 1.0}, {"description": "Section 5 presents additional analysis of the TranAD model", "from": "SECTION 5", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "TRANAD", "width": 1.0}, {"description": "Self-conditioning is used in algorithm 1, representing the two-phase inference approach", "from": "ALGORITHM 1", "source_id": "a65c0f1eae6e779357739df141f75d36", "to": "SELF-CONDITIONING", "width": 1.0}, {"description": "Dominick is related to ERCOT Load as both are datasets of energy usage data", "from": "ERCOT LOAD", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "DOMINICK", "width": 1.0}, {"description": "Brazilian Cities Temperature is related to Mexico City Bikes as both datasets contain temperature data", "from": "BRAZILIAN CITIES TEMPERATURE", "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "to": "MEXICO CITY BIKES", "width": 1.0}, {"description": "Brazilian Cities Temperature dataset is sourced from Brazilian weather data", "from": "BRAZILIAN CITIES TEMPERATURE", "source_id": "d4bc1397526f236029876d4fbc22a721", "to": "BRAZILIAN WEATHER DATA", "width": 1.0}, {"description": "Mexico City Bikes is related to Solar as both datasets contain energy data", "from": "MEXICO CITY BIKES", "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "to": "SOLAR", "width": 1.0}, {"description": "Solar is related to Spanish Energy and Weather as both datasets contain energy data", "from": "SOLAR", "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "to": "SPANISH ENERGY AND WEATHER", "width": 1.0}, {"description": "Solar dataset is sourced from US solar power data", "from": "SOLAR", "source_id": "d4bc1397526f236029876d4fbc22a721", "to": "US SOLAR POWER DATA", "width": 1.0}, {"description": "Beijing Multisite Sunspot is related to solar as solar is a specific type of Beijing Multisite Sunspot", "from": "SOLAR", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "BEIJING MULTISITE SUNSPOT", "width": 1.0}, {"description": "Spanish Energy and Weather is related to Taxi as both datasets contain energy data", "from": "SPANISH ENERGY AND WEATHER", "source_id": "6a222f9ed7fcf5fc945dfc22e16a3502", "to": "TAXI", "width": 1.0}, {"description": "Spanish Energy and Weather dataset is sourced from Spain energy and weather data", "from": "SPANISH ENERGY AND WEATHER", "source_id": "d4bc1397526f236029876d4fbc22a721", "to": "SPAIN ENERGY AND WEATHER DATA", "width": 1.0}, {"description": "Taxi is related to USHCN as both are datasets of transportation and climate data", "from": "TAXI", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "USHCN", "width": 1.0}, {"description": "New York is related to taxi as New York has taxi services", "from": "TAXI", "source_id": "6820cea275275e87630a6876e890c525", "to": "NEW YORK", "width": 1.0}, {"description": "USHCN is related to Weatherbench as both are datasets of climate and weather data", "from": "USHCN", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "WEATHERBENCH", "width": 1.0}, {"description": "Precipitation is related to USHCN as USHCN provides precipitation data", "from": "USHCN", "source_id": "6820cea275275e87630a6876e890c525", "to": "PRECIPITATION", "width": 1.0}, {"description": "WeatherBench is related to car parts as car parts can be affected by weather", "from": "WEATHERBENCH", "source_id": "6820cea275275e87630a6876e890c525", "to": "CAR PARTS", "width": 1.0}, {"description": "KDD Cup 2018 is related to London Smart Meters as both are datasets of energy usage data", "from": "KDD CUP 2018", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "LONDON SMART METERS", "width": 1.0}, {"description": "KDD Cup 2018 dataset is sourced from air quality data", "from": "KDD CUP 2018", "source_id": "d4bc1397526f236029876d4fbc22a721", "to": "AIR QUALITY DATA", "width": 1.0}, {"description": "Nature is related to KDD Cup 2018 as KDD Cup 2018 is a dataset used for pretraining and fine-tuning", "from": "KDD CUP 2018", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "NATURE", "width": 1.0}, {"description": "Based on the provided information, the primary entity is \"KDD CUP 2018\" and the secondary entity is \"SUNSPOT\". \n\nAfter analyzing the descriptions, it appears that there are two possible connections between the entities. The first description suggests that both \"KDD Cup 2018\" and \"SUNSPOT\" are concepts related to nature. However, the second description provides a more specific connection, stating that both are datasets used for pretraining and fine-tuning.\n\nConsidering the second description as the primary connection, it can be inferred that \"KDD Cup 2018\" and \"SUNSPOT\" are datasets that are related to each other in terms of their usage in machine learning tasks. \n\nHere is a comprehensive summary of the data:\n\nThe \"KDD CUP 2018\" and \"SUNSPOT\" datasets are two related entities that are used in machine learning tasks, specifically for pretraining and fine-tuning. They are likely datasets that are used to train and evaluate models in a specific domain, possibly related to nature or environmental studies. \n\nThis summary is based on the second description, which provides a more specific connection between the two entities. If the first description is considered, the summary would be:\n\nThe \"KDD CUP 2018\" and \"SUNSPOT\" concepts are related to nature, but the exact nature of this connection is unclear.\n\nHowever, considering the context of the KDD Cup 2018, which is a machine learning competition, it is more likely that the connection between the two entities is related to their usage in machine learning tasks, rather than a general connection to nature.", "from": "KDD CUP 2018", "source_id": "4c09f35749179fe18c7d7290eaa57955,9e88afa28686ff93769bfc5eb0f1095e", "to": "SUNSPOT", "width": 2.0}, {"description": "KDD Cup 2018 is related to Australian electricity demand as Australian electricity demand is a specific type of KDD Cup 2018", "from": "KDD CUP 2018", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "AUSTRALIAN ELECTRICITY DEMAND", "width": 1.0}, {"description": "KDD Cup 2018 is related to Air Quality UCL as Air Quality UCL is a specific type of KDD Cup 2018", "from": "KDD CUP 2018", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "AIR QUALITY UCL", "width": 1.0}, {"description": "London Smart Meters is related to M4 as both are datasets of energy and economic data", "from": "LONDON SMART METERS", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "M4", "width": 1.0}, {"description": "Australian electricity demand is related to London smart meters as both are datasets used for pretraining and fine-tuning", "from": "LONDON SMART METERS", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "AUSTRALIAN ELECTRICITY DEMAND", "width": 1.0}, {"description": "London smart meters is related to wind farms as both are datasets used for pretraining and fine-tuning", "from": "LONDON SMART METERS", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "WIND FARMS", "width": 1.0}, {"description": "London Smart Meters is related to pedestrian counts as pedestrian counts are a specific type of London Smart Meters", "from": "LONDON SMART METERS", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "PEDESTRIAN COUNTS", "width": 1.0}, {"description": "London Smart Meters is related to ETT M2 as ETT M2 is a specific type of London Smart Meters", "from": "LONDON SMART METERS", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "ETT M2", "width": 1.0}, {"description": "M4 and M5 are forecasting competitions that contain data from various domains", "from": "M4", "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "to": "M5", "width": 1.0}, {"description": "M4 is related to ETSformer as ETSformer is used for short-term time series forecasting on M4", "from": "M4", "source_id": "d4551c2839eaa68a7cb7324089956581", "to": "ETSFORMER", "width": 1.0}, {"description": "M4 is related to LightTS as LightTS is used for short-term time series forecasting on M4", "from": "M4", "source_id": "d4551c2839eaa68a7cb7324089956581", "to": "LIGHTTS", "width": 1.0}, {"description": "The M4 benchmark and the M3-Quarterly dataset are both benchmarks", "from": "M4", "source_id": "50eeacd99c68b2581be90310bedcbc2c", "to": "M3-QUARTERLY", "width": 1.0}, {"description": "Dominick is related to Mexico City as Dominick has a store in Mexico City", "from": "DOMINICK", "source_id": "6820cea275275e87630a6876e890c525", "to": "MEXICO CITY", "width": 1.0}, {"description": "Exchange Rate is related to FRED-MD as both are datasets of economic data", "from": "EXCHANGE RATE", "source_id": "e067234b24b0625a3f95ecc18035f915", "to": "FRED-MD", "width": 1.0}, {"description": "Exchange Rate dataset is sourced from exchange rate data", "from": "EXCHANGE RATE", "source_id": "d4bc1397526f236029876d4fbc22a721", "to": "EXCHANGE RATE DATA", "width": 1.0}, {"description": "Exchange Rate is related to ETT H1 as both are concepts related to energy", "from": "EXCHANGE RATE", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "ETT H1", "width": 1.0}, {"description": "Electricity weather CPU usage is related to exchange rate as exchange rate is a specific type of electricity weather CPU usage", "from": "EXCHANGE RATE", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "ELECTRICITY WEATHER CPU USAGE", "width": 1.0}, {"description": "FRED-MD is related to economics as it provides data on economic variables", "from": "FRED-MD", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "ECONOMICS", "width": 1.0}, {"description": "FRED-MD dataset is sourced from Federal Reserve Bank data", "from": "FRED-MD", "source_id": "d4bc1397526f236029876d4fbc22a721", "to": "FEDERAL RESERVE BANK DATA", "width": 1.0}, {"description": "Air Quality is related to Economics as both are domains used for fine-tuning forecasting tasks", "from": "ECONOMICS", "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26", "to": "AIR QUALITY", "width": 1.0}, {"description": "Cloud is related to Economics as both are domains used for fine-tuning forecasting tasks", "from": "ECONOMICS", "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26", "to": "CLOUD", "width": 1.0}, {"description": "Instances are related to economics as economics is a specific type of instance", "from": "ECONOMICS", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "INSTANCES", "width": 1.0}, {"description": "M1 (Monthly) is related to M1 (Quarterly) as both are time series datasets of economic and financial variables", "from": "M1 (MONTHLY)", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "M1 (QUARTERLY)", "width": 1.0}, {"description": "M1 (Monthly) is related to M1 (Yearly) as both are time series datasets of economic and financial variables", "from": "M1 (MONTHLY)", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "M1 (YEARLY)", "width": 1.0}, {"description": "M1 (Quarterly) is related to M1 (Yearly) as both are time series datasets of economic and financial variables", "from": "M1 (QUARTERLY)", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "M1 (YEARLY)", "width": 1.0}, {"description": "M3 (Monthly) is related to M3 (Quarterly) as both are time series datasets of economic and financial variables", "from": "M3 (MONTHLY)", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "M3 (QUARTERLY)", "width": 1.0}, {"description": "M3 (Monthly) is related to M3 (Yearly) as both are time series datasets of economic and financial variables", "from": "M3 (MONTHLY)", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "M3 (YEARLY)", "width": 1.0}, {"description": "M3 (Quarterly) is related to M3 (Yearly) as both are time series datasets of economic and financial variables", "from": "M3 (QUARTERLY)", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "M3 (YEARLY)", "width": 1.0}, {"description": "M4 (Quarterly) is related to M4 (Yearly) as both are time series datasets of economic and financial variables", "from": "M4 (QUARTERLY)", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "M4 (YEARLY)", "width": 1.0}, {"description": "M5 is related to NN5 (Daily) as both are time series datasets of economic and financial variables", "from": "M5", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "NN5 (DAILY)", "width": 1.0}, {"description": "M5 is related to NN5 (Weekly) as both are time series datasets of economic and financial variables", "from": "M5", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "NN5 (WEEKLY)", "width": 1.0}, {"description": "Tourism (Monthly) is related to Tourism (Quarterly) as both are time series datasets of tourism-related data", "from": "TOURISM (MONTHLY)", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "TOURISM (QUARTERLY)", "width": 1.0}, {"description": "Tourism (Monthly) is related to Tourism (Yearly) as both are time series datasets of tourism-related data", "from": "TOURISM (MONTHLY)", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "TOURISM (YEARLY)", "width": 1.0}, {"description": "Tourism (Quarterly) is related to Tourism (Yearly) as both are time series datasets of tourism-related data", "from": "TOURISM (QUARTERLY)", "source_id": "4eb417cb4bd15ceba2949a1358623cb8", "to": "TOURISM (YEARLY)", "width": 1.0}, {"description": "ETERS dataset is sourced from smartmeter energy use data in London households", "from": "SMARTMETER ENERGY USE DATA", "source_id": "d4bc1397526f236029876d4fbc22a721", "to": "ETERS", "width": 1.0}, {"description": "Wind Farms dataset is sourced from Australia wind farm data", "from": "WIND FARMS", "source_id": "d4bc1397526f236029876d4fbc22a721", "to": "AUSTRALIA WIND FARM DATA", "width": 1.0}, {"description": "Australian electricity demand is related to wind farms as both are datasets used for pretraining and fine-tuning", "from": "WIND FARMS", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "AUSTRALIAN ELECTRICITY DEMAND", "width": 1.0}, {"description": "Weather series is related to NOAA as NOAA provides weather data", "from": "NOAA", "source_id": "6820cea275275e87630a6876e890c525", "to": "WEATHER SERIES", "width": 1.0}, {"description": "PM2.5 is related to PM10 as both are measures of particulate matter in the air", "from": "PM2.5", "source_id": "6820cea275275e87630a6876e890c525", "to": "PM10", "width": 1.0}, {"description": "NO2 is related to CO as both are measures of air pollution", "from": "NO2", "source_id": "6820cea275275e87630a6876e890c525", "to": "CO", "width": 1.0}, {"description": "CO is related to O3 as both are measures of air pollution", "from": "CO", "source_id": "6820cea275275e87630a6876e890c525", "to": "O3", "width": 1.0}, {"description": "O3 is related to SO2 as both are measures of air pollution", "from": "O3", "source_id": "6820cea275275e87630a6876e890c525", "to": "SO2", "width": 1.0}, {"description": "Beijing is related to London as both are cities with air quality data", "from": "BEIJING", "source_id": "6820cea275275e87630a6876e890c525", "to": "LONDON", "width": 1.0}, {"description": "The Beijing PM2.5 dataset includes data from Beijing", "from": "BEIJING", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "BEIJING PM2.5 DATASET", "width": 1.0}, {"description": "Temperature is related to rain as rain can affect temperature", "from": "TEMPERATURE", "source_id": "6820cea275275e87630a6876e890c525", "to": "RAIN", "width": 1.0}, {"description": "Snow is related to snow depth as snow depth can affect snow", "from": "SNOW", "source_id": "6820cea275275e87630a6876e890c525", "to": "SNOW DEPTH", "width": 1.0}, {"description": "Minimum temperature is related to maximum temperature as both are measures of temperature", "from": "MINTEMP", "source_id": "6820cea275275e87630a6876e890c525", "to": "MAXTEMP", "width": 1.0}, {"description": "Climate stations are related to Australia as Australia has climate stations", "from": "CLIMATE STATIONS", "source_id": "6820cea275275e87630a6876e890c525", "to": "AUSTRALIA", "width": 1.0}, {"description": "Bikes are related to pedestrian counts as both are measures of transportation", "from": "BIKES", "source_id": "6820cea275275e87630a6876e890c525", "to": "PEDESTRIAN COUNTS", "width": 1.0}, {"description": "Transport and tourism is related to pedestrian counts as pedestrian counts is a dataset used for pretraining and fine-tuning", "from": "PEDESTRIAN COUNTS", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "TRANSPORT \u0026 TOURISM", "width": 1.0}, {"description": "San Francisco traffic is related to pedestrian counts as both are datasets used for pretraining and fine-tuning", "from": "PEDESTRIAN COUNTS", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "SAN FRANCISCO TRAFFIC", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nUber and Lyft are two prominent rideshare services that offer various hourly statistics. Both companies are related as they operate in the same industry, specifically as ride-hailing companies. This connection suggests that they may share similar characteristics, such as providing transportation services to customers, and may have comparable hourly statistics, including metrics like demand, supply, and pricing.\n\nGiven the context of rideshare services, it is likely that the hourly statistics provided for Uber and Lyft include data on the number of rides, revenue, driver availability, and passenger demand. These statistics can be useful for understanding the dynamics of the rideshare market and for making informed decisions about pricing, supply, and demand.\n\nThe relationship between Uber and Lyft as ride-hailing companies also implies that they may face similar challenges and opportunities, such as managing driver supply, optimizing routes, and responding to changes in demand. By analyzing the hourly statistics of both companies, researchers and analysts can gain insights into the rideshare market and develop strategies to improve the efficiency and effectiveness of these services.\n\nOverall, the summary highlights the connection between Uber and Lyft as ride-hailing companies and provides context for understanding the hourly statistics provided for each entity.", "from": "UBER", "source_id": "6820cea275275e87630a6876e890c525,a875a1c0bede47a1c4e3823be81c42c6", "to": "LYFT", "width": 2.0}, {"description": "New York and Melbourne are locations of various data sources", "from": "NEW YORK", "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "to": "MELBOURNE", "width": 1.0}, {"description": "New York and San Francisco Bay area are locations of various data sources", "from": "NEW YORK", "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "to": "SAN FRANCISCO BAY AREA", "width": 1.0}, {"description": "The city of Melbourne is the location where hourly pedestrian counts were recorded", "from": "MELBOURNE", "source_id": "76cc338e586223647fd3dbe4e7a7c131", "to": "PEDIESTRIAN COUNTS", "width": 1.0}, {"description": "Electricity weather CPU usage is related to Uber TLC as Uber TLC is a specific type of electricity weather CPU usage", "from": "UBER TLC", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "ELECTRICITY WEATHER CPU USAGE", "width": 1.0}, {"description": "M1 and M3 are forecasting competitions that contain time series data from various domains", "from": "M1", "source_id": "a875a1c0bede47a1c4e3823be81c42c6", "to": "M3", "width": 1.0}, {"description": "English Wikipedia is related to page views as it contains information on the number of times a Wikipedia article is viewed", "from": "ENGLISH WIKIPEDIA", "source_id": "2565ae205d4c98342168bb67a4f7a309", "to": "PAGE VIEWS", "width": 1.0}, {"description": "NHITS and TimeGPT are both types of time series forecasting models", "from": "NHITS", "source_id": "f912df936d0b735fe654d1a9c53caa3b", "to": "TIMEGPT", "width": 1.0}, {"description": "OneFitsAll is related to Lag-Llama as both are methods for probabilistic forecasting", "from": "LAG-LAGMA", "source_id": "f0c52387a7b3a5c3850fe6991f0a7c83", "to": "ONEFITSSALL", "width": 1.0}, {"description": "Lag-Lagma is related to CRPS as CRPS is a specific type of Lag-Lagma", "from": "LAG-LAGMA", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "CRPS", "width": 1.0}, {"description": "Lag-Lagma is related to supervised baselines as supervised baselines are a specific type of Lag-Lagma", "from": "LAG-LAGMA", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "SUPERVISED BASELINES", "width": 1.0}, {"description": "Multiplier is related to prediction length as prediction length is set based on multiplier", "from": "PREDICTION LENGTH", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "MULTIPLIER", "width": 1.0}, {"description": "Dataset split is related to prediction length as prediction length is used to evaluate models", "from": "PREDICTION LENGTH", "source_id": "51ee17c1f0212d4c94010d8e376f649b", "to": "DATASET SPLIT", "width": 1.0}, {"description": "Window length is related to prediction length as window length and prediction length are used to control the length of the predictions made by a model", "from": "PREDICTION LENGTH", "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "to": "WINDOW LENGTH", "width": 1.0}, {"description": "Frequency is related to multiplier as multiplier is used to set context length based on frequency", "from": "FREQUENCY", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "MULTIPLIER", "width": 1.0}, {"description": "Frequency and sparsity are characteristics of time series that TimeGPT can handle", "from": "FREQUENCY", "source_id": "518bfcd6711530089fe3914ca16459c2", "to": "SPARSITY", "width": 1.0}, {"description": "STATSFORCAST is used with SEASONALNAIVE as a statistical baseline", "from": "STATSFORCAST", "source_id": "1ddbab2dca370c9ef7b5a724075518cc", "to": "SEASONALNAIVE", "width": 1.0}, {"description": "Point forecasts are related to predictions as point forecasts are a type of prediction", "from": "POINT FORECASTS", "source_id": "f0808ad97bc4d26391284145427770e5", "to": "PREDICTIONS", "width": 1.0}, {"description": "MASE is related to point forecasts as MASE is used to evaluate point forecasts", "from": "POINT FORECASTS", "source_id": "f0808ad97bc4d26391284145427770e5", "to": "MEAN ABSOLUTE SCALING ERROR", "width": 1.0}, {"description": "Probabilistic forecasts are related to predictions as probabilistic forecasts are a type of prediction", "from": "PROBABILISTIC FORECASTS", "source_id": "f0808ad97bc4d26391284145427770e5", "to": "PREDICTIONS", "width": 1.0}, {"description": "Seasonal naive model is related to MASE as seasonal naive model is used to estimate the empirical error of a time series", "from": "MEAN ABSOLUTE SCALING ERROR", "source_id": "f0808ad97bc4d26391284145427770e5", "to": "SEASONAL NAIVE MODEL", "width": 1.0}, {"description": "Seasonal Naive model is used as a baseline for comparison with Relative Mean Absolute Error (rMAE)", "from": "SEASONAL NAIVE MODEL", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "RELATIVE MEAN ABSOLUTE ERROR", "width": 1.0}, {"description": "Seasonal Naive model is used as a baseline for comparison with Relative Root Mean Square Error (rRMSE)", "from": "SEASONAL NAIVE MODEL", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "RELATIVE ROOT MEAN SQUARE ERROR", "width": 1.0}, {"description": "Predicted quantiles are related to quantiles as predicted quantiles are a type of quantile", "from": "QUANTILES", "source_id": "f0808ad97bc4d26391284145427770e5", "to": "PREDICTED QUANTILES", "width": 1.0}, {"description": "Continuous ranked probability score is related to Lag-Llama model as continuous ranked probability score is used to evaluate the performance of a model", "from": "CONTINUOUS RANKED PROBABILITY SCORE", "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "to": "LAG-LLAMA MODEL", "width": 1.0}, {"description": "Uncertainty is related to predictions as predictions aim to reduce uncertainty", "from": "PREDICTIONS", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "UNCERTAINTY", "width": 1.0}, {"description": "Ground truths are related to predictions as they are the actual values that the model is trying to predict", "from": "PREDICTIONS", "source_id": "3174231a67593609c727151c9df31d0a", "to": "GROUND TRUTHS", "width": 1.0}, {"description": "Predictions are related to output patches as they are the values that the model outputs", "from": "PREDICTIONS", "source_id": "3174231a67593609c727151c9df31d0a", "to": "OUTPUT PATCHES", "width": 1.0}, {"description": "CRPS is related to data history as CRPS is a metric that is affected by data history", "from": "CRPS", "source_id": "d08a82328191907abfcdb72efab9a6cf", "to": "DATA HISTORY", "width": 1.0}, {"description": "NBEATS is related to CRPS as CRPS is used to evaluate the performance of NBEATS", "from": "CRPS", "source_id": "990578022879395d00b7a5b229863c2f", "to": "NBEATS", "width": 1.0}, {"description": "ETSFORMER is related to CRPS as CRPS is used to evaluate the performance of ETSFORMER", "from": "CRPS", "source_id": "990578022879395d00b7a5b229863c2f", "to": "ETSFORMER", "width": 1.0}, {"description": "TimeGPT is related to Azul Garza as Azul Garza is one of the authors of the paper introducing TimeGPT", "from": "TIMEGPT", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "AZUL GARZA", "width": 1.0}, {"description": "TimeGPT is related to Cristian Challu as Cristian Challu is one of the authors of the paper introducing TimeGPT", "from": "TIMEGPT", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "CRISTIAN CHALLU", "width": 1.0}, {"description": "TimeGPT is related to Max Mergenthaler-Canseco as Max Mergenthaler-Canseco is one of the authors of the paper introducing TimeGPT", "from": "TIMEGPT", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "MAX MERGENTHALER-CANSECO", "width": 1.0}, {"description": "TimeGPT is a pre-trained model that can produce accurate predictions across a diverse array of domains and applications without additional training", "from": "TIMEGPT", "source_id": "6771b6279846e780d6807b15184ae008", "to": "PRE-TRAINED MODEL", "width": 1.0}, {"description": "TimeGPT is based on the Transformer architecture introduced by Vaswani et al.", "from": "TIMEGPT", "source_id": "518bfcd6711530089fe3914ca16459c2", "to": "VASWANI ET AL.", "width": 1.0}, {"description": "TimeGPT uses conformal predictions to estimate prediction intervals based on historic errors", "from": "TIMEGPT", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "CONFORMAL PREDICTIONS", "width": 1.0}, {"description": "Historical values are used as inputs to TimeGPT to produce forecasts", "from": "TIMEGPT", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "HISTORICAL VALUES", "width": 1.0}, {"description": "Exogenous variables are used as inputs to TimeGPT to produce forecasts", "from": "TIMEGPT", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "EXOGENOUS VARIABLES", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nTimeGPT is a machine learning model that is utilized for zero-shot inference. Zero-shot inference, in turn, is closely related to TimeGPT as it has been tested in various zero-shot inference tasks. This suggests that TimeGPT is capable of performing well in scenarios where it has not been explicitly trained on the specific task at hand, relying on its general knowledge and understanding to make accurate predictions.\n\nThe use of TimeGPT for zero-shot inference is a significant aspect of its functionality, and its performance in this area has been evaluated through various tasks. This highlights the potential of TimeGPT as a versatile tool for making predictions and generating insights in a wide range of domains.\n\nOverall, the relationship between TimeGPT and zero-shot inference is a key aspect of its capabilities, and further research and evaluation are likely to be conducted to fully understand its potential and limitations in this area.\n\nRelevant information from the nearby text:\n\n* The text is written in English, which is a formal and academic language.\n* The text contains technical terms, mathematical equations, and references to academic papers, which are all written in English.\n* The use of English words and phrases, such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention,\" further supports the conclusion that the text is written in English.\n\nNote: The provided descriptions are not contradictory, and the summary is a coherent representation of the information provided.", "from": "TIMEGPT", "source_id": "f8afc5a341360ab82dfbe9ebbb7f8e84,f912df936d0b735fe654d1a9c53caa3b", "to": "ZERO-SHOT INFERENCE", "width": 2.0}, {"description": "TimeGPT is compared to benchmark models", "from": "TIMEGPT", "source_id": "f8afc5a341360ab82dfbe9ebbb7f8e84", "to": "BENCHMARK MODELS", "width": 1.0}, {"description": "TimeGPT is related to rMAE as rMAE is a metric used to evaluate the performance of TimeGPT", "from": "TIMEGPT", "source_id": "f912df936d0b735fe654d1a9c53caa3b", "to": "RMAE", "width": 1.0}, {"description": "TimeGPT is related to rRMSE as rRMSE is a metric used to evaluate the performance of TimeGPT", "from": "TIMEGPT", "source_id": "f912df936d0b735fe654d1a9c53caa3b", "to": "RRMSE", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe primary entities involved are \"TIMEGPT\" and \"PYTHON SDK\". \n\nTIMEGPT is a time series forecasting model that can be utilized through a software development kit (SDK). The PYTHON SDK is specifically designed for this purpose, allowing developers to leverage the capabilities of TIMEGPT in their applications.\n\nThe relationship between TIMEGPT and PYTHON SDK is that the latter provides a platform for accessing and utilizing the former. This enables developers to integrate TIMEGPT\u0027s time series forecasting capabilities into their projects using the PYTHON SDK.\n\nOverall, the connection between TIMEGPT and PYTHON SDK is one of integration and accessibility, with the SDK serving as a gateway to the TIMEGPT model\u0027s functionality.", "from": "TIMEGPT", "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce,b8ce119147e4d1454453c514e68dc4dc", "to": "PYTHON SDK", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTimeGPT and REST API are two entities that are closely related. TimeGPT is a software interface that can be accessed through a REST API, which is a software interface for accessing TimeGPT. This suggests a client-server architecture where TimeGPT is the server and the REST API is the interface through which clients can interact with TimeGPT.\n\nIn other words, the REST API serves as a gateway to TimeGPT, allowing users to access its functionality and features through a standardized interface. This relationship between TimeGPT and REST API enables seamless communication and interaction between the two entities, facilitating the exchange of data and services.\n\nOverall, the summary highlights the interdependence of TimeGPT and REST API, with the latter providing a critical interface for accessing the former\u0027s capabilities.", "from": "TIMEGPT", "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce,b8ce119147e4d1454453c514e68dc4dc", "to": "REST API", "width": 2.0}, {"description": "Nixtla is the company behind TimeGPT", "from": "TIMEGPT", "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce", "to": "NIXTLA", "width": 1.0}, {"description": "Azul Garza and Cristian Challu are related as they are co-authors of the paper introducing TimeGPT", "from": "AZUL GARZA", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "CRISTIAN CHALLU", "width": 1.0}, {"description": "Azul Garza and Max Mergenthaler-Canseco are related as they are co-authors of the paper introducing TimeGPT", "from": "AZUL GARZA", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "MAX MERGENTHALER-CANSECO", "width": 1.0}, {"description": "Nixtla is related to Azul Garza as Azul Garza is affiliated with Nixtla", "from": "AZUL GARZA", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "NIXTLA", "width": 1.0}, {"description": "Cristian Challu and Max Mergenthaler-Canseco are related as they are co-authors of the paper introducing TimeGPT", "from": "CRISTIAN CHALLU", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "MAX MERGENTHALER-CANSECO", "width": 1.0}, {"description": "Nixtla is related to Cristian Challu as Cristian Challu is affiliated with Nixtla", "from": "CRISTIAN CHALLU", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "NIXTLA", "width": 1.0}, {"description": "References are related to Cristian Challu as Cristian Challu is an author of one of the referenced papers", "from": "CRISTIAN CHALLU", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "REFERENCES", "width": 1.0}, {"description": "Nixtla is related to Max Mergenthaler-Canseco as Max Mergenthaler-Canseco is affiliated with Nixtla", "from": "MAX MERGENTHALER-CANSECO", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "NIXTLA", "width": 1.0}, {"description": "San Francisco is related to Nixtla as Nixtla is located in San Francisco", "from": "NIXTLA", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "SAN FRANCISCO", "width": 1.0}, {"description": "USA is related to Nixtla as Nixtla is located in the USA", "from": "NIXTLA", "source_id": "f8f6fec6d1d7eda0349d3c52c4c96d97", "to": "USA", "width": 1.0}, {"description": "API key is used to access TimeGPT through Nixtla", "from": "NIXTLA", "source_id": "6383536333b1a09cc9e6f8d4ea5e9bce", "to": "API KEY", "width": 1.0}, {"description": "CES and XGBoost are machine learning models used for time series forecasting", "from": "CES", "source_id": "6771b6279846e780d6807b15184ae008", "to": "XGBOOST", "width": 1.0}, {"description": "Benidis et al. and Kunz et al. are researchers who have contributed to the field of time series analysis", "from": "BENIDIS ET AL.", "source_id": "6771b6279846e780d6807b15184ae008", "to": "KUNZ ET AL.", "width": 1.0}, {"description": "Hybrid forecasting is related to literature review as literature review is a document that summarizes and analyzes the existing research on a particular topic", "from": "HYBRID FORECASTING", "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "to": "LITERATURE REVIEW", "width": 1.0}, {"description": "Review and taxonomy is related to M4 competition as M4 competition is a competition that challenges teams to develop and evaluate time series forecasting models", "from": "REVIEW AND TAXONOMY", "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "to": "M4 COMPETITION", "width": 1.0}, {"description": "ESRNN is related to DPMN as DPMN is a type of neural network that is designed for time series forecasting", "from": "ESRNN", "source_id": "4de223d7df157faf857c3e17d9e6e5b6", "to": "DPMN", "width": 1.0}, {"description": "Transformer-based models are used in models like MQ-Transformer [Eisenach et al., 2020]", "from": "TRANSFORMER-BASED MODELS", "source_id": "7d5d82d600620153153772a9bc498ac0", "to": "MQ-TRANSFORMER", "width": 1.0}, {"description": "Scaling laws refer to the existence of laws on data and model sizes for Transformer architectures on time series forecasting tasks, which is related to transformer-based models", "from": "TRANSFORMER-BASED MODELS", "source_id": "7d5d82d600620153153772a9bc498ac0", "to": "SCALING LAWS", "width": 1.0}, {"description": "Input embedding is related to output embedding as output embedding is used to enrich the output", "from": "INPUT EMBEDDING", "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "to": "OUTPUT EMBEDDING", "width": 1.0}, {"description": "Output embedding is related to inputs (shifted right) as inputs (shifted right) is used to enrich the input", "from": "OUTPUT EMBEDDING", "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "to": "INPUTS (SHIFTED RIGHT)", "width": 1.0}, {"description": "Forecasting task is related to conditional distribution as conditional distribution is used to estimate the forecasting task", "from": "FORECASTING TASK", "source_id": "89e5f6a93205d5e87ad7e7641c0bbb91", "to": "CONDITIONAL DISTRIBUTION", "width": 1.0}, {"description": "Encoder-decoder structure uses residual connections", "from": "ENCODER-DECODER", "source_id": "518bfcd6711530089fe3914ca16459c2", "to": "RESIDUAL CONNECTIONS", "width": 1.0}, {"description": "Encoder-decoder structure uses layer normalization", "from": "ENCODER-DECODER", "source_id": "518bfcd6711530089fe3914ca16459c2", "to": "LAYER NORMALIZATION", "width": 1.0}, {"description": "Air Quality is related to Transport as both are domains used for fine-tuning forecasting tasks", "from": "TRANSPORT", "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26", "to": "AIR QUALITY", "width": 1.0}, {"description": "Instances are related to transport as transport is a specific type of instance", "from": "TRANSPORT", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "INSTANCES", "width": 1.0}, {"description": "GPU cluster is related to PyTorch as PyTorch was used to implement TimeGPT on the GPU cluster", "from": "GPU CLUSTER", "source_id": "42e1bb44edbe787e104f589e74b95d6c", "to": "PYTORCH", "width": 1.0}, {"description": "PyTorch is related to Adam as Adam was used to optimize TimeGPT\u0027s performance", "from": "PYTORCH", "source_id": "42e1bb44edbe787e104f589e74b95d6c", "to": "ADAM", "width": 1.0}, {"description": "Matplotlib and PyTorch are open-source libraries used in the project", "from": "PYTORCH", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "MATPLOTLIB", "width": 1.0}, {"description": "Adam is related to confidence interval as confidence intervals were used to estimate TimeGPT\u0027s uncertainty", "from": "ADAM", "source_id": "42e1bb44edbe787e104f589e74b95d6c", "to": "CONFIDENCE INTERVAL", "width": 1.0}, {"description": "Error and confidence interval are related as they are both concepts used to evaluate the performance of models, as indicated by the use of the \u00b1 symbol in the table", "from": "CONFIDENCE INTERVAL", "source_id": "1dc665214307b1656d1a0094a1918ece", "to": "ERROR", "width": 1.0}, {"description": "Conformal predictions are used to estimate prediction intervals", "from": "CONFORMAL PREDICTIONS", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "PREDICTION INTERVALS", "width": 1.0}, {"description": "Input is embedded as a vector in a high-dimensional space", "from": "INPUT", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "EMBEDDING", "width": 1.0}, {"description": "Embedding is used to represent a data point as a vector in a high-dimensional space", "from": "EMBEDDING", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "OUTPUT", "width": 1.0}, {"description": "Relative Mean Absolute Error (rMAE) and Relative Root Mean Square Error (rRMSE) are metrics used to evaluate the performance of a model", "from": "RELATIVE MEAN ABSOLUTE ERROR", "source_id": "fb67fcff21ac521a5ed8b202412ec1fc", "to": "RELATIVE ROOT MEAN SQUARE ERROR", "width": 1.0}, {"description": "rMAE and rRMSE are metrics used to evaluate the performance of a model", "from": "RMAE", "source_id": "f8afc5a341360ab82dfbe9ebbb7f8e84", "to": "RRMSE", "width": 1.0}, {"description": "Equation 2 is related to rMAE as Equation 2 is used to compute the rMAE metric", "from": "RMAE", "source_id": "f912df936d0b735fe654d1a9c53caa3b", "to": "EQUATION 2", "width": 1.0}, {"description": "Task-specific dataset refers to a dataset that is tailored to a specific task or domain, and fine-tuning steps refer to the process of adjusting model parameters on a task-specific dataset", "from": "TASK-SPECIFIC DATASET", "source_id": "55eb54ef455d14cd1a11760924f99eb8", "to": "FINE-TUNING STEPS", "width": 1.0}, {"description": "Model parameters refer to the adjustable values in a model that are adjusted during fine-tuning, and fine-tuning steps refer to the process of adjusting model parameters on a task-specific dataset", "from": "MODEL PARAMETERS", "source_id": "55eb54ef455d14cd1a11760924f99eb8", "to": "FINE-TUNING STEPS", "width": 1.0}, {"description": "Task-specific task refers to a task that is tailored to a specific domain or context, and domain-specific applications refer to tasks or domains that require specialized models or techniques", "from": "TASK-SPECIFIC TASK", "source_id": "55eb54ef455d14cd1a11760924f99eb8", "to": "DOMAIN-SPECIFIC APPLICATIONS", "width": 1.0}, {"description": "Parallel computing-optimized statistical methods refer to methods that are optimized for parallel computing, and Numba compiling is a method used to optimize code for parallel computing", "from": "PARALLEL COMPUTING-OPTIMIZED STATISTICAL METHODS", "source_id": "55eb54ef455d14cd1a11760924f99eb8", "to": "NUMBA COMPILING", "width": 1.0}, {"description": "OpenAI and Boris N Oreshkin are authors of the paper \"N-beats: Neural basis expansion analysis for interpretable time series forecasting\"", "from": "OPENAI", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "BORIS N ORESHKIN", "width": 1.0}, {"description": "OpenAI and Dmitri Carpov are authors of the paper \"N-beats: Neural basis expansion analysis for interpretable time series forecasting\"", "from": "OPENAI", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "DMITRI CARPOV", "width": 1.0}, {"description": "OpenAI and Nicolas Chapados are authors of the paper \"N-beats: Neural basis expansion analysis for interpretable time series forecasting\"", "from": "OPENAI", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "NICOLAS CHAPADOS", "width": 1.0}, {"description": "OpenAI and Yoshua Bengio are authors of the paper \"N-beats: Neural basis expansion analysis for interpretable time series forecasting\"", "from": "OPENAI", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "YOSHUA BEN-GIO", "width": 1.0}, {"description": "Several domains are represented in the time series data used to train forecasting models", "from": "DOMAINS", "source_id": "c7167cf92abe7513ccb936ca79871932", "to": "FORECASTING MODELS", "width": 1.0}, {"description": "Domains are related to demographic as demographic is a specific domain or category of data or information", "from": "DOMAINS", "source_id": "1b48e9ca066ac5ba037066bb762d3458", "to": "DEMOGRAPHIC", "width": 1.0}, {"description": "Domains are related to micro as micro is a specific domain or category of data or information", "from": "DOMAINS", "source_id": "1b48e9ca066ac5ba037066bb762d3458", "to": "MICRO", "width": 1.0}, {"description": "Domains are related to macro as macro is a specific domain or category of data or information", "from": "DOMAINS", "source_id": "1b48e9ca066ac5ba037066bb762d3458", "to": "MACRO", "width": 1.0}, {"description": "Domains are related to industry as industry is a specific domain or category of data or information", "from": "DOMAINS", "source_id": "1b48e9ca066ac5ba037066bb762d3458", "to": "INDUSTRY", "width": 1.0}, {"description": "Forecasting models are compared to prior deep learning approaches", "from": "FORECASTING MODELS", "source_id": "c7167cf92abe7513ccb936ca79871932", "to": "DEEP LEARNING APPROACHES", "width": 1.0}, {"description": "Open time series data is related to Lag-Llama as it was used to train Lag-Llama", "from": "LAG-LAGA", "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "to": "OPEN TIME SERIES DATA", "width": 1.0}, {"description": "Unseen time series datasets are related to Lag-Llama as Lag-Llama was evaluated on these datasets", "from": "LAG-LAGA", "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "to": "UNSEEN TIME SERIES DATASETS", "width": 1.0}, {"description": "State-of-the-art dataset-specific models are related to Lag-Llama as Lag-Llama was compared to these models", "from": "LAG-LAGA", "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "to": "STATE-OF-THE-ART DATASET-SPECIFIC MODELS", "width": 1.0}, {"description": "Few-shot adaptation is related to Lag-Llama as Lag-Llama demonstrated strong few-shot adaptation performance", "from": "LAG-LAGA", "source_id": "ca3dfe42c66d68dd6dbad037936b2360", "to": "FEW-SHOT ADAPTATION", "width": 1.0}, {"description": "Few-shot adaptation is related to Lag-Llama as Lag-Llama is a model that can perform few-shot adaptation", "from": "FEW-SHOT ADAPTATION", "source_id": "d08a82328191907abfcdb72efab9a6cf", "to": "LAG-LAGGAGE", "width": 1.0}, {"description": "Few-shot adaptation is related to data history as few-shot adaptation is a scenario where data history is limited", "from": "FEW-SHOT ADAPTATION", "source_id": "d08a82328191907abfcdb72efab9a6cf", "to": "DATA HISTORY", "width": 1.0}, {"description": "Statistical models are related to theta models as theta models are a type of statistical model", "from": "THETA MODELS", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "STATISTICAL MODELS", "width": 1.0}, {"description": "AutoGluon is related to statistical models as statistical models are used to make predictions", "from": "STATISTICAL MODELS", "source_id": "51ee17c1f0212d4c94010d8e376f649b", "to": "AUTOGLUON", "width": 1.0}, {"description": "Statistical models are related to deep neural networks as deep neural networks are used to make predictions", "from": "STATISTICAL MODELS", "source_id": "51ee17c1f0212d4c94010d8e376f649b", "to": "DEEP NEURAL NETWORKS", "width": 1.0}, {"description": "Neural forecasting is related to RNN-based models as RNN-based models are a type of neural model used in time series forecasting", "from": "NEURAL FORECASTING", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "RNN-BASED MODELS", "width": 1.0}, {"description": "Neural forecasting is related to LSTM-based models as LSTM-based models are a type of neural model used in time series forecasting", "from": "NEURAL FORECASTING", "source_id": "2e2e2fa4e717e09d31996f7f22bd50a0", "to": "LSTM-BASED MODELS", "width": 1.0}, {"description": "The foundation model approach is related to transfer capabilities as it enables models to perform well on different tasks or domains", "from": "FOUNDATION MODEL APPROACH", "source_id": "c4e47033b8ecc85beacde9d560e66062", "to": "TRANSFER CAPABILITIES", "width": 1.0}, {"description": "Meta learning is related to neural network as it is used to learn temporal trends in the input training time-series with limited data", "from": "NEURAL NETWORK", "source_id": "6ea15432a841705c2e74cbc01f6004e9", "to": "META LEARNING", "width": 1.0}, {"description": "Neural network is related to loss function as it is used to evaluate the performance of the model", "from": "NEURAL NETWORK", "source_id": "6ea15432a841705c2e74cbc01f6004e9", "to": "LOSSES FUNCTION", "width": 1.0}, {"description": "Lagged features are constructed using lag indices", "from": "LAGGED FEATURES", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "LAG INDICES", "width": 1.0}, {"description": "Lag indices include second-level frequency", "from": "LAG INDICES", "source_id": "ac37bdb991d1513e44e4c5fc7a28b187", "to": "SECOND-LEVEL FREQUENCY", "width": 1.0}, {"description": "Lagged features are combined with date-time features to create a larger window of historical points", "from": "LAG-LAGGED FEATURES", "source_id": "55099ca8e21d1db3bdaf7bd04e590b72", "to": "DATE-TIME FEATURES", "width": 1.0}, {"description": "Lagged features are used in the Lag-Llama architecture", "from": "LAG-LAGGED FEATURES", "source_id": "55099ca8e21d1db3bdaf7bd04e590b72", "to": "LLAMA", "width": 1.0}, {"description": "Date-time features are used in the Lag-Llama architectureThe Lag-Llama architecture incorporates date-time features", "from": "DATE-TIME FEATURES", "source_id": "55099ca8e21d1db3bdaf7bd04e590b72", "to": "LLAMA", "width": 2.0}, {"description": "LLama is a foundation language model that can perform well on a diverse range of NLP tasks in a few-shot or even zero-shot setting", "from": "LLAMA", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "FOUNDATION LANGUAGE MODELS", "width": 1.0}, {"description": "LLAMA is a foundation language model developed by Baptiste Rozi\u00e8re", "from": "LLAMA", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "BAPTISTE ROZIERE", "width": 1.0}, {"description": "LLAMA is related to GPT-4 as they are both language models", "from": "LLAMA", "source_id": "e6a6bc6fbd362394320961ac10cdd230", "to": "GPT-4", "width": 1.0}, {"description": "Touvron et al. (2023) is a paper that discusses Llama", "from": "TOUVRON ET AL. (2023)", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "FOUNDATION LANGUAGE MODELS", "width": 1.0}, {"description": "Parametric distribution head is used to output the parameters of a Student\u2019s t-distribution", "from": "PARAMETRIC DISTRIBUTION HEAD", "source_id": "b305a0d76a0159dc10338467e16d6761", "to": "STUDENT\u0027S T-DISTRIBUTION", "width": 1.0}, {"description": "Parametric distribution head is used to output the parameters of a normalizing flow", "from": "PARAMETRIC DISTRIBUTION HEAD", "source_id": "b305a0d76a0159dc10338467e16d6761", "to": "NORMALIZING FLOWS", "width": 1.0}, {"description": "Parametric distribution head is used to output the parameters of a copula", "from": "PARAMETRIC DISTRIBUTION HEAD", "source_id": "b305a0d76a0159dc10338467e16d6761", "to": "COPULAS", "width": 1.0}, {"description": "RMSNorm is used in conjunction with parametric distribution head in Lag-Llama", "from": "PARAMETRIC DISTRIBUTION HEAD", "source_id": "b305a0d76a0159dc10338467e16d6761", "to": "RMSNORM", "width": 1.0}, {"description": "Rotary Positional Encoding is used in conjunction with RMSNorm in Lag-Llama", "from": "ROTARY POSITIONAL ENCODING", "source_id": "b305a0d76a0159dc10338467e16d6761", "to": "RMSNORM", "width": 1.0}, {"description": "Model training is related to model optimization as optimization is a process that is used to improve the performance of a model", "from": "MODEL OPTIMIZATION", "source_id": "00007df4774d6122e3848802a24f9536", "to": "MODEL TRAINING", "width": 1.0}, {"description": "A parametric distributional head is related to model training as it is a component of a model that is used to generate probability distributions", "from": "PARAMETRIC DISTRIBUTIONAL HEAD", "source_id": "00007df4774d6122e3848802a24f9536", "to": "MODEL TRAINING", "width": 1.0}, {"description": "A scaling heuristic is related to the mean value as it is an algorithm that is used to scale the values of a time series", "from": "SCALING HEURISTIC", "source_id": "00007df4774d6122e3848802a24f9536", "to": "MEAN VALUE", "width": 1.0}, {"description": "Time series augmentation is related to Freq-mix as it is a process that is used to modify a time series to create new data points", "from": "TIME SERIES AUGMENTATION", "source_id": "00007df4774d6122e3848802a24f9536", "to": "FREQ-MIX", "width": 1.0}, {"description": "Based on the provided information, the expert Data Scientist has analyzed the data and generated a comprehensive summary of the entities and their descriptions.\n\nThe entities in question are \"HYPERPARAMETER SEARCH\" and \"MODEL TRAINING\". \n\nThe descriptions provided for these entities are as follows:\n\n1. \"Hyperparameter search is related to model training as hyperparameter search is used in model training\"\n2. \"Hyperparameter search is related to model training as hyperparameter search is used to select the best hyperparameters for a model\"\n\nAfter analyzing these descriptions, the Data Scientist has determined that they are not contradictory, but rather complementary. Both descriptions highlight the relationship between hyperparameter search and model training, but from different perspectives.\n\nThe first description emphasizes that hyperparameter search is a part of the model training process, implying that it is an integral component of the training workflow.\n\nThe second description takes a more specific approach, stating that hyperparameter search is used to select the best hyperparameters for a model, which is a crucial step in the model training process.\n\nTaking both descriptions into account, the Data Scientist has generated a comprehensive summary of the entities and their relationships:\n\n\"Hyperparameter search is a critical component of model training, as it is used to select the best hyperparameters for a model. This process is an integral part of the model training workflow, enabling the development of high-performing models. By leveraging hyperparameter search, model training can be optimized, leading to improved model accuracy and efficiency.\"\n\nThis summary provides a clear and concise understanding of the relationship between hyperparameter search and model training, highlighting the importance of hyperparameter search in the model training process.", "from": "HYPERPARAMETER SEARCH", "source_id": "f0c52387a7b3a5c3850fe6991f0a7c83,fa01b75dccc556af8aca0d6c47e1970f", "to": "MODEL TRAINING", "width": 2.0}, {"description": "Pretraining data is related to the corpus of datasets, as indicated by the use of the phrase \"corpus of datasets\"", "from": "CORPUS OF DATASETS", "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "to": "PRETRAINING DATA", "width": 1.0}, {"description": "The corpus of datasets is related to zero-shot generalization, as indicated by the use of the phrase \"zero-shot generalization\"", "from": "CORPUS OF DATASETS", "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "to": "ZERO-SHOT GENERALIZATION", "width": 1.0}, {"description": "Nature is related to sunspot as sunspot is a dataset used for pretraining and fine-tuning", "from": "NATURE", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "SUNSPOT", "width": 1.0}, {"description": "Air Quality is related to Nature as both are domains used for fine-tuning forecasting tasks", "from": "NATURE", "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26", "to": "AIR QUALITY", "width": 1.0}, {"description": "Instances are related to nature as nature is a specific type of instance", "from": "NATURE", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "INSTANCES", "width": 1.0}, {"description": "Air Quality is related to Cloud as both are domains used for fine-tuning forecasting tasks", "from": "AIR QUALITY", "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26", "to": "CLOUD", "width": 1.0}, {"description": "Instances are related to air quality as air quality is a specific type of instance", "from": "AIR QUALITY", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "INSTANCES", "width": 1.0}, {"description": "Model training is related to validation loss as validation loss is used to evaluate the performance of a model during training", "from": "MODEL TRAINING", "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "to": "VALIDATION LOSS", "width": 1.0}, {"description": "Pretraining corpus is related to dataset split as dataset split is used to divide data into training and testing sets", "from": "PRETRAINING CORPUS", "source_id": "51ee17c1f0212d4c94010d8e376f649b", "to": "DATASET SPLIT", "width": 1.0}, {"description": "Validation loss is related to pretraining corpus as pretraining corpus is used to pretrain a model", "from": "PRETRAINING CORPUS", "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "to": "VALIDATION LOSS", "width": 1.0}, {"description": "Diverse datasets are related to the pretraining corpus, as indicated by the use of the phrase \"pretraining corpus\"", "from": "PRETRAINING CORPUS", "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "to": "DIVERSE DATASETS", "width": 1.0}, {"description": "AutoML is related to AutoGluon as AutoGluon is used to automate machine learning tasks", "from": "AUTOML", "source_id": "51ee17c1f0212d4c94010d8e376f649b", "to": "AUTOGLUON", "width": 1.0}, {"description": "Croston\u0027s seasonal batch arrival model and dynamic optimization are both models used for time series forecasting", "from": "CROSTONSBA", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "DYNAMICOPTIMIZE", "width": 1.0}, {"description": "Croston\u0027s seasonal batch arrival model and NPts are both models used for time series forecasting", "from": "CROSTONSBA", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "NPTS", "width": 1.0}, {"description": "Croston\u0027s seasonal batch arrival model and Temporal Fusion Transformer are both models used for time series forecasting", "from": "CROSTONSBA", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "TEMPORALFUSIONT", "width": 1.0}, {"description": "Croston\u0027s seasonal batch arrival model and N-BEATS are both models used for time series forecasting", "from": "CROSTONSBA", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "NBEATS", "width": 1.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entities \"NPTS\" and \"DYNAMICOPTIMIZE\" are both models used for time series forecasting. They are dynamic optimization models that utilize techniques for predicting future values in a time series. The primary language of the text describing these entities is English, as indicated by the use of technical terms, mathematical equations, and references to academic papers written in English.\n\nThe models, \"NPTS\" and \"DYNAMICOPTIMIZE\", are likely used in various applications, including long-term series forecasting, frequency analysis, and multi-head cross-attention. The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" suggests that the text is from a research paper or a technical document related to these entities.\n\nOverall, the summary provides a clear understanding of the entities \"NPTS\" and \"DYNAMICOPTIMIZE\" as dynamic optimization models used for time series forecasting, with a primary language of English.", "from": "NPTS", "source_id": "376897a501ac50834f626fcc5fb13ece,6fa5f635ad5f7e6b67d5e467f130345c", "to": "DYNAMICOPTIMIZE", "width": 2.0}, {"description": "NPts and Temporal Fusion Transformer are both models used for time series forecasting", "from": "NPTS", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "TEMPORALFUSIONT", "width": 1.0}, {"description": "NPts and N-BEATS are both models used for time series forecasting", "from": "NPTS", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "NBEATS", "width": 1.0}, {"description": "ETSFormer is related to OneFitsAll as both are methods for probabilistic forecasting", "from": "ETSFORMER", "source_id": "f0c52387a7b3a5c3850fe6991f0a7c83", "to": "ONEFITSSALL", "width": 1.0}, {"description": "NBEATS and ETSFORMER are models mentioned in the text, as indicated by the use of their names in the table", "from": "ETSFORMER", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "NBEATS", "width": 1.0}, {"description": "ETSFORMER and LAGLLAMA are models mentioned in the text, as indicated by the use of their names in the table", "from": "ETSFORMER", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "LAGLLAMA", "width": 1.0}, {"description": "ILI is related to ETSformer as it is a forecasting horizon used in the model", "from": "ETSFORMER", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "ILI", "width": 8.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"ETSFORMER\" and \"STATIONARY\" are related in the context of time series analysis and modeling. Specifically, \"STATIONARY\" is a type of time series that is used in the \"ETSFORMER\" model, which is a model mentioned in the text. This relationship suggests that the \"ETSFORMER\" model is designed to handle and analyze stationary time series data.\n\nIn the context of time series forecasting, \"ETSFORMER\" is a model that utilizes techniques such as frequency analysis and multi-head cross-attention to make predictions. The use of stationary time series data in this model is likely due to the fact that stationary time series exhibit constant mean and variance over time, making them more amenable to forecasting.\n\nOverall, the relationship between \"ETSFORMER\" and \"STATIONARY\" highlights the importance of stationary time series data in time series forecasting and modeling, and suggests that the \"ETSFORMER\" model is well-suited to handle such data.\n\nNote: The summary is written in third person and includes information collected from all the descriptions. Any contradictions have been resolved to provide a single, coherent summary. Relevant information from the nearby text has been incorporated to enrich the summary.", "from": "ETSFORMER", "source_id": "9ba0189af2ef0720a721c16eef0f0788,fd1092903d83bf6e90a6caa371d7c514", "to": "STATIONARY", "width": 2.0}, {"description": "Former and ETSformer are both types of models used for time series forecasting", "from": "ETSFORMER", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c", "to": "FORMER", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe primary language of the text is English, as indicated by the use of English words and phrases, mathematical equations, and references to academic papers written in English. The language used is formal and academic, suggesting that the text is from a research paper or a technical document.\n\nThe text discusses two entities, \"ETSFORMER\" and \"LIGHTTS\", which are both types of models used for time series forecasting. Specifically, ETSformer and LightTS are related to each other as both are models mentioned in the text. This suggests that they may share similar characteristics or applications in the context of time series forecasting.\n\nGiven the information provided, it appears that ETSformer and LightTS are both models designed for time series forecasting, and they are mentioned together in the text, indicating a potential connection or similarity between them. However, without further information, it is not possible to determine the exact nature of their relationship or how they differ from each other.\n\nOverall, the summary provides a concise description of the entities and their relationship, while also highlighting the primary language of the text and the context in which they are discussed.", "from": "ETSFORMER", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c,fd1092903d83bf6e90a6caa371d7c514", "to": "LIGHTTS", "width": 2.0}, {"description": "ETSformer and Reformer are both types of models used for time series forecasting", "from": "ETSFORMER", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c", "to": "REFORMER", "width": 1.0}, {"description": "Validation loss is related to performance as it is a measure of the model\u0027s performance on a validation set", "from": "VALIDATION LOSS", "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "to": "PERFORMANCE", "width": 1.0}, {"description": "Early stopping criterion is related to window length as early stopping criterion and window length are used to control the length of the windows used during training", "from": "EARLY STOPPING CRITERION", "source_id": "fa01b75dccc556af8aca0d6c47e1970f", "to": "WINDOW LENGTH", "width": 1.0}, {"description": "Lag-Llama is related to cold-start problem as Lag-Llama is a model that can handle cold-start problem", "from": "LAG-LAGGAGE", "source_id": "d08a82328191907abfcdb72efab9a6cf", "to": "COLD-START PROBLEM", "width": 1.0}, {"description": "40% and 60% are related as they are both percentages, as indicated by the use of the percentage sign in the table", "from": "40 %", "source_id": "1dc665214307b1656d1a0094a1918ece", "to": "60 %", "width": 1.0}, {"description": "Lag-Llama achieves strong performance in the ETT-M2 and requests datasets", "from": "ETT-M2", "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "to": "REQUESTS", "width": 1.0}, {"description": "Lag-Llama achieves comparable performance to the state-of-the-art in the exchange-rate dataset", "from": "REQUESTS", "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "to": "EXCHANGE-RATE", "width": 1.0}, {"description": "Function delay is related to requests as requests are a specific type of function delay", "from": "REQUESTS", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "FUNCTION DELAY", "width": 1.0}, {"description": "Lag-Llama achieves significantly better performance than OneFitsAll model in all datasets except for beijing-p", "from": "ONEFITSAALL", "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "to": "LAG-LAG", "width": 1.0}, {"description": "Tay et al. and Zhou et al. studied the influence of inductive bias at scale in the NLP community and proposed the OneFitsAll model, respectively", "from": "TAY ET AL.", "source_id": "bcf830b85e12e1ab9498e3ac3593a88e", "to": "ZHOU ET AL.", "width": 1.0}, {"description": "Zhu et al. is related to OneFitsAll model as they are the authors of the paper OneFitsAll model", "from": "ONEFITSALL MODEL", "source_id": "6ae1ee8f0c4bcf7ec4d04b1048451e96", "to": "ZHU ET AL.", "width": 1.0}, {"description": "Canonical time series characteristics are used in PCA to assess diversity across datasets", "from": "CANONICAL TIME SERIES CHARACTERISTICS", "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "to": "PCA", "width": 1.0}, {"description": "Catch22 is related to PCA as PCA is a specific type of Catch22", "from": "PCA", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "CATCH22", "width": 1.0}, {"description": "HCTSA library is used in the appendix to select features for classification ability", "from": "HCTSA LIBRARY", "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "to": "APPENDIX", "width": 1.0}, {"description": "Appendix discusses the performance of Lag-Llama in subsection 6.1", "from": "APPENDIX", "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "to": "SUBSECTION 6.1", "width": 1.0}, {"description": "Subsection 6.1 discusses the performance of Lag-Llama compared to state-of-the-art", "from": "SUBSECTION 6.1", "source_id": "6c11bd339c9630f1d61f2024e90bce5e", "to": "STATE-OF-THE-ART", "width": 1.0}, {"description": "Dataset-specific models are related to state-of-the-art performance, as indicated by the use of the phrase \"state-of-the-art performance\"", "from": "DATASET-SPECIFIC MODELS", "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "to": "STATE-OF-THE-ART PERFORMANCE", "width": 1.0}, {"description": "Monash time series repository dataset is related to Hugging Face datasets, as indicated by the use of the phrase \"Hugging Face datasets\"", "from": "MONASH TIME SERIES REPOSITORY DATASET", "source_id": "1727ee77a376bbef1c7625a001e4ac3d", "to": "HUGGING FACE DATASETS", "width": 1.0}, {"description": "Roland worked with the code and experiments of the One-FitsAll model in the project", "from": "ROLAND", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "ONE-FITS-ALL MODEL", "width": 1.0}, {"description": "Nadhir integrated the N-BEATS model in the project", "from": "NADHIR", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "N-BEATS MODEL", "width": 1.0}, {"description": "Marin wrote the initial code for sampling windows using GluonTS code in the project", "from": "MARIN", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "GLUONTS CODE", "width": 1.0}, {"description": "Sahil advised the project as a whole and provided feedback on the experiments and the paper", "from": "SAHIL", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "PROJECT", "width": 1.0}, {"description": "Anderson advised the project as a whole and provided feedback on the experiments and the paper", "from": "ANDERSON", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "PROJECT", "width": 1.0}, {"description": "Nicolas advised the project as a whole and provided feedback on the experiments and the paper", "from": "NICOLAS", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "PROJECT", "width": 1.0}, {"description": "Alexandre advised the project as a whole and provided feedback on the experiments and the paper", "from": "ALEXANDRE", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "PROJECT", "width": 1.0}, {"description": "The project is related to the paper as the paper is a part of the project", "from": "PAPER", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "PROJECT", "width": 1.0}, {"description": "Valentina advised the project as a whole and provided feedback on the experiments and the paper", "from": "VALENTINA", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "PROJECT", "width": 1.0}, {"description": "Yuriy advised the project as a whole and provided feedback on the experiments and the paper", "from": "YURIY", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "PROJECT", "width": 1.0}, {"description": "Irina advised the project as a whole and provided feedback on the experiments and the paper", "from": "IRINA", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "PROJECT", "width": 1.0}, {"description": "Viatcheslav Gurev provided useful discussions during the project", "from": "VIATCHESLAV GUREV", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "PROJECT", "width": 1.0}, {"description": "GluonTS code was used in experimental setups in the project", "from": "GLUONTS CODE", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "EXPERIMENTAL SETUPS", "width": 1.0}, {"description": "NumPy and Pandas are open-source libraries used in the project", "from": "NUMPY", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "PANDAS", "width": 1.0}, {"description": "The Canada CIFAR AI Chair Program and the Canada Excellence Research Chairs (CERC) Program supported the project", "from": "CANADA CIFAR AI CHAIR PROGRAM", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "CANADA EXCELLENCE RESEARCH CHAIRS (CERC) PROGRAM", "width": 1.0}, {"description": "The Oak Ridge Leadership Computing Facility and the Oak Ridge National Laboratory provided compute resources for the project", "from": "OAK RIDGE LEADERSHIP COMPUTING FACILITY", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "OAK RIDGE NATIONAL LABORATORY", "width": 1.0}, {"description": "ServiceNow and Mila provided compute resources for the project", "from": "SERVICE NOW", "source_id": "88d96b5f76042688e9dd745eb822d919", "to": "MILA", "width": 1.0}, {"description": "The Australian Electricity Demand dataset includes data from Victoria", "from": "AUSTRALIAN ELECTRICITY DEMAND DATASET", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "VICTORIA", "width": 1.0}, {"description": "The Australian Electricity Demand dataset includes data from New South Wales", "from": "AUSTRALIAN ELECTRICITY DEMAND DATASET", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "NEW SOUTH WALES", "width": 1.0}, {"description": "The Australian Electricity Demand dataset includes data from Queensland", "from": "AUSTRALIAN ELECTRICITY DEMAND DATASET", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "QUEENSLAND", "width": 1.0}, {"description": "The Australian Electricity Demand dataset includes data from Tasmania", "from": "AUSTRALIAN ELECTRICITY DEMAND DATASET", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "TASMANIA", "width": 1.0}, {"description": "The Australian Electricity Demand dataset includes data from South Australia", "from": "AUSTRALIAN ELECTRICITY DEMAND DATASET", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "SOUTH AUSTRALIA", "width": 1.0}, {"description": "The Beijing PM2.5 dataset includes data from the US Embassy in Beijing", "from": "BEIJING PM2.5 DATASET", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "US EMBASSY", "width": 1.0}, {"description": "The Beijing PM2.5 dataset includes data from Beijing Capital International Airport", "from": "BEIJING PM2.5 DATASET", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "BEIJING CAPITAL INTERNATIONAL AIRPORT", "width": 1.0}, {"description": "Electricity Hourly dataset is related to ETT-H2 dataset as both are collections of data used to train and evaluate machine learning models", "from": "ELECTRICITY HOURLY DATASET", "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "to": "ETT-H2 DATASET", "width": 1.0}, {"description": "The Exchange Rate compilation includes data from China", "from": "EXCHANGE RATE COMPILATION", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "CHINA", "width": 1.0}, {"description": "The Exchange Rate compilation includes data from Japan", "from": "EXCHANGE RATE COMPILATION", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "JAPAN", "width": 1.0}, {"description": "The Exchange Rate compilation includes data from New Zealand", "from": "EXCHANGE RATE COMPILATION", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "NEW ZEALAND", "width": 1.0}, {"description": "The Exchange Rate compilation includes data from Singapore", "from": "EXCHANGE RATE COMPILATION", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "SINGAPORE", "width": 1.0}, {"description": "The London Smart Meters dataset includes data from UK Power Networks", "from": "LONDON SMART METERS DATASET", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "UK POWER NETWORKS", "width": 1.0}, {"description": "The London Smart Meters dataset includes data from the Low Carbon London project", "from": "LONDON SMART METERS DATASET", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "LOW CARBON LONDON PROJECT", "width": 1.0}, {"description": "The KDD Cup 2018 dataset includes data from 59 stations in Beijing and London", "from": "KDD CUP 2018 DATASET", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "59 STATIONS", "width": 1.0}, {"description": "The Pedestrian Counts dataset includes data from 66 sensors in the city of Melbourne", "from": "PEDESTRIAN COUNTS DATASET", "source_id": "85902d07a5ac3acacd692810072fa1c6", "to": "66 SENSORS", "width": 1.0}, {"description": "The Solar dataset comprises 6000 simulated time series for 5-minute solar power and hourly forecasts of photovoltaic power plants in the U.S. in 2006", "from": "SOLAR DATASET", "source_id": "76cc338e586223647fd3dbe4e7a7c131", "to": "SOLAR POWER", "width": 1.0}, {"description": "The Sunspot dataset comprises a singular extensive daily time series of sunspot numbers spanning from January 1818 to May 2020", "from": "SUNSPOT DATASET", "source_id": "76cc338e586223647fd3dbe4e7a7c131", "to": "SUNSPOTS", "width": 1.0}, {"description": "The Uber TLC Hourly dataset consists data of 4.5 million Uber pickups in NYC (April-September 2014) and 14.3 million pickups (January-June 2015)", "from": "UBER TLC HOURLY DATASET", "source_id": "76cc338e586223647fd3dbe4e7a7c131", "to": "UBER PICKUPS", "width": 1.0}, {"description": "The Wind Farms dataset contains minute-frequency time series data tracking the wind power production of 339 wind farms in Australia", "from": "WIND FARMS DATASET", "source_id": "76cc338e586223647fd3dbe4e7a7c131", "to": "WIND POWER PRODUCTION", "width": 1.0}, {"description": "Training split is related to validation set as validation set is used to evaluate a model\u0027s performance", "from": "TRAINING SPLIT", "source_id": "76cc338e586223647fd3dbe4e7a7c131", "to": "VALIDATION SET", "width": 1.0}, {"description": "Pretraining datasets are related to test split as pretraining datasets are used to pretrain the model", "from": "TEST SPLIT", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "PRETRAINING DATASETS", "width": 1.0}, {"description": "Test split is related to validation split as test split and validation split are both subsets of the dataset", "from": "TEST SPLIT", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "VALIDATION SPLIT", "width": 1.0}, {"description": "Transport and tourism is related to San Francisco traffic as San Francisco traffic is a dataset used for pretraining and fine-tuning", "from": "TRANSPORT \u0026 TOURISM", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "SAN FRANCISCO TRAFFIC", "width": 1.0}, {"description": "Transport and tourism is related to Uber TLC hourly as Uber TLC hourly is a dataset used for pretraining and fine-tuning", "from": "TRANSPORT \u0026 TOURISM", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "UBER TLC HOURLY", "width": 1.0}, {"description": "San Francisco traffic is related to Australian electricity demand as both are datasets used for pretraining and fine-tuning", "from": "SAN FRANCISCO TRAFFIC", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "AUSTRALIAN ELECTRICITY DEMAND", "width": 1.0}, {"description": "San Francisco traffic is related to Uber TLC hourly as both are datasets used for pretraining and fine-tuning", "from": "SAN FRANCISCO TRAFFIC", "source_id": "4c09f35749179fe18c7d7290eaa57955", "to": "UBER TLC HOURLY", "width": 1.0}, {"description": "ETT H1 is related to ETT H2 as both are concepts related to energy", "from": "ETT H1", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "ETT H2", "width": 1.0}, {"description": "ETT H1 is related to ETT M1 as both are concepts related to energy", "from": "ETT H1", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "ETT M1", "width": 1.0}, {"description": "ETT H1 is related to ETT M2 as both are concepts related to energy", "from": "ETT H1", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "ETT M2", "width": 1.0}, {"description": "Electricity weather CPU usage is related to ETT H2 as ETT H2 is a specific type of electricity weather CPU usage", "from": "ETT H2", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "ELECTRICITY WEATHER CPU USAGE", "width": 1.0}, {"description": "UCI is related to Beijing Multisite as both are concepts related to air quality", "from": "BEIJING MULTISITE", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "UCI", "width": 1.0}, {"description": "Beijing PM2.5 is related to Beijing Multisite as both are concepts related to air quality", "from": "BEIJING MULTISITE", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "BEIJING PM2.5", "width": 1.0}, {"description": "UCI is related to Beijing PM2.5 as both are concepts related to air quality", "from": "UCI", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "BEIJING PM2.5", "width": 1.0}, {"description": "Requests Minute is related to CPU Limit Minute as both are concepts related to cloud computing", "from": "CPU LIMIT MINUTE", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "REQUESTS MINUTE", "width": 1.0}, {"description": "Requests Minute is related to CPU Usage Minute as both are concepts related to cloud computing", "from": "CPU USAGE MINUTE", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "REQUESTS MINUTE", "width": 1.0}, {"description": "Requests Minute is related to Memory Limit Minute as both are concepts related to cloud computing", "from": "MEMORY LIMIT MINUTE", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "REQUESTS MINUTE", "width": 1.0}, {"description": "Requests Minute is related to Memory Usage Minute as both are concepts related to cloud computing", "from": "MEMORY USAGE MINUTE", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "REQUESTS MINUTE", "width": 1.0}, {"description": "Requests Minute is related to Function Delay Minute as both are concepts related to cloud computing", "from": "REQUESTS MINUTE", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "FUNCTION DELAY MINUTE", "width": 1.0}, {"description": "Requests Minute is related to Platform Delay Minute as both are concepts related to cloud computing", "from": "REQUESTS MINUTE", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "PLATFORM DELAY MINUTE", "width": 1.0}, {"description": "Requests Minute is related to Instances Minute as both are concepts related to cloud computing", "from": "REQUESTS MINUTE", "source_id": "9e88afa28686ff93769bfc5eb0f1095e", "to": "INSTANCES MINUTE", "width": 1.0}, {"description": "Embedding dimensions per head is related to context length C as both are hyperparameters that determine the size of the context window", "from": "EMBEDDING DIMENSIONS PER HEAD", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "CONTEXT LENGTH C \u2020", "width": 1.0}, {"description": "Context length C is related to augmentation probability as both are hyperparameters that determine the data augmentation strategy", "from": "CONTEXT LENGTH C \u2020", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "AUGMENTATION PROBABILITY", "width": 1.0}, {"description": "Augmentation probability is related to frequency masking rate as both are hyperparameters that determine the data augmentation strategy", "from": "AUGMENTATION PROBABILITY", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "FREQUENCY MASKING RATE", "width": 1.0}, {"description": "Frequency masking rate is related to frequency mixing rate as both are hyperparameters that determine the data augmentation strategy", "from": "FREQUENCY MASKING RATE", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "FREQUENCY MIXING RATE", "width": 1.0}, {"description": "Frequency mixing rate is related to weight decay as both are hyperparameters that determine the optimization strategy", "from": "FREQUENCY MIXING RATE", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "WEIGHT DECAY", "width": 1.0}, {"description": "Supervised models are related to pretraining datasets as supervised models are trained on pretraining datasets", "from": "SUPERVISED MODELS", "source_id": "c5dc13d7191b625e7373e79907b5782a", "to": "PRETRAINING DATASETS", "width": 1.0}, {"description": "Time series foundation model is related to data repositories as data repositories are used to train and evaluate the model", "from": "TIME SERIES FOUNDATION MODEL", "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "to": "DATA REPOSITORIES", "width": 1.0}, {"description": "Laws are related to relations as laws describe the relationships between variables", "from": "LAWS", "source_id": "c60a8238db3d56eff9cc9692e7ac5b1c", "to": "RELATIONS", "width": 1.0}, {"description": "Forecast is related to target as forecast is a prediction of target", "from": "FORECAST", "source_id": "3ba0bc9230706fb8f4a61d16ecf8fd26", "to": "TARGET", "width": 1.0}, {"description": "Instances are related to cloud as cloud is a specific type of instance", "from": "CLOUD", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "INSTANCES", "width": 1.0}, {"description": "Electricity weather CPU usage is related to Beijing PM25 as Beijing PM25 is a specific type of electricity weather CPU usage", "from": "BEIJING PM25", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "ELECTRICITY WEATHER CPU USAGE", "width": 1.0}, {"description": "Limit is related to memory limit as memory limit is a specific type of limit", "from": "MEMORY LIMIT", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "LIMIT", "width": 1.0}, {"description": "Instances are related to memory limit as memory limit is affected by instances", "from": "MEMORY LIMIT", "source_id": "990578022879395d00b7a5b229863c2f", "to": "INSTANCES", "width": 1.0}, {"description": "Memory limit is related to memory usage as memory usage is affected by memory limit", "from": "MEMORY LIMIT", "source_id": "990578022879395d00b7a5b229863c2f", "to": "MEMORY USAGE", "width": 1.0}, {"description": "Function delay is related to instances as instances affect function delay", "from": "INSTANCES", "source_id": "990578022879395d00b7a5b229863c2f", "to": "FUNCTION DELAY", "width": 1.0}, {"description": "Platform delay is related to function delay as function delay is a specific type of platform delay", "from": "PLATFORM DELAY", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "FUNCTION DELAY", "width": 1.0}, {"description": "CPU usage is related to function delay as function delay is affected by CPU usage", "from": "FUNCTION DELAY", "source_id": "990578022879395d00b7a5b229863c2f", "to": "CPU USAGE", "width": 1.0}, {"description": "Beijing Multisite Sunspot is related to ETT ML as ETT ML is a specific type of Beijing Multisite Sunspot", "from": "BEIJING MULTISITE SUNSPOT", "source_id": "ec7705b83cf4fe3aa18662c917b18c1a", "to": "ETT ML", "width": 1.0}, {"description": "Dynamic optimization and Temporal Fusion Transformer are both models used for time series forecasting", "from": "DYNAMICOPTIMIZE", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "TEMPORALFUSIONT", "width": 1.0}, {"description": "Dynamic optimization and N-BEATS are both models used for time series forecasting", "from": "DYNAMICOPTIMIZE", "source_id": "376897a501ac50834f626fcc5fb13ece", "to": "NBEATS", "width": 1.0}, {"description": "Temporal Fusion Transformer and N-BEATS are both models used for time series forecasting", "from": "TEMPORALFUSIONT", "source_id": "376897a501ac50834f626fcc5fb13ece,6fa5f635ad5f7e6b67d5e467f130345c", "to": "NBEATS", "width": 2.0}, {"description": "NBEATS and LAGLLAMA are models mentioned in the text, as indicated by the use of their names in the table", "from": "NBEATS", "source_id": "bb87457fce8d4214bfe1f398b7ea35f2", "to": "LAGLLAMA", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe technical terms entity is described in relation to a text document entity. The text document contains technical terms, which are used to describe specialized concepts or ideas. These technical terms are likely to be used in a formal and academic context, as suggested by the presence of mathematical equations and formulas, as well as references to academic papers and conferences.\n\nThe technical terms entity is closely related to the text document entity, as they are used to describe the content and purpose of the document. The text document is likely to be a research paper or a technical document, given the formal and academic language used.\n\nSome specific indicators of the language being English include the use of English words and phrases, such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention.\" The presence of mathematical equations and formulas, written in a standard mathematical notation used in English-language academic papers, also supports this conclusion.\n\nThe text document also contains English-language abbreviations, such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals. Additionally, the citation of English-language academic papers and authors suggests that the text is written in English.\n\nOverall, based on the language and content of the text, it is clear that the primary language of the text document is English, and that the technical terms entity is closely related to the text document entity, used to describe specialized concepts or ideas in a formal and academic context.", "from": "TECHNICAL TERMS", "source_id": "0654926be53a9cf18e4def1c94371576,9f70fa3e2d0ba714627b9ce1a442fd21,c80929fa1aa2a6f9652319844c4ca742", "to": "TEXT DOCUMENT", "width": 3.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe text document, which is written in English, contains mathematical equations that are used to describe mathematical relationships and solve problems. These equations are a crucial component of the document, and they are likely used to support the research or technical content presented in the text. The presence of mathematical equations and formulas, as well as references to academic papers and authors, further suggests that the text is from a research paper or a technical document.\n\nThe entity \"MATHEMATICAL EQUATIONS\" is a key component of the text document, and they are used to convey complex mathematical concepts and relationships. The entity \"TEXT DOCUMENT\" is the primary source of the mathematical equations, and it is likely a research paper or technical document that presents mathematical models and solutions.\n\nOverall, the text document is a technical document that contains mathematical equations used to describe mathematical relationships and solve problems, and it is written in English.", "from": "MATHEMATICAL EQUATIONS", "source_id": "0654926be53a9cf18e4def1c94371576,9f70fa3e2d0ba714627b9ce1a442fd21,c80929fa1aa2a6f9652319844c4ca742", "to": "TEXT DOCUMENT", "width": 3.0}, {"description": "The text document contains references, which are used to support or validate information", "from": "REFERENCES", "source_id": "0654926be53a9cf18e4def1c94371576,9f70fa3e2d0ba714627b9ce1a442fd21,c80929fa1aa2a6f9652319844c4ca742", "to": "TEXT DOCUMENT", "width": 3.0}, {"description": "References are related to Shaojie Bai as Shaojie Bai is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "SHAOJIE BAI", "width": 1.0}, {"description": "References are related to J Zico Kolter as J Zico Kolter is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "J ZICO KOLTER", "width": 1.0}, {"description": "References are related to Vladlen Koltun as Vladlen Koltun is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "VLADLEN KOLTUN", "width": 1.0}, {"description": "References are related to George EP Box as George EP Box is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "GEORGE EP BOX", "width": 1.0}, {"description": "References are related to Gwilym M Jenkins as Gwilym M Jenkins is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "GWILYM M JENKINS", "width": 1.0}, {"description": "References are related to Gregory C Reinsel as Gregory C Reinsel is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "GREGORY C REINSEL", "width": 1.0}, {"description": "References are related to Greta M Ljung as Greta M Ljung is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "GWETA M LJUNG", "width": 1.0}, {"description": "References are related to Tom Brown as Tom Brown is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "TOM BROWN", "width": 1.0}, {"description": "References are related to Benjamin Mann as Benjamin Mann is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "BENJAMIN MANN", "width": 1.0}, {"description": "References are related to Nick Ryder as Nick Ryder is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "NICK RYDER", "width": 1.0}, {"description": "References are related to Melanie Subbiah as Melanie Subbiah is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "MELANIE SUBBIAH", "width": 1.0}, {"description": "References are related to Jared D Kaplan as Jared D Kaplan is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "JARED D KAPLAN", "width": 1.0}, {"description": "References are related to Prafulla Dhariwal as Prafulla Dhariwal is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "PRAFULLA DHARIWAL", "width": 1.0}, {"description": "References are related to Arvind Neelakantan as Arvind Neelakantan is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "ARVIND NEELAKANTAN", "width": 1.0}, {"description": "References are related to Pranav Shyam as Pranav Shyam is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "PRANAV SHYAM", "width": 1.0}, {"description": "References are related to Girish Sastry as Girish Sastry is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "GIRISH SASTRY", "width": 1.0}, {"description": "References are related to Amanda Askell as Amanda Askell is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "AMANDA ASKELL", "width": 1.0}, {"description": "References are related to Kin G Olivares as Kin G Olivares is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "KIN G OLIVARES", "width": 1.0}, {"description": "References are related to Boris N Oreshkin as Boris N Oreshkin is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "BORIS N ORESHKIN", "width": 1.0}, {"description": "References are related to Federico Garza as Federico Garza is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "FEDERICO GARZA", "width": 1.0}, {"description": "References are related to Max Mergenthaler as Max Mergenthaler is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "MAX MERGENTHALER", "width": 1.0}, {"description": "References are related to Artur Dubrawski as Artur Dubrawski is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "ARTUR DUBRAWSKI", "width": 1.0}, {"description": "References are related to Ching Chang as Ching Chang is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "CHING CHANG", "width": 1.0}, {"description": "References are related to Wen-Chih Peng as Wen-Chih Peng is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "WEN-CHIH PENG", "width": 1.0}, {"description": "References are related to Tien-Fu Chen as Tien-Fu Chen is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "TIEN-FU CHEN", "width": 1.0}, {"description": "References are related to Pin-Yu Chen as Pin-Yu Chen is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "PIN-YU CHEN", "width": 1.0}, {"description": "References are related to Zhixuan Chu as Zhixuan Chu is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "ZHIXUAN CHU", "width": 1.0}, {"description": "References are related to Hongyan Hao as Hongyan Hao is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "HONGYAN HAO", "width": 1.0}, {"description": "References are related to Xin Ouyang as Xin Ouyang is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "XIN OUYANG", "width": 1.0}, {"description": "References are related to Simeng Wang as Simeng Wang is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "SIMENG WANG", "width": 1.0}, {"description": "References are related to Yan Wang as Yan Wang is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "YAN WANG", "width": 1.0}, {"description": "References are related to Yue Shen as Yue Shen is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "YUE SHEN", "width": 1.0}, {"description": "References are related to Jinjie Gu as Jinjie Gu is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "JINJIE GU", "width": 1.0}, {"description": "References are related to Qing Cui as Qing Cui is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "QING CUI", "width": 1.0}, {"description": "References are related to Longfei Li as Longfei Li is an author of one of the referenced papers", "from": "REFERENCES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "LONGFEI LI", "width": 1.0}, {"description": "CPU limit is related to CPU usage as CPU usage is affected by CPU limit", "from": "CPU LIMIT", "source_id": "990578022879395d00b7a5b229863c2f", "to": "CPU USAGE", "width": 1.0}, {"description": "Prototype representations are related to Prompt-as-Prefix as PaP is used to enrich the input time series with additional context and provide task instructions in natural language", "from": "PROMPT-AS-PREFIX", "source_id": "89b79391630ac478085efea89fad5736", "to": "PROTOTYPE REPRESENTATIONS", "width": 1.0}, {"description": "Prompt-as-Prefix is related to time series forecasts as the output of the language model is projected to generate time series forecasts", "from": "PROMPT-AS-PREFIX", "source_id": "89b79391630ac478085efea89fad5736", "to": "TIME SERIES FORECASTS", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entity \"PROMPT-AS-PREFIX\" is related to the entity \"DATASET CONTEXT\" in a specific context. Specifically, \"PROMPT-AS-PREFIX\" uses \"DATASET CONTEXT\" to provide background information to the language model about the input time series. This implies that \"DATASET CONTEXT\" is used to train \"PROMPT-AS-PREFIX\" and is essential for its functionality.\n\nIn other words, \"PROMPT-AS-PREFIX\" relies on \"DATASET CONTEXT\" to understand the input time series, which suggests that \"DATASET CONTEXT\" contains relevant information about the time series data. This relationship highlights the importance of \"DATASET CONTEXT\" in enabling \"PROMPT-AS-PREFIX\" to perform its intended function.\n\nOverall, the summary provides a clear understanding of the relationship between \"PROMPT-AS-PREFIX\" and \"DATASET CONTEXT\", highlighting the crucial role of \"DATASET CONTEXT\" in training and enabling \"PROMPT-AS-PREFIX\" to process time series data.", "from": "PROMPT-AS-PREFIX", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,e6a6bc6fbd362394320961ac10cdd230", "to": "DATASET CONTEXT", "width": 2.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe entities \"PROMPT-AS-PREFIX\" and \"PATCH REPROGRAMMING\" are both techniques used in the context of Large Language Models (LLMs) and computer vision. Specifically, they are utilized for effective time series forecasting and are related to each other as they share commonalities in their application.\n\nPatch reprogramming and prompt-as-prefix are both techniques that have been employed in the LLM for the purpose of time series forecasting. This suggests that these methods have been found to be effective in this domain. Additionally, patch reprogramming is also related to prompt-as-prefix as both are techniques used in computer vision, indicating that there may be some overlap or commonalities between these two fields.\n\nOverall, the summary highlights the connection between these two entities and their applications in LLMs and computer vision, as well as their effectiveness in time series forecasting.", "from": "PROMPT-AS-PREFIX", "source_id": "072b166b9a1b6afecf5874f45af61699,e6a6bc6fbd362394320961ac10cdd230", "to": "PATCH REPROGRAMMING", "width": 2.0}, {"description": "A sequence of historical observations is related to an input time series as it is used to make predictions", "from": "INPUT TIME SERIES", "source_id": "3174231a67593609c727151c9df31d0a", "to": "SEQUENCE OF HISTORICAL OBSERVATIONS", "width": 1.0}, {"description": "An input time series is related to ground truths as it is trying to predict the actual values", "from": "INPUT TIME SERIES", "source_id": "3174231a67593609c727151c9df31d0a", "to": "GROUND TRUTHS", "width": 1.0}, {"description": "Natural language is related to time series patterns as the knowledge and reasoning capabilities to interpret time series data are not naturally present within LLMs\u2019 pre-training", "from": "NATURAL LANGUAGE", "source_id": "89b79391630ac478085efea89fad5736", "to": "TIME SERIES PATTERNS", "width": 1.0}, {"description": "Multimodal models are related to natural language as multimodal models can process and integrate data from multiple sources, including natural language", "from": "NATURAL LANGUAGE", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "MULTIMODAL MODELS", "width": 1.0}, {"description": "IBM Research is related to Alibaba Group as Alibaba Group is a partner of IBM Research", "from": "IBM RESEARCH", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "ALIBABA GROUP", "width": 1.0}, {"description": "Alibaba Group is related to Monash University as Monash University is a partner of Alibaba Group", "from": "ALIBABA GROUP", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "MONASH UNIVERSITY", "width": 1.0}, {"description": "Monash University is related to Griffith University as Griffith University is a partner of Monash University", "from": "MONASH UNIVERSITY", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "GRIFFITH UNIVERSITY", "width": 1.0}, {"description": "Griffith University is related to The Hong Kong University of Science and Technology (Guangzhou) as The Hong Kong University of Science and Technology (Guangzhou) is a partner of Griffith University", "from": "GRIFFITH UNIVERSITY", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "THE HONG KONG UNIVERSITY OF SCIENCE AND TECHNOLOGY (GUANGZHOU)", "width": 1.0}, {"description": "The Hong Kong University of Science and Technology (Guangzhou) is related to Ant Group as Ant Group is a partner of The Hong Kong University of Science and Technology (Guangzhou)", "from": "THE HONG KONG UNIVERSITY OF SCIENCE AND TECHNOLOGY (GUANGZHOU)", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "ANT GROUP", "width": 1.0}, {"description": "Ant Group is related to Zhixuan Chu as Zhixuan Chu is a researcher at Ant Group", "from": "ANT GROUP", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "ZHIXUAN CHU", "width": 1.0}, {"description": "Shiyu Wang is related to Zhixuan Chu as Zhixuan Chu is a co-author of Shiyu Wang", "from": "ZHIXUAN CHU", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "SHIYU WANG", "width": 1.0}, {"description": "Lintao Ma is related to Zhixuan Chu as Zhixuan Chu is a co-author of Lintao Ma", "from": "ZHIXUAN CHU", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "LINTAO MA", "width": 1.0}, {"description": "Yuan-Fang Li is related to Zhixuan Chu as Zhixuan Chu is a co-author of Yuan-Fang Li", "from": "ZHIXUAN CHU", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "YUAN-FANG LI", "width": 1.0}, {"description": "James Y. Zhang is related to Zhixuan Chu as Zhixuan Chu is a co-author of James Y. Zhang", "from": "ZHIXUAN CHU", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "JAMES Y. ZHANG", "width": 1.0}, {"description": "Xiaoming Shi is related to Zhixuan Chu as Zhixuan Chu is a co-author of Xiaoming Shi", "from": "ZHIXUAN CHU", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "XIAOMING SHI", "width": 1.0}, {"description": "Pin-Yu Chen is related to Zhixuan Chu as Zhixuan Chu is a co-author of Pin-Yu Chen", "from": "ZHIXUAN CHU", "source_id": "8f4724ff6541b8924f0cebe9872ed040", "to": "PIN-YU CHEN", "width": 1.0}, {"description": "Zhixuan Chu is related to pre-trained recommender systems as he has published a paper on the topic", "from": "ZHIXUAN CHU", "source_id": "a73df99fe49b288e1c8751be2008b191", "to": "PRE-TRAINED RECOMMENDER SYSTEMS", "width": 1.0}, {"description": "Yan Wang is an author of the paper \"Enhancing recommender systems with large language model reasoning graphs\" along with Zhixuan Chu", "from": "ZHIXUAN CHU", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "YAN WANG", "width": 1.0}, {"description": "Zhixuan Chu is an author of the paper \"Enhancing recommender systems with large language model reasoning graphs\" along with Xin Ouyang", "from": "ZHIXUAN CHU", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "XIN OUYANG", "width": 1.0}, {"description": "Yunyi Zhou is related to Zhixuan Chu as they are co-authors of the research paper", "from": "ZHIXUAN CHU", "source_id": "a69a914fb6c895c7202532b69ad3e094", "to": "YUNYI ZHOU", "width": 1.0}, {"description": "Zhixuan Chu is related to Yijia Ruan as they are co-authors of the research paper", "from": "ZHIXUAN CHU", "source_id": "a69a914fb6c895c7202532b69ad3e094", "to": "YIJIA RUAN", "width": 1.0}, {"description": "Pin-Yu Chen is related to model reprogramming as he has published a paper on the topic", "from": "PIN-YU CHEN", "source_id": "a73df99fe49b288e1c8751be2008b191", "to": "MODEL REPROGRAMMING", "width": 1.0}, {"description": "Ria Vinod is an author of the paper \"Reprogramming language models for molecular representation learning\" along with Pin-Yu Chen", "from": "PIN-YU CHEN", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "RIA VINOD", "width": 1.0}, {"description": "Pin-Yu Chen is an author of the paper \"Reprogramming language models for molecular representation learning\" along with Payel Das", "from": "PIN-YU CHEN", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "PAYEL DAS", "width": 1.0}, {"description": "Foundation language models have the potential for generalizable forecasting across domains without requiring per-task retraining from scratch", "from": "FOUNDATION LANGUAGE MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "GENERALIZABILITY", "width": 1.0}, {"description": "Foundation language models have shown the ability to perform new tasks with only a few examples, enabling data efficiency", "from": "FOUNDATION LANGUAGE MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "DATA EFFICIENCY", "width": 1.0}, {"description": "Foundation language models exhibit sophisticated reasoning and pattern recognition capabilities", "from": "FOUNDATION LANGUAGE MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "REASONING", "width": 1.0}, {"description": "Foundation language models gain more diverse knowledge across modalities like vision, speech, and text", "from": "FOUNDATION LANGUAGE MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "MULTIMODAL KNOWLEDGE", "width": 1.0}, {"description": "Foundation language models are trained once on massive computing and then can be applied to forecasting tasks without learning from scratch", "from": "FOUNDATION LANGUAGE MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "EASY OPTIMIZATION", "width": 1.0}, {"description": "GPT-4 is a foundation language model that can perform well on a diverse range of NLP tasks in a few-shot or even zero-shot setting", "from": "FOUNDATION LANGUAGE MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "GPT-4", "width": 1.0}, {"description": "Brown et al. (2020) is a paper that discusses foundation language models", "from": "FOUNDATION LANGUAGE MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "BROWN ET AL. (2020)", "width": 1.0}, {"description": "OpenAI (2023) is a paper that discusses GPT-4", "from": "FOUNDATION LANGUAGE MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "OPENAI (2023)", "width": 1.0}, {"description": "Mirchandani et al. (2023) is a paper that discusses reasoning and pattern recognition capabilities", "from": "FOUNDATION LANGUAGE MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "MIRCHANDANI ET AL. (2023)", "width": 1.0}, {"description": "Wang et al. (2023) is a paper that discusses reasoning and pattern recognition capabilities", "from": "FOUNDATION LANGUAGE MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "WANG ET AL. (2023)", "width": 1.0}, {"description": "Chu et al. (2023) is a paper that discusses reasoning and pattern recognition capabilities", "from": "FOUNDATION LANGUAGE MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "CHU ET AL. (2023)", "width": 1.0}, {"description": "Ma et al. (2023) is a paper that discusses multimodal knowledge", "from": "FOUNDATION LANGUAGE MODELS", "source_id": "b67d18d306fde251ee94b0a831d1e075", "to": "MA ET AL. (2023)", "width": 1.0}, {"description": "Text prototype representations are augmented with declarative prompts to guide LLM reasoning", "from": "TEXT PROTOTYPE REPRESENTATIONS", "source_id": "b27c89cb0db6646b1203b2701e017aeb", "to": "DECLARATIVE PROMPTS", "width": 1.0}, {"description": "Declarative prompts are used to incorporate domain expert knowledge and task instructions", "from": "DECLARATIVE PROMPTS", "source_id": "b27c89cb0db6646b1203b2701e017aeb", "to": "DOMAIN EXPERT KNOWLEDGE", "width": 1.0}, {"description": "Domain expert knowledge and task instructions are used to guide LLM reasoning", "from": "DOMAIN EXPERT KNOWLEDGE", "source_id": "b27c89cb0db6646b1203b2701e017aeb", "to": "TASK INSTRUCTIONS", "width": 1.0}, {"description": "Task instructions and input context are beneficial for facilitating the task", "from": "TASK INSTRUCTIONS", "source_id": "2bb4fc2b46b9c8bdd052b2755d986aa8", "to": "INPUT CONTEXT", "width": 1.0}, {"description": "Task-specific learning and model fine-tuning are compared to TIME-LLM in the text", "from": "TASK-SPECIFIC LEARNING", "source_id": "b27c89cb0db6646b1203b2701e017aeb", "to": "MODEL FINE-TUNING", "width": 1.0}, {"description": "A multivariate time series is related to input transformation as it is used to transform the input data", "from": "MULTIVARIATE TIME SERIES", "source_id": "3174231a67593609c727151c9df31d0a", "to": "INPUT TRANSFORMATION", "width": 1.0}, {"description": "Univariate time series data is different from multivariate time series data, which is a key feature of TranAD", "from": "MULTIVARIATE TIME SERIES", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "UNIVARIATE TIME SERIES", "width": 1.0}, {"description": "Input transformation is related to a pre-trained and frozen LLM as it uses the pre-trained model to transform the input data", "from": "INPUT TRANSFORMATION", "source_id": "3174231a67593609c727151c9df31d0a", "to": "PRE-TRAINED AND FROZEN LLM", "width": 1.0}, {"description": "Input transformation is related to output projection as output projection uses the output of input transformation", "from": "INPUT TRANSFORMATION", "source_id": "fececbac281c1e2b13921f378df30919", "to": "OUTPUT PROJECTION", "width": 1.0}, {"description": "A pre-trained and frozen LLM is related to output projection as it uses the pre-trained model to project the output", "from": "PRE-TRAINED AND FROZEN LLM", "source_id": "3174231a67593609c727151c9df31d0a", "to": "OUTPUT PROJECTION", "width": 1.0}, {"description": "LLAMA-7B uses patch reprogramming for effective time series forecasting", "from": "PATCH REPROGRAMMING", "source_id": "072b166b9a1b6afecf5874f45af61699", "to": "LLAMA-7B", "width": 1.0}, {"description": "QLORA is related to patch reprogramming as it is used to fine-tune patch reprogramming", "from": "PATCH REPROGRAMMING", "source_id": "e6a6bc6fbd362394320961ac10cdd230", "to": "QLORA", "width": 1.0}, {"description": "Statistical context is related to patch reprogramming as patch reprogramming is used to modify statistical context", "from": "PATCH REPROGRAMMING", "source_id": "e6a6bc6fbd362394320961ac10cdd230", "to": "STATISTICAL CONTEXT", "width": 1.0}, {"description": "Source data representation space is related to target modalities as target modalities are aligned in the source data representation space", "from": "SOURCE DATA REPRESENTATION SPACE", "source_id": "fececbac281c1e2b13921f378df30919", "to": "TARGET MODALITIES", "width": 1.0}, {"description": "Update is related to fine-tune as fine-tune involves updating the parameters of a pre-trained model", "from": "UPDATE", "source_id": "fececbac281c1e2b13921f378df30919", "to": "FINE-TUNE", "width": 1.0}, {"description": "Optimized is related to update as update involves adjusting the parameters of the model to improve its performance", "from": "UPDATE", "source_id": "fececbac281c1e2b13921f378df30919", "to": "OPTIMIZED", "width": 1.0}, {"description": "Domain-specific models are related to resource constraints as domain-specific models require more resources than TIME-LLM", "from": "DOMAIN-SPECIFIC MODELS", "source_id": "fececbac281c1e2b13921f378df30919", "to": "RESOURCE CONSTRAINTS", "width": 1.0}, {"description": "RV is related to D\u02c6(i) as D\u02c6(i) is a space used to represent pre-trained word embeddings", "from": "RV", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "D\u02c6(I)", "width": 1.0}, {"description": "RV is related to V as V is the vocabulary size", "from": "RV", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "V", "width": 1.0}, {"description": "RV \u2032 is related to V \u2032 as V \u2032 is a smaller vocabulary size", "from": "V \u2032", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "RV \u2032", "width": 1.0}, {"description": "E is related to E\u2032 as E\u2032 is a model used to represent text prototypes", "from": "E", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "E\u2032", "width": 1.0}, {"description": "RV \u2032 is related to D as D is a space used to represent text prototypes", "from": "RV \u2032", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "D", "width": 1.0}, {"description": "Wk Q is related to Wk K as Wk K is a matrix used in multi-head cross-attention", "from": "WK Q", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "WK K", "width": 1.0}, {"description": "Wk Q is related to Wk V as Wk V is a matrix used in multi-head cross-attention", "from": "WK Q", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "WK V", "width": 1.0}, {"description": "Wk K is related to Wk V as Wk V is a matrix used in multi-head cross-attention", "from": "WK K", "source_id": "a8638786e37b5d4f9d005cc1dbf2b8cb", "to": "WK V", "width": 1.0}, {"description": "Electricity consumption is related to transformer load as transformer load is affected by electricity consumption", "from": "ELECTRICITY CONSUMPTION", "source_id": "509b431231e669be373f593b31412eed", "to": "TRANSFORMER LOAD", "width": 1.0}, {"description": "Transformer load is related to electricity transformer temperature as electricity transformer temperature is affected by transformer load", "from": "TRANSFORMER LOAD", "source_id": "509b431231e669be373f593b31412eed", "to": "ELECTRICITY TRANSFORMER TEMPERATURE (ETT)", "width": 1.0}, {"description": "Prompting is related to time series patches as prompting is used to reprogram time series patches", "from": "PROMPTING", "source_id": "509b431231e669be373f593b31412eed", "to": "TIME SERIES PATCHES", "width": 1.0}, {"description": "Text prototypes are related to time series patches as text prototypes are used to represent time series patches", "from": "TIME SERIES PATCHES", "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "to": "TEXT PROTOTYPES", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nThe \"DATASET CONTEXT\" is closely related to the \"TASK INSTRUCTION\" in the context of guiding a language model for specific tasks. The \"TASK INSTRUCTION\" serves as a guide for the \"DATASET CONTEXT\", which is used to transform patch embeddings for the purpose of task-specific processing. This suggests a symbiotic relationship between the two entities, where the \"TASK INSTRUCTION\" provides the necessary guidance for the \"DATASET CONTEXT\" to facilitate effective task execution.\n\nThis summary is derived from the provided descriptions, which collectively convey the interdependent nature of the \"DATASET CONTEXT\" and the \"TASK INSTRUCTION\". The first description establishes a direct relationship between the two entities, while the second description provides further insight into the specific role of the \"DATASET CONTEXT\" in task execution, guided by the \"TASK INSTRUCTION\".", "from": "DATASET CONTEXT", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f,e6a6bc6fbd362394320961ac10cdd230", "to": "TASK INSTRUCTION", "width": 2.0}, {"description": "Task instruction uses input statistics to facilitate pattern recognition and reasoning in the language model", "from": "TASK INSTRUCTION", "source_id": "3e937ba8de0e7eca993c50506ceb8f1f", "to": "INPUT STATISTICS", "width": 1.0}, {"description": "Task instruction is related to statistical context as statistical context is used to train task instruction", "from": "TASK INSTRUCTION", "source_id": "e6a6bc6fbd362394320961ac10cdd230", "to": "STATISTICAL CONTEXT", "width": 1.0}, {"description": "ILI is related to Reformer as it is a forecasting horizon used in the model", "from": "REFORMER", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "ILI", "width": 8.0}, {"description": "LightTS and Reformer are both types of models used for time series forecasting", "from": "REFORMER", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c", "to": "LIGHTTS", "width": 1.0}, {"description": "Former and Reformer are both types of models used for time series forecasting", "from": "REFORMER", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c", "to": "FORMER", "width": 1.0}, {"description": "ILI is related to LightTS as it is a forecasting horizon used in the model", "from": "LIGHTTS", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "ILI", "width": 8.0}, {"description": "Former and LightTS are both types of models used for time series forecasting", "from": "LIGHTTS", "source_id": "c84edbea28fbbed451e8d0b7df4ffb7c", "to": "FORMER", "width": 1.0}, {"description": "ECL is related to ILI as ECL is used to predict ILI-related metrics", "from": "ILI", "source_id": "bb03c20a6fc1d9af2f3cbfa55b50cfb8", "to": "ECL", "width": 1.0}, {"description": "ILI is related to Stationary as it is a forecasting horizon used in the model", "from": "ILI", "source_id": "9ba0189af2ef0720a721c16eef0f0788", "to": "STATIONARY", "width": 8.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nET T M1 and ET T M2 are two entities that are related to each other in the context of time series forecasting. They are both models or algorithms used to make predictions or forecasts, as indicated by the presence of numerical values and performance metrics in the text. Specifically, ET T M2 is a transformation of ET T M1, suggesting a hierarchical or sequential relationship between the two models. The text also mentions that ET T M1 and ET T M2 are metrics, which implies that they are used to evaluate the performance of time series forecasting models. Overall, the relationship between ET T M1 and ET T M2 is one of transformation and hierarchical relationship, where ET T M2 builds upon the predictions or forecasts made by ET T M1.\n\nIt is worth noting that the text is written in English, as indicated by the use of technical terms, mathematical equations, and references to academic papers. The language used is formal and academic, suggesting that the text is from a research paper or a technical document. The presence of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further supports this conclusion.", "from": "ET T M2", "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,919ff66615400ea06113a2a59ff34ef0,bb03c20a6fc1d9af2f3cbfa55b50cfb8,f05d25f1f55ce768a5d240373d5283a6", "to": "ET T M1", "width": 4.0}, {"description": "Based on the provided information, the primary entities are \"ET T M2\" and \"ET T H2\". After analyzing the descriptions, it can be concluded that these two entities are related in a hierarchical or transformational manner.\n\nThe first description states that \"ET T h2 and ET T m2 are related as both are models or algorithms used to make predictions or forecasts.\" This suggests that both \"ET T M2\" and \"ET T H2\" are predictive models or algorithms.\n\nThe second description further clarifies the relationship between the two entities, stating that \"ET T m2 is related to ET T h2 as ET T h2 is a transformation of ET T m2.\" This indicates that \"ET T H2\" is derived from or built upon \"ET T M2\", possibly through some form of transformation or modification.\n\nTaking both descriptions into account, it can be inferred that \"ET T M2\" is a foundational model or algorithm, and \"ET T H2\" is a transformed or enhanced version of it, likely used for more advanced or specialized predictive tasks.\n\nIn terms of the language and content of the text, it appears to be formal and academic, suggesting that the text is from a research paper or technical document. The use of technical terms, mathematical equations, and references to academic papers further supports this conclusion.\n\nOverall, the relationship between \"ET T M2\" and \"ET T H2\" can be summarized as follows:\n\n\"ET T M2\" and \"ET T H2\" are predictive models or algorithms, with \"ET T H2\" being a transformation or enhancement of \"ET T M2\", likely used for more advanced or specialized predictive tasks.", "from": "ET T M2", "source_id": "919ff66615400ea06113a2a59ff34ef0,f05d25f1f55ce768a5d240373d5283a6", "to": "ET T H2", "width": 2.0}, {"description": "Average is related to ET T m2 as average is calculated from ET T m2", "from": "ET T M2", "source_id": "f05d25f1f55ce768a5d240373d5283a6", "to": "AVERAGE", "width": 1.0}, {"description": "ET T m2 \u2192 ET T m1 is related to ET T m2 as ET T m2 is a specific type of data or model", "from": "ET T M2", "source_id": "b90805909f7b1f31ddc03014f161d3ba", "to": "ET T M2 \u2192 ET T M1", "width": 1.0}, {"description": "1stCount and ECL are concepts mentioned in the text, as indicated by the use of their names", "from": "ECL", "source_id": "1d6fb60c5060c25ae4791b03a0513a7f", "to": "1STCOUNT", "width": 1.0}, {"description": "ET T h1 and ET T m1 are related as both are models or algorithms used to make predictions or forecasts", "from": "ET T M1", "source_id": "919ff66615400ea06113a2a59ff34ef0", "to": "ET T H1", "width": 1.0}, {"description": "Average is related to ET T m1 as average is calculated from ET T m1", "from": "ET T M1", "source_id": "f05d25f1f55ce768a5d240373d5283a6", "to": "AVERAGE", "width": 1.0}, {"description": "ET T m2 \u2192 ET T m1 is related to ET T m1 as ET T m1 is a specific type of data or model", "from": "ET T M1", "source_id": "b90805909f7b1f31ddc03014f161d3ba", "to": "ET T M2 \u2192 ET T M1", "width": 1.0}, {"description": "OWA and SMAPE are related as both are metrics used to evaluate the performance of time series forecasting models", "from": "SMAPE", "source_id": "d540ee970ce7debedc6b1b95e9c88be8", "to": "OWA", "width": 1.0}, {"description": "TIME-LLM reprogrammed is a SOTA model in forecasting tasks", "from": "TIME-LLM (REPROGRAMMED)", "source_id": "6d66c2ea37e25b646a13d751c05a8e4d", "to": "SOTA MODEL", "width": 1.0}, {"description": "Time series machines are SOTA models in forecasting tasks", "from": "TIME SERIES MACHINES", "source_id": "6d66c2ea37e25b646a13d751c05a8e4d", "to": "RECENT SOTA MODELS", "width": 1.0}, {"description": "Time series machines are related to Time-LLM framework as Time-LLM framework is a specific approach for using LLMs for time series forecasting", "from": "TIME SERIES MACHINES", "source_id": "ec3fbfb800fd9bf1d913584fda4ae925", "to": "TIME-LLM FRAMEWORK", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nET T H1 and ET T H2 are two metrics mentioned in the text, which are related as both are models or algorithms used to make predictions or forecasts. The text suggests that these models are likely used in the context of time series analysis, as indicated by the use of technical terms such as \"time series\" and \"long-term series forecasting.\" Additionally, the presence of mathematical equations and formulas in the text, written in standard mathematical notation, further supports the idea that ET T H1 and ET T H2 are used for predictive modeling.\n\nThe fact that ET T H1 and ET T H2 are mentioned together in the text suggests that they may be used in conjunction with each other, possibly as part of a larger framework or system for making predictions or forecasts. The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" in the text also suggests that the context in which ET T H1 and ET T H2 are used is likely an academic or research setting.\n\nOverall, based on the information provided, it appears that ET T H1 and ET T H2 are two related models or algorithms used for predictive modeling, likely in the context of time series analysis, and possibly in an academic or research setting.", "from": "ET T H1", "source_id": "1d6fb60c5060c25ae4791b03a0513a7f,919ff66615400ea06113a2a59ff34ef0", "to": "ET T H2", "width": 2.0}, {"description": "Average is related to ET T h2 as average is calculated from ET T h2", "from": "ET T H2", "source_id": "f05d25f1f55ce768a5d240373d5283a6", "to": "AVERAGE", "width": 1.0}, {"description": "Informer (2021) and Reformer (2020) are models mentioned in the text, as indicated by the use of their names", "from": "REFORMER (2020)", "source_id": "1d6fb60c5060c25ae4791b03a0513a7f", "to": "INFORMER (2021)", "width": 1.0}, {"description": "MSE reduction is related to cross-domain adaptation as it enables a model to adapt to a new task or domain without any prior training", "from": "CROSS-DOMAIN ADAPTATION", "source_id": "7e03744dea80e2138baff03611104fa8", "to": "MSE REDUCTION", "width": 1.0}, {"description": "Memory is related to speed as speed is a measure of the model\u0027s computational efficiency", "from": "MEMORY", "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "to": "SPEED", "width": 1.0}, {"description": "Memory and computational footprint are related as both are factors that affect the performance of a model", "from": "MEMORY", "source_id": "1902e651467179a9a1d4c4df0035e980", "to": "COMPUTATIONAL FOOTPRINT", "width": 1.0}, {"description": "LLAMA (32) is related to LLAMA (8) as LLAMA (8) is a variant of LLAMA (32)", "from": "LLAMA (32)", "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "to": "LLAMA (8)", "width": 1.0}, {"description": "LLAMA (32) is related to LLAMA (w/o LLM) as LLAMA (w/o LLM) is a variant of LLAMA (32)", "from": "LLAMA (32)", "source_id": "50bf8b7f27ec843882dbc6eadb2bf158", "to": "LLAMA (W/O LLM)", "width": 1.0}, {"description": "Tim Dettmers is related to Qlora as he has published a paper on the model", "from": "QLORA", "source_id": "a73df99fe49b288e1c8751be2008b191", "to": "TIM DETTMERS", "width": 1.0}, {"description": "Simeng Wang is an author of the paper \"Enhancing recommender systems with large language model reasoning graphs\" along with Hongyan Hao", "from": "HONGYAN HAO", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "SIMENG WANG", "width": 1.0}, {"description": "Hongyan Hao is an author of the paper \"Enhancing recommender systems with large language model reasoning graphs\" along with Yue Shen", "from": "HONGYAN HAO", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "YUE SHEN", "width": 1.0}, {"description": "Xin Ouyang is an author of the paper \"Enhancing recommender systems with large language model reasoning graphs\" along with Simeng Wang", "from": "XIN OUYANG", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "SIMENG WANG", "width": 1.0}, {"description": "Yue Shen is an author of the paper \"Enhancing recommender systems with large language model reasoning graphs\" along with Jinjie Gu", "from": "YUE SHEN", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "JINJIE GU", "width": 1.0}, {"description": "Jinjie Gu is an author of the paper \"Enhancing recommender systems with large language model reasoning graphs\" along with Siqiao Xue", "from": "JINJIE GU", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "SIQIAO XUE", "width": 1.0}, {"description": "James Y Zhang is an author of the paper \"Enhancing recommender systems with large language model reasoning graphs\" along with Qing Cui", "from": "QING CUI", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "JAMES Y ZHANG", "width": 1.0}, {"description": "Cross-modality adaptation is related to model reprogramming as model reprogramming is a type of cross-modality adaptation", "from": "MODEL REPROGRAMMING", "source_id": "f7266cfccedb1d9840d10afa689a05e9", "to": "CROSS-MODALITY ADAPTATION", "width": 1.0}, {"description": "Shohreh Deldari is related to self-supervised representation learning as she has published a paper on the topic", "from": "SHOHREH DELDARI", "source_id": "a73df99fe49b288e1c8751be2008b191", "to": "SELF-SUPERVISED REPRESENTATION LEARNING", "width": 1.0}, {"description": "Nate Gruver is related to zero-shot time series forecasters as he has published a paper on the topic", "from": "NATE GRUVER", "source_id": "a73df99fe49b288e1c8751be2008b191", "to": "ZERO-SHOT TIME SERIES FORECASTERS", "width": 1.0}, {"description": "Sepp Hochreiter and J\u00a8urgen Schmidhuber are related as they are co-authors of the paper on Long short-term memory", "from": "SEPP HOCHREITER", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "J\u00a8URGEN SCHMIDHUBER", "width": 1.0}, {"description": "Taesung Kim and Jinhee Kim are related as they are co-authors of the paper on Reversible instance normalization for accurate time-series forecasting against distribution shift", "from": "TAESUNG KIM", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "JINHEE KIM", "width": 1.0}, {"description": "Taesung Kim and Yunwon Tae are related as they are co-authors of the paper on Reversible instance normalization for accurate time-series forecasting against distribution shift", "from": "TAESUNG KIM", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "YUNWON TAE", "width": 1.0}, {"description": "Taesung Kim and Cheonbok Park are related as they are co-authors of the paper on Reversible instance normalization for accurate time-series forecasting against distribution shift", "from": "TAESUNG KIM", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "CHEONBOK PARK", "width": 1.0}, {"description": "Taesung Kim and Jang-Ho Choi are related as they are co-authors of the paper on Reversible instance normalization for accurate time-series forecasting against distribution shift", "from": "TAESUNG KIM", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "JANG-HO CHOI", "width": 1.0}, {"description": "Taesung Kim and Jaegul Choo are related as they are co-authors of the paper on Reversible instance normalization for accurate time-series forecasting against distribution shift", "from": "TAESUNG KIM", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "JAEGUL CHOO", "width": 1.0}, {"description": "Diederik P Kingma and Jimmy Ba are related as they are co-authors of the paper on Adam: A method for stochastic optimization", "from": "DIEDERIK P KINGMA", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "JIMMY BA", "width": 1.0}, {"description": "Nikita Kitaev and \u0141ukasz Kaiser are related as they are co-authors of the paper on Reformer: The efficient transformer", "from": "NIKITA KITAEV", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "\u0141UKASZ KAISER", "width": 1.0}, {"description": "Nikita Kitaev and Anselm Levskaya are related as they are co-authors of the paper on Reformer: The efficient transformer", "from": "NIKITA KITAEV", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "ANSLEM LEVSKAYA", "width": 1.0}, {"description": "Aidan N Gomez is an author of the paper \"Attention is all you need\" along with \u0141ukasz Kaiser", "from": "\u0141UKASZ KAISER", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "AIDAN N GOMEZ", "width": 1.0}, {"description": "\u0141ukasz Kaiser is an author of the paper \"Attention is all you need\" along with Illia Polosukhin", "from": "\u0141UKASZ KAISER", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "ILLIA POLOSUKHIN", "width": 1.0}, {"description": "Takeshi Kojima and Shixiang Shane Gu are related as they are co-authors of the paper on Large language models are zero-shot reasoners", "from": "TAKEISHI KOJIMA", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "SHIXIANG SHANE GU", "width": 1.0}, {"description": "Takeshi Kojima and Machel Reid are related as they are co-authors of the paper on Large language models are zero-shot reasoners", "from": "TAKEISHI KOJIMA", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "MACHEL REID", "width": 1.0}, {"description": "Takeshi Kojima and Yutaka Matsuo are related as they are co-authors of the paper on Large language models are zero-shot reasoners", "from": "TAKEISHI KOJIMA", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "YUTAKA MATSUO", "width": 1.0}, {"description": "Takeshi Kojima and Yusuke Iwasawa are related as they are co-authors of the paper on Large language models are zero-shot reasoners", "from": "TAKEISHI KOJIMA", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "YUSUKE IWASAWA", "width": 1.0}, {"description": "Michael Leonard and Na Li are related as they are co-authors of the paper on Promotional analysis and forecasting for demand planning: a practical time series approach", "from": "MICHAEL LEONARD", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "NA LI", "width": 1.0}, {"description": "Na Li and Donald M Arnold are related as they are co-authors of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "from": "NA LI", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "DONALD M ARNOLD", "width": 1.0}, {"description": "Na Li and Douglas G Down are related as they are co-authors of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "from": "NA LI", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "DOUGLAS G DOWN", "width": 1.0}, {"description": "Na Li and Rebecca Barty are related as they are co-authors of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "from": "NA LI", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "REBECCA BARTY", "width": 1.0}, {"description": "Na Li and John Blake are related as they are co-authors of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "from": "NA LI", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "JOHN BLAKE", "width": 1.0}, {"description": "Na Li and Fei Chiang are related as they are co-authors of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "from": "NA LI", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "FEI CHIANG", "width": 1.0}, {"description": "Na Li and Tom Courtney are related as they are co-authors of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "from": "NA LI", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "TOM COURTNEY", "width": 1.0}, {"description": "Na Li and Marianne Waito are related as they are co-authors of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "from": "NA LI", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "MARIANNE WAITO", "width": 1.0}, {"description": "Na Li and Rick Trifunov are related as they are co-authors of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "from": "NA LI", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "RICK TRIFUNOV", "width": 1.0}, {"description": "Na Li and Nancy M Heddle are related as they are co-authors of the paper on From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization", "from": "NA LI", "source_id": "97c1f318683bb63131ece0a51f096c5b", "to": "NANCY M HEDDLE", "width": 1.0}, {"description": "Rick Trifunov and Nancy M Heddle are authors of the paper \"From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization\"", "from": "RICK TRIFUNOV", "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "to": "NANCY M HEDDLE", "width": 1.0}, {"description": "Hengbo Liu and Ziqing Ma are authors of the paper \"Sadi: A self-adaptive decomposed interpretable framework for electric load forecasting under extreme events\"", "from": "HENGBO LIU", "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "to": "ZIQING MA", "width": 1.0}, {"description": "Xin Liu and Daniel McDuff are authors of the paper \"Large language models are few-shot health learners\"", "from": "XIN LIU", "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "to": "DANIEL MCDUFF", "width": 1.0}, {"description": "Yong Liu and Haixu Wu are authors of the paper \"Non-stationary transformers: Exploring the stationarity in time series forecasting\"", "from": "YONG LIU", "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "to": "HAIXU WU", "width": 1.0}, {"description": "Ziyang Ma and Wen Wu are authors of the paper \"Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition\"", "from": "ZIYANG MA", "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "to": "WEN WU", "width": 1.0}, {"description": "Spyros Makridakis and Michele Hibon are authors of the paper \"The m3-competition: results, conclusions and implications\"", "from": "SPYROS MAKRIDAKIS", "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "to": "MICHELE HIBON", "width": 1.0}, {"description": "Spyros Makridakis and Evangelos Spiliotis are authors of the paper \"The m4 competition: Results, findings, conclusion and way forward\"", "from": "SPYROS MAKRIDAKIS", "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "to": "EVANGELOS SPILIOTIS", "width": 1.0}, {"description": "Igor Melnyk and Vijil Chenthamarakshan are authors of the paper \"Reprogramming pretrained language models for antibody sequence infilling\"", "from": "IGOR MELNYK", "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "to": "VIJIL CHENTHAMARAKSHAN", "width": 1.0}, {"description": "Suvir Mirchandani and Fei Xia are authors of the paper \"Large language models as general pattern machines\"", "from": "SUVIR MIRCHANDANI", "source_id": "a4bb4cfc468e2f87ad5d8a4b451bcbbf", "to": "FEI XIA", "width": 1.0}, {"description": "Xia and Pete Florence are authors of the paper \"Large language models as general pattern machines\"", "from": "PETE FLORENCE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "XIA", "width": 1.0}, {"description": "Xia and Danny Driess are authors of the paper \"Large language models as general pattern machines\"", "from": "DANNY DRIESS", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "XIA", "width": 1.0}, {"description": "Xia and Montserrat Gonzalez Arenas are authors of the paper \"Large language models as general pattern machines\"", "from": "MONTSERRAT GONZALEZ ARENAS", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "XIA", "width": 1.0}, {"description": "Xia and Kanishka Rao are authors of the paper \"Large language models as general pattern machines\"", "from": "KANISHKA RAO", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "XIA", "width": 1.0}, {"description": "Xia and Andy Zeng are authors of the paper \"Large language models as general pattern machines\"", "from": "ANDY ZENG", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "XIA", "width": 1.0}, {"description": "Diganta Misra and Agam Goyal are authors of the paper \"Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets\"", "from": "DIGANTA MISRA", "source_id": "81b15ffb0d853301758618e61757cca9,a4bb4cfc468e2f87ad5d8a4b451bcbbf", "to": "AGAM GOYAL", "width": 2.0}, {"description": "Diganta Misra and Bharat Runwal are authors of the paper \"Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets\"", "from": "DIGANTA MISRA", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "BHARAT RUNWAL", "width": 1.0}, {"description": "Diganta Misra and Pin Yu Chen are authors of the paper \"Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets\"", "from": "DIGANTA MISRA", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "PIN YU CHEN", "width": 1.0}, {"description": "Xia and Dorsa Sadigh are authors of the paper \"Large language models as general pattern machines\"", "from": "DORSA SADIGH", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "XIA", "width": 1.0}, {"description": "Adam Paszke and Sam Gross are authors of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "from": "ADAM PASZKE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "SAM GROSS", "width": 1.0}, {"description": "Adam Paszke and Francisco Massa are authors of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "from": "ADAM PASZKE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "FRANCISCO MASSA", "width": 1.0}, {"description": "Adam Paszke and Adam Lerer are authors of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "from": "ADAM PASZKE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "ADAM LERER", "width": 1.0}, {"description": "Adam Paszke and James Bradbury are authors of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "from": "ADAM PASZKE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "JAMES BRADBURY", "width": 1.0}, {"description": "Adam Paszke and Gregory Chanan are authors of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "from": "ADAM PASZKE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "GREGORY CHANAN", "width": 1.0}, {"description": "Adam Paszke and Trevor Killeen are authors of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "from": "ADAM PASZKE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "TREVOR KILLEEN", "width": 1.0}, {"description": "Adam Paszke and Zeming Lin are authors of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "from": "ADAM PASZKE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "ZEMING LIN", "width": 1.0}, {"description": "Adam Paszke and Natalia Gimelshein are authors of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "from": "ADAM PASZKE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "NATALIA GIMELSHEIN", "width": 1.0}, {"description": "Adam Paszke and Luca Antiga are authors of the paper \"Pytorch: An imperative style, high-performance deep learning library\"", "from": "ADAM PASZKE", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "LUCA ANTIGA", "width": 1.0}, {"description": "Alec Radford and Jeffrey Wu are authors of the paper \"Language models are unsupervised multitask learners\"", "from": "ALEC RADFORD", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "JEFFREY WU", "width": 1.0}, {"description": "Alec Radford and Rewon Child are authors of the paper \"Language models are unsupervised multitask learners\"", "from": "ALEC RADFORD", "source_id": "81b15ffb0d853301758618e61757cca9", "to": "REWON CHILD", "width": 1.0}, {"description": "Baptiste Rozi\u00e8re is an author of the paper \"Llama: Open and efficient foundation language models\" along with Naman Goyal", "from": "BAPTISTE ROZIERE", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "NAMAN GOYAL", "width": 1.0}, {"description": "Naman Goyal is an author of the paper \"Llama: Open and efficient foundation language models\" along with Eric Hambro", "from": "NAMAN GOYAL", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "ERIC HAMBRO", "width": 1.0}, {"description": "Eric Hambro is an author of the paper \"Llama: Open and efficient foundation language models\" along with Faisal Azhar", "from": "ERIC HAMBRO", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "FAISAL AZHAR", "width": 1.0}, {"description": "Faisal Azhar is an author of the paper \"Llama: Open and efficient foundation language models\" along with Maria Tsimpoukelli", "from": "FAISAL AZHAR", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "MARIA TSIMPOUKELLI", "width": 1.0}, {"description": "Maria Tsimpoukelli is an author of the paper \"Multimodal few-shot learning with frozen language models\" along with Jacob L Menick", "from": "MARIA TSIMPOUKELLI", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "JACOB L MENICK", "width": 1.0}, {"description": "Jacob L Menick is an author of the paper \"Multimodal few-shot learning with frozen language models\" along with Serkan Cabi", "from": "JACOB L MENICK", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "SERKAN CABI", "width": 1.0}, {"description": "Serkan Cabi is an author of the paper \"Multimodal few-shot learning with frozen language models\" along with SM Eslami", "from": "SERKAN CABI", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "SM ESLAMI", "width": 1.0}, {"description": "SM Eslami is an author of the paper \"Multimodal few-shot learning with frozen language models\" along with Oriol Vinyals", "from": "SM ESLAMI", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "ORIOL VINYALS", "width": 1.0}, {"description": "Oriol Vinyals is an author of the paper \"Multimodal few-shot learning with frozen language models\" along with Felix Hill", "from": "ORIOL VINYALS", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "FELIX HILL", "width": 1.0}, {"description": "Ashish Vaswani is an author of the paper \"Attention is all you need\" along with Noam Shazeer", "from": "ASHISH VASWANI", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "NOAM SHAZEEER", "width": 1.0}, {"description": "Noam Shazeer is an author of the paper \"Attention is all you need\" along with Niki Parmar", "from": "NOAM SHAZEEER", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "NIKI PARMAR", "width": 1.0}, {"description": "Niki Parmar is an author of the paper \"Attention is all you need\" along with Jakob Uszkoreit", "from": "NIKI PARMAR", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "JAKOB USZKOREIT", "width": 1.0}, {"description": "Jakob Uszkoreit is an author of the paper \"Attention is all you need\" along with Llion Jones", "from": "JAKOB USZKOREIT", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "LLION JONES", "width": 1.0}, {"description": "Llion Jones is an author of the paper \"Attention is all you need\" along with Aidan N Gomez", "from": "LLION JONES", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "AIDAN N GOMEZ", "width": 1.0}, {"description": "Illia Polosukhin is an author of the paper \"Reprogramming language models for molecular representation learning\" along with Ria Vinod", "from": "ILLIA POLOSUKHIN", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "RIA VINOD", "width": 1.0}, {"description": "Siqiao Xue is an author of the paper \"Enhancing recommender systems with large language model reasoning graphs\" along with James Y Zhang", "from": "SIQIAO XUE", "source_id": "8c4fb3f97d731ab00c60399045cd97bd", "to": "JAMES Y ZHANG", "width": 1.0}, {"description": "Hao Xue and Flora D Salim are authors of the paper mentioned in the text", "from": "HAO XUE", "source_id": "98c6b5003112ab7110e45414a2fa468b", "to": "FLORA D SALIM", "width": 1.0}, {"description": "Self-supervised contrastive pre-training is related to Hao Xue as Hao Xue is an author of a paper on self-supervised contrastive pre-training", "from": "HAO XUE", "source_id": "98c6b5003112ab7110e45414a2fa468b", "to": "SELF-SUPERVISED CONTRASTIVE PRE-TRAINING", "width": 1.0}, {"description": "Multimodal large language models are related to Ailing Zeng as Ailing Zeng is an author of a paper on multimodal large language models", "from": "MULTIMODAL LARGE LANGUAGE MODELS", "source_id": "98c6b5003112ab7110e45414a2fa468b", "to": "AILING ZENG", "width": 1.0}, {"description": "Less is more is related to Qiang Xu as Qiang Xu is an author of a paper on less is more", "from": "QIANG XU", "source_id": "98c6b5003112ab7110e45414a2fa468b", "to": "LESS IS MORE", "width": 1.0}, {"description": "Long sequence time-series forecasting is related to AAAI Conference as research papers on time series forecasting are presented at the conference", "from": "AAAI CONFERENCE", "source_id": "a69a914fb6c895c7202532b69ad3e094", "to": "LONG SEQUENCE TIME-SERIES FORECASTING", "width": 1.0}, {"description": "AAAI Conference is related to Proceedings of the AAAI Conference on Artificial Intelligence as research papers presented at the conference are published in the document", "from": "AAAI CONFERENCE", "source_id": "a69a914fb6c895c7202532b69ad3e094", "to": "PROCEEDINGS OF THE AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE", "width": 1.0}, {"description": "Tian Zhou 2023a is related to Peisong Niu as they are co-authors of the research paper", "from": "TIAN ZHOU 2023A", "source_id": "a69a914fb6c895c7202532b69ad3e094", "to": "PEISONG NIU", "width": 1.0}, {"description": "Peisong Niu is related to Yunyi Zhou as they are co-authors of the research paper", "from": "PEISONG NIU", "source_id": "a69a914fb6c895c7202532b69ad3e094", "to": "YUNYI ZHOU", "width": 1.0}, {"description": "Dependency discovery is related to representation aggregation as representation aggregation is used to combine multiple representations of a time series", "from": "DEPENDENCY DISCOVERY", "source_id": "f7266cfccedb1d9840d10afa689a05e9", "to": "REPRESENTATION AGGREGATION", "width": 1.0}, {"description": "Wu et al. are related to Makridakis et al. as they are both authors of papers mentioned in the text", "from": "WU ET AL.", "source_id": "74527a4337ed6919731be520311ae774", "to": "MAKRIDAKIS ET AL.", "width": 1.0}, {"description": "Makridakis et al. are related to Makridakis as they are both authors of papers mentioned in the text", "from": "MAKRIDAKIS ET AL.", "source_id": "74527a4337ed6919731be520311ae774", "to": "MAKRIDAKIS", "width": 1.0}, {"description": "Each entry within the ETT datasets includes six power load features and a target variable, termed \u201coil temperature\u201d", "from": "POWER LOAD FEATURES", "source_id": "50eeacd99c68b2581be90310bedcbc2c", "to": "OIL TEMPERATURE", "width": 1.0}, {"description": "Patch dimensions are related to heads as both are parameters used in the TIME-LLM model", "from": "PATCH DIMENSIONS", "source_id": "1b48e9ca066ac5ba037066bb762d3458", "to": "HEADS", "width": 1.0}, {"description": "LTF - ETTh1 and LTF - ETTh2 are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH1", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ETTH2", "width": 1.0}, {"description": "LTF - ETTh1 and LTF - ETTm1 are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH1", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ETTM1", "width": 1.0}, {"description": "LTF - ETTh1 and LTF - ETTm2 are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH1", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ETTM2", "width": 1.0}, {"description": "LTF - ETTh1 and LTF - Weather are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH1", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - WEATHER", "width": 1.0}, {"description": "LTF - ETTh1 and LTF - Electricity are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH1", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ELECTRICITY", "width": 1.0}, {"description": "LTF - ETTh1 and LTF - Traffic are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH1", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - TRAFFIC", "width": 1.0}, {"description": "LTF - ETTh1 and LTF - ILI are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH1", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ILI", "width": 1.0}, {"description": "LTF - ETTh2 and LTF - ETTm1 are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH2", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ETTM1", "width": 1.0}, {"description": "LTF - ETTh2 and LTF - ETTm2 are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH2", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ETTM2", "width": 1.0}, {"description": "LTF - ETTh2 and LTF - Weather are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH2", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - WEATHER", "width": 1.0}, {"description": "LTF - ETTh2 and LTF - Electricity are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH2", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ELECTRICITY", "width": 1.0}, {"description": "LTF - ETTh2 and LTF - Traffic are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH2", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - TRAFFIC", "width": 1.0}, {"description": "LTF - ETTh2 and LTF - ILI are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTH2", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ILI", "width": 1.0}, {"description": "LTF - ETTm1 and LTF - ETTm2 are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTM1", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ETTM2", "width": 1.0}, {"description": "LTF - ETTm1 and LTF - Weather are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTM1", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - WEATHER", "width": 1.0}, {"description": "LTF - ETTm1 and LTF - Electricity are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTM1", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ELECTRICITY", "width": 1.0}, {"description": "LTF - ETTm1 and LTF - Traffic are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTM1", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - TRAFFIC", "width": 1.0}, {"description": "LTF - ETTm1 and LTF - ILI are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTM1", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ILI", "width": 1.0}, {"description": "LTF - ETTm2 and LTF - Weather are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTM2", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - WEATHER", "width": 1.0}, {"description": "LTF - ETTm2 and LTF - Electricity are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTM2", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ELECTRICITY", "width": 1.0}, {"description": "LTF - ETTm2 and LTF - Traffic are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTM2", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - TRAFFIC", "width": 1.0}, {"description": "LTF - ETTm2 and LTF - ILI are related as they are both configurations used for long-term forecasting", "from": "LTF - ETTM2", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ILI", "width": 1.0}, {"description": "LTF - Weather and LTF - Electricity are related as they are both configurations used for long-term forecasting", "from": "LTF - WEATHER", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ELECTRICITY", "width": 1.0}, {"description": "LTF - Weather and LTF - Traffic are related as they are both configurations used for long-term forecasting", "from": "LTF - WEATHER", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - TRAFFIC", "width": 1.0}, {"description": "LTF - Weather and LTF - ILI are related as they are both configurations used for long-term forecasting", "from": "LTF - WEATHER", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ILI", "width": 1.0}, {"description": "LTF - Electricity and LTF - Traffic are related as they are both configurations used for long-term forecasting", "from": "LTF - ELECTRICITY", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - TRAFFIC", "width": 1.0}, {"description": "LTF - Electricity and LTF - ILI are related as they are both configurations used for long-term forecasting", "from": "LTF - ELECTRICITY", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ILI", "width": 1.0}, {"description": "LTF - Traffic and LTF - ILI are related as they are both configurations used for long-term forecasting", "from": "LTF - TRAFFIC", "source_id": "306c29917039736b5e882376c7647704", "to": "LTF - ILI", "width": 1.0}, {"description": "STF - M3-Quarterly and STF - M4 are related as they are both configurations used for short-term forecasting", "from": "STF - M3-QUARTERLY", "source_id": "306c29917039736b5e882376c7647704", "to": "STF - M4", "width": 1.0}, {"description": "Time series input length is related to forecasting accuracy as time series input length affects the model\u0027s forecasting accuracy", "from": "TIME SERIES INPUT LENGTH", "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "to": "FORECASTING ACCURACY", "width": 1.0}, {"description": "Patch reprogramming cross-attention heads are related to forecasting accuracy as patch reprogramming cross-attention heads affect the model\u0027s forecasting accuracy", "from": "PATCH REPROGRAMMING CROSS-ATTENTION HEADS", "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "to": "FORECASTING ACCURACY", "width": 1.0}, {"description": "LLAMAs are related to SOTA as LLAMAs are used to achieve SOTA performance", "from": "LLAMAS", "source_id": "e57d44d23f5a3a82ce9a9b9532d31cbe", "to": "SOTA", "width": 1.0}, {"description": "Traffic flow is related to velocity as velocity is a measure of traffic flow", "from": "TRAFFIC FLOW", "source_id": "2dbfdb45630a023a5a6979b9573a868f", "to": "VELOCITY", "width": 1.0}, {"description": "Velocity is related to acceleration as acceleration is the rate of change of velocity", "from": "VELOCITY", "source_id": "2dbfdb45630a023a5a6979b9573a868f", "to": "ACCELERATION", "width": 1.0}, {"description": "Velocity 1 is related to velocity 2 as velocity 2 is an example of velocity 1", "from": "VELOCITY 1", "source_id": "2dbfdb45630a023a5a6979b9573a868f", "to": "VELOCITY 2", "width": 1.0}, {"description": "Velocity 3 is related to velocity 1 as velocity 1 is an example of velocity 3", "from": "VELOCITY 1", "source_id": "2dbfdb45630a023a5a6979b9573a868f", "to": "VELOCITY 3", "width": 1.0}, {"description": "Velocity 2 is related to velocity 3 as velocity 3 is an example of velocity 2", "from": "VELOCITY 2", "source_id": "2dbfdb45630a023a5a6979b9573a868f", "to": "VELOCITY 3", "width": 1.0}, {"description": "Acceleration 1 is related to acceleration 2 as acceleration 2 is an example of acceleration 1", "from": "ACCELERATION 1", "source_id": "2dbfdb45630a023a5a6979b9573a868f", "to": "ACCELERATION 2", "width": 1.0}, {"description": "Acceleration 3 is related to acceleration 1 as acceleration 1 is an example of acceleration 3", "from": "ACCELERATION 1", "source_id": "2dbfdb45630a023a5a6979b9573a868f", "to": "ACCELERATION 3", "width": 1.0}, {"description": "Acceleration 2 is related to acceleration 3 as acceleration 3 is an example of acceleration 2", "from": "ACCELERATION 2", "source_id": "2dbfdb45630a023a5a6979b9573a868f", "to": "ACCELERATION 3", "width": 1.0}, {"description": "Table 3 is a specific table mentioned in the text", "from": "TABLE", "source_id": "deb67d5386710136cf24bcbf135a66c4", "to": "TABLE 3", "width": 1.0}, {"description": "The table contains the results of the abalation studies", "from": "TABLE", "source_id": "15c3350fad556f666d93817c5036109c", "to": "RESULTS", "width": 1.0}, {"description": "Table 3 presents the results of the study", "from": "TABLE 3", "source_id": "deb67d5386710136cf24bcbf135a66c4", "to": "RESULTS", "width": 1.0}, {"description": "The results of the study are related to ablation studies", "from": "RESULTS", "source_id": "deb67d5386710136cf24bcbf135a66c4", "to": "ABALATION STUDIES", "width": 1.0}, {"description": "F1-scores are used to evaluate experimental conditions", "from": "F1-SCORES", "source_id": "deb67d5386710136cf24bcbf135a66c4", "to": "EXPERIMENTAL CONDITIONS", "width": 1.0}, {"description": "Baseline models are used for evaluating the performance of models in short-term forecasting on M4-Yearly dataset", "from": "BASELINE MODELS", "source_id": "ef2ef6c95c36f51706c54ca3f2893641", "to": "M4-YEARLY", "width": 1.0}, {"description": "TranAD is related to baseline models as TranAD is compared to baseline models", "from": "BASELINE MODELS", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "TRANAD", "width": 1.0}, {"description": "M4-Yearly and M4-Hourly are datasets used for evaluating the performance of models in short-term forecasting", "from": "M4-YEARLY", "source_id": "ef2ef6c95c36f51706c54ca3f2893641", "to": "M4-HOURLY", "width": 1.0}, {"description": "M4-Hourly and M4-Daily are datasets used for evaluating the performance of models in short-term forecasting", "from": "M4-HOURLY", "source_id": "ef2ef6c95c36f51706c54ca3f2893641", "to": "M4-DAILY", "width": 1.0}, {"description": "M4-Daily and M4-Weekly are datasets used for evaluating the performance of models in short-term forecasting", "from": "M4-DAILY", "source_id": "ef2ef6c95c36f51706c54ca3f2893641", "to": "M4-WEEKLY", "width": 1.0}, {"description": "Reprogrammed LLM is used for few-shot forecasting tasks", "from": "FEW-SHOT FORECASTING", "source_id": "5fa25d3abec59ccd2718e00c0e0eb440", "to": "REPROGRAMMED LLM", "width": 1.0}, {"description": "Long-term forecasting is related to few-shot forecasting as both are concepts in forecasting", "from": "FEW-SHOT FORECASTING", "source_id": "b90805909f7b1f31ddc03014f161d3ba", "to": "LONG-TERM FORECASTING", "width": 1.0}, {"description": "Table 17 contains information about few-shot forecasting", "from": "FEW-SHOT FORECASTING", "source_id": "b90805909f7b1f31ddc03014f161d3ba", "to": "TABLE 17", "width": 1.0}, {"description": "ETTM1-96 is related to few-shot forecasting as it is used to evaluate the performance of few-shot forecasting models", "from": "FEW-SHOT FORECASTING", "source_id": "e6a6bc6fbd362394320961ac10cdd230", "to": "ETTM1-96", "width": 1.0}, {"description": "ETTM1-192 is related to few-shot forecasting as it is used to evaluate the performance of few-shot forecasting models", "from": "FEW-SHOT FORECASTING", "source_id": "e6a6bc6fbd362394320961ac10cdd230", "to": "ETTM1-192", "width": 1.0}, {"description": "ETTH1-96 is related to few-shot forecasting as it is used to evaluate the performance of few-shot forecasting models", "from": "FEW-SHOT FORECASTING", "source_id": "e6a6bc6fbd362394320961ac10cdd230", "to": "ETTH1-96", "width": 1.0}, {"description": "ETTH1-192 is related to few-shot forecasting as it is used to evaluate the performance of few-shot forecasting models", "from": "FEW-SHOT FORECASTING", "source_id": "e6a6bc6fbd362394320961ac10cdd230", "to": "ETTH1-192", "width": 1.0}, {"description": "Average is related to 96 as average is a measure of central tendency at time point 96", "from": "AVERAGE", "source_id": "a014d14e003d0998b877eacffa8ebfc4", "to": "96", "width": 1.0}, {"description": "Average is related to 192 as average is a measure of central tendency at time point 192", "from": "AVERAGE", "source_id": "a014d14e003d0998b877eacffa8ebfc4", "to": "192", "width": 1.0}, {"description": "Average is related to 336 as average is a measure of central tendency at time point 336", "from": "AVERAGE", "source_id": "a014d14e003d0998b877eacffa8ebfc4", "to": "336", "width": 1.0}, {"description": "Average is related to 720 as average is a measure of central tendency at time point 720", "from": "AVERAGE", "source_id": "a014d14e003d0998b877eacffa8ebfc4", "to": "720", "width": 1.0}, {"description": "96 is related to 192 as 192 is a subsequent time point", "from": "96", "source_id": "a014d14e003d0998b877eacffa8ebfc4", "to": "192", "width": 1.0}, {"description": "192 is related to 336 as 336 is a subsequent time point", "from": "192", "source_id": "a014d14e003d0998b877eacffa8ebfc4", "to": "336", "width": 1.0}, {"description": "336 is related to 720 as 720 is a subsequent time point", "from": "336", "source_id": "a014d14e003d0998b877eacffa8ebfc4", "to": "720", "width": 1.0}, {"description": "Short-term time series forecasting is related to full short-term time series forecasting results as full short-term time series forecasting results is a specific type of short-term time series forecasting", "from": "SHORT-TERM TIME SERIES FORECASTING", "source_id": "f49330b6fd81d86d14e7a9d4b8e45576", "to": "FULL SHORT-TERM TIME SERIES FORECASTING RESULTS", "width": 1.0}, {"description": "Full short-term time series forecasting results is related to forecasting horizons as forecasting horizons are used in full short-term time series forecasting results", "from": "FULL SHORT-TERM TIME SERIES FORECASTING RESULTS", "source_id": "f49330b6fd81d86d14e7a9d4b8e45576", "to": "FORECASTING HORIZONS", "width": 1.0}, {"description": "MASE values and OWA values are related as both are used to evaluate the performance of time series forecasting models", "from": "MASE VALUES", "source_id": "d540ee970ce7debedc6b1b95e9c88be8", "to": "OWA VALUES", "width": 1.0}, {"description": "MASE values and SMAPE values are related as both are used to evaluate the performance of time series forecasting models", "from": "MASE VALUES", "source_id": "d540ee970ce7debedc6b1b95e9c88be8", "to": "SMAPE VALUES", "width": 1.0}, {"description": "OWA values and SMAPE values are related as both are used to evaluate the performance of time series forecasting models", "from": "OWA VALUES", "source_id": "d540ee970ce7debedc6b1b95e9c88be8", "to": "SMAPE VALUES", "width": 1.0}, {"description": "Tab. 17 is related to Tab. 18 as Tab. 18 is a continuation of Tab. 17", "from": "TAB. 17", "source_id": "7e97089185883c456c798ddc5ec86373", "to": "TAB. 18", "width": 1.0}, {"description": "GPU memory is related to running time as running time is affected by GPU memory", "from": "GPU MEMORY", "source_id": "7e97089185883c456c798ddc5ec86373", "to": "RUNNING TIME", "width": 1.0}, {"description": "Table 17 contains information about the transformation or conversion between ET T m2 and ET T m1", "from": "ET T M2 \u2192 ET T M1", "source_id": "b90805909f7b1f31ddc03014f161d3ba", "to": "TABLE 17", "width": 1.0}, {"description": "Table 17 contains information about long-term forecasting", "from": "LONG-TERM FORECASTING", "source_id": "b90805909f7b1f31ddc03014f161d3ba", "to": "TABLE 17", "width": 1.0}, {"description": "ETTh1-96 is related to ETTh1-192 as both are specific types of data or models that predict 96 and 192 steps ahead", "from": "ET T H1-96", "source_id": "b90805909f7b1f31ddc03014f161d3ba", "to": "ET T H1-192", "width": 1.0}, {"description": "ETTh1-96 is related to ETTm1-96 as both are specific types of data or models that predict 96 steps ahead", "from": "ET T H1-96", "source_id": "b90805909f7b1f31ddc03014f161d3ba", "to": "ET T M1-96", "width": 1.0}, {"description": "ETTh1-192 is related to ETTm1-192 as both are specific types of data or models that predict 192 steps ahead", "from": "ET T H1-192", "source_id": "b90805909f7b1f31ddc03014f161d3ba", "to": "ET T M1-192", "width": 1.0}, {"description": "ETTm1-96 is related to ETTm1-192 as both are specific types of data or models that predict 96 and 192 steps ahead", "from": "ET T M1-96", "source_id": "b90805909f7b1f31ddc03014f161d3ba", "to": "ET T M1-192", "width": 1.0}, {"description": "ETTM1-96 is related to ETTM1-192 as they are both time series datasets", "from": "ETTM1-96", "source_id": "e6a6bc6fbd362394320961ac10cdd230", "to": "ETTM1-192", "width": 1.0}, {"description": "ETTH1-96 is related to ETTH1-192 as they are both time series datasets", "from": "ETTH1-96", "source_id": "e6a6bc6fbd362394320961ac10cdd230", "to": "ETTH1-192", "width": 1.0}, {"description": "TranAD is a deep transformer network based anomaly detection and diagnosis model", "from": "DEEP TRANSFORMER NETWORKS", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "to": "TRANAD", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTranAD is a model designed specifically for multivariate time series data, which consists of multiple variables measured over time. This type of data is characterized by its complexity, as it involves multiple variables that are interrelated and change over time. TranAD is well-suited to handle this complexity, making it a valuable tool for analyzing and forecasting multivariate time series data.\n\nThe primary application of TranAD is in the analysis of multivariate time series data, which is a key aspect of its design. This suggests that TranAD is intended to be used in scenarios where multiple variables are measured over time, and the relationships between these variables are of interest.\n\nIn terms of the type of data that TranAD is designed to handle, it is clear that it is intended for use with multivariate time series data. This is consistent with the description provided, which states that TranAD is designed for multivariate time series data.\n\nOverall, TranAD appears to be a specialized model designed to handle the complexities of multivariate time series data. Its primary application is in the analysis of this type of data, and it is well-suited to handle the multiple variables and temporal relationships that are characteristic of multivariate time series data.\n\nRelevant information from the nearby text:\n\n* The text mentions that TranAD is designed for multivariate time series data, which consists of multiple variables measured over time.\n* The text also mentions that TranAD is used for multivariate time series data, which is consistent with its design and primary application.\n\nNote: There is no information provided about the \"MULTIVARIATE TIME SERIES DATA\" entity, so it is not included in the summary.", "from": "MULTIVARIATE TIME SERIES DATA", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3,78ee4a3d7a2bffd4405a03d94a4f6cb1", "to": "TRANAD", "width": 2.0}, {"description": "Shreshth Tuli is an author of the paper that proposes TranAD", "from": "SHRESHTH TULI", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "to": "TRANAD", "width": 1.0}, {"description": "Shreshth Tuli is affiliated with Imperial College London", "from": "SHRESHTH TULI", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "to": "IMPERIAL COLLEGE LONDON", "width": 1.0}, {"description": "Shreshth Tuli is affiliated with London, UK", "from": "SHRESHTH TULI", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "to": "LONDON, UK", "width": 1.0}, {"description": "A type of operating system", "from": "SHRESHTH TULI", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "WINDOWS 11 OS", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nSHRESHTH TULI and GIULIANO CASALE are researchers who have collaborated on a research paper. They are co-authors of the paper, which is mentioned in the text and cited as \"Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings\". This paper is likely related to the development of the TranAD model, as SHRESHTH TULI is described as a researcher who contributed to its development. The paper is likely an academic publication, given the presence of English-language technical terms, mathematical equations, and references to academic conferences and journals, such as ICLR, AAAI, and PMLR.", "from": "SHRESHTH TULI", "source_id": "5277ea101e78f34ea2c62fa10007b2ac,9b35a2c607e0f4b8cbc9179f424f180a,a4a241e471ad258932241bc441b96155,fa91ecd90e1c12d64176de581c6220c7", "to": "GIULIANO CASALE", "width": 4.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nShreshth Tuli and Nicholas R. Jennings are co-authors of a paper, as indicated by the citation \"Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings\". This suggests that they have collaborated on a research project, likely in the field of computer science or a related discipline. The paper is mentioned in the text, implying that it is a significant contribution to the field and has been referenced by the author.\n\nGiven the context of the text, which appears to be a technical document or research paper, it is likely that the paper written by Shreshth Tuli and Nicholas R. Jennings deals with topics such as time series analysis, long-term series forecasting, frequency analysis, or multi-head cross-attention, as these terms are mentioned in the text. The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR\" further suggests that the paper is written in English and has been published in an English-language academic conference or journal.\n\nOverall, Shreshth Tuli and Nicholas R. Jennings are co-authors of a paper that has been referenced in the text, and their work likely contributes to the field of computer science or a related discipline.", "from": "SHRESHTH TULI", "source_id": "5277ea101e78f34ea2c62fa10007b2ac,9b35a2c607e0f4b8cbc9179f424f180a,a4a241e471ad258932241bc441b96155", "to": "NICHOLAS R. JENNINGS", "width": 3.0}, {"description": "Giuliano Casale is an author of the paper that proposes TranAD", "from": "GIULIANO CASALE", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "to": "TRANAD", "width": 1.0}, {"description": "Shreshth Tuli and Giuliano Casale are authors of the paper", "from": "GIULIANO CASALE", "source_id": "bd73ee0439609823e12a877840c6ebae", "to": "SHRESTH TULI", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nGiuliano Casale and Nicholas R. Jennings are two researchers who have made significant contributions to the field of research, particularly in the development of the TranAD model. They are co-authors of a paper, as indicated by the citation \"Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings\", which suggests that they have collaborated on a research project. This paper is likely a technical document or research paper, given the formal and academic language used, which includes technical terms such as \"time series,\" \"long-term series forecasting,\" \"frequency analysis,\" and \"multi-head cross-attention.\" The presence of mathematical equations and formulas, as well as English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" further supports the notion that the paper is written in English and is likely from an academic conference or journal.\n\nOverall, Giuliano Casale and Nicholas R. Jennings are notable researchers who have contributed to the development of the TranAD model and have collaborated on a research paper, which is a testament to their expertise and collaboration in the field of research.", "from": "GIULIANO CASALE", "source_id": "5277ea101e78f34ea2c62fa10007b2ac,9b35a2c607e0f4b8cbc9179f424f180a,a4a241e471ad258932241bc441b96155,bd73ee0439609823e12a877840c6ebae,fa91ecd90e1c12d64176de581c6220c7", "to": "NICHOLAS R. JENNINGS", "width": 5.0}, {"description": "Nicholas R. Jennings is an author of the paper that proposes TranAD", "from": "NICHOLAS R. JENNINGS", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "to": "TRANAD", "width": 1.0}, {"description": "Nicholas R. Jennings is affiliated with Loughborough University", "from": "NICHOLAS R. JENNINGS", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "to": "LOUGHBOROUGH UNIVERSITY", "width": 1.0}, {"description": "Shreshth Tuli and Nicholas R. Jennings are authors of the paper", "from": "NICHOLAS R. JENNINGS", "source_id": "bd73ee0439609823e12a877840c6ebae", "to": "SHRESTH TULI", "width": 1.0}, {"description": "Imperial College London is related to President\u0027s PhD scholarship as Shreshth Tuli is supported by this award", "from": "IMPERIAL COLLEGE LONDON", "source_id": "a39d9d28126733c33db624ccc25f5782", "to": "PRESIDENT\u0027S PHD SCHOLARSHIP", "width": 1.0}, {"description": "TranAD uses model-agnostic meta learning", "from": "MODEL-AGNOSTIC META LEARNING", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "to": "TRANAD", "width": 1.0}, {"description": "TranAD is related to model-agnostic meta learning as it is a technique used in TranAD to improve generalization and adaptability", "from": "MODEL-AGNOSTIC META LEARNING", "source_id": "bd73ee0439609823e12a877840c6ebae", "to": "TRANAAD", "width": 1.0}, {"description": "TranAD uses adversarial training", "from": "ADVERSARIAL TRAINING", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "to": "TRANAD", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"ADVERSARIAL TRAINING\" and \"TRANAAD\" are related in that they are connected through the technique of adversarial training. Specifically, TranAD utilizes adversarial training to amplify reconstruction errors and improve anomaly detection. Furthermore, this technique is also used in TranAD to amplify errors and gain training stability. This suggests that adversarial training plays a crucial role in the development and functionality of TranAD, enabling it to effectively detect anomalies and improve its overall performance.\n\nIn terms of the relationship between the two entities, it appears that TranAD is a technique or method that leverages adversarial training to achieve its goals. The use of adversarial training in TranAD allows it to improve its anomaly detection capabilities and gain stability during the training process. This highlights the importance of adversarial training in the context of TranAD and its potential applications in various fields.\n\nOverall, the connection between ADVERSARIAL TRAINING and TRANAAD is one of technique and application, with adversarial training being a key component of TranAD\u0027s functionality and effectiveness.", "from": "ADVERSARIAL TRAINING", "source_id": "a39d9d28126733c33db624ccc25f5782,bd73ee0439609823e12a877840c6ebae", "to": "TRANAAD", "width": 2.0}, {"description": "Adversarial training and self-conditioning are used in TranAD to amplify errors and gain training stability", "from": "ADVERSARIAL TRAINING", "source_id": "78ee4a3d7a2bffd4405a03d94a4f6cb1", "to": "SELF-CONDITIONING", "width": 1.0}, {"description": "TranAD uses focus score-based self-conditioning", "from": "FOCUS SCORE-BASED SELF-CONDITIONING", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "to": "TRANAD", "width": 1.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nMTAD-GAT and TRANAD are two deep learning models used for anomaly detection. They have been compared in the text, indicating that they are both relevant models in the field of anomaly detection. Specifically, TRANAD has been found to outperform MTAD-GAT in terms of detection and diagnosis performance.\n\nThis summary is based on the information collected from all the descriptions, and it resolves any potential contradictions by providing a clear and concise overview of the two entities, MTAD-GAT and TRANAD. The summary is written in third person and includes the entity names for context.\n\nRelevant information from the nearby text has been incorporated into the summary, including the fact that both models are deep learning models and have been compared in the text. The summary also highlights the key finding that TRANAD outperforms MTAD-GAT in detection and diagnosis performance.", "from": "MTAD-GAT", "source_id": "00973c1c3c962d9234a38037709824b1,0c4c072869e10b0b4bd4bc19a60a23a3,2b44aebc638544dcf835db30c4270d09,a4a241e471ad258932241bc441b96155", "to": "TRANAD", "width": 4.0}, {"description": "LSTMs are related to MTAD-GAT as MTAD-GAT uses deep neural networks, which are similar to LSTMs", "from": "MTAD-GAT", "source_id": "bd73ee0439609823e12a877840c6ebae", "to": "LSTMS", "width": 1.0}, {"description": "MTAD-GAT is related to graph-attention network as both are used for modeling feature and temporal correlations", "from": "MTAD-GAT", "source_id": "0259287b914980606371cd1161c6a420", "to": "GRAPH-ATTENTION NETWORK", "width": 1.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nMTAD-GAT and CAE-M are two deep learning models for anomaly detection. Both models have been evaluated in the text, with their performance metrics indicating their effectiveness in detecting anomalies. Specifically, MTAD-GAT and CAE-M are both models used for anomaly detection, with CAE-M having a higher F1 score than MTAD-GAT. However, despite CAE-M\u0027s higher F1 score, MTAD-GAT outperforms CAE-M when compared across all datasets and also when given the complete WADI dataset.\n\nThis summary is based on the information collected from all the descriptions, and it resolves any potential contradictions by providing a clear and concise overview of the relationship between MTAD-GAT and CAE-M. The summary is written in third person and includes the entity names for context.\n\nIt is worth noting that the text does not provide any information about the specific architecture or implementation details of MTAD-GAT and CAE-M. However, based on their names, it can be inferred that they are both graph-based models, with MTAD-GAT likely being a graph attention network (GAT) and CAE-M possibly being a convolutional autoencoder (CAE) with a modified architecture.\n\nOverall, the summary provides a clear understanding of the relationship between MTAD-GAT and CAE-M, and it highlights their strengths and weaknesses in anomaly detection tasks.", "from": "MTAD-GAT", "source_id": "00973c1c3c962d9234a38037709824b1,2b44aebc638544dcf835db30c4270d09,2fc273b26b3ec71da711daaa40c0355d,70a97858b727e5af1466ae5eb9183921,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb", "to": "CAE-M", "width": 6.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMTAD-GAT and MAD-GAN are two deep learning models designed for anomaly detection. Both models are utilized for identifying anomalies, as indicated by their application in the table. A comparison of their performance suggests that MTAD-GAT outperforms MAD-GAN in terms of F1 score, indicating its superior ability in detecting anomalies.\n\nThis summary is based on the information collected from all the descriptions provided, and it resolves any potential contradictions by highlighting the specific performance difference between the two models. The relevant details from the nearby text, such as the use of deep learning models and anomaly detection, are also incorporated into the summary to provide a comprehensive understanding of the entities and their characteristics.", "from": "MTAD-GAT", "source_id": "00973c1c3c962d9234a38037709824b1,2b44aebc638544dcf835db30c4270d09,d5c8b72da09cebefa0c26285ad5272eb", "to": "MAD-GAN", "width": 3.0}, {"description": "MERLIN and MTAD-GAT are both models used for anomaly detection, with MTAD-GAT having a higher F1 score than MERLIN", "from": "MTAD-GAT", "source_id": "d5c8b72da09cebefa0c26285ad5272eb", "to": "MERLIN", "width": 1.0}, {"description": "LSTM-NDT and MTAD-GAT are both models used for anomaly detection, with MTAD-GAT having a higher F1 score than LSTM-NDT", "from": "MTAD-GAT", "source_id": "d5c8b72da09cebefa0c26285ad5272eb", "to": "LSTM-NDT", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMTAD-GAT and DAGMM are two deep learning models used for anomaly detection. Both models are utilized for identifying anomalies, with MTAD-GAT and DAGMM being employed in this context. Notably, MTAD-GAT has been observed to have a higher F1 score compared to DAGMM, indicating its superior performance in anomaly detection tasks.\n\nThis summary is based on the information provided in the description list, which states that both models are used for anomaly detection and that MTAD-GAT has a higher F1 score than DAGMM. The entities \"MTAD-GAT\" and \"DAGMM\" are mentioned as deep learning models, and the context suggests that they are being used for anomaly detection tasks.\n\nIt is worth noting that the text does not provide any information about the language or content of the text, which was analyzed separately. However, the provided description list contains technical terms and mathematical equations, which are consistent with the analysis of the text language.", "from": "MTAD-GAT", "source_id": "2b44aebc638544dcf835db30c4270d09,d5c8b72da09cebefa0c26285ad5272eb", "to": "DAGMM", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entities \"LSTM-NDT\" and \"TRANAD\" are two models mentioned in the text. Specifically, TranAD is related to LSTM-NDT as both are models that have been compared in terms of their performance. According to the description, TranAD outperforms LSTM-NDT in detection and diagnosis performance. This suggests that TranAD has demonstrated superior capabilities in these areas compared to LSTM-NDT.\n\nIt is worth noting that the text does not provide further information on the specific context in which these models are being used or the exact nature of their comparison. However, based on the information provided, it appears that TranAD has a competitive advantage over LSTM-NDT in terms of detection and diagnosis performance.\n\nIn terms of the language used, the text is written in English, as indicated by the use of technical terms, mathematical equations, and references to academic papers. The language is formal and academic, suggesting that the text is from a research paper or a technical document.", "from": "LSTM-NDT", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3,971e73b638469366cfdd3af2e7c7a824", "to": "TRANAD", "width": 2.0}, {"description": "Based on the provided descriptions, a comprehensive summary of the data can be generated as follows:\n\nMERLIN and LSTM-NDT are two state-of-the-art models for multivariate time-series anomaly detection. Both methods are used for anomaly detection and diagnosis in multivariate time series data. They are models that have been compared in the text, with MERLIN having a higher F1 score than LSTM-NDT. MERLIN and LSTM-NDT are both models used for anomaly detection, with LSTM-NDT relying on an LSTM-based deep neural network model that uses the input sequence as training data and forecasts data for the next timestamp.\n\nThe relationship between MERLIN and LSTM-NDT can be understood as follows: LSTM-NDT is a method that uses an LSTM-based deep neural network model, whereas MERLIN is a model that has been compared to LSTM-NDT in terms of their performance on multivariate time-series anomaly detection tasks.\n\nIn terms of their technical characteristics, both MERLIN and LSTM-NDT are used for anomaly detection in multivariate time series data. They are both models that have been mentioned in the text, indicating their relevance and importance in the field of time-series anomaly detection.\n\nOverall, MERLIN and LSTM-NDT are two prominent models for multivariate time-series anomaly detection, with MERLIN having a higher F1 score than LSTM-NDT. Their comparison in the text highlights their strengths and weaknesses, providing valuable insights for researchers and practitioners working in this field.", "from": "LSTM-NDT", "source_id": "00973c1c3c962d9234a38037709824b1,1413d358c623ac2d4c70be6547eb218b,1902e651467179a9a1d4c4df0035e980,1b51ec337efd822ca3a0b3eb819c1b91,971e73b638469366cfdd3af2e7c7a824,a4a241e471ad258932241bc441b96155,d5c8b72da09cebefa0c26285ad5272eb,ffbbbf29ffb8d038e241f023079cb0a2", "to": "MERLIN", "width": 8.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nLSTM-NDT and DAGMM are two state-of-the-art models for multivariate time-series anomaly detection. Both models have been evaluated in the text, as indicated by the presence of their names and performance metrics. Specifically, LSTM-NDT and DAGMM are both used for anomaly detection, with DAGMM having a higher F1 score than LSTM-NDT. This suggests that DAGMM may be a more effective model for this task. The text also indicates that both models are related, as they are both mentioned in the document.\n\nIt is worth noting that the text is written in English, as indicated by the presence of technical terms, mathematical equations, and references to academic papers. The language used is formal and academic, suggesting that the text is from a research paper or a technical document. The use of English words and phrases, such as \"time series,\" \"long-term series forecasting,\" and \"frequency analysis,\" further supports this conclusion. Additionally, the presence of English-language abbreviations, such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" and the citation of English-language academic papers and authors, also suggest that the text is written in English.\n\nOverall, the summary provides a clear understanding of the two models, LSTM-NDT and DAGMM, their relationship, and their performance in multivariate time-series anomaly detection.", "from": "LSTM-NDT", "source_id": "1b51ec337efd822ca3a0b3eb819c1b91,2fc273b26b3ec71da711daaa40c0355d,971e73b638469366cfdd3af2e7c7a824,d16a81565fcd654ab21768510dcc5d7f,d5c8b72da09cebefa0c26285ad5272eb", "to": "DAGMM", "width": 5.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nLSTM-NDT and MAD-GAN are two machine learning models used for anomaly detection. Both models are mentioned in the text, indicating their relevance to the topic. Specifically, LSTM-NDT and MAD-GAN are compared in terms of their performance, with MAD-GAN having a higher F1 score than LSTM-NDT. This suggests that MAD-GAN is a more effective model for anomaly detection, at least based on the F1 score metric.\n\nThe text also implies that both models are used for time series analysis, as they are mentioned in the context of anomaly detection. This is consistent with the use of LSTM-NDT and MAD-GAN in the field of time series forecasting, where anomaly detection is a critical component.\n\nOverall, the summary highlights the key similarities and differences between LSTM-NDT and MAD-GAN, including their use for anomaly detection, their performance comparison, and their relevance to time series analysis.\n\nRelevant information from the nearby text:\n\n* The text mentions that the models are used for anomaly detection, which is a critical component of time series analysis.\n* The text implies that the models are used for long-term series forecasting, which is a common application of time series analysis.\n* The text mentions the use of frequency analysis, which is a technique used in time series analysis to identify patterns and trends in data.\n\nNote: The text does not provide any information about the specific applications or use cases of LSTM-NDT and MAD-GAN, so this information is not included in the summary.", "from": "LSTM-NDT", "source_id": "971e73b638469366cfdd3af2e7c7a824,d5c8b72da09cebefa0c26285ad5272eb", "to": "MAD-GAN", "width": 2.0}, {"description": "LSTM-NDT is related to MSDS as MSDS is a metric used to evaluate model performance", "from": "LSTM-NDT", "source_id": "00973c1c3c962d9234a38037709824b1", "to": "MSDS", "width": 1.0}, {"description": "LSTM-NDT and CAE-M are both models used for anomaly detection, with CAE-M having a higher F1 score than LSTM-NDT", "from": "LSTM-NDT", "source_id": "d5c8b72da09cebefa0c26285ad5272eb", "to": "CAE-M", "width": 1.0}, {"description": "TranAD outperforms OpenGauss in detection and diagnosis performance", "from": "OPENGAUSS", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "to": "TRANAD", "width": 1.0}, {"description": "OpenGauss is related to tree-based LSTM as both are used for capturing temporal trends even with noisy data", "from": "OPENGAUSS", "source_id": "0259287b914980606371cd1161c6a420", "to": "TREE-BASED LSTM", "width": 1.0}, {"description": "TranAD outperforms SAND in detection and diagnosis performance", "from": "SAND", "source_id": "0c4c072869e10b0b4bd4bc19a60a23a3", "to": "TRANAD", "width": 1.0}, {"description": "MAML helps keep optimum detection performance even with limited data, which is a key feature of TranAD", "from": "TRANAD", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "MAML", "width": 1.0}, {"description": "Section 3 outlines the working of the TranAD model", "from": "TRANAD", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "SECTION 3", "width": 1.0}, {"description": "Section 4 shows a performance evaluation of the proposed method, TranAD", "from": "TRANAD", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "SECTION 4", "width": 1.0}, {"description": "Section 6 concludes the paper, which discusses the TranAD model", "from": "TRANAD", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "SECTION 6", "width": 1.0}, {"description": "Classical methods are different from TranAD, which is a key feature of TranAD", "from": "TRANAD", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "CLASSICAL METHODS", "width": 1.0}, {"description": "POT is related to TranAD as TranAD is a deep transformer network for anomaly detection in multivariate time series data", "from": "TRANAD", "source_id": "ffbbbf29ffb8d038e241f023079cb0a2", "to": "POT", "width": 1.0}, {"description": "TranAD is related to multi-scale convolutional recursive encoder as multi-scale convolutional recursive encoder is a model used for anomaly detection in multivariate time series data", "from": "TRANAD", "source_id": "ffbbbf29ffb8d038e241f023079cb0a2", "to": "MULTI-SCALE CONVOLUTIONAL RECURSIVE ENCODER", "width": 1.0}, {"description": "TranAD learns broad level trends to detect anomalies", "from": "TRANAD", "source_id": "1902e651467179a9a1d4c4df0035e980", "to": "BROAD LEVEL TRENDS", "width": 1.0}, {"description": "A deep learning model for anomaly detection", "from": "TRANAD", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "TRAINING TIME-SERIES", "width": 1.0}, {"description": "NDCG is related to TranAD as TranAD is an anomaly detection model that uses NDCG to evaluate its performance", "from": "TRANAD", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "NDCG", "width": 1.0}, {"description": "TranAD uses the POT technique to set more accurate threshold values", "from": "TRANAD", "source_id": "8e075f1de7293e0cd1724bd167c263be", "to": "POT TECHNIQUE", "width": 1.0}, {"description": "TranAD performs better than DAGMM model for longer sequences", "from": "TRANAD", "source_id": "8e075f1de7293e0cd1724bd167c263be", "to": "DAGMM MODEL", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nThe entities \"TRANAD\" and \"MERLIN\" are two models that are compared and related in the context of their performance and characteristics. Specifically, TRANAD is compared to MERLIN, a baseline model, in terms of training time and anomaly detection performance. This comparison suggests that TRANAD and MERLIN are both models that are used for anomaly detection, with TRANAD being evaluated against MERLIN as a benchmark.\n\nBoth TRANAD and MERLIN are models that are mentioned in the text, indicating that they are relevant to the discussion or research being presented. The comparison between TRANAD and MERLIN highlights their differences and similarities in terms of their performance and characteristics, which is likely to be of interest to researchers or practitioners in the field.\n\nOverall, the summary provides a clear understanding of the relationship between TRANAD and MERLIN, including their comparison and relevance to the context in which they are mentioned.", "from": "TRANAD", "source_id": "78ee4a3d7a2bffd4405a03d94a4f6cb1,971e73b638469366cfdd3af2e7c7a824,d16a81565fcd654ab21768510dcc5d7f", "to": "MERLIN", "width": 3.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nTranAD and DAGMM are two deep learning models for anomaly detection. They are both mentioned in the text as being related to each other, indicating a connection or similarity between the two models. Specifically, TranAD is associated with DAGMM, suggesting that they share common characteristics or applications in the field of anomaly detection.\n\nThe use of their names in the table further supports the fact that both TranAD and DAGMM are deep learning models, implying that they utilize complex neural network architectures to detect anomalies in data. This summary is based on the information provided in the description list, which indicates that both models are related to each other and are used for anomaly detection.\n\nIt is worth noting that the text does not provide further details about the specific characteristics or differences between TranAD and DAGMM. However, based on the information provided, it is clear that both models are related to each other and are used for anomaly detection purposes.", "from": "TRANAD", "source_id": "2b44aebc638544dcf835db30c4270d09,971e73b638469366cfdd3af2e7c7a824", "to": "DAGMM", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMAD-GAN and TRANAD are two deep learning models used for anomaly detection. They are both mentioned in the text as models related to anomaly detection, indicating their shared purpose and application. The use of their names in the table further supports their classification as deep learning models for anomaly detection. \n\nGiven the context of the text, which appears to be a technical document or research paper, it is likely that MAD-GAN and TRANAD are advanced models used in the field of anomaly detection, possibly leveraging techniques such as frequency analysis and multi-head cross-attention. The mention of ICLR, AAAI, and PMLR, which are English-language academic conferences and journals, suggests that the text is written in English and may be related to academic research in the field of anomaly detection.\n\nOverall, MAD-GAN and TRANAD are two deep learning models used for anomaly detection, with potential applications in fields such as time series forecasting and long-term series forecasting, as hinted by the use of technical terms and mathematical equations in the text.", "from": "TRANAD", "source_id": "00973c1c3c962d9234a38037709824b1,2b44aebc638544dcf835db30c4270d09,971e73b638469366cfdd3af2e7c7a824", "to": "MAD-GAN", "width": 3.0}, {"description": "Based on the provided information, the primary entities are \"TRANAD\" and \"CAE-M\". These two entities are both models used for anomaly detection. \n\nThe descriptions provided confirm that both \"TRANAD\" and \"CAE-M\" are deep learning models designed for anomaly detection. They are listed in a table, which further supports their association with anomaly detection.\n\nGiven the information collected from all the descriptions, a comprehensive summary can be formed. \n\n\"TRANAD\" and \"CAE-M\" are two deep learning models specifically designed for anomaly detection. They are both listed in a table, indicating their relevance to this task. Their names and presence in the table suggest that they are models used for identifying anomalies in data.\n\nThis summary is written in third person and includes the entity names for context. It also resolves any potential contradictions by confirming that both models are used for anomaly detection.", "from": "TRANAD", "source_id": "00973c1c3c962d9234a38037709824b1,2b44aebc638544dcf835db30c4270d09,69afb4bcb8c1e03975c837102e1d0b32", "to": "CAE-M", "width": 3.0}, {"description": "TranAD achieves the best rank across all models with a significant statistical difference when given the complete MSDS dataset", "from": "TRANAD", "source_id": "70a97858b727e5af1466ae5eb9183921", "to": "MSDS", "width": 1.0}, {"description": "ANAD and TRANAD are both deep transformer networks for anomaly detection in multivariate time series data", "from": "TRANAD", "source_id": "1413d358c623ac2d4c70be6547eb218b", "to": "ANAD", "width": 1.0}, {"description": "TranAD is related to MBA as both are models that use a multi-task learning approach", "from": "TRANAD", "source_id": "9b35a2c607e0f4b8cbc9179f424f180a", "to": "MBA", "width": 1.0}, {"description": "TranAD is related to UCR as UCR is a dataset used to evaluate the performance of anomaly detection models", "from": "TRANAD", "source_id": "9b35a2c607e0f4b8cbc9179f424f180a", "to": "UCR", "width": 1.0}, {"description": "TranAD is related to NAB as NAB is a dataset used to evaluate the performance of anomaly detection models", "from": "TRANAD", "source_id": "9b35a2c607e0f4b8cbc9179f424f180a", "to": "NAB", "width": 1.0}, {"description": "Position encoding is related to transformer models as it is a technique used in transformer models to encode the position of each token in a sequence", "from": "TRANSFORMER MODELS", "source_id": "bd73ee0439609823e12a877840c6ebae", "to": "POSITION ENCODING", "width": 1.0}, {"description": "Position Encoding is related to input matrices as it is used to add positional information to them", "from": "POSITION ENCODING", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "INPUT MATRICES", "width": 1.0}, {"description": "The window encoder uses position encoding to encode the input window", "from": "POSITION ENCODING", "source_id": "4bdf596e75e1cb06d11b25e95491037e", "to": "WINDOW ENCODER", "width": 1.0}, {"description": "Anomaly detector is related to TranAD as TranAD is a transformer-based anomaly detection model", "from": "ANOMALY DETECTOR", "source_id": "bd73ee0439609823e12a877840c6ebae", "to": "TRANAAD", "width": 1.0}, {"description": "Based on the provided information, the primary entity of interest is \"TRANAAD\" and \"SELF-CONDITIONING\". \n\nAfter analyzing the descriptions, it appears that there are two different perspectives on the relationship between TRANAAD and SELF-CONDITIONING. \n\nThe first description suggests that TRANAAD uses SELF-CONDITIONING to improve robustness and generalization. \n\nThe second description, however, states that TRANAAD uses SELF-CONDITIONING to amplify errors and gain training stability.\n\nTo resolve the contradiction, it can be inferred that TRANAAD employs SELF-CONDITIONING in a way that serves multiple purposes, including improving robustness and generalization, as well as amplifying errors and gaining training stability.\n\nHere is a comprehensive summary of the data:\n\nTRANAAD is a technique that utilizes SELF-CONDITIONING to achieve improved robustness and generalization, as well as to amplify errors and gain training stability. This suggests that SELF-CONDITIONING plays a crucial role in enhancing the performance and stability of TRANAAD. \n\nThe use of SELF-CONDITIONING in TRANAAD is likely to be a key factor in its ability to handle complex tasks and adapt to changing environments. \n\nOverall, the relationship between TRANAAD and SELF-CONDITIONING is one of mutual benefit, with SELF-CONDITIONING enhancing the capabilities of TRANAAD and TRANAAD providing a platform for the effective application of SELF-CONDITIONING.", "from": "TRANAAD", "source_id": "a39d9d28126733c33db624ccc25f5782,bd73ee0439609823e12a877840c6ebae", "to": "SELF-CONDITIONING", "width": 2.0}, {"description": "Transformer based encoder-decoder is related to TranAD as TranAD is a model that uses this architecture", "from": "TRANAAD", "source_id": "a39d9d28126733c33db624ccc25f5782", "to": "TRANSFORMER BASED ENCODER-DECODER", "width": 1.0}, {"description": "TranAD is related to meta-learning as it uses this technique to allow it to identify data trends even with limited data", "from": "TRANAAD", "source_id": "a39d9d28126733c33db624ccc25f5782", "to": "META-LEARNING", "width": 1.0}, {"description": "Attention weights are modified using self-conditioning in the second phase", "from": "SELF-CONDITIONING", "source_id": "a65c0f1eae6e779357739df141f75d36", "to": "ATTENTION WEIGHTS", "width": 1.0}, {"description": "Self-conditioning and meta-learning are used in TranAD to allow it to identify data trends even with limited data", "from": "SELF-CONDITIONING", "source_id": "78ee4a3d7a2bffd4405a03d94a4f6cb1", "to": "META-LEARNING", "width": 1.0}, {"description": "Simple transformers underperform compared to state-of-the-art methods, which is a key feature of TranAD", "from": "STATE-OF-THE-ART METHODS", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "SIMPLE TRANSFORMERS", "width": 1.0}, {"description": "TranAD is able to outperform baselines by increasing prediction scores by up to 17% while reducing training time overheads by up to 99%", "from": "PREDICTION SCORES", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "TRAINING TIME OVERHEADS", "width": 1.0}, {"description": "Section 2 overviews related work in the VLDB community", "from": "SECTION 2", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "VLDB COMMUNITY", "width": 1.0}, {"description": "K-Mean clustering and Support Vector Machines are different from TranAD, which is a key feature of TranAD", "from": "K-MEAN CLUSTERING", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "SUPPORT VECTOR MACHINES", "width": 1.0}, {"description": "Wavelet theory and Hilbert transform are different from TranAD, which is a key feature of TranAD", "from": "WAVELET THEORY", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "HILBERT TRANSFORM", "width": 1.0}, {"description": "Principal Component Analysis and process regression are different from TranAD, which is a key feature of TranAD", "from": "PRINCIPAL COMPONENT ANALYSIS", "source_id": "203f9117f8528750ca0c22a768a02cd9", "to": "PROCESS REGRESSION", "width": 1.0}, {"description": "Time series discords are related to matrix profiling as matrix profiling is used for anomaly and motif discovery by detecting time series discords", "from": "TIME SERIES DISCORDS", "source_id": "ffbbbf29ffb8d038e241f023079cb0a2", "to": "MATRIX PROFILING", "width": 1.0}, {"description": "Matrix profiling is related to MERLIN as MERLIN uses a parameter-free version of time series discord discovery by iteratively comparing subsequences of varying length with their immediate neighbors", "from": "MATRIX PROFILING", "source_id": "ffbbbf29ffb8d038e241f023079cb0a2", "to": "MERLIN", "width": 1.0}, {"description": "F1* is related to MERLIN as MERLIN is a baseline model that uses F1* to evaluate its performance", "from": "MERLIN", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "F1*", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMERLIN and DAGMM are two machine learning models used for anomaly detection. Both models have been compared in the text, as indicated by the use of their respective abbreviations. According to the available data, DAGMM has a higher F1 score than MERLIN, suggesting that DAGMM may be a more effective model for anomaly detection. These models are mentioned in the text as part of a comparison or evaluation, indicating that they are being considered for their performance in anomaly detection tasks.\n\nIn terms of their relationship, MERLIN and DAGMM are related as both are models mentioned in the text, and they have been compared in the context of anomaly detection. This comparison suggests that both models are being evaluated for their ability to detect anomalies, with DAGMM showing a higher F1 score than MERLIN.\n\nOverall, the available data suggests that MERLIN and DAGMM are two machine learning models used for anomaly detection, with DAGMM being a more effective model based on its higher F1 score.", "from": "MERLIN", "source_id": "971e73b638469366cfdd3af2e7c7a824,a4a241e471ad258932241bc441b96155,d5c8b72da09cebefa0c26285ad5272eb", "to": "DAGMM", "width": 3.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMERLIN and MAD-GAN are two models used for anomaly detection. Both models are mentioned in the text, indicating their relevance to the topic. Notably, MAD-GAN has been found to have a higher F1 score compared to MERLIN, suggesting its superior performance in anomaly detection tasks. This comparison highlights the strengths and weaknesses of each model, providing valuable insights for researchers and practitioners working in the field of anomaly detection.\n\nIn terms of their relationship, MERLIN and MAD-GAN are closely related as they are both models used for anomaly detection. Their shared application domain and the comparison of their performance metrics (F1 score) suggest that they are competing models, each with its own strengths and weaknesses. However, further analysis of the text reveals that both models are used for anomaly detection, indicating that they may be complementary rather than mutually exclusive.\n\nOverall, the summary highlights the key characteristics of MERLIN and MAD-GAN, including their application domain, performance metrics, and relationship to each other. This information can be useful for researchers and practitioners looking to understand the strengths and weaknesses of each model and how they can be applied in real-world scenarios.", "from": "MERLIN", "source_id": "971e73b638469366cfdd3af2e7c7a824,d5c8b72da09cebefa0c26285ad5272eb", "to": "MAD-GAN", "width": 2.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMERLIN and MSDS are two entities that are related to each other in the context of machine learning and time series forecasting. MERLIN is used to find anomalies in MSDS, suggesting that MSDS is a dataset or a metric that is being analyzed for anomalies. Additionally, MSDS is a metric used to evaluate model performance, indicating that it is a key component in assessing the effectiveness of machine learning models.\n\nThe relationship between MERLIN and MSDS is likely centered around the use of MSDS as a benchmark or evaluation metric for MERLIN\u0027s anomaly detection capabilities. This implies that MERLIN is designed to identify unusual patterns or outliers in MSDS, which is a critical aspect of time series forecasting and model performance evaluation.\n\nOverall, the connection between MERLIN and MSDS highlights the importance of anomaly detection and model evaluation in the context of time series forecasting, and suggests that MERLIN is a valuable tool for identifying and addressing potential issues in MSDS-based models.", "from": "MERLIN", "source_id": "00973c1c3c962d9234a38037709824b1,f6e57fa18831bcc732a631240536777a", "to": "MSDS", "width": 2.0}, {"description": "MERLIN and CAE-M are both models used for anomaly detection, with CAE-M having a higher F1 score than MERLIN", "from": "MERLIN", "source_id": "d5c8b72da09cebefa0c26285ad5272eb", "to": "CAE-M", "width": 1.0}, {"description": "Kate Highnam is related to MERLIN as she provided constructive comments on improving the manuscript writing", "from": "MERLIN", "source_id": "a39d9d28126733c33db624ccc25f5782", "to": "KATE HIGHNAM", "width": 1.0}, {"description": "MERLIN is related to Python as the code is implemented in Python", "from": "MERLIN", "source_id": "a39d9d28126733c33db624ccc25f5782", "to": "PYTHON", "width": 1.0}, {"description": "Peak Over Threshold (POT) is related to NDT as both are methods for anomaly threshold selection", "from": "NDT", "source_id": "0259287b914980606371cd1161c6a420", "to": "PEAK OVER THRESHOLD (POT)", "width": 1.0}, {"description": "Based on the provided information, a comprehensive summary of the data can be generated as follows:\n\nDAGMM and MAD-GAN are two deep learning models used for anomaly detection. Both models are mentioned in the text and are related to each other, as indicated by their presence in the table and the comparison of their performance metrics. Specifically, it is noted that MAD-GAN has a higher F1 score than DAGMM, suggesting that MAD-GAN may be a more effective model for anomaly detection in certain scenarios. However, it is essential to note that the text does not provide a comprehensive comparison of the two models, and further analysis would be necessary to fully understand their strengths and weaknesses.\n\nRelevant information from the nearby text suggests that the models are likely being compared in the context of time series analysis, as the text mentions \"time series\" and \"long-term series forecasting.\" Additionally, the use of technical terms such as \"frequency analysis\" and \"multi-head cross-attention\" suggests that the models are being applied to complex data sets that require sophisticated processing techniques.\n\nOverall, DAGMM and MAD-GAN are two deep learning models for anomaly detection that are related to each other and are being compared in the context of time series analysis. Further research would be necessary to fully understand their performance and applications.", "from": "DAGMM", "source_id": "2b44aebc638544dcf835db30c4270d09,971e73b638469366cfdd3af2e7c7a824,d5c8b72da09cebefa0c26285ad5272eb", "to": "MAD-GAN", "width": 3.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nDAGMM and CAE-M are two deep learning models used for anomaly detection. Both models are utilized for identifying anomalies, with DAGMM and CAE-M being employed in a specific context, as indicated by their presence in a table. A comparison of their performance suggests that CAE-M has a higher F1 score than DAGMM, indicating a potential difference in their effectiveness in detecting anomalies.", "from": "DAGMM", "source_id": "2b44aebc638544dcf835db30c4270d09,d5c8b72da09cebefa0c26285ad5272eb", "to": "CAE-M", "width": 2.0}, {"description": "Ariational Autoencoder is related to Planar Normalizing Flow as both are used for generating reconstruction probabilities", "from": "PLANAR NORMALIZING FLOW", "source_id": "0259287b914980606371cd1161c6a420", "to": "ARIATIONAL AUTOENCODER", "width": 1.0}, {"description": "Multi-Scale Convectional Recursive Encoder-Decoder (MSCRED) is related to ConvLSTM as both are used for capturing complex inter-modal correlations and temporal information", "from": "MULTI-SCALE CONVECTIONAL RECURSIVE ENCODER-DECODER (MSCRED)", "source_id": "0259287b914980606371cd1161c6a420", "to": "CONVLSTM", "width": 1.0}, {"description": "MAD-GAN is related to LSTM based GAN as both are used for modeling time-series distribution", "from": "MAD-GAN", "source_id": "0259287b914980606371cd1161c6a420", "to": "LSTM BASED GAN", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nMAD-GAN and CAE-M are two deep learning models specifically designed for anomaly detection. Both models are utilized for identifying anomalies, as indicated by their application in the table. Notably, CAE-M has demonstrated a higher F1 score compared to MAD-GAN, suggesting its superior performance in anomaly detection tasks. These models are likely employed in various applications, such as time series analysis, where anomaly detection is crucial for accurate forecasting and decision-making.\n\nThe use of deep learning models like MAD-GAN and CAE-M for anomaly detection implies that they are capable of learning complex patterns and relationships within the data, enabling them to identify anomalies that may not be apparent through traditional methods. The higher F1 score of CAE-M indicates its potential for more accurate and reliable anomaly detection, making it a valuable tool in various domains, including but not limited to, time series analysis and forecasting.\n\nIt is worth noting that the use of these models in anomaly detection is likely facilitated by their ability to handle large datasets and complex patterns, which is a common challenge in time series analysis. The performance of these models can be further improved by incorporating additional techniques, such as frequency analysis and multi-head cross-attention, which are commonly used in time series forecasting and anomaly detection.\n\nOverall, MAD-GAN and CAE-M are two powerful deep learning models for anomaly detection, with CAE-M demonstrating superior performance in terms of F1 score. Their application in time series analysis and forecasting can lead to more accurate and reliable results, making them valuable tools in various domains.", "from": "MAD-GAN", "source_id": "00973c1c3c962d9234a38037709824b1,2b44aebc638544dcf835db30c4270d09,d5c8b72da09cebefa0c26285ad5272eb", "to": "CAE-M", "width": 3.0}, {"description": "GRU is related to GRU as both are types of neural network layers used for aiding detection without severe overheads", "from": "GRU", "source_id": "0259287b914980606371cd1161c6a420", "to": "GRU", "width": 1.0}, {"description": "CAE-M is related to convolutional autoencoding memory network as both are used for anomaly detection", "from": "CAE-M", "source_id": "0259287b914980606371cd1161c6a420", "to": "CONVOLUTIONAL AUTOENCODING MEMORY NETWORK", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant details:\n\nHitAnomaly and Vanilla Transformers are two entities related to anomaly detection. Specifically, HitAnomaly is connected to Vanilla Transformers as both are utilized for anomaly detection purposes. Furthermore, HitAnomaly employs Vanilla Transformers as encoder-decoder networks, highlighting their collaborative role in this context.\n\nThis summary is derived from the provided descriptions, which are not contradictory. The information collected from all the descriptions is used to create a coherent and concise summary.", "from": "HITANOMALY", "source_id": "0259287b914980606371cd1161c6a420,1902e651467179a9a1d4c4df0035e980", "to": "VANILLA TRANSFORMERS", "width": 2.0}, {"description": "Temporal trends are related to noisy data as noisy data can affect the accuracy of temporal trends", "from": "TEMPORAL TRENDS", "source_id": "1902e651467179a9a1d4c4df0035e980", "to": "NOISY DATA", "width": 1.0}, {"description": "Time series windows is related to local contextual window as local contextual window is used to create time series windows", "from": "TIME SERIES WINDOWS", "source_id": "477c5b7f23f4e00030ab389788a4c88d", "to": "LOCAL CONTEXTUAL WINDOW", "width": 1.0}, {"description": "Local contextual window is related to replication padding as replication padding is used to create local contextual window", "from": "LOCAL CONTEXTUAL WINDOW", "source_id": "477c5b7f23f4e00030ab389788a4c88d", "to": "REPPLICATION PADDING", "width": 1.0}, {"description": "Threshold value is related to anomaly label as anomaly label is determined using threshold value", "from": "THRESHOLD VALUE", "source_id": "477c5b7f23f4e00030ab389788a4c88d", "to": "ANOMALY LABEL", "width": 1.0}, {"description": "The input window is related to the complete sequence as the complete sequence is used to encode the input window", "from": "INPUT WINDOW", "source_id": "4bdf596e75e1cb06d11b25e95491037e", "to": "COMPLETE SEQUENCE", "width": 1.0}, {"description": "Encoder is related to decoder as decoder is a component of the transformer model that decodes the encoded representation of the input sequence", "from": "ENCODER", "source_id": "f2e78b25a535b82e2743a0ea4052eb9a", "to": "DECODER", "width": 1.0}, {"description": "Window encoder is related to encoder as encoder is a component of the transformer model that encodes the input sequence", "from": "ENCODER", "source_id": "f2e78b25a535b82e2743a0ea4052eb9a", "to": "WINDOW ENCODER", "width": 1.0}, {"description": "The decoder uses multihead attention to perform attention operations", "from": "DECODER", "source_id": "4bdf596e75e1cb06d11b25e95491037e", "to": "MULTIHEAD ATTENTION", "width": 1.0}, {"description": "The window encoder uses masking to hide window sequences for future timestamps", "from": "WINDOW ENCODER", "source_id": "4bdf596e75e1cb06d11b25e95491037e", "to": "MASK", "width": 1.0}, {"description": "Scaled-dot product attention is related to multi-head self attention as multi-head self attention is a type of attention mechanism used in the transformer model", "from": "SCALEDDOT PRODUCT ATTENTION", "source_id": "f2e78b25a535b82e2743a0ea4052eb9a", "to": "MULTI-HEAD SELF ATTENTION", "width": 1.0}, {"description": "Feed-forward layer is related to scaled-dot product attention as scaled-dot product attention is a type of attention mechanism used in the transformer model", "from": "SCALEDDOT PRODUCT ATTENTION", "source_id": "f2e78b25a535b82e2743a0ea4052eb9a", "to": "FEED-FORWARD LAYER", "width": 1.0}, {"description": "Multi-Head Self Attention is related to feed-forward layers as it uses them to transform the input matrices", "from": "MULTI-HEAD SELF ATTENTION", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "FEED-FORWARD LAYERS", "width": 1.0}, {"description": "Multi-Head Self Attention is related to scaled-dot product attention as it uses it to compute the attention weights", "from": "MULTI-HEAD SELF ATTENTION", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "SCALED-DOT PRODUCT ATTENTION", "width": 1.0}, {"description": "GAN is related to time-efficient GAN style adversarial training as it is a type of generative adversarial network that can be trained using this method", "from": "GAN", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "TIME-EFFICIENT GAN STYLE ADVERSARIAL TRAINING", "width": 1.0}, {"description": "Transformer Encoders are related to decoders as they are used together to form a neural network", "from": "TRANSFORMER ENCODERS", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "DECODERS", "width": 1.0}, {"description": "Input Matrices are related to \ud835\udc4a as it is an example of an input matrix", "from": "INPUT MATRICES", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc4a", "width": 1.0}, {"description": "Input Matrices are related to \ud835\udc36 as it is an example of an input matrix", "from": "INPUT MATRICES", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc36", "width": 1.0}, {"description": "\ud835\udc4a is related to \ud835\udc36 as they are both input matrices", "from": "\ud835\udc4a", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc36", "width": 1.0}, {"description": "\ud835\udc44 is related to \ud835\udc3e as they are both input matrices", "from": "\ud835\udc44", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc3e", "width": 1.0}, {"description": "\ud835\udc44 is related to \ud835\udc49 as they are both input matrices", "from": "\ud835\udc44", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc49", "width": 1.0}, {"description": "\ud835\udc3e is related to \ud835\udc49 as they are both input matrices", "from": "\ud835\udc3e", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc49", "width": 1.0}, {"description": "\ud835\udc56 is related to \u210e as it is used to iterate over the input matrices", "from": "\ud835\udc56", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\u210e", "width": 1.0}, {"description": "\ud835\udc44\ud835\udc56 is related to \ud835\udc3e\ud835\udc56 as they are both transformed input matrices", "from": "\ud835\udc44\ud835\udc56", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc3e\ud835\udc56", "width": 1.0}, {"description": "\ud835\udc44\ud835\udc56 is related to \ud835\udc49\ud835\udc56 as they are both transformed input matrices", "from": "\ud835\udc44\ud835\udc56", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc49\ud835\udc56", "width": 1.0}, {"description": "\ud835\udc3e\ud835\udc56 is related to \ud835\udc49\ud835\udc56 as they are both transformed input matrices", "from": "\ud835\udc3e\ud835\udc56", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc49\ud835\udc56", "width": 1.0}, {"description": "\ud835\udc3c1 is related to \ud835\udc3c11 as it is a transformed input matrix", "from": "\ud835\udc3c1", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc3c11", "width": 1.0}, {"description": "\ud835\udc3c1 is related to \ud835\udc3c12 as it is a transformed input matrix", "from": "\ud835\udc3c1", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc3c12", "width": 1.0}, {"description": "\ud835\udc3c11 is related to \ud835\udc3c12 as they are both transformed input matrices", "from": "\ud835\udc3c11", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc3c12", "width": 3.0}, {"description": "\ud835\udc3c2 is related to \ud835\udc3c11 as they are both input matrices", "from": "\ud835\udc3c11", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc3c2", "width": 2.0}, {"description": "\ud835\udc3c2 is related to \ud835\udc3c12 as they are both input matrices", "from": "\ud835\udc3c12", "source_id": "4d2961a17ee532a47fa53a41f53ebdd3", "to": "\ud835\udc3c2", "width": 2.0}, {"description": "Input time-series window is used in the encoded-decoder network", "from": "INPUT TIME-SERIES WINDOW", "source_id": "a65c0f1eae6e779357739df141f75d36", "to": "ENCODED-DECODER NETWORK", "width": 1.0}, {"description": "Encoded-decoder network is used in the auto-regressive inference", "from": "ENCODED-DECODER NETWORK", "source_id": "a65c0f1eae6e779357739df141f75d36", "to": "AUTO-REGRESSIVE INFERENCE", "width": 1.0}, {"description": "Auto-regressive inference is used in phase 1", "from": "AUTO-REGRESSIVE INFERENCE", "source_id": "a65c0f1eae6e779357739df141f75d36", "to": "PHASE 1", "width": 1.0}, {"description": "Phase 1 is used in phase 2, as the focus score is generated in the first phase", "from": "PHASE 1", "source_id": "a65c0f1eae6e779357739df141f75d36", "to": "PHASE 2", "width": 1.0}, {"description": "Phase 2 is used to generate the focus score", "from": "PHASE 2", "source_id": "a65c0f1eae6e779357739df141f75d36", "to": "FOCUS SCORE", "width": 1.0}, {"description": "Focus score is used to modify the attention weights in the second phase", "from": "FOCUS SCORE", "source_id": "a65c0f1eae6e779357739df141f75d36", "to": "ATTENTION WEIGHTS", "width": 1.0}, {"description": "Masked multi-head attention is related to meta learning as it is used to learn temporal trends in the input training time-series with limited data", "from": "MASKED MULTI-HEAD ATTENTION", "source_id": "6ea15432a841705c2e74cbc01f6004e9", "to": "META LEARNING", "width": 1.0}, {"description": "Loss function is related to meta step-size as it is used to evaluate the performance of the model", "from": "LOSSES FUNCTION", "source_id": "6ea15432a841705c2e74cbc01f6004e9", "to": "META STEP-SIZE", "width": 1.0}, {"description": "Meta step-size is related to model weights as it is used to update the model weights during the training process", "from": "META STEP-SIZE", "source_id": "6ea15432a841705c2e74cbc01f6004e9", "to": "MODEL WEIGHTS", "width": 1.0}, {"description": "The Generalized Pareto Distribution is used to fit the data distribution and choose the threshold automatically using the Peak Over Threshold method", "from": "GENERALIZED PARETO DISTRIBUTION", "source_id": "1b51ec337efd822ca3a0b3eb819c1b91", "to": "PEAK OVER THRESHOLD", "width": 1.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entities \"NAB\" and \"UCR\" are two datasets used for anomaly detection. They are both utilized in the text, as indicated by the presence of their abbreviations \"NAB\" and \"UCR\". Furthermore, both datasets are related as they have been used in experiments to evaluate the performance of anomaly detection models. Specifically, they are used to assess the effectiveness of various models in identifying anomalies within the datasets.\n\nIn the context of time series analysis, both NAB and UCR datasets are employed to evaluate the performance of long-term series forecasting models, which is a crucial aspect of anomaly detection. The use of frequency analysis and multi-head cross-attention mechanisms is also mentioned in the text, suggesting that these techniques are applied to the NAB and UCR datasets to improve the accuracy of anomaly detection models.\n\nOverall, the summary highlights the relationship between the NAB and UCR datasets, their use in anomaly detection, and their application in time series analysis and forecasting.", "from": "NAB", "source_id": "69afb4bcb8c1e03975c837102e1d0b32,9b35a2c607e0f4b8cbc9179f424f180a,9c6e74299923071f9fc2b7cce49efc90,a4a241e471ad258932241bc441b96155", "to": "UCR", "width": 4.0}, {"description": "The DAGMM model performs well for short datasets like NAB", "from": "NAB", "source_id": "8e075f1de7293e0cd1724bd167c263be", "to": "DAGMM MODEL", "width": 1.0}, {"description": "Based on the provided information, the primary entities are \"NAB\" and \"MBA\", which are both datasets used for anomaly detection. \n\nThe descriptions provided indicate that NAB is a dataset used to evaluate the performance of anomaly detection models, and both NAB and MBA are datasets used for anomaly detection. \n\nConsidering the information collected from all the descriptions, it can be concluded that NAB and MBA are two datasets that have been used in the context of anomaly detection, with NAB being specifically used to evaluate the performance of anomaly detection models.\n\nHere is a comprehensive summary of the data in third person, including the entity names and relevant information:\n\nThe datasets \"NAB\" and \"MBA\" are both used for anomaly detection. Specifically, NAB is a dataset used to evaluate the performance of anomaly detection models, while both NAB and MBA are datasets used for anomaly detection, as indicated by their presence in the table.", "from": "NAB", "source_id": "69afb4bcb8c1e03975c837102e1d0b32,9b35a2c607e0f4b8cbc9179f424f180a,a4a241e471ad258932241bc441b96155", "to": "MBA", "width": 3.0}, {"description": "NAB and MSDS are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "NAB", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "MSDS", "width": 1.0}, {"description": "NAB is related to P as P is a metric used to evaluate the models on the NAB dataset", "from": "NAB", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "P", "width": 1.0}, {"description": "NAB is related to R as R is a metric used to evaluate the models on the NAB dataset", "from": "NAB", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "R", "width": 1.0}, {"description": "NAB is related to AUC as AUC is a metric used to evaluate the models on the NAB dataset", "from": "NAB", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "AUC", "width": 1.0}, {"description": "NAB is related to F1 as F1 is a metric used to evaluate the models on the NAB dataset", "from": "NAB", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "F1", "width": 1.0}, {"description": "Based on the provided information, the comprehensive summary of the data is as follows:\n\nThe entities \"UCR\" and \"MBA\" are both datasets used for anomaly detection. They are related in that they have been used in the text to evaluate the performance of anomaly detection models and have been utilized in experiments. Specifically, the UCR dataset is used to evaluate the performance of anomaly detection models, and both UCR and MBA are datasets that have been used in the text, as indicated by the use of their respective abbreviations.\n\nThis summary is based on the information collected from all the descriptions provided, and it resolves any potential contradictions by highlighting the common thread of anomaly detection between the two datasets. The summary is written in the third person and includes the entity names for context.", "from": "UCR", "source_id": "69afb4bcb8c1e03975c837102e1d0b32,9b35a2c607e0f4b8cbc9179f424f180a,9c6e74299923071f9fc2b7cce49efc90,a4a241e471ad258932241bc441b96155", "to": "MBA", "width": 4.0}, {"description": "The DAGMM model performs well for short datasets like UCR", "from": "UCR", "source_id": "8e075f1de7293e0cd1724bd167c263be", "to": "DAGMM MODEL", "width": 1.0}, {"description": "UCR and MSDS are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "UCR", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "MSDS", "width": 1.0}, {"description": "UCR is related to P as P is a metric used to evaluate the models on the UCR dataset", "from": "UCR", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "P", "width": 1.0}, {"description": "UCR is related to R as R is a metric used to evaluate the models on the UCR dataset", "from": "UCR", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "R", "width": 1.0}, {"description": "UCR is related to AUC as AUC is a metric used to evaluate the models on the UCR dataset", "from": "UCR", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "AUC", "width": 1.0}, {"description": "UCR is related to F1 as F1 is a metric used to evaluate the models on the UCR dataset", "from": "UCR", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "F1", "width": 1.0}, {"description": "The DAGMM model performs well for short datasets like MBA", "from": "MBA", "source_id": "8e075f1de7293e0cd1724bd167c263be", "to": "DAGMM MODEL", "width": 1.0}, {"description": "MBA and MSDS are both datasets used for anomaly detection, as indicated by their presence in the table", "from": "MBA", "source_id": "69afb4bcb8c1e03975c837102e1d0b32", "to": "MSDS", "width": 1.0}, {"description": "MBA is related to P as P is a metric used to evaluate the models on the MBA dataset", "from": "MBA", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "P", "width": 1.0}, {"description": "MBA is related to R as R is a metric used to evaluate the models on the MBA dataset", "from": "MBA", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "R", "width": 1.0}, {"description": "MBA is related to AUC as AUC is a metric used to evaluate the models on the MBA dataset", "from": "MBA", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "AUC", "width": 1.0}, {"description": "MBA is related to F1 as F1 is a metric used to evaluate the models on the MBA dataset", "from": "MBA", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "F1", "width": 1.0}, {"description": "A collection of electrocardiogram recordings from four patients, containing multiple instances of two different kinds of anomalies", "from": "ELECTROCARDIOGRAM RECORDINGS", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "MIT-BIH SUPRAVENTRICULAR ARRHYTHMIA DATABASE", "width": 1.0}, {"description": "Recordings of the electrical activity of the heart", "from": "ELECTROCARDIOGRAM RECORDINGS", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "PATIENTS", "width": 1.0}, {"description": "Abnormalities or irregularities in the electrocardiogram recordings", "from": "PATIENTS", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "ANOMALIES", "width": 1.0}, {"description": "Abnormal heartbeats originating from the atria", "from": "ANOMALIES", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "SUPRAVENTRICULAR CONTRACTIONS", "width": 1.0}, {"description": "Abnormal heartbeats occurring before the normal heartbeat", "from": "ANOMALIES", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "PREMATURE HEARTBEATS", "width": 1.0}, {"description": "A dataset of soil samples and telemetry information using the Mars rover by NASA", "from": "SOIL MOISTURE ACTIVE PASSIVE DATASET", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "SOIL SAMPLES", "width": 1.0}, {"description": "Data collected from the Mars rover\u0027s sensors and instruments", "from": "SOIL SAMPLES", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "TELEMETRY INFORMATION", "width": 1.0}, {"description": "Data collected from the Mars rover\u0027s sensors and instruments", "from": "TELEMETRY INFORMATION", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "MARS ROVER", "width": 1.0}, {"description": "A robotic vehicle designed to explore the planet Mars", "from": "MARS ROVER", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "NASA", "width": 1.0}, {"description": "A dataset similar to the SMAP dataset but corresponds to the sensor and actuator data for the Mars rover itself", "from": "MARS SCIENCE LABORATORY DATASET", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "SENSOR AND ACTUATOR DATA", "width": 1.0}, {"description": "Time-series data used to train the TranAD model", "from": "TRAINING TIME-SERIES", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "VALIDATION DATA", "width": 1.0}, {"description": "Data used to evaluate the performance of the TranAD model", "from": "VALIDATION DATA", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "PARALLEL TRANSFORMER", "width": 1.0}, {"description": "A deep learning model for parallel processing", "from": "PARALLEL TRANSFORMER", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "INTEL I7-10700K CPU", "width": 1.0}, {"description": "A type of central processing unit", "from": "INTEL I7-10700K CPU", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "64GB RAM", "width": 1.0}, {"description": "A type of random access memory", "from": "64GB RAM", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "NVIDIA RTX 3080", "width": 1.0}, {"description": "A type of graphics processing unit", "from": "NVIDIA RTX 3080", "source_id": "fa91ecd90e1c12d64176de581c6220c7", "to": "WINDOWS 11 OS", "width": 1.0}, {"description": "Predicted candidates are related to dimensions as the dimensions are used to determine the predicted candidates", "from": "DIMENSIONS", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "PREDICTED CANDIDATES", "width": 1.0}, {"description": "HitRate is related to NDCG as NDCG is used to evaluate the performance of the anomaly detection model", "from": "HITRATE", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "NDCG", "width": 1.0}, {"description": "Based on the provided information, here is a comprehensive summary of the data in third person, including the entity names and relevant information from the nearby text:\n\nThe entities \"RECALL\" and \"AUC\" are metrics used to evaluate the performance of an anomaly detection model. Specifically, \"RECALL\" and \"AUC\" are related in that \"AUC\" is a metric used to evaluate the performance of the anomaly detection model, which is also related to \"RECALL\". This suggests that \"RECALL\" and \"AUC\" are both important metrics in the context of anomaly detection and model evaluation.\n\nThe use of \"RECALL\" and \"AUC\" in the table indicates that they are both relevant metrics in this context. Furthermore, the fact that \"AUC\" is used to evaluate the performance of the anomaly detection model suggests that \"RECALL\" is also related to this evaluation process.\n\nOverall, the summary suggests that \"RECALL\" and \"AUC\" are both important metrics in the context of anomaly detection and model evaluation, and that they are related in their use and application.\n\nRelevant information from the nearby text that supports this summary includes:\n\n* The use of technical terms and mathematical equations in the text, which suggests a formal and academic tone.\n* The presence of references to academic papers and authors, which suggests that the text is written in a formal and academic style.\n* The use of English-language abbreviations such as \"ICLR,\" \"AAAI,\" and \"PMLR,\" which are commonly used in English-language academic conferences and journals.\n\nThis information supports the conclusion that the text is written in English and is likely from a research paper or technical document.", "from": "RECALL", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce,f6e57fa18831bcc732a631240536777a", "to": "AUC", "width": 2.0}, {"description": "AUC is related to F1 as F1 is a metric used to evaluate the performance of the anomaly detection model", "from": "AUC", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "F1", "width": 1.0}, {"description": "AUC and F1 score are metrics, as indicated by their use in the table", "from": "AUC", "source_id": "f6e57fa18831bcc732a631240536777a", "to": "F1 SCORE", "width": 1.0}, {"description": "F1 is related to AUC* as AUC* is a metric used to evaluate the performance of the anomaly detection model", "from": "F1", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "AUC*", "width": 1.0}, {"description": "AUC* is related to F1* as F1* is a metric used to evaluate the performance of the anomaly detection model", "from": "AUC*", "source_id": "052d1d9614f084eb2b4b0cd58ad476ce", "to": "F1*", "width": 1.0}, {"description": "Friedman test is used to reject the null hypothesis using the Wilcoxon paired signed-rank test", "from": "FREIDMAN TEST", "source_id": "70a97858b727e5af1466ae5eb9183921", "to": "WILCOXON PAIRED SIGNED-RANK TEST", "width": 1.0}, {"description": "Critical difference analysis is used to assess the significance of the differences among the performance of the models using the Friedman test", "from": "FREIDMAN TEST", "source_id": "70a97858b727e5af1466ae5eb9183921", "to": "CRITICAL DIFFERENCE ANALYSIS", "width": 1.0}, {"description": "Wilcoxon paired signed-rank test is used to assess the significance of the differences among the performance of the models using critical difference analysis", "from": "WILCOXON PAIRED SIGNED-RANK TEST", "source_id": "70a97858b727e5af1466ae5eb9183921", "to": "CRITICAL DIFFERENCE ANALYSIS", "width": 1.0}, {"description": "H@100% and H@150% are both metrics used to evaluate the performance of anomaly detection methods", "from": "H@100%", "source_id": "1413d358c623ac2d4c70be6547eb218b", "to": "H@150%", "width": 1.0}, {"description": "N@100% and N@150% are both metrics used to evaluate the performance of anomaly detection methods", "from": "N@100%", "source_id": "1413d358c623ac2d4c70be6547eb218b", "to": "N@150%", "width": 1.0}, {"description": "F1 score is related to ROC/AUC score as both are measures of model performance", "from": "F1 SCORE", "source_id": "9b35a2c607e0f4b8cbc9179f424f180a", "to": "ROC/AUC SCORE", "width": 1.0}, {"description": "F1 score is related to training times as training times can affect model performance", "from": "F1 SCORE", "source_id": "9b35a2c607e0f4b8cbc9179f424f180a", "to": "TRAINING TIMES", "width": 1.0}, {"description": "F1 score is related to root causes as the model is able to correctly identify root causes for up to 75% of the detected anomalies", "from": "F1 SCORE", "source_id": "a39d9d28126733c33db624ccc25f5782", "to": "ROOT CAUSES", "width": 1.0}, {"description": "F1 score and training time are metrics, as indicated by their use in the table", "from": "F1 SCORE", "source_id": "f6e57fa18831bcc732a631240536777a", "to": "TRAINING TIME", "width": 1.0}, {"description": "Our implementation gives F1 scores close to MATLAB codeOur implementation is compared to MATLAB code in terms of F1 score", "from": "F1 SCORE", "source_id": "0e3a4048d3d693cb8fd969fd606f1e8a", "to": "IMPLEMENTATION", "width": 2.0}, {"description": "MATLAB code is compared to our implementation in terms of F1 scoreMATLAB code gives F1 scores compared to our implementation", "from": "F1 SCORE", "source_id": "0e3a4048d3d693cb8fd969fd606f1e8a", "to": "MATLAB CODE", "width": 2.0}, {"description": "ROC/AUC score is related to training times as training times can affect model performance", "from": "ROC/AUC SCORE", "source_id": "9b35a2c607e0f4b8cbc9179f424f180a", "to": "TRAINING TIMES", "width": 1.0}, {"description": "Our implementation gives training time gains compared to MATLAB codeOur implementation is compared to MATLAB code in terms of training time", "from": "TRAINING TIME", "source_id": "0e3a4048d3d693cb8fd969fd606f1e8a", "to": "IMPLEMENTATION", "width": 2.0}, {"description": "MATLAB code gives training time compared to our implementationMATLAB code is compared to our implementation in terms of training time", "from": "TRAINING TIME", "source_id": "0e3a4048d3d693cb8fd969fd606f1e8a", "to": "MATLAB CODE", "width": 2.0}, {"description": "Series data is related to transformer based encoder-decoder as the model is used to analyze series data", "from": "TRANSFORMER BASED ENCODER-DECODER", "source_id": "a39d9d28126733c33db624ccc25f5782", "to": "SERIES DATA", "width": 1.0}, {"description": "State-of-the-art models are related to bidirectional neural networks as they are a type of model that allows for model generalization to diverse temporal trends in data", "from": "STATE-OF-THE-ART MODELS", "source_id": "a39d9d28126733c33db624ccc25f5782", "to": "BIDIRECTIONAL NEURAL NETWORKS", "width": 1.0}, {"description": "Cost-benefit analysis is related to GitHub as the code and relevant training scripts are made publicly available on GitHub", "from": "COST-BENEFIT ANALYSIS", "source_id": "a39d9d28126733c33db624ccc25f5782", "to": "GITHUB", "width": 1.0}, {"description": "GitHub is related to BSD-3 licence as the code and relevant training scripts are made publicly available under this licence", "from": "GITHUB", "source_id": "a39d9d28126733c33db624ccc25f5782", "to": "BSD-3 LICENCE", "width": 1.0}, {"description": "MATLAB is related to grid-search as the original code uses grid-search to find optimal hyperparameters", "from": "MATLAB", "source_id": "a39d9d28126733c33db624ccc25f5782", "to": "GRID-SEARCH", "width": 1.0}, {"description": "The original model is being compared to the proposed model, Ours", "from": "OURS", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "ORIGINAL", "width": 1.0}, {"description": "The proposed model, Ours, is related to deviation as deviation is the difference between the original and proposed models", "from": "OURS", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "DEVIATION", "width": 1.0}, {"description": "The original model is related to deviation as deviation is the difference between the original and proposed models", "from": "DEVIATION", "source_id": "b01517b8d09acabed7145d9ffa4a409b", "to": "ORIGINAL", "width": 1.0}, {"description": "MATLAB code and our implementation are implementations, as indicated by their use in the text", "from": "MATLAB CODE", "source_id": "f6e57fa18831bcc732a631240536777a", "to": "OUR IMPLEMENTATION", "width": 1.0}, {"description": "Our implementation is compared to MATLAB code to evaluate its performance", "from": "MATLAB CODE", "source_id": "0e3a4048d3d693cb8fd969fd606f1e8a", "to": "IMPLEMENTATION", "width": 1.0}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>