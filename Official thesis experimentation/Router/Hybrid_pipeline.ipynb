{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizzazione di un sistema ibrido\n",
    "\n",
    "Il sistema sarà in grado di riconoscere la tipologia di domanda (globale/locale) ed utilizzare la pipeline più adeguata a quella tipologia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificatore di domande"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classificazione viene effettuata tramite una chiamata ad un LLM con apposito prompt, che fornisce descrizioni ed esempi delle due tipologie di domande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt per la classificazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_prompt = \"\"\"\n",
    "Task: You will classify the following question as either \"local\" or \"global\". Please respond only with the word \"local\" or \"global\".\n",
    "\n",
    "Context:\n",
    "We are analyzing a set of 9 papers related to time-series models. The titles of the 9 papers are:\n",
    "\n",
    "1. TranAD\n",
    "2. AnomalyBERT\n",
    "3. RestAD\n",
    "4. TimeGPT\n",
    "5. TimeLLM\n",
    "6. LagLlama\n",
    "7. Chronos\n",
    "8. Foundation Models for Time Series Analysis\n",
    "9. TimeSFM\n",
    "\n",
    "Definitions:\n",
    "- **Local questions**: These questions focus on specific details found within a single paper. These questions often reference sections, figures, or tables of a document. They do not require synthesizing information from multiple papers on the list, even if they involve comparisons between different methods within the same paper.\n",
    " Characteristics of Local Questions:\n",
    "  1. They reference specific sections, tables, or figures within a single document.\n",
    "  2. Formed sometimes by very specific questions, dealing with very particular technical details.\n",
    "  3. If it says 'according to the abstract of the paper... ', 'in accordance with table ..', 'according to section ..', the specific chapter, or uses phrases like that, the question is almost certainly local.\n",
    "  4. They do not require synthesizing information from multiple papers on the list, even if they involve comparisons between different methods within the same paper.\n",
    "  5. Sometimes it focuses on detailed results, scores, metrics, precise comparisons with numerical results.\n",
    " Examples of local questions:\n",
    "    1. \"What is the architecture of the model described in Section 4 of the paper?\"\n",
    "    2. \"How does the model's performance in Table 2 compare to other baselines presented in the same paper?\"\n",
    "    3. \"What preprocessing steps are necessary for the model described in this paper to handle seasonal data?\"\n",
    "    4. \"What evaluation metrics were used in the analysis presented in Section 5 of the paper?\"\n",
    "\n",
    "    \n",
    "- **Global questions**: These questions require the synthesis of information. They require understanding an entire paper and synthesizing it, or they require comparisons between the papers mentioned above. Global questions are also those that address general themes common to all papers.\n",
    " Characteristics of Global Questions:\n",
    "  1. They compare or reference multiple papers or models from the list of 9 papers.\n",
    "  2. They often ask about the trade-offs, similarities, or differences between methods from different documents on the list.\n",
    "  3. They require a broader understanding or a synthesis of information across papers or models from the list.\n",
    "  4. They can also be just about a specific paper but require a summary or the main contents of the paper \n",
    " Examples of Global Questions (synthetic examples):\n",
    "    1. \"How does the model Y compare to model Z for anomaly detection?\"\n",
    "    2. \"What are the key differences between the forecasting models described in Papers C and D?\"\n",
    "    3. \"How do the models from Paper F and Paper I handle non-stationary data, and why is this important?\"\n",
    "    4. \"How does Model X work?\"\n",
    "\n",
    "Additional Instructions:\n",
    " - If different papers, found in the list posed above, are mentioned in the question, the question will definitely be “global”.\n",
    " - If the question includes a comparison but one of the models mentioned is not in the list of papers, then it refers to a comparison within a paper and the question is “local”.\n",
    " - If the question mentions a specific section of a paper, such as the abstract, chapter number, table, code, image, or other, the question will definitely be “local”.\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chiamata al LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vengono definite le funzioni per la chiamata al LLM per la classificazione della domanda.\n",
    "\n",
    "Vari prompt sono stati testati a parte per valutarne le performance di classificazione.\n",
    "\n",
    "Le funzioni verranno poi usate nella pipeline definitiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = 'http://172.18.21.132:8000/v1/completions'\n",
    "model_name = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "\n",
    "def generate_payload(question, temperature=0.0, max_tokens=1):\n",
    "    prompt = classification_prompt.format(query=question)\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification(question):\n",
    "    payload = generate_payload(question)\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(model_url, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"text\"].strip()\n",
    "    else:\n",
    "        print(f\"Errore nella richiesta: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definizione delle due pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphRAG per pipeline globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "from graphrag.query.indexer_adapters import read_indexer_entities, read_indexer_reports\n",
    "from graphrag.query.llm.oai.chat_openai import ChatOpenAI\n",
    "from graphrag.query.llm.oai.typing import OpenaiApiType\n",
    "from graphrag.query.structured_search.global_search.community_context import (\n",
    "    GlobalCommunityContext,\n",
    ")\n",
    "from graphrag.query.structured_search.global_search.search import GlobalSearch\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path exists and is not empty.\n"
     ]
    }
   ],
   "source": [
    "file_path = '../../output/20240925-154939/artifacts'\n",
    "\n",
    "if not os.path.exists(file_path) or not os.listdir(file_path):\n",
    "    print(\"The specified path is empty or does not exist.\")\n",
    "else:\n",
    "    print(\"The path exists and is not empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = file_path\n",
    "LANCEDB_URI = f\"{INPUT_DIR}/lancedb\"\n",
    "\n",
    "COMMUNITY_REPORT_TABLE = \"create_final_community_reports\"\n",
    "ENTITY_TABLE = \"create_final_nodes\"\n",
    "ENTITY_EMBEDDING_TABLE = \"create_final_entities\"\n",
    "RELATIONSHIP_TABLE = \"create_final_relationships\"\n",
    "COVARIATE_TABLE = \"create_final_covariates\"\n",
    "TEXT_UNIT_TABLE = \"create_final_text_units\"\n",
    "\n",
    "COMMUNITY_LEVEL = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "nodes_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet\")\n",
    "relationship_df = pd.read_parquet(f\"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet\")\n",
    "df_report = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encoder = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)\n",
    "\n",
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "entity_embedding_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet\")\n",
    "entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"GRAPHRAG_API_KEY\"]\n",
    "llm_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "api_base = \"http://172.18.21.132:8000/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    model=llm_model,\n",
    "    api_type=OpenaiApiType.OpenAI,  \n",
    "    api_base=api_base,  \n",
    "    max_retries=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_builder = GlobalCommunityContext(\n",
    "    community_reports=reports,\n",
    "    entities=entities,  \n",
    "    token_encoder=token_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_builder_params = {\n",
    "    \"use_community_summary\": True,  \n",
    "    \"shuffle_data\": True,\n",
    "    \"include_community_rank\": True,\n",
    "    \"min_community_rank\": 0,\n",
    "    \"community_rank_name\": \"rank\",\n",
    "    \"include_community_weight\": True,\n",
    "    \"community_weight_name\": \"occurrence weight\",\n",
    "    \"normalize_community_weight\": True,\n",
    "    \"max_tokens\": 6000,  \n",
    "    \"context_name\": \"Reports\",\n",
    "}\n",
    "\n",
    "map_llm_params = {\n",
    "    \"max_tokens\": 1500,\n",
    "    \"temperature\": 0.0,\n",
    "    \"response_format\": {\"type\": \"json_object\"},\n",
    "}\n",
    "\n",
    "reduce_llm_params = {\n",
    "    \"max_tokens\": 1500,  \n",
    "    \"temperature\": 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine = GlobalSearch(\n",
    "    llm=llm,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    max_data_tokens=6000, \n",
    "    map_llm_params=map_llm_params,\n",
    "    reduce_llm_params=reduce_llm_params,\n",
    "    allow_general_knowledge=False,  \n",
    "    json_mode=True,  \n",
    "    context_builder_params=context_builder_params,\n",
    "    concurrent_coroutines=32,\n",
    "    response_type=\"Single page\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask_question(question):\n",
    "    try:\n",
    "        result = await search_engine.asearch(question)  \n",
    "        answer = result.response\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\\nException: {e}\")\n",
    "        answer = \"Error: Unable to retrieve answer.\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def global_pipeline(question):\n",
    "    print(f\"Running global pipeline for question: {question}\")\n",
    "    answer = await ask_question(question) \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG tradizionale per pipeline locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import glob\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbeddings(Embeddings):\n",
    "    def __init__(self, endpoint_url):\n",
    "        self.endpoint_url = endpoint_url\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            payload = {\n",
    "                \"input\": text,\n",
    "                \"model\": \"intfloat/multilingual-e5-large-instruct\"\n",
    "            }\n",
    "            response = requests.post(f\"{self.endpoint_url}/embeddings\", json=payload)\n",
    "            if response.status_code == 200:\n",
    "                embedding = response.json()['data'][0]['embedding']\n",
    "                embeddings.append(embedding)\n",
    "            else:\n",
    "                raise Exception(f\"Errore nell'embedder: {response.text}\")\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "embedder = CustomEmbeddings(endpoint_url=\"http://172.18.21.138:80/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_paths = glob.glob('../input/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file_path in document_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        documents.append(Document(page_content=content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di documenti caricati: 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numero di documenti caricati: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero totale di chunk: 718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,       \n",
    "    chunk_overlap=200,     \n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Numero totale di chunk: {len(docs)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    endpoint_url: str\n",
    "    model_name: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    temperature: float = 0.0\n",
    "    max_tokens: int = 1500\n",
    "    repetition_penalty: float = 1.2\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom_llm\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        payload = {\n",
    "            \"prompt\": prompt,\n",
    "            \"model\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"repetition_penalty\": self.repetition_penalty,\n",
    "            \"stop\": stop or [\"I don't know.\"],\n",
    "        }\n",
    "        print(\"Payload inviato all'API:\", payload)  \n",
    "        response = requests.post(f\"{self.endpoint_url}/completions\", json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['choices'][0]['text']\n",
    "        else:\n",
    "            raise Exception(f\"Errore nel LLM: {response.text}\")\n",
    "\n",
    "llm = CustomLLM(\n",
    "    endpoint_url=\"http://172.18.21.132:8000/v1\",\n",
    "    temperature=0.0,\n",
    "    max_tokens= 500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_naive(query):  \n",
    "    result = qa_chain({\"query\": query})\n",
    "    return result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_pipeline(question):\n",
    "    print(f\"Running local pipeline (naive RAG) for question: {question}\")\n",
    "    answer = ask_question_naive(question)  \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_question(question):\n",
    "    classification = get_classification(question)  \n",
    "    if classification == \"local\":\n",
    "        result = local_pipeline(question)  \n",
    "    elif classification == \"global\":\n",
    "        result = await global_pipeline(question)  \n",
    "    else:\n",
    "        result = \"Error: Invalid classification\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_all_questions(questions):\n",
    "    tasks = [process_question(question) for question in questions]\n",
    "    answers = await asyncio.gather(*tasks)  \n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    answers = await process_all_questions(questions_list)\n",
    "    \n",
    "    model_answers = []\n",
    "    for question, answer in zip(questions_list, answers):\n",
    "        model_answers.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    \n",
    "\n",
    "    output_file = 'questions_answers.json'\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(model_answers, f, indent=4)\n",
    "    \n",
    "    print(f\"Results saved in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione di tutte le domande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_path = '../DatasetCreation/Labeled_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0       1\n",
      "0  What are the main topics covered by the data i...  global\n",
      "1  How does RestAD leverage both statistical meth...  global\n",
      "2  What are the key features and benefits of Rest...  global\n",
      "3  What are the key features and benefits of Rest...  global\n",
      "4  How does TimeLLM differ from other models in t...  global\n",
      "RangeIndex(start=0, stop=2, step=1)\n"
     ]
    }
   ],
   "source": [
    "labeled_data = pd.read_json(questions_path)\n",
    "print(labeled_data.head())\n",
    "print(labeled_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What are the main topics covered by the data in the set of time-series papers?'\n",
      " 'How does RestAD leverage both statistical methods and machine learning to achieve robust anomaly detection in noisy time-series data?'\n",
      " 'What are the key features and benefits of RestAD in anomaly detection for time-series data?'\n",
      " 'What are the key features and benefits of RestAD in anomaly detection for time-series data?'\n",
      " 'How does TimeLLM differ from other models in time-series forecasting?'\n",
      " 'How does AnomalyBERT work?'\n",
      " 'How does TimeGPT approach time-series forecasting?'\n",
      " 'What types of real-world applications can benefit from models like TimeLLM, RestAD, TimeGPT, AnomalyBERT, LagLLama and the other models described?'\n",
      " 'What distinguishes LagLLama in its approach to time-series analysis?'\n",
      " 'How do models like AnomalyBERT handle non-stationary data, and why is this important?'\n",
      " 'What are the main trade-offs when choosing between transformer-based models and traditional time-series models?'\n",
      " 'How does TimeLLM incorporate historical data for better forecasting accuracy?'\n",
      " 'What kind of preprocessing steps are typically necessary for models like TimeGPT and AnomalyBERT to perform effectively on time-series data?'\n",
      " 'How do models like TimeGPT and RestAD handle seasonality and trends in time-series data?'\n",
      " 'How does Chronos approach time-series forecasting compared to traditional statistical models?'\n",
      " 'What are the strengths and weaknesses of TimesFM in handling multivariate time-series forecasting?'\n",
      " 'How does TranAD address the challenges of detecting anomalies in streaming time-series data?'\n",
      " 'How does Foundation Models for Time Series Analysis (FMTS) improve time-series forecasting compared to deep learning models?'\n",
      " 'What are the key differences between TimeLLM and TimeGPT in terms of model architecture and performance on time-series forecasting tasks?'\n",
      " 'How does LagLLama differ from Chronos in handling long-term dependencies in time-series forecasting?'\n",
      " 'What are the most common pitfalls when using anomaly detection models in financial datasets?'\n",
      " 'How has the use of foundation models impacted the field of time-series forecasting in terms of model accuracy and scalability?'\n",
      " 'How are transformer-based models improving the accuracy of anomaly detection in real-time applications?'\n",
      " 'What are the most effective techniques for handling seasonality and trend in time-series forecasting models?'\n",
      " 'How do foundation models like TimeGPT reduce the need for manual feature engineering in time-series forecasting?'\n",
      " 'How do foundation models such as TimeGPT, LagLLama, and TimesFM redefine time series forecasting compared to traditional statistical methods, and what are the implications of their zero-shot or few-shot capabilities in diverse industries?'\n",
      " 'What are the architectural innovations and challenges of adapting language models, like those in Chronos and TimeLLM, for time series data, particularly in terms of tokenization, temporal dependencies, and forecast accuracy?'\n",
      " 'In what ways do models like AnomalyBERT, TranAD, and RestAD approach anomaly detection in multivariate time series, and how do their self-supervised learning techniques compare in handling unlabeled data for real-time anomaly detection?'\n",
      " 'What role does probabilistic forecasting play in models like LagLLama and TimesFM, and how do these models handle uncertainty quantification across long prediction horizons?'\n",
      " 'How do models like TimeGPT and Chronos utilize synthetic data and real-world benchmarks in their training, and what trade-offs exist between their generalization abilities on new datasets and performance on specialized, domain-specific tasks?'\n",
      " 'How do the different models balance the trade-off between model complexity and interpretability when applied to real-world time series tasks such as anomaly detection and forecasting?'\n",
      " 'What are the shared architectural principles across the foundation models, and how do these designs influence their scalability and performance in large-scale forecasting tasks?'\n",
      " 'How do anomaly detection models like AnomalyBERT, TranAD, and RestAD compare in terms of their robustness and adaptability to various types of anomalies (contextual, point, collective) across different time series domains?'\n",
      " 'What are the main differences in how models like Chronos, TimesFM, and LagLLama handle long-term versus short-term dependencies in time series forecasting, and what impact does this have on their practical applications?'\n",
      " 'What challenges and breakthroughs have been encountered in applying self-supervised learning to time series data, and how does this compare with traditional supervised learning approaches in time series tasks?'\n",
      " 'How do state-of-the-art models for time series forecasting address the challenge of data sparsity and missing values in real-world datasets, and what techniques are most effective in mitigating the impact of such issues?'\n",
      " 'What are the main solutions presented in the paper Foundation Models for Time Series Data? What comparisons and tests were carried out?'\n",
      " \"According to the paper 'Chronos', what is the primary goal of the proposed framework?\"\n",
      " 'How does the Chronos framework tokenize time series values, as described in the paper?'\n",
      " 'What is the significance of the synthetic dataset generated via Gaussian processes in the Chronos framework, as mentioned in the paper?'\n",
      " 'How does the Chronos framework differ from other LLM-based forecasting models, such as PromptCast and LLMTime, as discussed in the paper?'\n",
      " 'What is the result of the comprehensive evaluation of Chronos models on 42 datasets, as reported in the paper?'\n",
      " 'According to the abstract of the TimeGPT paper, what is the main contribution of the paper?'\n",
      " 'What is the architecture of TimeGPT, as described in Section 5.1 of the paper?'\n",
      " 'What is the size of the training dataset used for TimeGPT, as mentioned in Section 5.2 of the paper?'\n",
      " 'What is the evaluation metric used to compare the performance of TimeGPT with other models, as described in Section 6 of the paper?'\n",
      " 'What is the average GPU inference speed of TimeGPT for zero-shot inference, as reported in Section 6.3 of the paper?'\n",
      " \"What is the main contribution of the paper 'Timesfm'?\"\n",
      " \"What is the composition of the pretraining dataset used in the paper 'Timesfm'?\"\n",
      " 'What is the architecture of the TimesFM model, and how does it differ from other models like PatchTST?'\n",
      " 'How does the performance of TimesFM compare to other baselines on the Monash and Darts datasets?'\n",
      " 'What are some potential limitations and future directions for the TimesFM model, as discussed in the paper?'\n",
      " 'What is the primary goal of the Lag-Llama model, as described in the abstract of the paper?'\n",
      " 'How does the tokenization scheme of Lag-Llama work, as described in Section 4.1 of the paper?'\n",
      " 'What is the architecture of Lag-Llama, as described in Section 4.2 of the paper?'\n",
      " 'What is the purpose of the distribution head in Lag-Llama, as described in Section 4.3 of the paper?'\n",
      " 'How does Lag-Llama handle the diversity of time series data in the pretraining corpus, as described in Section 5.1 of the paper?'\n",
      " \"According to the paper 'Foundation Models for Time Series Analysis: A Tutorial and Survey', what is the primary motivation behind the use of deep learning and transformers in time series analysis?\"\n",
      " \"What is the main contribution of the paper 'Foundation Models for Time Series Analysis: A Tutorial and Survey' in terms of taxonomy?\"\n",
      " \"According to the paper 'Foundation Models for Time Series Analysis: A Tutorial and Survey', what is the primary difference between the proposed taxonomy and previous taxonomies?\"\n",
      " 'What is the main advantage of using pre-trained models from other domains, such as large language, vision, and acoustic models, in time series analysis?'\n",
      " \"According to the paper 'Foundation Models for Time Series Analysis: A Tutorial and Survey', what is the primary benefit of using multi-modal time series analysis?\"\n",
      " \"According to the abstract of the paper 'TimeLLM', what are the benefits of leveraging large language models (LLMs) for time series forecasting?\"\n",
      " 'What is the main idea behind the proposed framework TIME-LLM, as described in the paper?'\n",
      " 'How does the patch reprogramming process work in TIME-LLM, as described in Section 3.1 of the paper?'\n",
      " 'What are the three pivotal components for constructing effective prompts in TIME-LLM, as described in the paper?'\n",
      " 'What is the significance of the output projection step in TIME-LLM, as described in Section 3.1 of the paper?'\n",
      " \"According to the abstract of the paper 'RESTAD', what is the primary limitation of using reconstruction error for anomaly detection in time series data?\"\n",
      " 'What is the role of the RBF layer in the RESTAD model, and how does it contribute to anomaly detection?'\n",
      " 'According to Table 1, what is the F1-score of the RESTAD model with random initialization on the SMD dataset?'\n",
      " 'What is the effect of integrating the RBF layer into the Transformer model, as shown in Figure 4?'\n",
      " 'According to the discussion section, what is the primary reason for the significant performance gains of the RESTAD model?'\n",
      " 'What is the main contribution of the AnomalyBERT paper, as described in the abstract?'\n",
      " 'What is the purpose of the data degradation scheme in AnomalyBERT, as described in Section 3.2?'\n",
      " 'What are the four types of synthetic outliers proposed in the AnomalyBERT paper, as described in Section 3.2?'\n",
      " 'What is the purpose of the 1D relative position bias in the AnomalyBERT model, as described in Section 3.1?'\n",
      " 'What is the evaluation metric used to compare the performance of AnomalyBERT with previous works, as described in Section 4.2?'\n",
      " \"What is the proposed model in the paper titled 'TranAD' and what are its key features?\"\n",
      " \"What is the problem formulation for anomaly detection and diagnosis in the paper 'TranAD'?\"\n",
      " 'What is the architecture of the neural network used in TranAD, as shown in Figure 1?'\n",
      " 'What is the two-phase adversarial training process in TranAD, as described in Algorithm 1?'\n",
      " 'What are the results of the ablation study in Table 6, and what do they indicate about the importance of each component of the TranAD model?']\n"
     ]
    }
   ],
   "source": [
    "questions_list = labeled_data[0].values\n",
    "print(questions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esecuzione della pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing event loop...\n",
      "Running global pipeline for question: What are the main topics covered by the data in the set of time-series papers?\n",
      "Running global pipeline for question: How does RestAD leverage both statistical methods and machine learning to achieve robust anomaly detection in noisy time-series data?\n",
      "Running global pipeline for question: What are the key features and benefits of RestAD in anomaly detection for time-series data?\n",
      "Running global pipeline for question: What are the key features and benefits of RestAD in anomaly detection for time-series data?\n",
      "Running global pipeline for question: How does TimeLLM differ from other models in time-series forecasting?\n",
      "Running local pipeline (naive RAG) for question: How does AnomalyBERT work?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n. Our model converts multivariate data points into temporal representations with relative position bias and yields anomaly scores from these representations. Our method, AnomalyBERT, shows a great capability of detecting anomalies contained in complex time series and surpasses previous state-of-the-art methods on five real-world benchmarks. Our code is available at https://github.com/Jhryu30/AnomalyBERT.\\n\\n# 4.4 MAIN RESULTS\\n\\nWe report the results of AnomalyBERT on the five real-world datasets introduced in Section 4.1, and compare them to those of the previous works. The reproduced evaluation scores are brought from Kim et al. (2022). In Table 2, we show that AnomalyBERT outperforms the previous methods on\\n# Presented at the ICLR 2023 workshop on Machine Learning for IoT: Datasets, Perception, and Understanding\\n\\n# Table 2: F1-scores for various anomaly detection methods and AnomalyBERT on five benchmark datasets.\\n\\nWe report the standard F1 and F1-scores after point adjustment (F1PA) following the protocol in Kim et al. (2022). Our method outperforms all existing methods with F1.\\n\\n# Figure 1\\n\\nExamples of anomaly detection for abnormal time series in SWaT (features in our model). Our method, AnomalyBERT, predicts anomaly scores that quantify abnormalities of data points in time series. We also visualize the original data and the final latent features in our model using t-SNE and show that our method separates abnormal points effectively.\\n\\n|(a) Abnormal component of the original data|(b) 2D-visualization of the original data and latent features in our model.|\\n|---|---|\\n|Predicted Score|Latent feature|\\n|Normal|Abnormal|\\n|Normal|Abnormal|\\n\\n# 1 INTRODUCTION\\n\\nIn many industrial environments, time series data is mainly dealt with to monitor machines, IT devices, spacecrafts, or engines. Anomaly detection is one of the essential tasks for time series analysis, which can find defects in machines and prevent potential harm. Recently, many deep learning-based approaches have been applied to this work. Several studies design recurrent neural networks.\\n\\n# Table 1: Examination of relationships between the typical types of outliers in Lai et al. (2021) and our proposed synthetic outliers.\\n\\n|Proposed|Typical| | | | |\\n|---|---|---|---|---|---|\\n|Global|Contextual|Shapelet|Seasonal|Trend| |\\n|Soft replacement|✔|✔|✔|✔|✔|\\n|Uniform replacement|✔|✔| |✔| |\\n|Length adjustment| | | |✔| |\\n|Peak noise| |✔| | | |\\n\\n# Figure 4: Qualitative results of AnomalyBERT on abnormal sequences from SWaT, WADI, and SMAP datasets.\\n\\nWe visualize an abnormal component where the outlier occurs and anomaly scores for each sequence. Without any post-processing, our method finds out diverse types of anomalies.\\n\\n# B VISUAL RESULTS\\n\\nWe visualize the prediction results of AnomalyBERT and present the 2D projections of the original data and latent features in the model. Similar to Figure 1, we select three abnormal windows in each of SWaT, WADI, and SMAP test sets and fetch the corresponding anomaly scores after the score prediction process. We then project the original data points and the last latent features from the abnormal windows into 2D planes using t-SNE. As shown in Figure 5, our method distinguishes anomalies successfully and separates abnormal data points from normal points well.\\n\\n# SWaT\\n\\n# WADI\\n\\n# SMAP\\n\\n# Figure 5: Visualization of anomaly score predictions on abnormal sequences in SWaT, WADI, and SMAP datasets.\\n\\nQuestion:\\nHow does AnomalyBERT work?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running global pipeline for question: How does TimeGPT approach time-series forecasting?\n",
      "Running global pipeline for question: What types of real-world applications can benefit from models like TimeLLM, RestAD, TimeGPT, AnomalyBERT, LagLLama and the other models described?\n",
      "Running global pipeline for question: What distinguishes LagLLama in its approach to time-series analysis?\n",
      "Running global pipeline for question: How do models like AnomalyBERT handle non-stationary data, and why is this important?\n",
      "Running global pipeline for question: What are the main trade-offs when choosing between transformer-based models and traditional time-series models?\n",
      "Running global pipeline for question: How does TimeLLM incorporate historical data for better forecasting accuracy?\n",
      "Running global pipeline for question: What kind of preprocessing steps are typically necessary for models like TimeGPT and AnomalyBERT to perform effectively on time-series data?\n",
      "Running global pipeline for question: How do models like TimeGPT and RestAD handle seasonality and trends in time-series data?\n",
      "Running global pipeline for question: How does Chronos approach time-series forecasting compared to traditional statistical models?\n",
      "Running global pipeline for question: What are the strengths and weaknesses of TimesFM in handling multivariate time-series forecasting?\n",
      "Running global pipeline for question: How does TranAD address the challenges of detecting anomalies in streaming time-series data?\n",
      "Running global pipeline for question: How does Foundation Models for Time Series Analysis (FMTS) improve time-series forecasting compared to deep learning models?\n",
      "Running global pipeline for question: What are the key differences between TimeLLM and TimeGPT in terms of model architecture and performance on time-series forecasting tasks?\n",
      "Running global pipeline for question: How does LagLLama differ from Chronos in handling long-term dependencies in time-series forecasting?\n",
      "Running global pipeline for question: What are the most common pitfalls when using anomaly detection models in financial datasets?\n",
      "Running global pipeline for question: How has the use of foundation models impacted the field of time-series forecasting in terms of model accuracy and scalability?\n",
      "Running global pipeline for question: How are transformer-based models improving the accuracy of anomaly detection in real-time applications?\n",
      "Running global pipeline for question: What are the most effective techniques for handling seasonality and trend in time-series forecasting models?\n",
      "Running global pipeline for question: How do foundation models like TimeGPT reduce the need for manual feature engineering in time-series forecasting?\n",
      "Running global pipeline for question: How do foundation models such as TimeGPT, LagLLama, and TimesFM redefine time series forecasting compared to traditional statistical methods, and what are the implications of their zero-shot or few-shot capabilities in diverse industries?\n",
      "Running global pipeline for question: What are the architectural innovations and challenges of adapting language models, like those in Chronos and TimeLLM, for time series data, particularly in terms of tokenization, temporal dependencies, and forecast accuracy?\n",
      "Running global pipeline for question: In what ways do models like AnomalyBERT, TranAD, and RestAD approach anomaly detection in multivariate time series, and how do their self-supervised learning techniques compare in handling unlabeled data for real-time anomaly detection?\n",
      "Running global pipeline for question: What role does probabilistic forecasting play in models like LagLLama and TimesFM, and how do these models handle uncertainty quantification across long prediction horizons?\n",
      "Running global pipeline for question: How do models like TimeGPT and Chronos utilize synthetic data and real-world benchmarks in their training, and what trade-offs exist between their generalization abilities on new datasets and performance on specialized, domain-specific tasks?\n",
      "Running global pipeline for question: How do the different models balance the trade-off between model complexity and interpretability when applied to real-world time series tasks such as anomaly detection and forecasting?\n",
      "Running global pipeline for question: What are the shared architectural principles across the foundation models, and how do these designs influence their scalability and performance in large-scale forecasting tasks?\n",
      "Running global pipeline for question: How do anomaly detection models like AnomalyBERT, TranAD, and RestAD compare in terms of their robustness and adaptability to various types of anomalies (contextual, point, collective) across different time series domains?\n",
      "Running global pipeline for question: What are the main differences in how models like Chronos, TimesFM, and LagLLama handle long-term versus short-term dependencies in time series forecasting, and what impact does this have on their practical applications?\n",
      "Running global pipeline for question: What challenges and breakthroughs have been encountered in applying self-supervised learning to time series data, and how does this compare with traditional supervised learning approaches in time series tasks?\n",
      "Running global pipeline for question: How do state-of-the-art models for time series forecasting address the challenge of data sparsity and missing values in real-world datasets, and what techniques are most effective in mitigating the impact of such issues?\n",
      "Running global pipeline for question: What are the main solutions presented in the paper Foundation Models for Time Series Data? What comparisons and tests were carried out?\n",
      "Running local pipeline (naive RAG) for question: According to the paper 'Chronos', what is the primary goal of the proposed framework?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nChronos represents one of the first endeavours in practical pretrained time series forecasting models, with remarkable zero-shot performance on a comprehensive collection of test datasets. This work opens up various research avenues, some of which we discuss below.\\n\\n# 6.1 Beyond Zero-shot Univariate Forecasting\\n\\n# Fine tuning\\n\\nMotivated by the remarkable zero-shot performance of Chronos models, we conducted a preliminary investigation into fine-tuning Chronos models individually on datasets from Benchmark II.\\n\\n|Chronos-T5 (Small)|Zero Shot|Fine Tuned|\\n|---|---|---|\\n|WQL|0.672|0.608|\\n|MASE|0.856|0.782|\\n\\n. An example of this behaviour is given in Figure 16b. Improving the tokenization to overcome these edge cases is subject for future work, but the results from Section 5.5 suggest that the Chronos models performs well on real-world data despite the limitations.\\n\\nWe selected T5 (Raffel et al., 2020) as the main architecture for Chronos in our experiments, since it is available in a variety of sizes, ranging from 16M (Tiny) to 11B (XXL) parameters (Tay et al., 2021). We also conducted experiments with the decoder-only GPT-2 model to demonstrate the applicability of the Chronos framework to decoder-only models. In the following, we discuss the training configurations used for our main results (Section 5.5) and explore alternatives for some of the hyperparameters in Section 5.6.\\n\\nOur comprehensive evaluation across 42 datasets establishes Chronos as a benchmark for both in-domain and zero-shot forecasting, surpassing both traditional models and task-specific deep learning approaches. Notably, Chronos achieves impressive zero-shot forecasting performance out of the box, without necessitating task-specific adjustments. Its accuracy, coupled with its relatively modest model size, positions it as a preferable alternative to larger, more computationally demanding models for zero-shot forecasting applications.\\n# cations. By its very nature as a language model operating over a fixed vocabulary, Chronos can seamlessly integrate with future advancements in LLMs, making it an ideal candidate for further development as a generalist time series model.\\n\\nQuestion:\\nAccording to the paper \\'Chronos\\', what is the primary goal of the proposed framework?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: How does the Chronos framework tokenize time series values, as described in the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Chronos: A Language Modeling Framework for Time Series\\n\\nIn this section we introduce Chronos, a framework adapting existing language model architectures and training procedures to probabilistic time series forecasting. While both language and time series are sequential in nature, they differ in terms of their representation — natural language consists of words from a finite\\n# 3.1 Time Series Tokenization\\n\\nConsider a time series x1:C+H = [x1, . . . , xC+H], where the first C time steps constitute the historical context, and the remaining H represent the forecast horizon. Language models operate on tokens from a finite vocabulary, so using them for time series data requires mapping the observations xi ∈ R to a finite set of tokens. To this end, we first scale and then quantize observations into a fixed number of bins.\\n\\n# Scaling\\n\\n. Chronos tokenizes time series into discrete bins through simple scaling and quantization of real values. In this way, we can train off-the-shelf language models on this “language of time series,” with no changes to the model architecture (see Figure 1 for a high-level depiction of Chronos). Remarkably, this straightforward approach proves to be effective and efficient, underscoring the potential for language model architectures to address a broad range of time series problems with minimal modifications.\\n\\n. An example of this behaviour is given in Figure 16b. Improving the tokenization to overcome these edge cases is subject for future work, but the results from Section 5.5 suggest that the Chronos models performs well on real-world data despite the limitations.\\n\\nIn this section, we analyze forecasts generated by Chronos models qualitatively, and we also highlight some limitations of our tokenization technique. We primarily focus on synthetically generated time series for a controlled analysis of different types of time series patterns. For example forecasts from real datasets, see Figures 22 to 24 in Appendix E.\\n\\n# I.I.D. Noise.\\n\\nWe generated time series comprised purely of Gaussian observations, N (0, 1) and N (100, 10), and used Chronos-T5 (Base) to forecast these. Figure 12a shows that Chronos generates plausible forecasts for such time series and the predicted 80% interval coincides with the ground truth 80% interval shown by the dashed blue lines.\\n\\n# Trend and seasonality.\\n\\n# Chronos: Learning the Language of Time Series\\n\\n# Abdul Fatir Ansari1∗ Lorenzo Stella1∗ Caner Turkmen1, Xiyuan Zhang3† Pedro Mercado4†\\n\\n# Huibin Shen1, Oleksandr Shchur1, Syama Sundar Rangapuram1, Sebastian Pineda Arango1, Shubham Kapoor1, Jasper Zschiegner, Danielle C. Maddix1, Hao Wang1,5†\\n\\n# honey2,6† Kari Torkkola2, Andrew Gordon Wilson2,7† Michael Bohlke-Schneider1, Yuyang†, Michael W. Ma-Wang1\\n\\n# 1AWS AI Labs, 2Amazon Supply Chain Optimization Technologies, 3UC San Diego, {ansarnd, stellalo}@amazon.com 4University of Freiburg, 5Rutgers University, 6UC Berkeley, 7New York University\\n\\n# Abstract\\n\\nQuestion:\\nHow does the Chronos framework tokenize time series values, as described in the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running global pipeline for question: What is the significance of the synthetic dataset generated via Gaussian processes in the Chronos framework, as mentioned in the paper?\n",
      "Running global pipeline for question: How does the Chronos framework differ from other LLM-based forecasting models, such as PromptCast and LLMTime, as discussed in the paper?\n",
      "Running global pipeline for question: What is the result of the comprehensive evaluation of Chronos models on 42 datasets, as reported in the paper?\n",
      "Running local pipeline (naive RAG) for question: According to the abstract of the TimeGPT paper, what is the main contribution of the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 5 TimeGPT\\n\\n# 5.1 Architecture\\n\\nTimeGPT is a Transformer-based time series model with self-attention mechanisms based on [Vaswani et al., 2017]. TimeGPT takes a window of historical values to produce the forecast, adding local positional encoding to enrich the input. The architecture consists of an encoder-decoder structure with multiple layers, each with residual connections and layer normalization. Finally, a linear layer maps the decoder’s output to the forecasting window dimension. The general intuition is that attention-based mechanisms are able to capture the diversity of past events and correctly extrapolate potential future distributions.\\n\\n# 5.3 Training TimeGPT\\n\\nTimeGPT underwent a multi-day training period on a cluster of NVIDIA A10G GPUs. During this process, we carried out extensive hyperparameter exploration to optimize learning rates, batch sizes, and other related parameters. We observed a pattern in alignment with findings from [Brown et al., 2020], where a larger batch size and a smaller learning rate proved beneficial. Implemented in PyTorch, TimeGPT was trained using the Adam with a learning rate decay strategy that reduced the rate to 12% of its initial value.\\n\\n# 5.4 Uncertainty quantification\\n\\n# 6.3 Time Comparison\\n\\nFor zero-shot inference, our internal tests recorded an average GPU inference speed of 0.6 milliseconds per series for TimeGPT, which nearly mirrors that of the simple Seasonal Naive. As points of comparison, we consider parallel computing-optimized statistical methods, which, when complemented with Numba compiling, averaged a speed of 600 milliseconds per series for training and inference. On the other hand, global models such as LGBM, LSTM, and NHITS demonstrated a more prolonged average of 57 milliseconds per series, considering both training and inference. Due to its zero-shot capabilities, TimeGPT outperforms traditional statistical methods and global models with total speed by orders of magnitude.\\n\\n# 7 Discussion and Future Research\\n\\nTimeGPT was benchmarked against a broad spectrum of baseline, statistical, machine learning, and neural forecasting models to provide a comprehensive performance analysis. Baselines and statistical models are individually trained on each time series of the test set, utilizing the historical values preceding the last forecasting window. We opted for a global model approach for machine learning and deep learning methods for each frequency, leveraging all time series in the test set. Some popular models like Prophet [Taylor and Letham, 2018] and ARIMA were excluded from the analysis due to their prohibitive computational requirements and extensive training times.\\n\\n# 7 Discussion and Future Research\\n\\nCurrent forecasting practice usually involves a complex pipeline, encompassing multiple steps from data processing to model training and selection. TimeGPT greatly simplifies this process by reducing pipelines to the inference step, substantially reducing complexity and time investment while still achieving state-of-the-art performance. Perhaps most significantly, TimeGPT democratizes the advantages of large transformers models, nowadays restricted to organizations with vast amounts of data, computational resources, and technical expertise. We believe that foundational models are set to profoundly impact the forecasting field and can redefine current practices.\\n\\nQuestion:\\nAccording to the abstract of the TimeGPT paper, what is the main contribution of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the architecture of TimeGPT, as described in Section 5.1 of the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 5 TimeGPT\\n\\n# 5.1 Architecture\\n\\nTimeGPT is a Transformer-based time series model with self-attention mechanisms based on [Vaswani et al., 2017]. TimeGPT takes a window of historical values to produce the forecast, adding local positional encoding to enrich the input. The architecture consists of an encoder-decoder structure with multiple layers, each with residual connections and layer normalization. Finally, a linear layer maps the decoder’s output to the forecasting window dimension. The general intuition is that attention-based mechanisms are able to capture the diversity of past events and correctly extrapolate potential future distributions.\\n\\n# 5.3 Training TimeGPT\\n\\nTimeGPT underwent a multi-day training period on a cluster of NVIDIA A10G GPUs. During this process, we carried out extensive hyperparameter exploration to optimize learning rates, batch sizes, and other related parameters. We observed a pattern in alignment with findings from [Brown et al., 2020], where a larger batch size and a smaller learning rate proved beneficial. Implemented in PyTorch, TimeGPT was trained using the Adam with a learning rate decay strategy that reduced the rate to 12% of its initial value.\\n\\n# 5.4 Uncertainty quantification\\n\\nWe selected T5 (Raffel et al., 2020) as the main architecture for Chronos in our experiments, since it is available in a variety of sizes, ranging from 16M (Tiny) to 11B (XXL) parameters (Tay et al., 2021). We also conducted experiments with the decoder-only GPT-2 model to demonstrate the applicability of the Chronos framework to decoder-only models. In the following, we discuss the training configurations used for our main results (Section 5.5) and explore alternatives for some of the hyperparameters in Section 5.6.\\n\\n# A PREPRINT\\n\\n# Figure 1\\n\\nWe provide an illustration of the TimesFM model architecture during training, where we show a input time-series of a specific length that can be broken down into input patches. Each patch along is processed into a vector by a residual block (as defined in the model definition) to the model dimension of the transformer layers. The vector is then added to positional encodings and fed into nl stacked transformer layers. SA refers to self-attention (note that we use multi-head causal attention) and FFN is the fully connected layer in the transformer. The output tokens are then mapped through a residual block to an output of size output_patch_len, which is the forecast for the time window following the last input patch seen by the model so far.\\n\\n# Network\\n\\n# 6.3 Time Comparison\\n\\nFor zero-shot inference, our internal tests recorded an average GPU inference speed of 0.6 milliseconds per series for TimeGPT, which nearly mirrors that of the simple Seasonal Naive. As points of comparison, we consider parallel computing-optimized statistical methods, which, when complemented with Numba compiling, averaged a speed of 600 milliseconds per series for training and inference. On the other hand, global models such as LGBM, LSTM, and NHITS demonstrated a more prolonged average of 57 milliseconds per series, considering both training and inference. Due to its zero-shot capabilities, TimeGPT outperforms traditional statistical methods and global models with total speed by orders of magnitude.\\n\\n# 7 Discussion and Future Research\\n\\nQuestion:\\nWhat is the architecture of TimeGPT, as described in Section 5.1 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the size of the training dataset used for TimeGPT, as mentioned in Section 5.2 of the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 5.3 Training TimeGPT\\n\\nTimeGPT underwent a multi-day training period on a cluster of NVIDIA A10G GPUs. During this process, we carried out extensive hyperparameter exploration to optimize learning rates, batch sizes, and other related parameters. We observed a pattern in alignment with findings from [Brown et al., 2020], where a larger batch size and a smaller learning rate proved beneficial. Implemented in PyTorch, TimeGPT was trained using the Adam with a learning rate decay strategy that reduced the rate to 12% of its initial value.\\n\\n# 5.4 Uncertainty quantification\\n\\nTimeGPT was trained on, to our knowledge, the largest collection of publicly available time series, collectively encompassing over 100 billion data points. This training set incorporates time series from a broad array of domains, including finance, economics, demographics, healthcare, weather, IoT sensor data, energy, web traffic, sales, transport, and banking. Due to this diverse set of domains, the training dataset contains time series with a wide range of characteristics.\\n\\nWe trained T5 models of 4 sizes, namely, Mini (20M), Small (46M), Base (200M) and Large (710M), and the GPT-2 base model (90M), on 10M TSMixup augmentations generated from the 28 training datasets, with K = 3 in Algorithm 1, and 1M synthetic time series generated using Gaussian processes. Note that with this setup, original time series are adequately represented since they are included in the TSMixup augmentations with probability 1/3. We sampled time series from the augmentations and synthetic data in the ratio 9:1 during training. Each model is trained with an effective batch size of 256 sequences, using distributed data parallelism and gradient accumulation, whenever necessary. These sequences are constructed by slicing random windows from the time series, and then scaling and quantizing them into equal-sized bins within the interval [l= − 15, r=15], as described in Section 3.1\\n\\n.1. The context length of the sequences was set to 512, the default for T5 models, and the prediction length is set to 64, a value greater than the prediction lengths of all tasks we consider in our evaluation.\\n\\n# Table 2: MAE of different methods on ETT datasets.\\n\\nAll methods use 10% of the original training set for training or finetuning. The baseline numbers are from Table 13 in [ZZP+21].\\n\\nQuestion:\\nWhat is the size of the training dataset used for TimeGPT, as mentioned in Section 5.2 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the evaluation metric used to compare the performance of TimeGPT with other models, as described in Section 6 of the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nIt must be noted that the validity of a forecasting model can only be assessed relative to its performance against competing alternatives. Although accuracy is commonly seen as the only relevant metric, computational cost and implementation complexity are key factors for practical applications. In this regard, it is noteworthy that the reported results of TimeGPT are the result of a simple and extremely fast invocation of the prediction method of a pre-trained model. In comparison, other models require a complete pipeline for training and then predicting.\\n\\n# 6.1.1 Comparison with recent Foundation Models\\n\\nWork in progress.\\n\\n# 6.2 Fine Tuning\\n\\nOur selected evaluation metrics include the relative Mean Absolute Error (rMAE) and the relative Root Mean Square Error (rRMSE), both normalized against the performance of the Seasonal Naive model. This choice is justified by the additional insights offered by these relative errors, as they show performance gains in relation to a known baseline, improving the interpretability of our results. The relative error metrics bring the additional benefit of scale independence, enabling comparisons.\\n\\n4Future work would profit from expanding and varying this testing set.\\n# Table 1: Main performance results for TimeGPT with zero-shot inference and benchmark models\\n\\nMeasured with rMAE and rRMSE, lower scores are better. The best model for each frequency and metric is highlighted in bold, the second best underlined, and the third best underlined with a dotted line.\\n\\nTimeGPT was benchmarked against a broad spectrum of baseline, statistical, machine learning, and neural forecasting models to provide a comprehensive performance analysis. Baselines and statistical models are individually trained on each time series of the test set, utilizing the historical values preceding the last forecasting window. We opted for a global model approach for machine learning and deep learning methods for each frequency, leveraging all time series in the test set. Some popular models like Prophet [Taylor and Letham, 2018] and ARIMA were excluded from the analysis due to their prohibitive computational requirements and extensive training times.\\n\\nThe evaluation is performed in the last forecasting window of each time series, varying in length by the sampling frequency. TimeGPT uses the previous historical values as inputs, as shown in Figure 3, without re-training its weights (zero-shot). We specify a different forecasting horizon based on the frequency to represent common practical applications: 12 for monthly, 1 for weekly, 7 for daily, and 24 for hourly data. 4\\n\\n# 5.3 Training TimeGPT\\n\\nTimeGPT underwent a multi-day training period on a cluster of NVIDIA A10G GPUs. During this process, we carried out extensive hyperparameter exploration to optimize learning rates, batch sizes, and other related parameters. We observed a pattern in alignment with findings from [Brown et al., 2020], where a larger batch size and a smaller learning rate proved beneficial. Implemented in PyTorch, TimeGPT was trained using the Adam with a learning rate decay strategy that reduced the rate to 12% of its initial value.\\n\\n# 5.4 Uncertainty quantification\\n\\nQuestion:\\nWhat is the evaluation metric used to compare the performance of TimeGPT with other models, as described in Section 6 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the average GPU inference speed of TimeGPT for zero-shot inference, as reported in Section 6.3 of the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 6.3 Time Comparison\\n\\nFor zero-shot inference, our internal tests recorded an average GPU inference speed of 0.6 milliseconds per series for TimeGPT, which nearly mirrors that of the simple Seasonal Naive. As points of comparison, we consider parallel computing-optimized statistical methods, which, when complemented with Numba compiling, averaged a speed of 600 milliseconds per series for training and inference. On the other hand, global models such as LGBM, LSTM, and NHITS demonstrated a more prolonged average of 57 milliseconds per series, considering both training and inference. Due to its zero-shot capabilities, TimeGPT outperforms traditional statistical methods and global models with total speed by orders of magnitude.\\n\\n# 7 Discussion and Future Research\\n\\nAcross the results for each frequency. To ensure both robust numerical stability and consistency in evaluation, we apply this normalization at a global scale for each comprehensive dataset. The specific computations for these metrics, applicable to a dataset with n time series and a forecast horizon of h, are described in Equation 2.\\n\\nrMAE = Pn=1Ph=1 yi,t − ˆi,ti / Ph=1 |yi,t − ˆi,t|t\\n\\nrRMSE = Pn=1qPh=1 (yi,t − ˆi,t)2 / ybase\\n\\n# 6.1 Zero-shot inference\\n\\nWe first test TimeGPT capabilities on zero-shot inference, meaning that no additional fine-tuning is performed on the test set. Table 1 presents the zero-shot results. Remarkably, TimeGPT outperforms a comprehensive collection of battle-tested statistical models and SoTA deep learning approaches, ranking among the top-3 performers across frequencies.\\n\\n# 5.3 Training TimeGPT\\n\\nTimeGPT underwent a multi-day training period on a cluster of NVIDIA A10G GPUs. During this process, we carried out extensive hyperparameter exploration to optimize learning rates, batch sizes, and other related parameters. We observed a pattern in alignment with findings from [Brown et al., 2020], where a larger batch size and a smaller learning rate proved beneficial. Implemented in PyTorch, TimeGPT was trained using the Adam with a learning rate decay strategy that reduced the rate to 12% of its initial value.\\n\\n# 5.4 Uncertainty quantification\\n\\nThe evaluation is performed in the last forecasting window of each time series, varying in length by the sampling frequency. TimeGPT uses the previous historical values as inputs, as shown in Figure 3, without re-training its weights (zero-shot). We specify a different forecasting horizon based on the frequency to represent common practical applications: 12 for monthly, 1 for weekly, 7 for daily, and 24 for hourly data. 4\\n\\n. To the best of our knowledge, the very recent work in TimeGPT-1 [GMC23] is the only other parallel work on a zero-shot foundation model for time-series forecasting. However the model is not public access, and several model details and the benchmark dataset have not been revealed.\\n\\nQuestion:\\nWhat is the average GPU inference speed of TimeGPT for zero-shot inference, as reported in Section 6.3 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running global pipeline for question: What is the main contribution of the paper 'Timesfm'?\n",
      "Running local pipeline (naive RAG) for question: What is the composition of the pretraining dataset used in the paper 'Timesfm'?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# A PREPRINT\\n\\n# Table 1: Composition of TimesFM pretraining dataset.\\n\\n|Dataset|Granularity|# Time series|# Time points|\\n|---|---|---|---|\\n|Synthetic| |3,000,000|6,144,000,000|\\n|Electricity|Hourly|321|8,443,584|\\n|Traffic| | | |\\n|Weather [ZZP+21]|Hourly|862|15,122,928|\\n| |10 Min|42|2,213,232|\\n|Favorita Sales [23]| | | |\\n|LibCity [WJJ]|Daily|111,840|6,159|\\n| |15 Min|139,179,538|34,253,622|\\n|M4 hourly|Hourly|414|353,500|\\n|M4 daily|Daily|4,227|9,964,658|\\n|M4 monthly|Monthly|48,000|10,382,411|\\n|M4 quarterly|Quarterly|24,000|2,214,108|\\n|M4 yearly|Yearly|22,739|840,644|\\n|Wiki hourly|Hourly|5,608,693|239,110,787,496|\\n|Wiki daily|Daily|68,448,204|115,143,501,240|\\n|Wiki weekly|Weekly|66,579,850|16,414,251,948|\\n|Wiki monthly|Monthly|63,151,306|3,789,760,907|\\n|Trends hourly|Hourly|22,435|393,043,680|\\n|Trends daily|Daily|22,435|122,921,365|\\n|Trends weekly|Weekly|22,435|16,585,438|\\n|Trends monthly|Monthly|22,435|3,821,760|\\n\\n# Figure 2:\\n\\n# A PREPRINT\\n\\n# Figure 1\\n\\nWe provide an illustration of the TimesFM model architecture during training, where we show a input time-series of a specific length that can be broken down into input patches. Each patch along is processed into a vector by a residual block (as defined in the model definition) to the model dimension of the transformer layers. The vector is then added to positional encodings and fed into nl stacked transformer layers. SA refers to self-attention (note that we use multi-head causal attention) and FFN is the fully connected layer in the transformer. The output tokens are then mapped through a residual block to an output of size output_patch_len, which is the forecast for the time window following the last input patch seen by the model so far.\\n\\n# Network\\n\\n# A PREPRINT\\n\\nIn particular, we design TimesFM, a single foundation model for time-series forecasting that, when applied to a variety of previously-unseen forecasting datasets across different domains, obtains close to state-of-the-art zero-shot accuracy (compared to the best supervised models trained individually for these datasets). Our model can work well across different forecasting history lengths, prediction lengths and time granularities at inference time. The key elements of our foundation model are twofold: 1) a large-scale time-series corpus built using both real-world (mostly time-series data from web search queries1 and Wikipedia page visits2) and synthetic data, which meets the volume and diversity of data needed for training our foundation model, and 2) a decoder style attention architecture with input patching, that can be efficiently pre-trained on this time-series corpus.\\n\\n# Table 3: Datasets used in the pretraining corpus and the unseen datasets on which we evaluate, grouped by the domains they are labelled against.\\n\\n|Transport & Tourism|Energy|Nature|\\n|---|---|---|\\n|Pretraining|San Francisco Traffic|Australian Electricity Demand|\\n| |Uber TLC Hourly|Electricity Hourly|\\n| |London Smart Meters|Solar|\\n| |Wind Farms|ETT H1|\\n| |ETT H2|ETT M1|\\n| |ETT M2| |\\n|Unseen|Pedestrian Counts|ETT M2|\\n| | |Weather|\\n|Air Quality|Cloud|Banking & Econ|\\n|Beijing Multisite|CPU Limit Minute| |\\n|UCI|CPU Usage Minute| |\\n| |Function Delay Minute| |\\n| |Instances Minute| |\\n| |Memory Limit Minute| |\\n| |Memory Usage Minute| |\\n|Beijing PM2.5|Requests Minute|Exchange Rate|\\n| |Platform Delay Minute| |\\n\\n# Table 4: Statistics of all the datasets used in the paper. Frequencies H stands for Hourly, T for minute, and B for business day. Tokens refers to the total number of windows of size 1 in the dataset, computed as the aggregate number of timesteps across all series in that dataset.\\n\\n# Synthetic Data\\n\\nAnother major component of our pretraining data is of synthetic origin. We create generators for ARMA processes, seasonal patterns (mixture of sines and cosines of different frequencies), trends (linear, exponential with a few change-points) and step functions. A synthetic time-series can be an additive combination of one or more of these processes. We create 3M synthetic time-series each of length 2048 time-points. More details about our synthetic data generation are presented in Appendix A.8.\\n\\n# Other real-world data sources\\n\\nQuestion:\\nWhat is the composition of the pretraining dataset used in the paper \\'Timesfm\\'?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running global pipeline for question: What is the architecture of the TimesFM model, and how does it differ from other models like PatchTST?\n",
      "Running global pipeline for question: How does the performance of TimesFM compare to other baselines on the Monash and Darts datasets?\n",
      "Running global pipeline for question: What are some potential limitations and future directions for the TimesFM model, as discussed in the paper?\n",
      "Running local pipeline (naive RAG) for question: What is the primary goal of the Lag-Llama model, as described in the abstract of the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n¶This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nas in LLaMA (Touvron et al., 2023). After passing through the causally masked transformer layers, the model predicts the parameters ϕ of the forecast log prob distribution of the next timestep, where the parameters are output by a parametric distribution head, as described in Sec. 4.3. The negative log-likelihood of the predicted distribution of all predicted timesteps is minimized.\\n\\n. This clearly establishes Lag-Llama as a strong foundation model that can be used on a wide range of downstream datasets, without prior knowledge of these data distribution — a key property that a foundation model should satisfy.\\n\\n# Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting\\n\\n# Authors\\n\\n*Kashif Rasul1 *Arjun Ashok2 3 4 ♢Andrew Robert Williams3 4 ♢Hena Ghonia3 4 Rishika Bhagwatkar ♠Roland Riachi3 4 ♠Arian Khorasani3 4 ♠Mohammad Javad Darvishi Bayazi3 ♠George Adamopoulos5 Nadhir Hassen3 4 Marin Biloš1 △Sahil Garg1 △Anderson Schneider1 △Nicolas Chapados2 4 △Alexandre Drouin2 4 △Valentina Zantedeschi2 ♣Yuriy Nevmyvaka1 ♣Irina Rish3 4\\n\\n# Abstract\\n\\n# D. Hyperparameters of Lag-Llama\\n\\nWe perform a random search of 100 different hyperparameter configurations and use the average validation loss over all datasets in the pretraining corpus to select our model. We list the possible hyperparameters of Lag-Llama and the optimal values obtained by our hyperparameter search in Tab. 5. Our final model obtained by hyperparameter search contains 2,449,299 parameters.\\n\\n# E. Forecast Visualizations\\n\\n. This is reflected in the results, as Lag-Llama is not the best performing model in each dataset. Still, Lag-Llama achieves a comparable average rank, and is among the models achieving the top average ranks.\\n\\nQuestion:\\nWhat is the primary goal of the Lag-Llama model, as described in the abstract of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: How does the tokenization scheme of Lag-Llama work, as described in Section 4.1 of the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 4.1. Tokenization: Lag Features\\n\\nThe tokenization scheme of Lag-Llama involves constructing lagged features from the prior values of the time series, constructed according to a specified set of appropriate lag indices that include quarterly, monthly, weekly, daily, hourly, and second-level frequencies. Given a sorted set of positive lag indices L = {1, . . . , L}*, we define the lag operation:\\n\\n*Note that L refers to the list of lag indices, while L is the last lag index in the sorted list L\\n\\nlag indices:\\n\\n- sec(t)\\n- min(t)\\n- month(t)\\n\\ntime\\n\\nFigure 1: For a time series, we depict the tokenization at the timestep t of the value xt which contains lag features constructed using an example set of lag indices L, where each value in the vector is from the past of xt (in blue), and F possible temporal covariates (date-time features) constructed from timestamp t (red).\\n\\n¶This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nas in LLaMA (Touvron et al., 2023). After passing through the causally masked transformer layers, the model predicts the parameters ϕ of the forecast log prob distribution of the next timestep, where the parameters are output by a parametric distribution head, as described in Sec. 4.3. The negative log-likelihood of the predicted distribution of all predicted timesteps is minimized.\\n\\n# 4.5 MODEL ANALYSIS\\n\\n# Language Model Variants\\n\\nWe compare two representative backbones with varying capacities. Our results indicate that the scaling law retains after the LLM reprogramming. We adopt Llama-7B by default in its full capacity, which manifestly outperforms its 1/4 capacity variant by 14.5%. An average MSE reduction of 14.7% is observed over GPT-2, which slightly outperforms its variant GPT-2 (6) by 2.7%.\\n\\n# Cross-modality Alignment\\n\\n# 4.4. Value Scaling\\n\\nWhen training on a large corpus of time series data from different datasets and domains, each time series can be of different numerical magnitude. Since we pretrain a foundation model over such data, we utilize the scaling heuristic (Salinas et al., 2019b) where for each univariate window, we calculate its mean value μi = ∑t=1C xi / C and variance σi. We can then replace the time series xt with {(xit - μi) / σi}t=1. We also incorporate μi and σi as time independent real-valued covariates for each token, to give the model information of the statistics of the inputs, which we call summary statistics.\\n\\n# 4.5. Training Strategies\\n\\nWe employ a series of training strategies to effectively pre-train Lag-Llama on the corpus of datasets. Firstly, we find that employing a stratified sampling approach where the datasets in the corpus are weighed by the amount of total.\\n# Lag-Llama\\n\\n# 4.2. Lag-Llama Architecture\\n\\nLag-Llama’s architecture is based on the decoder-only transformer-based architecture LLaMA (Touvron et al., 2023). Fig. 2 shows a general schematic of this model with M decoder layers. A univariate sequence of length xi−L:C along with its covariates is tokenized by concatenating the covariate vectors to a sequence of C tokens xi1:C. These tokens are passed through a shared linear projection layer that maps the features to the hidden dimension of the attention module. Similar to in Touvron et al. (2023), Lag-Llama incorporates pre-normalization via the RMSNorm (Zhang & Sennrich, 2019) and Rotary Positional Encoding (RoPE) (Su et al., 2021) at each attention layer’s query and key representations.\\n\\n¶This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nQuestion:\\nHow does the tokenization scheme of Lag-Llama work, as described in Section 4.1 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the architecture of Lag-Llama, as described in Section 4.2 of the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 4.2. Lag-Llama Architecture\\n\\nLag-Llama’s architecture is based on the decoder-only transformer-based architecture LLaMA (Touvron et al., 2023). Fig. 2 shows a general schematic of this model with M decoder layers. A univariate sequence of length xi−L:C along with its covariates is tokenized by concatenating the covariate vectors to a sequence of C tokens xi1:C. These tokens are passed through a shared linear projection layer that maps the features to the hidden dimension of the attention module. Similar to in Touvron et al. (2023), Lag-Llama incorporates pre-normalization via the RMSNorm (Zhang & Sennrich, 2019) and Rotary Positional Encoding (RoPE) (Su et al., 2021) at each attention layer’s query and key representations.\\n\\n¶This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\n# 4.5 MODEL ANALYSIS\\n\\n# Language Model Variants\\n\\nWe compare two representative backbones with varying capacities. Our results indicate that the scaling law retains after the LLM reprogramming. We adopt Llama-7B by default in its full capacity, which manifestly outperforms its 1/4 capacity variant by 14.5%. An average MSE reduction of 14.7% is observed over GPT-2, which slightly outperforms its variant GPT-2 (6) by 2.7%.\\n\\n# Cross-modality Alignment\\n\\n¶This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nas in LLaMA (Touvron et al., 2023). After passing through the causally masked transformer layers, the model predicts the parameters ϕ of the forecast log prob distribution of the next timestep, where the parameters are output by a parametric distribution head, as described in Sec. 4.3. The negative log-likelihood of the predicted distribution of all predicted timesteps is minimized.\\n\\n. This clearly establishes Lag-Llama as a strong foundation model that can be used on a wide range of downstream datasets, without prior knowledge of these data distribution — a key property that a foundation model should satisfy.\\n\\n# 4.1. Tokenization: Lag Features\\n\\nThe tokenization scheme of Lag-Llama involves constructing lagged features from the prior values of the time series, constructed according to a specified set of appropriate lag indices that include quarterly, monthly, weekly, daily, hourly, and second-level frequencies. Given a sorted set of positive lag indices L = {1, . . . , L}*, we define the lag operation:\\n\\n*Note that L refers to the list of lag indices, while L is the last lag index in the sorted list L\\n\\nlag indices:\\n\\n- sec(t)\\n- min(t)\\n- month(t)\\n\\ntime\\n\\nFigure 1: For a time series, we depict the tokenization at the timestep t of the value xt which contains lag features constructed using an example set of lag indices L, where each value in the vector is from the past of xt (in blue), and F possible temporal covariates (date-time features) constructed from timestamp t (red).\\n\\nQuestion:\\nWhat is the architecture of Lag-Llama, as described in Section 4.2 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the purpose of the distribution head in Lag-Llama, as described in Section 4.3 of the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 4.3. Choice of Distribution Head\\n\\nThe last layer of Lag-Llama is a distinct layer known as the distribution head, which projects the model’s features to the parameters of a probability distribution. We can combine different distribution heads with the representational capacity of the model to output the parameters ϕ of any parametric probability distribution. For our experiments, we adopt a Student’s t-distribution (Student, 1908) and output the three parameters corresponding to this distribution, namely its degrees of freedom, mean, and scale, with appropriate non-linearities to ensure the appropriate parameters stay positive.\\n\\n¶This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nas in LLaMA (Touvron et al., 2023). After passing through the causally masked transformer layers, the model predicts the parameters ϕ of the forecast log prob distribution of the next timestep, where the parameters are output by a parametric distribution head, as described in Sec. 4.3. The negative log-likelihood of the predicted distribution of all predicted timesteps is minimized.\\n\\nAt inference time, given a time series of size at least L, we can construct a feature vector that is passed to the model to obtain the distribution of the next time point. In this fashion, via greedy autoregressive decoding, we can obtain many simulated trajectories of the future up to our chosen prediction horizon P ≥ 1. From these empirical samples, we can calculate the uncertainty intervals for downstream decision-making tasks and metrics with respect to held-out data.\\n\\n# 4.3. Choice of Distribution Head\\n\\n# Table 5: Hyperparameter choices for Lag-Llama\\n\\nThe values with * represent the optimal values obtained by hyperparameter search.\\n\\n† Note that this is just the consecutive context that is sampled for each window; in practice we use a much larger context window due to the use of lags, as described in Sec. § 4.1\\n\\n|HYPERPARAMETER|LAG-LLAMA|\\n|---|---|\\n|NUMBER OF LAYERS|1,2,3,4,5,6,7,8*,9|\\n|NUMBER OF HEADS|1,2,3,4,5,6,7,8,9*|\\n|EMBEDDING DIMENSIONS PER HEAD| |\\n|CONTEXT LENGTH C †|16*, 32, 64, 128, 256, 512|\\n| |32*, 64, 128, 256, 512, 1024|\\n|AUGMENTATION PROBABILITY|0,0.25,0.5*,1.0|\\n|FREQUENCY MASKING RATE|0,0.25,0.5*,1.0|\\n|FREQUENCY MIXING RATE|0,0.25*,0.5,1.0|\\n|WEIGHT DECAY|0*,0.25,0.5,1.0|\\n|DROPOUT|0*,0.25,0.5,1.0|\\n\\nMore expressive choices of distributions, such as normalizing flows (Rasul et al., 2021b) and copulas (Salinas et al., 2019a; Drouin et al., 2022; Ashok et al., 2023) are potential choices of distribution heads, however with the potential overhead of difficulties in model training and optimization. The goal of our work was to keep the model as simple as possible, which led us to adopt a simple parametric distributional head. We leave the exploration of such distribution heads for future work.\\n\\n# 4.4. Value Scaling\\n\\nQuestion:\\nWhat is the purpose of the distribution head in Lag-Llama, as described in Section 4.3 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: How does Lag-Llama handle the diversity of time series data in the pretraining corpus, as described in Section 5.1 of the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# D. Hyperparameters of Lag-Llama\\n\\nWe perform a random search of 100 different hyperparameter configurations and use the average validation loss over all datasets in the pretraining corpus to select our model. We list the possible hyperparameters of Lag-Llama and the optimal values obtained by our hyperparameter search in Tab. 5. Our final model obtained by hyperparameter search contains 2,449,299 parameters.\\n\\n# E. Forecast Visualizations\\n\\nlishes Lag-Llama as one with strong adaptation capabilities across all levels of data. As the amount of history available increases, Lag-Llama achieves increasingly better perfor-\\n\\nmance across all datasets, and the gap between the rank of Lag-Llama and the baselines widens, as expected. Note, however, that Lag-Llama is most often outperformed by TFT in the exchange-rate dataset, which is from an entirely new domain and has a new unseen frequency. Our observa-\\n\\ntion demonstrates that, in cases where the data is most dis-\\n\\nsimilar, as compared to the pretraining corpus, Lag-Llama requires increasing amounts of history to train on, and, when given enough history to adapt, performs comparable to state-\\n\\nof-the-art (as discussed in subsection 6.1). Overall, our empirical results demonstrate that Lag-Llama has strong few-shot adaptation capabilities, and that, based on the characteristics of the downstream dataset, Lag-Llama can adapt and generalize with the appropriate amount of data.\\n\\n# 4.4. Value Scaling\\n\\nWhen training on a large corpus of time series data from different datasets and domains, each time series can be of different numerical magnitude. Since we pretrain a foundation model over such data, we utilize the scaling heuristic (Salinas et al., 2019b) where for each univariate window, we calculate its mean value μi = ∑t=1C xi / C and variance σi. We can then replace the time series xt with {(xit - μi) / σi}t=1. We also incorporate μi and σi as time independent real-valued covariates for each token, to give the model information of the statistics of the inputs, which we call summary statistics.\\n\\n# 4.5. Training Strategies\\n\\nWe employ a series of training strategies to effectively pre-train Lag-Llama on the corpus of datasets. Firstly, we find that employing a stratified sampling approach where the datasets in the corpus are weighed by the amount of total.\\n# Lag-Llama\\n\\nple decoder-only transformer architecture. We show that Lag-Llama, when pretrained from scratch on a large cor-\\n\\npus of datasets, has strong zero-shot generalization per-\\n\\nformance on unseen datasets, and performs comparably to dataset-specific models. Lag-Llama also demonstrates\\n\\nstate-of-the-art performance across diverse datasets from\\n\\ndifferent domains after finetuning, and emerges as the best general-purpose model without any knowledge of down-\\n\\nstream datasets. Lag-Llama also demonstrates a strong\\n\\nfew-shot adaptation performance across varying amounts\\n\\nof data history being available. Finally, we investigate the\\n\\ndiversity of the pretraining corpus used to train Lag-Llama. Our work opens up several potential directions for future\\n\\nwork. For now, collecting and collating a large scale time\\n\\nseries corpus of open dataset would be of high value, since\\n\\nthe largest time series dataset repositories (Godahewa et al., 2021) are themselves too small. Further, scaling up the\\n\\n# Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting\\n\\n# Authors\\n\\n*Kashif Rasul1 *Arjun Ashok2 3 4 ♢Andrew Robert Williams3 4 ♢Hena Ghonia3 4 Rishika Bhagwatkar ♠Roland Riachi3 4 ♠Arian Khorasani3 4 ♠Mohammad Javad Darvishi Bayazi3 ♠George Adamopoulos5 Nadhir Hassen3 4 Marin Biloš1 △Sahil Garg1 △Anderson Schneider1 △Nicolas Chapados2 4 △Alexandre Drouin2 4 △Valentina Zantedeschi2 ♣Yuriy Nevmyvaka1 ♣Irina Rish3 4\\n\\n# Abstract\\n\\nQuestion:\\nHow does Lag-Llama handle the diversity of time series data in the pretraining corpus, as described in Section 5.1 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running global pipeline for question: According to the paper 'Foundation Models for Time Series Analysis: A Tutorial and Survey', what is the primary motivation behind the use of deep learning and transformers in time series analysis?\n",
      "Running global pipeline for question: What is the main contribution of the paper 'Foundation Models for Time Series Analysis: A Tutorial and Survey' in terms of taxonomy?\n",
      "Running global pipeline for question: According to the paper 'Foundation Models for Time Series Analysis: A Tutorial and Survey', what is the primary difference between the proposed taxonomy and previous taxonomies?\n",
      "Running global pipeline for question: What is the main advantage of using pre-trained models from other domains, such as large language, vision, and acoustic models, in time series analysis?\n",
      "Running global pipeline for question: According to the paper 'Foundation Models for Time Series Analysis: A Tutorial and Survey', what is the primary benefit of using multi-modal time series analysis?\n",
      "Running local pipeline (naive RAG) for question: According to the abstract of the paper 'TimeLLM', what are the benefits of leveraging large language models (LLMs) for time series forecasting?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nOur contributions are significant in two key aspects. First, we show that existing language model architectures are capable of performing forecasting without time-series-specific customizations. This paves the way for accelerated progress by leveraging developments in the area of LLMs and through better data strategies. Second, on a practical level, the strong performance of Chronos models suggests that large (by forecasting standards) pretrained language models can greatly simplify forecasting pipelines without sacrificing accuracy, offering an inference-only alternative to the conventional approach involving training and tuning a model on individual tasks.\\n\\n# Acknowledgements\\n\\nTIME-LLM shows promise in adapting frozen large language models for time series forecasting by reprogramming time series data into text prototypes more natural for LLMs and providing natural language guidance via Prompt-as-Prefix to augment reasoning. Evaluations demonstrate the adapted LLMs can outperform specialized expert models, indicating their potential as effective time series machines. Our results also provide a novel insight that time series forecasting can be cast as yet another “language” task that can be tackled by an off-the-shelf LLM to achieve state-of-the-art performance through our Time-LLM framework. Further research should explore optimal reprogramming representations, enrich LLMs with explicit time series knowledge through continued pre-training, and build towards multimodal models with joint reasoning across time series, natural language, and other modalities\\n\\n# Easy optimization\\n\\nLLMs are trained once on massive computing and then can be applied to forecasting tasks without learning from scratch. Optimizing existing forecasting models often requires significant architecture search and hyperparameter tuning (Zhou et al., 2023b).\\n\\nIn summary, LLMs offer a promising path to make time series forecasting more general, efficient, synergistic, and accessible compared to current specialized modeling paradigms. Thus, adapting these powerful models for time series data can unlock significant untapped potential.\\n\\n# TIME-LLM: TIME SERIES FORECASTING BY REPROGRAMMING LARGE LANGUAGE MODELS\\n\\n# Ming Jin1∗, Shiyu Wang2∗, Lintao Ma2, Zhixuan Chu2, James Y. Zhang2, Xiaoming Shi2, Pin-Yu Chen3, Yuxuan Liang6, Yuan-Fang Li4, Shirui Pan4†, Qingsong Wen5†\\n\\n# Griffith University1, Alibaba Group5, Monash University1, Ant Group2, IBM Research3, The Hong Kong University of Science and Technology (Guangzhou)6\\n\\n{ming.jin, yuanfang.li}@monash.edu, pin-yu.chen@ibm.com, yuxliang@outlook.com, s.pan@griffith.edu.au, qingsongedu@gmail.com\\n\\n{weiming.wsy, lintao.mlt, chuzhixuan.czx, james.z, peter.sxm}@antgroup.com\\n\\n# ABSTRACT\\n\\n- We introduce a novel concept of reprogramming large language models for time series forecasting without altering the pre-trained backbone model. In doing so, we show that forecasting can be cast as yet another “language” task that can be effectively tackled by an off-the-shelf LLM.\\n- We propose a new framework, TIME-LLM, which encompasses reprogramming the input time series into text prototype representations that are more natural for the LLM, and augmenting the input context with declarative prompts (e.g., domain expert knowledge and task instructions) to guide LLM reasoning. Our technique points towards multimodal foundation models excelling in both language and time series.\\n\\nQuestion:\\nAccording to the abstract of the paper \\'TimeLLM\\', what are the benefits of leveraging large language models (LLMs) for time series forecasting?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the main idea behind the proposed framework TIME-LLM, as described in the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Input statistics:\\n\\n- &lt;time series statistic 1&gt;\\n- &lt;time series statistic 2&gt; …\\n\\n# Frozen Training\\n\\n# Prompt Embeddings\\n\\n# Patch Embeddings\\n\\n# Forward\\n\\n# Backward\\n\\nFigure 2: The model framework of TIME-LLM.\\nGiven an input time series, we first tokenize and embed it via 1 patching along with a 2 customized embedding layer. 3 These patch embeddings are then reprogrammed with condensed text prototypes to align two modalities. To augment the LLM’s reasoning ability, 4 additional prompt prefixes are added to the input to direct the transformation of input patches. 5 The output patches from the LLM are projected to generate the forecasts.\\n\\n# Table 9: An overview of the experimental configurations for TIME-LLM. “LTF” and “STF” denote long-term and short-term forecasting, respectively.\\n\\n# Proposed Framework\\n\\nIn this work, we propose TIME-LLM, a reprogramming framework to adapt large language models for time series forecasting while keeping the backbone model intact. The core idea is to reprogram the input time series into text prototype representations that are more naturally suited to language models’ capabilities. To further augment the model’s reasoning about time series concepts, we introduce Prompt-as-Prefix (PaP), a novel idea in enriching the input time series with additional context and providing task instructions in the modality of natural language. This provides declarative guidance about desired transformations to apply to the reprogrammed input. The output of the language model is then projected to generate time series forecasts.\\n\\n# Table 7: Efficiency analysis of TIME-LLM on ETTh1 in forecasting different steps ahead.\\n\\n|Length| |ETTh1-96| | |ETTh1-192| |ETTh1-336|ETTh1-512| | | | | | | |\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|Metric|Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|D.1 LLama (32)|3404.53|32136|0.517| |3404.57|33762|0.582| |3404.62|37988|0.632| |3404.69|39004|0.697|\\n|D.2 LLama (8)|975.83|11370|0.184| |975.87|12392|0.192| |975.92|13188|0.203| |976.11|13616|0.217|\\n|D.3 w/o LLM|6.39|3678|0.046| |6.42|3812|0.087| |6.48|3960|0.093| |6.55|4176|0.129|\\n\\nticipated as external knowledge can be naturally incorporated via prompting to facilitate the learning and inference. Additionally, providing the LLM with clear task instructions and input context (e.g., dataset captioning) is also beneficial (i.e., C.2 and C.1; eliciting over 7.7% and 9.6%, respectively).\\n\\n- TIME-LLM consistently exceeds state-of-the-art performance in mainstream forecasting tasks, especially in few-shot and zero-shot scenarios. Moreover, this superior performance is achieved while maintaining excellent model reprogramming efficiency. Thus, our research is a concrete step in unleashing LLMs’ untapped potential for time series and perhaps other sequential data.\\n\\nQuestion:\\nWhat is the main idea behind the proposed framework TIME-LLM, as described in the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: How does the patch reprogramming process work in TIME-LLM, as described in Section 3.1 of the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# (a) Patch Reprogramming\\n\\n# (b) Patch-as-Prefix and Prompt-as-Prefix\\n\\nFigure 3: Illustration of (a) patch reprogramming and (b) Patch-as-Prefix versus Prompt-as-Prefix.\\n\\nModalities that are identical or similar. Examples include repurposing a vision model to work with cross-domain images (Misra et al., 2023) or reprogramming an acoustic model to handle time series data (Yang et al., 2021). In both cases, there are explicit, learnable transformations between the source and target data, allowing for the direct editing of input samples. However, time series can neither be directly edited nor described losslessly in natural language, posing significant challenges to directly bootstrap the LLM for understanding time series without resource-intensive fine-tuning.\\n\\n# Patch Reprogramming.\\n\\nHere we reprogram patch embeddings into the source data representation space to align the modalities of time series and natural language to activate the backbone’s time series understanding and reasoning capabilities. A common practice is learning a form of “noise” that, when applied to target input samples, allows the pre-trained source model to produce the desired target outputs without requiring parameter updates. This is technically feasible for bridging data.\\n# Published as a conference paper at ICLR 2024\\n\\n# 0.6\\n\\n|Source|Target|the next value is|Projection|\\n|---|---|---|---|\\n|Vocab.|Prototypes|0.6|Pre-trained LLM|\\n|time|Patch Embeddings!| |Pre-trained LLM (Body + LM Head)|\\n|late| | | |\\n|early|Patch 1| | |\\n|down|Patch 2| | |\\n|up|…| | |\\n|steady|Patch| |Pre-trained LLM|\\n|short|Reprogram| |Pre-trained LLM (Text Embedder)|\\n|long|Patch 5| |Reprogram|\\n\\n# (a) Patch Reprogramming\\n\\n# (b) Patch-as-Prefix and Prompt-as-Prefix\\n\\n# Input statistics:\\n\\n- &lt;time series statistic 1&gt;\\n- &lt;time series statistic 2&gt; …\\n\\n# Frozen Training\\n\\n# Prompt Embeddings\\n\\n# Patch Embeddings\\n\\n# Forward\\n\\n# Backward\\n\\nFigure 2: The model framework of TIME-LLM.\\nGiven an input time series, we first tokenize and embed it via 1 patching along with a 2 customized embedding layer. 3 These patch embeddings are then reprogrammed with condensed text prototypes to align two modalities. To augment the LLM’s reasoning ability, 4 additional prompt prefixes are added to the input to direct the transformation of input patches. 5 The output patches from the LLM are projected to generate the forecasts.\\n\\n# 3.1 MODEL STRUCTURE\\n\\n# Input Embedding.\\n\\nEach input channel X(i) is first individually normalized to have zero mean and unit standard deviation via reversible instance normalization (RevIN) in mitigating the time series distribution shift (Kim et al., 2021). Then, we divide X(i) into several consecutive overlapped or non-overlapped patches (Nie et al., 2023) with length Lp; thus the total number of input patches is P = ⌊ (T −Lp)⌋ + 2, where S denotes the horizontal sliding stride. The underlying motivations are two-fold: (1) better preserving local semantic information by aggregating local information into each patch and (2) serving as tokenization to form a compact sequence of input tokens, reducing computational burdens. Given these patches XP ∈ RP ×Lp, we embed them as XP ∈ RP ×dm, adopting a simple linear layer as the patch embedder to create dimensions dm.\\n\\n# Patch Reprogramming.\\n\\n# Cross-modality Alignment\\n\\nOur results indicate that ablating either patch reprogramming or Prompt-as-Prefix hurts knowledge transfer in reprogramming the LLM for effective time series forecasting. In the absence of representation alignment, we observe a notable average performance degradation of 9.2%, which becomes more pronounced (exceeding 17%) in few-shot tasks. In TIME-LLM, the act of prompting stands as a pivotal element in harnessing the LLM’s capacity for understanding the inputs and tasks. Ablation of this component results in over 8% and 19% degradation in standard and few-shot forecasting tasks, respectively. We find that removing the input statistics hurts the most, resulting in an average increase of 10.2% MSE.\\n# Published as a conference paper at ICLR 2024\\n\\n# Table 6: Ablations on ETTh1 and ETTm1 in predicting 96 and 192 steps ahead (MSE reported). Red: the best.\\n\\nQuestion:\\nHow does the patch reprogramming process work in TIME-LLM, as described in Section 3.1 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What are the three pivotal components for constructing effective prompts in TIME-LLM, as described in the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Input statistics:\\n\\n- &lt;time series statistic 1&gt;\\n- &lt;time series statistic 2&gt; …\\n\\n# Frozen Training\\n\\n# Prompt Embeddings\\n\\n# Patch Embeddings\\n\\n# Forward\\n\\n# Backward\\n\\nFigure 2: The model framework of TIME-LLM.\\nGiven an input time series, we first tokenize and embed it via 1 patching along with a 2 customized embedding layer. 3 These patch embeddings are then reprogrammed with condensed text prototypes to align two modalities. To augment the LLM’s reasoning ability, 4 additional prompt prefixes are added to the input to direct the transformation of input patches. 5 The output patches from the LLM are projected to generate the forecasts.\\n\\nPrompt engineering is more specialized in LLM-based TSFMs. The prompt can be handcrafted with task-specific textual input and directly used to query the output for downstream prediction [87, 102] or intermediate embedding as feature enhancement [103]. Besides, the prompt can also be parameterized vectors and end-to-end learnable when optimizing the model on target datasets [10, 83]. In comparison to static prompts, the use of trainable prompts enhances the ability of LLMs to comprehend and match the context of given time series inputs. For example, TEMPO [10] constructs a trainable prompt pool with distinct key-value pairs, and retrieves the most representative prompt candidates with the highest similarity scores.\\n\\n# Table 7: Efficiency analysis of TIME-LLM on ETTh1 in forecasting different steps ahead.\\n\\n|Length| |ETTh1-96| | |ETTh1-192| |ETTh1-336|ETTh1-512| | | | | | | |\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|Metric|Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|D.1 LLama (32)|3404.53|32136|0.517| |3404.57|33762|0.582| |3404.62|37988|0.632| |3404.69|39004|0.697|\\n|D.2 LLama (8)|975.83|11370|0.184| |975.87|12392|0.192| |975.92|13188|0.203| |976.11|13616|0.217|\\n|D.3 w/o LLM|6.39|3678|0.046| |6.42|3812|0.087| |6.48|3960|0.093| |6.55|4176|0.129|\\n\\nticipated as external knowledge can be naturally incorporated via prompting to facilitate the learning and inference. Additionally, providing the LLM with clear task instructions and input context (e.g., dataset captioning) is also beneficial (i.e., C.2 and C.1; eliciting over 7.7% and 9.6%, respectively).\\n\\n† H represents the forecasting horizon of the M4 and M3 datasets.\\n\\n* LR means the initial learning rate.\\n# HYPERPARAMETER SENSITIVITY\\n\\nWe conduct a hyperparameter sensitivity analysis focusing on the four important hyperparameters within TIME-LLM: namely, the number of backbone model layers, the number of text prototypes V ′, the time series input length T, and the number of patch reprogramming cross-attention heads K. The correlated results can be found in Fig. 6. From our analysis, we derive the following observations:\\n\\nWe provide additional technical details of TIME-LLM in three aspects: (1) the learning of text prototypes, (2) the calculation of trends and lags in time series for use in prompts, and (3) the implementation of the output projection. To identify a small set of text prototypes E′ ∈ RV ′×D from E ∈ RV ×D, we learn a matrix W ∈ RV ′×V as the intermediary. To describe the overall time series trend in natural language, we calculate the sum of differences between consecutive time steps. A sum greater than 0 indicates an upward trend, while a lesser sum denotes a downward trend. In addition, we calculate the top-5 lags of the time series, identified by computing the autocorrelation using fast Fourier transformation and selecting the five lags with the highest correlation values. After we pack and feedforward the prompt and patch embeddings O(i) ∈ RP ×D through the frozen LLM, we discard the prefixal part and obtain the output representations, denoted as Oi ∈ RP ×D\\n\\nQuestion:\\nWhat are the three pivotal components for constructing effective prompts in TIME-LLM, as described in the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the significance of the output projection step in TIME-LLM, as described in Section 3.1 of the paper?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Table 7: Efficiency analysis of TIME-LLM on ETTh1 in forecasting different steps ahead.\\n\\n|Length| |ETTh1-96| | |ETTh1-192| |ETTh1-336|ETTh1-512| | | | | | | |\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|Metric|Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)| |Param. (M)|Mem. (MiB)|Speed(s/iter)|\\n|D.1 LLama (32)|3404.53|32136|0.517| |3404.57|33762|0.582| |3404.62|37988|0.632| |3404.69|39004|0.697|\\n|D.2 LLama (8)|975.83|11370|0.184| |975.87|12392|0.192| |975.92|13188|0.203| |976.11|13616|0.217|\\n|D.3 w/o LLM|6.39|3678|0.046| |6.42|3812|0.087| |6.48|3960|0.093| |6.55|4176|0.129|\\n\\nticipated as external knowledge can be naturally incorporated via prompting to facilitate the learning and inference. Additionally, providing the LLM with clear task instructions and input context (e.g., dataset captioning) is also beneficial (i.e., C.2 and C.1; eliciting over 7.7% and 9.6%, respectively).\\n\\n¶This is since a history of L points in time is needed for all points in the context, starting from the first point in the context.\\n# Lag-Llama\\n\\nas in LLaMA (Touvron et al., 2023). After passing through the causally masked transformer layers, the model predicts the parameters ϕ of the forecast log prob distribution of the next timestep, where the parameters are output by a parametric distribution head, as described in Sec. 4.3. The negative log-likelihood of the predicted distribution of all predicted timesteps is minimized.\\n\\n# Table 9: An overview of the experimental configurations for TIME-LLM. “LTF” and “STF” denote long-term and short-term forecasting, respectively.\\n\\n# Input statistics:\\n\\n- &lt;time series statistic 1&gt;\\n- &lt;time series statistic 2&gt; …\\n\\n# Frozen Training\\n\\n# Prompt Embeddings\\n\\n# Patch Embeddings\\n\\n# Forward\\n\\n# Backward\\n\\nFigure 2: The model framework of TIME-LLM.\\nGiven an input time series, we first tokenize and embed it via 1 patching along with a 2 customized embedding layer. 3 These patch embeddings are then reprogrammed with condensed text prototypes to align two modalities. To augment the LLM’s reasoning ability, 4 additional prompt prefixes are added to the input to direct the transformation of input patches. 5 The output patches from the LLM are projected to generate the forecasts.\\n\\n**Table 5: MAE for ETT datasets for prediction horizons 96 and 192. Owing to expensive evaluations for llmtime, the results are reported on the last test window of the original test split.**\\n|Dataset|llmtime(ZS)*|PatchTST|PatchTST(ZS)|FEDFormer|AutoFormer|Informer|TimesFM(ZS)|\\n|---|---|---|---|---|---|---|---|\\n|ETTh1 (horizon=96)|0.42|0.41|0.39|0.58|0.55|0.76|0.45|\\n|ETTh1 (horizon=192)|0.50|0.49|0.50|0.64|0.64|0.78|0.53|\\n|ETTh2 (horizon=96)|0.33|0.28|0.37|0.67|0.65|1.94|0.35|\\n|ETTh2 (horizon=192)|0.70|0.68|0.59|0.82|0.82|2.02|0.62|\\n|ETTm1 (horizon=96)|0.37|0.33|0.24|0.41|0.54|0.71|0.19|\\n|ETTm1 (horizon=192)|0.71|0.31|0.26|0.49|0.46|0.68|0.26|\\n|ETTm2 (horizon=96)|0.29|0.23|0.22|0.36|0.29|0.48|0.24|\\n|ETTm2 (horizon=192)|0.31|0.25|0.22|0.25|0.30|0.51|0.27|\\n|Avg|0.45|0.37|0.35|0.53|0.53|0.99|0.36|\\n\\n# A.6 More Details on Models\\n\\nWe now present implementation details about TimesFM and other baselines.\\n\\n# TimesFM\\n\\nQuestion:\\nWhat is the significance of the output projection step in TIME-LLM, as described in Section 3.1 of the paper?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: According to the abstract of the paper 'RESTAD', what is the primary limitation of using reconstruction error for anomaly detection in time series data?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nPreprint\\n\\nRamin Ghorbani1∗, Marcel J.T. Reinders1, and David M.J. Tax1\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nOur task is to determine if a given xi,t shows any anomalous behavior or not.\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Data\\n\\n|Input|Embedding|Encoder Layer 1|Encoder Layer 2|RBF Layer|Encoder Layer 3|Fully Connected Layer|Output Layer|\\n|---|---|---|---|---|---|---|---|\\n|Multi-Head Attention (8x)|Dropout|+|Norm|Feed-Forward|Norm|+| |\\n\\nFigure 2: Overview of the proposed RESTAD model. Here, the RBF layer is added after the second encoder layer.\\n\\n# Model Score\\n\\nRESTAD score(xi,t) = ϵr × ϵs (2)\\n\\nand ϵs = ||xi,t PM=1 ||2 represents the reconstruction error, where ϵr = 1 − M1 − ˆi,tzm,t measures dissimilarity. This combination highlights subtle anomalies characterized by both low reconstruction errors and RBF scores, as well as significant anomalies with high reconstruction errors or low RBF scores.\\n\\n# Initialization of RBF Layer Parameters\\n\\n# Evaluation\\n\\nAnomaly scores (Eq. 2) exceeding a threshold δ are identified as anomalies. Performance is evaluated using the F1-score for threshold-dependent evaluation. Here, we follow [17] by setting δ to label a predefined proportion of data points as anomalies (0.5% for SMD, 1% for others). For threshold-independent analysis, we use AUC-ROC, AUC-PR, VUS-ROC, and VUS-PR metrics [21]. We exclude the point-adjustment method [22] due to its overestimation [23]. Our model is compared against baselines and state-of-the-arts models: LSTM [9], vanilla Transformer [17], USAD [13], PatchAD [12], AnomalyTrans [17], and DCdetector [10].\\n\\n1 https://github.com/Raminghorbanii/RESTAD\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Table 1: Performance metrics of baselines and RESTAD on test sets.\\n\\nInitialization methods are denoted as (R) for Random and (K) for K-means. For all measures, a higher value indicates better anomaly detection performance.\\n\\nTo overcome the challenges of scoring based on reconstruction error and the limitations of the association discrepancy method, we propose combining the reconstruction error with a specialized non-linear transformation like the Radial Basis.\\n\\n*Corresponding Author: r.ghorbani@tudelft.nl\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n|Input|Transformer Layer|RBF Layer|Transformer Layer|Reconstructed Signal|\\n|---|---|---|---|---|\\n|Original Signal|Significant Anomaly|RBFCenter|Detected Anomaly|Normal Data Point|\\n|Reconstructed Signal|Subtle Anomaly|Missed Anomaly|Subtle Anomaly|Reconstruction Score|\\n|Threshold|Dis-Similarity Score|t0|t1|Time|\\n\\n# Figure 1: Comparison of traditional reconstruction and RBF-enhanced anomaly detection\\n\\n(a) Original signal with subtle and significant anomalies compared to its reconstruction.\\n\\n(b) Reconstruction errors for the signals in (a), highlighting challenges in detecting subtle anomalies.\\n\\nQuestion:\\nAccording to the abstract of the paper \\'RESTAD\\', what is the primary limitation of using reconstruction error for anomaly detection in time series data?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the role of the RBF layer in the RESTAD model, and how does it contribute to anomaly detection?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n(a) Original signal with subtle and significant anomalies compared to its reconstruction.\\n\\n(b) Reconstruction errors for the signals in (a), highlighting challenges in detecting subtle anomalies.\\n\\n(c) Visualization of a model integrated with an RBF, shown via a 2D scatter plot that includes typical data, subtle and significant anomalies, and the RBF center with its influence radius, showing the RBF’s ability to differentiate typical points from anomalies.\\n\\n(d) Enhanced anomaly score using the RBF, which shows improved detection of subtle anomalies.\\n\\n# 2.1 RESTAD Framework\\n\\nIn our study, we incorporate the anomaly detection mechanism into the vanilla Transformer through a specific layer of RBF neurons, see Figure 1.c. This RBF layer operates on the latent representations from the preceding layer, denoted by Hi = hi,t t=1, where hi,t ∈ Rdh.\\n\\nOur task is to determine if a given xi,t shows any anomalous behavior or not.\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Data\\n\\n|Input|Embedding|Encoder Layer 1|Encoder Layer 2|RBF Layer|Encoder Layer 3|Fully Connected Layer|Output Layer|\\n|---|---|---|---|---|---|---|---|\\n|Multi-Head Attention (8x)|Dropout|+|Norm|Feed-Forward|Norm|+| |\\n\\nFigure 2: Overview of the proposed RESTAD model. Here, the RBF layer is added after the second encoder layer.\\n\\n# Model Score\\n\\nRESTAD score(xi,t) = ϵr × ϵs (2)\\n\\nand ϵs = ||xi,t PM=1 ||2 represents the reconstruction error, where ϵr = 1 − M1 − ˆi,tzm,t measures dissimilarity. This combination highlights subtle anomalies characterized by both low reconstruction errors and RBF scores, as well as significant anomalies with high reconstruction errors or low RBF scores.\\n\\n# Initialization of RBF Layer Parameters\\n\\n. However, the optimal number of RBF centers is data-dependent. These findings motivate future studies for the exploration of integrating RBF layers into other deep learning architectures for anomaly detection tasks.\\n\\n# Figure 5: RESTAD Performance with varying RBF layer\\n\\n# Figure 6: RESTAD mean performance across varying numbers of RBF centers.\\n\\nShaded areas indicate ± standard deviation, illustrating variability across multiple runs.\\n\\n# Discussion and Conclusion\\n\\n# AUC-ROC Metric Value\\n\\n**Table 2: Effect of integrating RBF layer and the choice of anomaly score. For all measures, a higher value indicates better anomaly detection performance.**\\n|Architecture|Anomaly Criterion|SMD|SMD|SMD|MSL|MSL|MSL|PSM|PSM|PSM|\\n|---|---|---|---|---|\\n|Transformer|ϵr|0.11|0.75|0.19|0.80|0.22|0.06|0.56|0.14|0.63|\\n|RESTAD|ϵr|0.11|0.77|0.18|0.81|0.21|0.07|0.63|0.16|0.69|\\n|RESTAD|ϵr ϵs ϵs|0.01|0.44|0.03|0.52|0.07|0.01|0.43|0.08|0.48|\\n|RESTAD|ϵr × ϵs|0.23|0.78|0.23|0.82|0.24|0.07|0.68|0.18|0.72|\\n\\n# AUC-ROC\\n\\n# SMD Dataset\\n\\n# MSL Dataset\\n\\n# PSM Dataset\\n\\n# Figure 4: Effect of our composite anomaly score (ϵr × ϵs) compared to reconstruction error (ϵr) across segments of all datasets.\\n\\nThe highlighted regions in red indicate the true anomaly periods (labeled by an expert).\\n\\n# Figure 5: RESTAD Performance with varying RBF layer\\n\\n# Figure 6: RESTAD mean performance across varying numbers of RBF centers.\\n\\nQuestion:\\nWhat is the role of the RBF layer in the RESTAD model, and how does it contribute to anomaly detection?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: According to Table 1, what is the F1-score of the RESTAD model with random initialization on the SMD dataset?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Evaluation\\n\\nAnomaly scores (Eq. 2) exceeding a threshold δ are identified as anomalies. Performance is evaluated using the F1-score for threshold-dependent evaluation. Here, we follow [17] by setting δ to label a predefined proportion of data points as anomalies (0.5% for SMD, 1% for others). For threshold-independent analysis, we use AUC-ROC, AUC-PR, VUS-ROC, and VUS-PR metrics [21]. We exclude the point-adjustment method [22] due to its overestimation [23]. Our model is compared against baselines and state-of-the-arts models: LSTM [9], vanilla Transformer [17], USAD [13], PatchAD [12], AnomalyTrans [17], and DCdetector [10].\\n\\n1 https://github.com/Raminghorbanii/RESTAD\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Table 1: Performance metrics of baselines and RESTAD on test sets.\\n\\nInitialization methods are denoted as (R) for Random and (K) for K-means. For all measures, a higher value indicates better anomaly detection performance.\\n\\n# Figure 5: RESTAD Performance with varying RBF layer\\n\\n# Figure 6: RESTAD mean performance across varying numbers of RBF centers.\\n\\nShaded areas indicate ± standard deviation, illustrating variability across multiple runs.\\n\\n# Discussion and Conclusion\\n\\n# 4 Results\\n\\n# 4.1 Ablation Analysis\\n\\nOur empirical results, as detailed in Table 1, highlight the effectiveness of the RESTAD for anomaly detection. The ablation experiments are based on the RBF layer with random initialization. This decision is based on our findings that random initialization is as effective as the K-means strategy (see Table 1), while offering greater simplicity and computational efficiency. While there are slight performance differences between initialization methods, these variations are not significant enough to establish the superiority of one method over another.\\n\\nWe report the standard F1 and F1-scores after point adjustment (F1PA) following the protocol in Kim et al. (2022). Our method outperforms all existing methods with F1.\\n\\n| |SWaT|SWaT|WADI|WADI|MSL|MSL|SMAP|SMAP|SMD|SMD|\\n|---|---|---|---|---|---|\\n| |F1|F1PA|F1|F1PA|F1|F1PA|F1|F1PA|F1|F1PA|\\n|DAGMM (2018)|0.550|0.853|0.121|0.209|0.199|0.701|0.333|0.712|0.238|0.723|\\n|LSTM-VAE (2018)|0.775|0.805|0.227|0.380|0.212|0.678|0.235|0.756|0.435|0.808|\\n|OmniAnomaly (2019)|0.782|0.866|0.223|0.417|0.207|0.899|0.227|0.805|0.474|0.944|\\n|MSCRED (2019)|0.662|0.868|0.087|0.346|0.199|0.775|0.232|0.945|0.097|0.389|\\n|THOC (2020)|0.612|0.880|0.130|0.506|0.190|0.891|0.240|0.781|0.168|0.541|\\n|USAD (2020)|0.791|0.846|0.232|0.429|0.211|0.927|0.228|0.818|0.426|0.938|\\n|GDN (2021)|0.808|0.935|0.569|0.855|0.217|0.903|0.252|0.708|0.529|0.716|\\n|AnomalyBERT (Ours)|0.854|0.925|0.580|0.798|0.302|0.585|0.457|0.914|0.535|0.830|\\n\\n# Table 3: Results of ablation studies on combinations of the synthetic outlier types.\\n\\n# Table 4: Diagnosis Performance.\\n\\n|Method|SMD|SMD|SMD|SMD|MSDS|MSDS|MSDS|MSDS|\\n|---|---|---|\\n| |H@100%|H@150%|N@100%|N@150%|H@100%|H@150%|N@100%|N@150%|\\n|MERLIN|0.5907|0.6177|0.4150|0.4912|0.3816|0.5626|0.3010|0.3947|\\n|LSTM-NDT|0.3808|0.5225|0.3603|0.4451|0.1504|0.2959|0.1124|0.1993|\\n|DAGMM|0.4927|0.6091|0.5169|0.5845|0.2617|0.4333|0.3153|0.4154|\\n|OmniAnomaly|0.4567|0.5652|0.4545|0.5125|0.2839|0.4365|0.3338|0.4231|\\n|MSCRED|0.4272|0.5180|0.4609|0.5164|0.2322|0.3469|0.2297|0.2962|\\n|MAD-GAN|0.4630|0.5785|0.4681|0.5522|0.3856|0.5589|0.4277|0.5292|\\n|USAD|0.4925|0.6055|0.5179|0.5781|0.3095|0.4769|0.3534|0.4515|\\n|MTAD-GAT|0.3493|0.4777|0.3759|0.4530|0.5812|0.5885|0.5926|0.6522|\\n|CAE-M|0.4707|0.5878|0.5474|0.6178|0.2530|0.4171|0.2047|0.3010|\\n|GDN|0.3143|0.4386|0.2980|0.3724|0.2276|0.3382|0.2921|0.3570|\\n|TranAD|0.4981|0.6401|0.4941|0.6178|0.4630|0.7533|0.5981|0.6963|\\n\\n# Table 5: Comparison of training times in seconds per epoch.\\n\\nQuestion:\\nAccording to Table 1, what is the F1-score of the RESTAD model with random initialization on the SMD dataset?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the effect of integrating the RBF layer into the Transformer model, as shown in Figure 4?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 5.1 Architecture\\n\\nAs shown in Figure 4, we first delve into the architecture of TSFMs, including Transformer-based models, non-Transformer-based models and diffusion-based models, focusing on the underlying mechanisms that shape their capabilities, as well as how they could be applied on various time series.\\n\\n# 5.1.1 Transformer-based Models\\n\\n. However, the optimal number of RBF centers is data-dependent. These findings motivate future studies for the exploration of integrating RBF layers into other deep learning architectures for anomaly detection tasks.\\n\\n# Figure 2: An overview of our framework.\\n\\nWe design a Transformer-based model and a self-supervised training strategy. In the training stage, a portion of an input window is randomly replaced and the model is directed to classify the degraded part. The main Transformer body is composed of Transformer layers with 1D relative position bias.\\n\\n# 3 METHOD\\n\\n# 3.1 OVERALL ARCHITECTURE\\n\\n# 3.2 Implementation\\n\\nRESTAD Model: The RESTAD model is an adaptation of a vanilla Transformer, incorporating an RBF kernel layer as detailed in Figure 2. It includes a DataEmbedding module that combines both token and positional embeddings, followed by an encoder with three layers. Each layer includes a multi-head self-attention mechanism and feed-forward networks. The model has a latent dimension of 32, an intermediate feed-forward network layer with a dimension of 128, and 8 attention heads. The RBF layer is placed after the second encoder layer (other placements are also possible, see section 4.1). Optimization is performed using the ADAM optimizer, and hyperparameters are determined through systematic search to optimize reconstruction task performance. Additional hyperparameter details are available in our code repository1.\\n\\n# Evaluation\\n\\nTo overcome the challenges of scoring based on reconstruction error and the limitations of the association discrepancy method, we propose combining the reconstruction error with a specialized non-linear transformation like the Radial Basis.\\n\\n*Corresponding Author: r.ghorbani@tudelft.nl\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n|Input|Transformer Layer|RBF Layer|Transformer Layer|Reconstructed Signal|\\n|---|---|---|---|---|\\n|Original Signal|Significant Anomaly|RBFCenter|Detected Anomaly|Normal Data Point|\\n|Reconstructed Signal|Subtle Anomaly|Missed Anomaly|Subtle Anomaly|Reconstruction Score|\\n|Threshold|Dis-Similarity Score|t0|t1|Time|\\n\\n# Figure 1: Comparison of traditional reconstruction and RBF-enhanced anomaly detection\\n\\n(a) Original signal with subtle and significant anomalies compared to its reconstruction.\\n\\n(b) Reconstruction errors for the signals in (a), highlighting challenges in detecting subtle anomalies.\\n\\nQuestion:\\nWhat is the effect of integrating the RBF layer into the Transformer model, as shown in Figure 4?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: According to the discussion section, what is the primary reason for the significant performance gains of the RESTAD model?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Figure 5: RESTAD Performance with varying RBF layer\\n\\n# Figure 6: RESTAD mean performance across varying numbers of RBF centers.\\n\\nShaded areas indicate ± standard deviation, illustrating variability across multiple runs.\\n\\n# Discussion and Conclusion\\n\\n# 4 Results\\n\\n# 4.1 Ablation Analysis\\n\\nOur empirical results, as detailed in Table 1, highlight the effectiveness of the RESTAD for anomaly detection. The ablation experiments are based on the RBF layer with random initialization. This decision is based on our findings that random initialization is as effective as the K-means strategy (see Table 1), while offering greater simplicity and computational efficiency. While there are slight performance differences between initialization methods, these variations are not significant enough to establish the superiority of one method over another.\\n\\n# 3.2 Implementation\\n\\nRESTAD Model: The RESTAD model is an adaptation of a vanilla Transformer, incorporating an RBF kernel layer as detailed in Figure 2. It includes a DataEmbedding module that combines both token and positional embeddings, followed by an encoder with three layers. Each layer includes a multi-head self-attention mechanism and feed-forward networks. The model has a latent dimension of 32, an intermediate feed-forward network layer with a dimension of 128, and 8 attention heads. The RBF layer is placed after the second encoder layer (other placements are also possible, see section 4.1). Optimization is performed using the ADAM optimizer, and hyperparameters are determined through systematic search to optimize reconstruction task performance. Additional hyperparameter details are available in our code repository1.\\n\\n# Evaluation\\n\\n# Evaluation\\n\\nAnomaly scores (Eq. 2) exceeding a threshold δ are identified as anomalies. Performance is evaluated using the F1-score for threshold-dependent evaluation. Here, we follow [17] by setting δ to label a predefined proportion of data points as anomalies (0.5% for SMD, 1% for others). For threshold-independent analysis, we use AUC-ROC, AUC-PR, VUS-ROC, and VUS-PR metrics [21]. We exclude the point-adjustment method [22] due to its overestimation [23]. Our model is compared against baselines and state-of-the-arts models: LSTM [9], vanilla Transformer [17], USAD [13], PatchAD [12], AnomalyTrans [17], and DCdetector [10].\\n\\n1 https://github.com/Raminghorbanii/RESTAD\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Table 1: Performance metrics of baselines and RESTAD on test sets.\\n\\nInitialization methods are denoted as (R) for Random and (K) for K-means. For all measures, a higher value indicates better anomaly detection performance.\\n\\n# Results\\n\\nOur results are given in Tab. 18. We see that model reprogramming remarkably results in better efficiency compared to parameter-efficient fine-tuning (PEFT) with QLoRA on long-range.\\n\\n# Table 14: Full few-shot learning results on 10% training data\\n\\nWe use the same protocol as in Tab. 1.\\n\\nQuestion:\\nAccording to the discussion section, what is the primary reason for the significant performance gains of the RESTAD model?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the main contribution of the AnomalyBERT paper, as described in the abstract?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Table 1: Examination of relationships between the typical types of outliers in Lai et al. (2021) and our proposed synthetic outliers.\\n\\n|Proposed|Typical| | | | |\\n|---|---|---|---|---|---|\\n|Global|Contextual|Shapelet|Seasonal|Trend| |\\n|Soft replacement|✔|✔|✔|✔|✔|\\n|Uniform replacement|✔|✔| |✔| |\\n|Length adjustment| | | |✔| |\\n|Peak noise| |✔| | | |\\n\\n# Figure 4: Qualitative results of AnomalyBERT on abnormal sequences from SWaT, WADI, and SMAP datasets.\\n\\nWe visualize an abnormal component where the outlier occurs and anomaly scores for each sequence. Without any post-processing, our method finds out diverse types of anomalies.\\n\\n# 4.4 MAIN RESULTS\\n\\nWe report the results of AnomalyBERT on the five real-world datasets introduced in Section 4.1, and compare them to those of the previous works. The reproduced evaluation scores are brought from Kim et al. (2022). In Table 2, we show that AnomalyBERT outperforms the previous methods on\\n# Presented at the ICLR 2023 workshop on Machine Learning for IoT: Datasets, Perception, and Understanding\\n\\n# Table 2: F1-scores for various anomaly detection methods and AnomalyBERT on five benchmark datasets.\\n\\nWe report the standard F1 and F1-scores after point adjustment (F1PA) following the protocol in Kim et al. (2022). Our method outperforms all existing methods with F1.\\n\\n. Our model converts multivariate data points into temporal representations with relative position bias and yields anomaly scores from these representations. Our method, AnomalyBERT, shows a great capability of detecting anomalies contained in complex time series and surpasses previous state-of-the-art methods on five real-world benchmarks. Our code is available at https://github.com/Jhryu30/AnomalyBERT.\\n\\n# B VISUAL RESULTS\\n\\nWe visualize the prediction results of AnomalyBERT and present the 2D projections of the original data and latent features in the model. Similar to Figure 1, we select three abnormal windows in each of SWaT, WADI, and SMAP test sets and fetch the corresponding anomaly scores after the score prediction process. We then project the original data points and the last latent features from the abnormal windows into 2D planes using t-SNE. As shown in Figure 5, our method distinguishes anomalies successfully and separates abnormal data points from normal points well.\\n\\n# SWaT\\n\\n# WADI\\n\\n# SMAP\\n\\n# Figure 5: Visualization of anomaly score predictions on abnormal sequences in SWaT, WADI, and SMAP datasets.\\n\\n# 5 CONCLUSION\\n\\nThis paper presents AnomalyBERT, a novel method for time series anomaly detection that uses a data degradation scheme to train a Transformer-based model in a self-supervised manner. We design an appropriate Transformer architecture with 1D relative position embeddings for temporal data and propose four types of synthetic outliers that can cover all typical types of anomalies. Exploiting the synthetic outliers in the training phase, our proposed model can learn to distinguish anomalous behavior. We finally show that our method outperforms previous works and has a strong capability in detecting real-world anomalies in complex time series. Our data degradation scheme has the potential to improve the model performance by revising degradation algorithms to mimic real-world anomalies naturally or mixing proper types of outliers according to data characteristics. Therefore, future studies could be demonstrated on detailed analysis of outlier synthesis processes.\\n\\nQuestion:\\nWhat is the main contribution of the AnomalyBERT paper, as described in the abstract?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the purpose of the data degradation scheme in AnomalyBERT, as described in Section 3.2?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# ANOMALYBERT: SELF-SUPERVISED TRANSFORMER FOR TIME SERIES ANOMALY DETECTION USING DATA DEGRADATION SCHEME\\n\\n# Yungi Jeong, Eunseok Yang, Jung Hyun Ryu, Imseong Park, Myungjoo Kang*\\n\\nNumerical Computing & Image Analysis Lab\\n\\nSeoul National University, Seoul, Republic of Korea\\n\\n{jyg9628, mayth24, jhryu30, parkis, mkang}@snu.ac.kr\\n\\n# ABSTRACT\\n\\n# 5 CONCLUSION\\n\\nThis paper presents AnomalyBERT, a novel method for time series anomaly detection that uses a data degradation scheme to train a Transformer-based model in a self-supervised manner. We design an appropriate Transformer architecture with 1D relative position embeddings for temporal data and propose four types of synthetic outliers that can cover all typical types of anomalies. Exploiting the synthetic outliers in the training phase, our proposed model can learn to distinguish anomalous behavior. We finally show that our method outperforms previous works and has a strong capability in detecting real-world anomalies in complex time series. Our data degradation scheme has the potential to improve the model performance by revising degradation algorithms to mimic real-world anomalies naturally or mixing proper types of outliers according to data characteristics. Therefore, future studies could be demonstrated on detailed analysis of outlier synthesis processes.\\n\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\nPreprint\\n\\nRamin Ghorbani1∗, Marcel J.T. Reinders1, and David M.J. Tax1\\n\\n1Pattern Recognition Lab, Delft University of Technology, Delft, Netherlands\\n\\n# Abstract\\n\\n# B VISUAL RESULTS\\n\\nWe visualize the prediction results of AnomalyBERT and present the 2D projections of the original data and latent features in the model. Similar to Figure 1, we select three abnormal windows in each of SWaT, WADI, and SMAP test sets and fetch the corresponding anomaly scores after the score prediction process. We then project the original data points and the last latent features from the abnormal windows into 2D planes using t-SNE. As shown in Figure 5, our method distinguishes anomalies successfully and separates abnormal data points from normal points well.\\n\\n# SWaT\\n\\n# WADI\\n\\n# SMAP\\n\\n# Figure 5: Visualization of anomaly score predictions on abnormal sequences in SWaT, WADI, and SMAP datasets.\\n\\n# 2 RELATED WORKS\\n\\n# Time Series Anomaly Detection\\n\\nQuestion:\\nWhat is the purpose of the data degradation scheme in AnomalyBERT, as described in Section 3.2?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What are the four types of synthetic outliers proposed in the AnomalyBERT paper, as described in Section 3.2?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Table 1: Examination of relationships between the typical types of outliers in Lai et al. (2021) and our proposed synthetic outliers.\\n\\n|Proposed|Typical| | | | |\\n|---|---|---|---|---|---|\\n|Global|Contextual|Shapelet|Seasonal|Trend| |\\n|Soft replacement|✔|✔|✔|✔|✔|\\n|Uniform replacement|✔|✔| |✔| |\\n|Length adjustment| | | |✔| |\\n|Peak noise| |✔| | | |\\n\\n# Figure 4: Qualitative results of AnomalyBERT on abnormal sequences from SWaT, WADI, and SMAP datasets.\\n\\nWe visualize an abnormal component where the outlier occurs and anomaly scores for each sequence. Without any post-processing, our method finds out diverse types of anomalies.\\n\\nIn Table 1, we examine the relationships between the typical outlier types and our synthetic outliers using a simplified version of our model and the sine wave dataset. We measure F1 and area under ROC curve (AUROC) for comparisons of the model performances, and consider that a synthetic outlier covers a typical outlier type (✔) if a model trained with the synthetic one achieves both F1 and AUROC over 0.9 on the sine wave including the corresponding typical outlier. As shown in Table 1, the soft replacement covers all typical types of outliers, and the uniform replacement and peak noise partially cover them. Meanwhile, the length adjustment only covers the seasonal outlier. From the simple experiment, we draw inspiration and get ideas for imitating anomalous behavior using synthetic outliers.\\n\\n# 4.4 MAIN RESULTS\\n\\n# Table 3: Results of ablation studies on combinations of the synthetic outlier types.\\n\\nWe show the impacts of synthetic outliers by comparing F1-scores on various experimental conditions.\\n\\n# (a) Experimental results on the soft replacement, uniform replacement, and peak noise on WADI dataset.\\n\\n|Soft replacement|Uniform replacement|Peak noise|F1|F1PA|\\n|---|---|---|---|---|\\n|×|×|×|0.580|0.798|\\n|×|×| |0.504|0.756|\\n|×| |×|0.556|0.770|\\n| |×|×|0.402|0.757|\\n| | |×|0.403|0.706|\\n| |×| |0.330|0.888|\\n\\n# (b) Experimental results on the length adjustment on SWaT dataset.\\n\\n|Length adjustment|F1|F1PA|\\n|---|---|---|\\n|×|0.854|0.925|\\n| |0.837|0.914|\\n\\n# (c) Experimental results on the length adjustment on WADI dataset.\\n\\n|Length adjustment|F1|F1PA|\\n|---|---|---|\\n| |0.433|0.642|\\n|×|0.580|0.798|\\n\\n# Table 5: Different training settings for five benchmark datasets.\\n\\n|Setting|Dataset|SWaT|WADI|MSL|SMAP|SMD|\\n|---|---|---|---|---|---|---|\\n|Patch size| |14|8|2|4|4|\\n|Window size| |7,168|4,096|1,024|2,048|2,048|\\n|Max length % of outlier| |20%|15%|20%|15%|20%|\\n|Use of length adjustment| | |×|×|×| |\\n\\n# A.3 DETAILED RESULTS OF SECTION 4.3\\n\\nWe examine relationships between the typical types of outliers and our proposed synthetic outliers in Section 4.3 through F1 and AUROC. The detailed results of F1 and AUROC are presented in Table 7 and Table 6, respectively.\\n\\n# Table 6: AUROC results of the preliminary experiment in Section 4.3.\\n\\n# 4.4 MAIN RESULTS\\n\\nWe report the results of AnomalyBERT on the five real-world datasets introduced in Section 4.1, and compare them to those of the previous works. The reproduced evaluation scores are brought from Kim et al. (2022). In Table 2, we show that AnomalyBERT outperforms the previous methods on\\n# Presented at the ICLR 2023 workshop on Machine Learning for IoT: Datasets, Perception, and Understanding\\n\\n# Table 2: F1-scores for various anomaly detection methods and AnomalyBERT on five benchmark datasets.\\n\\nWe report the standard F1 and F1-scores after point adjustment (F1PA) following the protocol in Kim et al. (2022). Our method outperforms all existing methods with F1.\\n\\nQuestion:\\nWhat are the four types of synthetic outliers proposed in the AnomalyBERT paper, as described in Section 3.2?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the purpose of the 1D relative position bias in the AnomalyBERT model, as described in Section 3.1?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# B VISUAL RESULTS\\n\\nWe visualize the prediction results of AnomalyBERT and present the 2D projections of the original data and latent features in the model. Similar to Figure 1, we select three abnormal windows in each of SWaT, WADI, and SMAP test sets and fetch the corresponding anomaly scores after the score prediction process. We then project the original data points and the last latent features from the abnormal windows into 2D planes using t-SNE. As shown in Figure 5, our method distinguishes anomalies successfully and separates abnormal data points from normal points well.\\n\\n# SWaT\\n\\n# WADI\\n\\n# SMAP\\n\\n# Figure 5: Visualization of anomaly score predictions on abnormal sequences in SWaT, WADI, and SMAP datasets.\\n\\n# 4.4 MAIN RESULTS\\n\\nWe report the results of AnomalyBERT on the five real-world datasets introduced in Section 4.1, and compare them to those of the previous works. The reproduced evaluation scores are brought from Kim et al. (2022). In Table 2, we show that AnomalyBERT outperforms the previous methods on\\n# Presented at the ICLR 2023 workshop on Machine Learning for IoT: Datasets, Perception, and Understanding\\n\\n# Table 2: F1-scores for various anomaly detection methods and AnomalyBERT on five benchmark datasets.\\n\\nWe report the standard F1 and F1-scores after point adjustment (F1PA) following the protocol in Kim et al. (2022). Our method outperforms all existing methods with F1.\\n\\n# (c) Experimental results on the length adjustment on WADI dataset.\\n\\n|Length adjustment|F1|F1PA|\\n|---|---|---|\\n| |0.433|0.642|\\n|×|0.580|0.798|\\n\\nAll datasets with F1. Our method particularly surpasses the others on MSL and SMAP, which may contain unlabeled outliers in the training set. Despite the difficulty in training networks, our method detects anomalies well in this kind of data. AnomalyBERT also performs well with F1PA, although F1PA tends to distort the model performance. Our qualitative results are visualized in Figure 4.\\n\\n# 4.5 IMPACT OF SYNTHETIC OUTLIERS\\n\\n. Our model converts multivariate data points into temporal representations with relative position bias and yields anomaly scores from these representations. Our method, AnomalyBERT, shows a great capability of detecting anomalies contained in complex time series and surpasses previous state-of-the-art methods on five real-world benchmarks. Our code is available at https://github.com/Jhryu30/AnomalyBERT.\\n\\n# Table 1: Examination of relationships between the typical types of outliers in Lai et al. (2021) and our proposed synthetic outliers.\\n\\n|Proposed|Typical| | | | |\\n|---|---|---|---|---|---|\\n|Global|Contextual|Shapelet|Seasonal|Trend| |\\n|Soft replacement|✔|✔|✔|✔|✔|\\n|Uniform replacement|✔|✔| |✔| |\\n|Length adjustment| | | |✔| |\\n|Peak noise| |✔| | | |\\n\\n# Figure 4: Qualitative results of AnomalyBERT on abnormal sequences from SWaT, WADI, and SMAP datasets.\\n\\nWe visualize an abnormal component where the outlier occurs and anomaly scores for each sequence. Without any post-processing, our method finds out diverse types of anomalies.\\n\\nQuestion:\\nWhat is the purpose of the 1D relative position bias in the AnomalyBERT model, as described in Section 3.1?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the evaluation metric used to compare the performance of AnomalyBERT with previous works, as described in Section 4.2?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# 4.4 MAIN RESULTS\\n\\nWe report the results of AnomalyBERT on the five real-world datasets introduced in Section 4.1, and compare them to those of the previous works. The reproduced evaluation scores are brought from Kim et al. (2022). In Table 2, we show that AnomalyBERT outperforms the previous methods on\\n# Presented at the ICLR 2023 workshop on Machine Learning for IoT: Datasets, Perception, and Understanding\\n\\n# Table 2: F1-scores for various anomaly detection methods and AnomalyBERT on five benchmark datasets.\\n\\nWe report the standard F1 and F1-scores after point adjustment (F1PA) following the protocol in Kim et al. (2022). Our method outperforms all existing methods with F1.\\n\\n# B VISUAL RESULTS\\n\\nWe visualize the prediction results of AnomalyBERT and present the 2D projections of the original data and latent features in the model. Similar to Figure 1, we select three abnormal windows in each of SWaT, WADI, and SMAP test sets and fetch the corresponding anomaly scores after the score prediction process. We then project the original data points and the last latent features from the abnormal windows into 2D planes using t-SNE. As shown in Figure 5, our method distinguishes anomalies successfully and separates abnormal data points from normal points well.\\n\\n# SWaT\\n\\n# WADI\\n\\n# SMAP\\n\\n# Figure 5: Visualization of anomaly score predictions on abnormal sequences in SWaT, WADI, and SMAP datasets.\\n\\n# 4.2.2 Anomaly Diagnosis\\n\\nWe use commonly used metrics to measure the diagnosis performance of all models [62]. HitRate@P% is the measure of how many ground truth dimensions have been included in the top candidates predicted by the model [45]. P% is the percentage of the ground truth dimensions for each timestamp, which we use to consider the top predicted candidates. For instance, if at timestamp t, if 2 dimensions are labeled anomalous in the ground truth, HitRate@100% would consider top 2 dimensions and HitRate@150% would consider 3 dimensions (100 and 150 are chosen based on prior work [62]). We also measure the Normalized Discounted Cumulative Gain (NDCG) [24]. NDCG@P% considers the same number of top predicted candidates as HitRate@P%.\\n\\n# 4.3 Results\\n\\n# Evaluation\\n\\nAnomaly scores (Eq. 2) exceeding a threshold δ are identified as anomalies. Performance is evaluated using the F1-score for threshold-dependent evaluation. Here, we follow [17] by setting δ to label a predefined proportion of data points as anomalies (0.5% for SMD, 1% for others). For threshold-independent analysis, we use AUC-ROC, AUC-PR, VUS-ROC, and VUS-PR metrics [21]. We exclude the point-adjustment method [22] due to its overestimation [23]. Our model is compared against baselines and state-of-the-arts models: LSTM [9], vanilla Transformer [17], USAD [13], PatchAD [12], AnomalyTrans [17], and DCdetector [10].\\n\\n1 https://github.com/Raminghorbanii/RESTAD\\n# RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection\\n\\n# Table 1: Performance metrics of baselines and RESTAD on test sets.\\n\\nInitialization methods are denoted as (R) for Random and (K) for K-means. For all measures, a higher value indicates better anomaly detection performance.\\n\\n# 4 Experiments\\n\\nWe compare TranAD with state-of-the-art models for multivariate time-series anomaly detection, including MERLIN, LSTM-NDT (with autoencoder implementation from openGauss), DAGMM, OmniAnomaly, MSCRED, MAD-GAN, USAD, MTAD-GAT, CAE-M and GDN (with graph embedding implementation from GraphAn). For more details refer Section 2.1. We also tested the Isolation Forest method, but due to its low F1 scores, do not include the corresponding results in our discussion. Other classical methods have been omitted as well.\\n\\nQuestion:\\nWhat is the evaluation metric used to compare the performance of AnomalyBERT with previous works, as described in Section 4.2?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the proposed model in the paper titled 'TranAD' and what are its key features?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Figure 1: The TranAD Model.\\n\\n# 3.3 Transformer Model\\n\\nTransformers are popular deep learning models that have been used in various natural language and vision processing tasks [51]. However, we use insightful refactoring of the transformer architecture for the task of anomaly detection in time-series data. Just like other encoder-decoder models, in a transformer, an input sequence undergoes several attention-based transformations. Figure 1 shows the architecture of the neural network used in TranAD. The encoder encodes the complete sequence until the current timestamp 𝐶 with a focus score (more details later). The window encoder uses this to create an encoded representation of the input window 𝑊, which is then passed to two decoders to create its reconstruction.\\n\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\nThe Transformer model enables us to predict the reconstruction of each input time-series window. It does this by acting as an encoder-decoder network at each timestamp. However, traditional encoder-decoder models often are unable to capture short-term trends and tend to miss anomalies if the deviations are too small. To tackle this challenge, we develop an auto-regressive inference style that predicts the reconstructed window in two-phases. In the first phase, the model aims to generate an approximate reconstruction of the input window.\\n\\n# Phase 2 - Focused Input Reconstruction\\n\\nIn the second phase, we use the reconstruction loss for the first decoder as a focus score. Having the focus matrix for the second phase 𝐹 = 𝐿1, we rerun model inference to obtain the output of the second decoder as 𝑂ˆ2.\\n\\nFor the future, we propose to extend the method with other transformer models like bidirectional neural networks to allow model generalization to diverse temporal trends in data. We also wish to explore the direction of applying cost-benefit analysis for each model component based on the deployment setting to avoid expensive computation.\\n\\n# Software Availability\\n\\nThe code and relevant training scripts are made publicly available on GitHub under BSD-3 licence at https://github.com/imperial-qore/TranAD.\\n\\n# Acknowledgments\\n\\nShreshth Tuli is supported by the President’s PhD scholarship at Imperial College London. We thank Kate Highnam for constructive comments on improving the manuscript writing. We thank the providers of all datasets used in this work.\\n\\n# A MERLIN Implementation\\n\\nRecent models such as USAD, MTAD-GAT and GDN use attention mechanisms to focus on specific modes of the data. Moreover, these models try to capture the long-term trends by adjusting the weights of their neural network and only use a local window as an.\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Table 2: Performance comparison of TranAD with baseline methods on the complete dataset.\\n\\nP: Precision, R: Recall, AUC: Area under the ROC curve, F1: F1 score with complete training data. The best F1 and AUC scores are highlighted in bold.\\n\\n# 5.2 Overhead Analysis\\n\\nTable 5 shows the average training times for all models on every dataset in seconds per epoch. For comparison, we report the training time for MERLIN as the time it takes to discover discords in the test data. The training time of TranAD is 75% − 99% lower than those of the baseline methods. This clearly shows the advantage of having a transformer model with positional encoding to push the complete sequence as an input instead of sequentially inferring over local windows.\\n\\n# 5.3 Sensitivity Analysis\\n\\nQuestion:\\nWhat is the proposed model in the paper titled \\'TranAD\\' and what are its key features?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the problem formulation for anomaly detection and diagnosis in the paper 'TranAD'?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Anomaly Diagnosis\\n\\nThe anomaly diagnosis results in Table 4 where H and N correspond to HitRate and NDCG (with complete data used for model testing). We only present results on the multivariate SMD and MSDS datasets for the sake of brevity (TranAD yields better scores for others as well). We also ignore models that do not explicitly output anomaly class outputs for each dimension individually. Multi-head attention in TranAD allows it to attend to multiple modes simultaneously, making it more suitable for more inter-correlated anomalies.\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Table 4: Diagnosis Performance.\\n\\nThe rest of the paper is organized as follows. Section 2 overviews related work. Section 3 outlines the working of the TranAD model for multivariate anomaly detection and diagnosis. A performance evaluation of the proposed method is shown in Section 4. Section 5 presents additional analysis. Finally, Section 6 concludes.\\n\\n# 2 Related Work\\n\\nTime series anomaly detection is a long-studied problem in the VLDB community. The prior literature works on two types of time-series data: univariate and multivariate. For the former, various methods analyze and detect anomalies in time-series data with a single data source [34], while for the latter multiple time-series together [14, 45, 62].\\n\\n# Classical methods\\n\\nThe recently proposed HitAnomaly [19] method uses vanilla transformers as encoder-decoder networks, but is only applicable to natural-language log data and not appropriate for generic continuous time-series data as inputs. In our experiments, we compare TranAD against the state-of-the-art methods MERLIN, LSTM-NDT, DAGMM, OmniAnomaly, MSCRED, MAD-GAN, USAD, MTAD-GAT, CAE-M and GDN. These methods have shown superiority in anomaly detection and diagnosis, but complement one another in terms of performance across different time-series datasets. Out of these, only USAD aims to reduce training times, but does this to a limited extent. Just like reconstruction based prior work [4, 29, 45, 60, 61], we develop a TranAD model that learns broad level trends using training data to find anomalies in test data. We specifically improve anomaly detection and diagnosis performance with also reducing the training times in this work.\\n\\n# 3 Methodology\\n\\n# 3.1 Problem Formulation\\n\\nThis work uses various tools, including Transformer neural networks and model-agnostic meta learning, as building blocks. However, each of these different technologies cannot be directly used and need necessary adaptations to create a generalizable model for anomaly detection. Specifically, we propose a transformer-based anomaly detection model (TranAD), that uses self-conditioning and an adversarial training process. Its architecture makes it fast for training and testing while maintaining stability with large input sequences. Simple transformer-based encoder-decoder networks tend to miss anomalies if the deviation is too small, i.e., it is relatively close to normal data. One of our contributions is to show that this can be alleviated by an adversarial training procedure that can amplify reconstruction errors. Further, using self-conditioning for robust multi-modal feature extraction can help gain training stability and allow generalization [32]\\n\\nEfficient anomaly detection and diagnosis in multivariate time-series data is of great importance for modern industrial applications. However, building a system that is able to quickly and accurately pinpoint anomalous observations is a challenging problem. This is due to the lack of anomaly labels, high data volatility and the demands of ultra-low inference times in modern applications. Despite the recent developments of deep learning approaches for anomaly detection, only a few of them can address all of these challenges. In this paper, we propose TranAD, a deep transformer network based anomaly detection and diagnosis model which uses attention-based sequence encoders to swiftly perform inference with the knowledge of the broader temporal trends in the data. TranAD uses focus score-based self-conditioning to enable robust multi-modal feature extraction and adversarial training to gain stability\\n\\nQuestion:\\nWhat is the problem formulation for anomaly detection and diagnosis in the paper \\'TranAD\\'?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the architecture of the neural network used in TranAD, as shown in Figure 1?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Figure 1: The TranAD Model.\\n\\n# 3.3 Transformer Model\\n\\nTransformers are popular deep learning models that have been used in various natural language and vision processing tasks [51]. However, we use insightful refactoring of the transformer architecture for the task of anomaly detection in time-series data. Just like other encoder-decoder models, in a transformer, an input sequence undergoes several attention-based transformations. Figure 1 shows the architecture of the neural network used in TranAD. The encoder encodes the complete sequence until the current timestamp 𝐶 with a focus score (more details later). The window encoder uses this to create an encoded representation of the input window 𝑊, which is then passed to two decoders to create its reconstruction.\\n\\n# Figure 2: An overview of our framework.\\n\\nWe design a Transformer-based model and a self-supervised training strategy. In the training stage, a portion of an input window is randomly replaced and the model is directed to classify the degraded part. The main Transformer body is composed of Transformer layers with 1D relative position bias.\\n\\n# 3 METHOD\\n\\n# 3.1 OVERALL ARCHITECTURE\\n\\nWe now provide details on the working of TranAD. A multivariate sequence like 𝑊 or 𝐶 is transformed first into a matrix form with modality 𝑚. We define scaled-dot product attention [51] of three matrices 𝑄 (query), 𝐾 (key) and 𝑉 (value):\\n\\nAttention(𝑄, 𝐾, 𝑉) = softmax(𝑄𝐾T / √𝑚) 𝑉. (2)\\n\\nHere, the softmax forms the convex combination weights for the values in 𝑉, allowing us to compress the matrix 𝑉 into a smaller representative embedding for simplified inference in the downstream neural network operations. Unlike traditional attention operation, the scaled-dot product attention scales the weights by a √𝑚 term to reduce the variance of the weights, facilitating stable training [51]. For input matrices 𝑄, 𝐾 and 𝑉, we apply Multi-Head Self Attention [51] by first passing it through ℎ (number of heads) feed-forward layers to get 𝑄𝑖, 𝐾𝑖 and 𝑉𝑖 for 𝑖 ∈ {1, . . . , ℎ}, and then applying scaled-dot product attention as:\\n\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\nThe Transformer model enables us to predict the reconstruction of each input time-series window. It does this by acting as an encoder-decoder network at each timestamp. However, traditional encoder-decoder models often are unable to capture short-term trends and tend to miss anomalies if the deviations are too small. To tackle this challenge, we develop an auto-regressive inference style that predicts the reconstructed window in two-phases. In the first phase, the model aims to generate an approximate reconstruction of the input window.\\n\\n# Phase 2 - Focused Input Reconstruction\\n\\nIn the second phase, we use the reconstruction loss for the first decoder as a focus score. Having the focus matrix for the second phase 𝐹 = 𝐿1, we rerun model inference to obtain the output of the second decoder as 𝑂ˆ2.\\n\\n𝑂𝑖 = Sigmoid(FeedForward(𝐼23)), (6)\\n\\nwhere 𝑖 ∈ {1, 2} for the first and second decoder respectively. The Sigmoid activation is used to generate an output in the range [0, 1], to match the normalized input window. Thus, the TranAD model takes the inputs 𝐶 and 𝑊 to generate two outputs 𝑂1 and 𝑂2.\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Algorithm 1 The TranAD training algorithm\\n\\nRequire:\\n\\n- Encoder 𝐸, Decoders 𝐷1 and 𝐷2\\n- Dataset used for training W\\n- Evolutionary hyperparameter 𝜖\\n- Iteration limit 𝑁\\n\\n1. Initialize weights 𝐸, 𝐷1, 𝐷2\\n2. 𝑛 ← 0\\n3. do\\n\\nwhile 𝑛 < 𝑁\\n\\n# 3.4 Offline Two-Phase Adversarial Training\\n\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\nQuestion:\\nWhat is the architecture of the neural network used in TranAD, as shown in Figure 1?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What is the two-phase adversarial training process in TranAD, as described in Algorithm 1?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\nThe Transformer model enables us to predict the reconstruction of each input time-series window. It does this by acting as an encoder-decoder network at each timestamp. However, traditional encoder-decoder models often are unable to capture short-term trends and tend to miss anomalies if the deviations are too small. To tackle this challenge, we develop an auto-regressive inference style that predicts the reconstructed window in two-phases. In the first phase, the model aims to generate an approximate reconstruction of the input window.\\n\\n# Phase 2 - Focused Input Reconstruction\\n\\nIn the second phase, we use the reconstruction loss for the first decoder as a focus score. Having the focus matrix for the second phase 𝐹 = 𝐿1, we rerun model inference to obtain the output of the second decoder as 𝑂ˆ2.\\n\\n𝑂𝑖 = Sigmoid(FeedForward(𝐼23)), (6)\\n\\nwhere 𝑖 ∈ {1, 2} for the first and second decoder respectively. The Sigmoid activation is used to generate an output in the range [0, 1], to match the normalized input window. Thus, the TranAD model takes the inputs 𝐶 and 𝑊 to generate two outputs 𝑂1 and 𝑂2.\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Algorithm 1 The TranAD training algorithm\\n\\nRequire:\\n\\n- Encoder 𝐸, Decoders 𝐷1 and 𝐷2\\n- Dataset used for training W\\n- Evolutionary hyperparameter 𝜖\\n- Iteration limit 𝑁\\n\\n1. Initialize weights 𝐸, 𝐷1, 𝐷2\\n2. 𝑛 ← 0\\n3. do\\n\\nwhile 𝑛 < 𝑁\\n\\n# 3.4 Offline Two-Phase Adversarial Training\\n\\nWe now describe the adversarial training process and the two-phase inference approach in the TranAD model, summarized in Algorithm 1.\\n\\n# Phase 1 - Input Reconstruction\\n\\nThe focus score generated in the first phase indicates the deviations of the reconstructed output from the given input. This acts as a prior to modify the attention weights in the second phase and gives higher neural network activation to specific input sub-sequences to extract short-term temporal trends. We refer to this approach as “self-conditioning” in the rest of the paper.\\n\\nThis two-phase auto-regressive inference style has a three-fold benefit. First, it amplifies the deviations, as the reconstruction error acts as an activation in the attention part of the Encoder to generate an anomaly.\\n# Algorithm 2 The TranAD testing algorithm\\n\\nRequire:\\n\\n- Trained Encoder 𝐸, Decoders 𝐷1 and 𝐷2\\n- Test Dataset ˆ𝑇ˆW\\n\\n1. for(𝑡 = 1 to )\\n2. 𝑂1, 𝑂2 ← 𝐷1 ˆ𝑡 , ∥𝑂1 − 𝑊 ∥2ˆ ∥2𝑡 , 0\\n\\n𝑂2 ← 𝐷1 (𝐸 ( (𝐸 ( ˆ𝑡 ,®)), 𝐷2 (𝐸 ( 1∥𝑂1 − 𝑊𝑊 1∥ ˆ2 − )), 𝐷2 (®))ˆ𝑡 , ∥𝑂1 − 𝑊 ∥2))\\n3. 𝑠 = 2 𝑊ˆ ∥2 + 2 𝑂 𝑊\\n4. 𝑦𝑖 = 1(𝑠𝑖 ≥ POT(𝑠𝑖))\\n5. 𝑦 = ∨𝑦𝑖𝑖\\n\\n𝜖−𝑛 in the training process with a small positive constant parameter 𝜖.\\n\\n# Figure 2: An overview of our framework.\\n\\nWe design a Transformer-based model and a self-supervised training strategy. In the training stage, a portion of an input window is randomly replaced and the model is directed to classify the degraded part. The main Transformer body is composed of Transformer layers with 1D relative position bias.\\n\\n# 3 METHOD\\n\\n# 3.1 OVERALL ARCHITECTURE\\n\\n# 5.2 Overhead Analysis\\n\\nTable 5 shows the average training times for all models on every dataset in seconds per epoch. For comparison, we report the training time for MERLIN as the time it takes to discover discords in the test data. The training time of TranAD is 75% − 99% lower than those of the baseline methods. This clearly shows the advantage of having a transformer model with positional encoding to push the complete sequence as an input instead of sequentially inferring over local windows.\\n\\n# 5.3 Sensitivity Analysis\\n\\nQuestion:\\nWhat is the two-phase adversarial training process in TranAD, as described in Algorithm 1?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Running local pipeline (naive RAG) for question: What are the results of the ablation study in Table 6, and what do they indicate about the importance of each component of the TranAD model?\n",
      "Payload inviato all'API: {'prompt': '\\nYou are a knowledgeable assistant specialized in answering questions based solely on the provided context. Provide a detailed and well-structured answer, including all relevant information from the context. Ensure your response is comprehensive, faithful to the context, and presented in clear, well-formed sentences. Do not add any information that is not present in the context. If the answer is not explicitly stated in the context, respond with \"I don\\'t know.\"\\n\\nContext:\\n# Analyses\\n\\n# Ablation Analysis\\n\\nTo study the relative importance of each component of the model, we exclude every major one and observe how it affects the performance in terms of the F1 scores for each dataset. First, we consider the TranAD model without the transformer-based encoder-decoder architecture but instead with a feed-forward network. Second, we consider the model without the self-conditioning, i.e., we fix the focus score 𝐹 = 0® in phase 2. Third, we study the model without the adversarial loss, i.e., a single-phase inference and only the reconstruction loss for model training. Finally, we consider the model without meta-learning. The results are summarized in Table 6 and provide the following findings:\\n\\n# Table 17: Full ablations on ETTh1 and ETTm1 in predicting 96 and 192 steps ahead (MSE reported).\\n\\n# Anomaly Diagnosis\\n\\nThe anomaly diagnosis results in Table 4 where H and N correspond to HitRate and NDCG (with complete data used for model testing). We only present results on the multivariate SMD and MSDS datasets for the sake of brevity (TranAD yields better scores for others as well). We also ignore models that do not explicitly output anomaly class outputs for each dimension individually. Multi-head attention in TranAD allows it to attend to multiple modes simultaneously, making it more suitable for more inter-correlated anomalies.\\n# TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\\n\\n# Table 4: Diagnosis Performance.\\n\\n# Figure 5: Predicted and Ground Truth labels for the MSDS test set using the TranAD model.\\n\\noutperforms it when compared across all datasets and also when given the complete WADI dataset.\\n\\nWe perform critical difference analysis to assess the significance of the differences among the performance of the models. Figure 4 depicts the critical difference diagrams for the F1 and AUC scores based on the Wilcoxon pair-wised signed-rank test (with 𝛼 = 0.05) after rejecting the null hypothesis using the Friedman test on all datasets [22]. TranAD achieves the best rank across all models with a significant statistical difference.\\n\\n# Anomaly Diagnosis\\n\\n# 4 Results\\n\\n# 4.1 Ablation Analysis\\n\\nOur empirical results, as detailed in Table 1, highlight the effectiveness of the RESTAD for anomaly detection. The ablation experiments are based on the RBF layer with random initialization. This decision is based on our findings that random initialization is as effective as the K-means strategy (see Table 1), while offering greater simplicity and computational efficiency. While there are slight performance differences between initialization methods, these variations are not significant enough to establish the superiority of one method over another.\\n\\nQuestion:\\nWhat are the results of the ablation study in Table 6, and what do they indicate about the importance of each component of the TranAD model?\\n\\nAnswer:\\n', 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'temperature': 0.0, 'max_tokens': 500, 'repetition_penalty': 1.2, 'stop': [\"I don't know.\"]}\n",
      "Results saved in questions_answers.json\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        print(\"Using existing event loop...\")\n",
    "        await main()  \n",
    "    else:\n",
    "        print(\"Creating a new event loop...\")\n",
    "        loop.run_until_complete(main())\n",
    "except RuntimeError:\n",
    "    print(\"Creating a new event loop...\")\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.1 (Python 3.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
