{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG per query globali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import os\n",
    "from graphrag.query.indexer_adapters import read_indexer_entities, read_indexer_reports\n",
    "from graphrag.query.llm.oai.chat_openai import ChatOpenAI\n",
    "from graphrag.query.llm.oai.typing import OpenaiApiType\n",
    "from graphrag.query.structured_search.global_search.community_context import (\n",
    "    GlobalCommunityContext,\n",
    ")\n",
    "from graphrag.query.structured_search.global_search.search import GlobalSearch\n",
    "from IPython.display import Markdown, display\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selezione dei file utili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path exists and is not empty.\n"
     ]
    }
   ],
   "source": [
    "file_path = '../../output/20240925-154939/artifacts'\n",
    "\n",
    "if not os.path.exists(file_path) or not os.listdir(file_path):\n",
    "    print(\"The specified path is empty or does not exist.\")\n",
    "else:\n",
    "    print(\"The path exists and is not empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = file_path\n",
    "LANCEDB_URI = f\"{INPUT_DIR}/lancedb\"\n",
    "\n",
    "COMMUNITY_REPORT_TABLE = \"create_final_community_reports\"\n",
    "ENTITY_TABLE = \"create_final_nodes\"\n",
    "ENTITY_EMBEDDING_TABLE = \"create_final_entities\"\n",
    "RELATIONSHIP_TABLE = \"create_final_relationships\"\n",
    "COVARIATE_TABLE = \"create_final_covariates\"\n",
    "TEXT_UNIT_TABLE = \"create_final_text_units\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "nodes_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet\")\n",
    "relationship_df = pd.read_parquet(f\"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet\")\n",
    "df_report = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encoder = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = file_path\n",
    "COMMUNITY_REPORT_TABLE = \"create_final_community_reports\"\n",
    "ENTITY_TABLE = \"create_final_nodes\"\n",
    "ENTITY_EMBEDDING_TABLE = \"create_final_entities\"\n",
    "\n",
    "COMMUNITY_LEVEL = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)\n",
    "\n",
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "entity_embedding_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet\")\n",
    "entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definizione del LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"GRAPHRAG_API_KEY\"]\n",
    "llm_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "api_base = \"http://172.18.21.132:8000/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    model=llm_model,\n",
    "    api_type=OpenaiApiType.OpenAI,  \n",
    "    api_base=api_base,  \n",
    "    max_retries=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurazione context builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_builder = GlobalCommunityContext(\n",
    "    community_reports=reports,\n",
    "    entities=entities,  \n",
    "    token_encoder=token_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_builder_params = {\n",
    "    \"use_community_summary\": True,  \n",
    "    \"shuffle_data\": True,\n",
    "    \"include_community_rank\": True,\n",
    "    \"min_community_rank\": 0,\n",
    "    \"community_rank_name\": \"rank\",\n",
    "    \"include_community_weight\": True,\n",
    "    \"community_weight_name\": \"occurrence weight\",\n",
    "    \"normalize_community_weight\": True,\n",
    "    \"max_tokens\": 6000,  \n",
    "    \"context_name\": \"Reports\",\n",
    "}\n",
    "\n",
    "map_llm_params = {\n",
    "    \"max_tokens\": 1500,\n",
    "    \"temperature\": 0.0,\n",
    "    \"response_format\": {\"type\": \"json_object\"},\n",
    "}\n",
    "\n",
    "reduce_llm_params = {\n",
    "    \"max_tokens\": 1500,  \n",
    "    \"temperature\": 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definizione Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine = GlobalSearch(\n",
    "    llm=llm,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    max_data_tokens=6000, \n",
    "    map_llm_params=map_llm_params,\n",
    "    reduce_llm_params=reduce_llm_params,\n",
    "    allow_general_knowledge=False,  \n",
    "    json_mode=True,  \n",
    "    context_builder_params=context_builder_params,\n",
    "    concurrent_coroutines=32,\n",
    "    response_type=\"Single page\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzione per esecuzione query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask_question(question):\n",
    "    try:\n",
    "        result = await search_engine.asearch(question)\n",
    "        answer = result.response\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\\nException: {e}\")\n",
    "        answer = \"Error: Unable to retrieve answer.\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione delle domande globali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../DatasetCreation/Global_questions.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What are the main topics covered by the data in the set of time-series papers?\n",
      "Question 2: How does RestAD leverage both statistical methods and machine learning to achieve robust anomaly detection in noisy time-series data?\n",
      "Question 3: What are the key features and benefits of RestAD in anomaly detection for time-series data?\n",
      "Question 4: What are the key features and benefits of RestAD in anomaly detection for time-series data?\n",
      "Question 5: How does TimeLLM differ from other models in time-series forecasting?\n",
      "Question 6: How does AnomalyBERT work?\n",
      "Question 7: How does TimeGPT approach time-series forecasting?\n",
      "Question 8: What types of real-world applications can benefit from models like TimeLLM, RestAD, TimeGPT, AnomalyBERT, LagLLama and the other models described?\n",
      "Question 9: What distinguishes LagLLama in its approach to time-series analysis?\n",
      "Question 10: How do models like AnomalyBERT handle non-stationary data, and why is this important?\n",
      "Question 11: What are the main trade-offs when choosing between transformer-based models and traditional time-series models?\n",
      "Question 12: How does TimeLLM incorporate historical data for better forecasting accuracy?\n",
      "Question 13: What kind of preprocessing steps are typically necessary for models like TimeGPT and AnomalyBERT to perform effectively on time-series data?\n",
      "Question 14: How do models like TimeGPT and RestAD handle seasonality and trends in time-series data?\n",
      "Question 15: How does Chronos approach time-series forecasting compared to traditional statistical models?\n",
      "Question 16: What are the strengths and weaknesses of TimesFM in handling multivariate time-series forecasting?\n",
      "Question 17: How does TranAD address the challenges of detecting anomalies in streaming time-series data?\n",
      "Question 18: How does Foundation Models for Time Series Analysis (FMTS) improve time-series forecasting compared to deep learning models?\n",
      "Question 19: What are the key differences between TimeLLM and TimeGPT in terms of model architecture and performance on time-series forecasting tasks?\n",
      "Question 20: How does LagLLama differ from Chronos in handling long-term dependencies in time-series forecasting?\n",
      "Question 21: What are the most common pitfalls when using anomaly detection models in financial datasets?\n",
      "Question 22: How has the use of foundation models impacted the field of time-series forecasting in terms of model accuracy and scalability?\n",
      "Question 23: How are transformer-based models improving the accuracy of anomaly detection in real-time applications?\n",
      "Question 24: What are the most effective techniques for handling seasonality and trend in time-series forecasting models?\n",
      "Question 25: How do foundation models like TimeGPT reduce the need for manual feature engineering in time-series forecasting?\n",
      "Question 26: How do foundation models such as TimeGPT, LagLLama, and TimesFM redefine time series forecasting compared to traditional statistical methods, and what are the implications of their zero-shot or few-shot capabilities in diverse industries?\n",
      "Question 27: What are the architectural innovations and challenges of adapting language models, like those in Chronos and TimeLLM, for time series data, particularly in terms of tokenization, temporal dependencies, and forecast accuracy?\n",
      "Question 28: In what ways do models like AnomalyBERT, TranAD, and RestAD approach anomaly detection in multivariate time series, and how do their self-supervised learning techniques compare in handling unlabeled data for real-time anomaly detection?\n",
      "Question 29: What role does probabilistic forecasting play in models like LagLLama and TimesFM, and how do these models handle uncertainty quantification across long prediction horizons?\n",
      "Question 30: How do models like TimeGPT and Chronos utilize synthetic data and real-world benchmarks in their training, and what trade-offs exist between their generalization abilities on new datasets and performance on specialized, domain-specific tasks?\n",
      "Question 31: How do the different models balance the trade-off between model complexity and interpretability when applied to real-world time series tasks such as anomaly detection and forecasting?\n",
      "Question 32: What are the shared architectural principles across the foundation models, and how do these designs influence their scalability and performance in large-scale forecasting tasks?\n",
      "Question 33: How do anomaly detection models like AnomalyBERT, TranAD, and RestAD compare in terms of their robustness and adaptability to various types of anomalies (contextual, point, collective) across different time series domains?\n",
      "Question 34: What are the main differences in how models like Chronos, TimesFM, and LagLLama handle long-term versus short-term dependencies in time series forecasting, and what impact does this have on their practical applications?\n",
      "Question 35: What challenges and breakthroughs have been encountered in applying self-supervised learning to time series data, and how does this compare with traditional supervised learning approaches in time series tasks?\n",
      "Question 36: How do state-of-the-art models for time series forecasting address the challenge of data sparsity and missing values in real-world datasets, and what techniques are most effective in mitigating the impact of such issues?\n",
      "Question 37: What are the main solutions presented in the paper Foundation Models for Time Series Data? What comparisons and tests were carried out?\n"
     ]
    }
   ],
   "source": [
    "questions_list = [item['question'] for item in data['questions']]\n",
    "\n",
    "# Print the questions to verify\n",
    "for idx, question in enumerate(questions_list, 1):\n",
    "    print(f\"Question {idx}: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risposte del modello alle domande globali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_all_questions(questions):\n",
    "    tasks = []\n",
    "    for question in questions:\n",
    "        tasks.append(ask_question(question))\n",
    "    answers = await asyncio.gather(*tasks)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "answers = await process_all_questions(questions_list)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time_global = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers = []\n",
    "for question, answer in zip(questions_list, answers):\n",
    "    model_answers.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvataggio risposte in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {\n",
    "    \"questions\": model_answers\n",
    "}\n",
    "\n",
    "with open('GraphRAG_responses.json', 'w') as outfile:\n",
    "    json.dump(output_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvataggio tempi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempi salvati in gr_global.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "time_data = [\n",
    "    [\"Type of question\", \"Time (seconds)\", \"Number of questions\"],\n",
    "    [\"Global\", total_time_global, 37]\n",
    "]\n",
    "\n",
    "output_file = 'gr_global.csv'\n",
    "with open(output_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(time_data)\n",
    "\n",
    "print(f\"Tempi salvati in {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.1 (Python 3.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
